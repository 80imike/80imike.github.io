<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>运维之美</title>
  
  <subtitle>种一棵树最好的时间是十年前，其次是现在。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.hi-linux.com/"/>
  <updated>2020-05-08T05:14:23.217Z</updated>
  <id>https://www.hi-linux.com/</id>
  
  <author>
    <name>Mike</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes 私有集群负载均衡器终极解决方案 MetalLB ( 贫苦 K8S 用户的 LoadBalancer )</title>
    <link href="https://www.hi-linux.com/posts/34820.html"/>
    <id>https://www.hi-linux.com/posts/34820.html</id>
    <published>2020-05-08T01:00:00.000Z</published>
    <updated>2020-05-08T05:14:23.217Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>私有云裸金属架构（这里是相对云上环境来说，不是说无操作系统）上部署的 Kubernetes 集群，通常是无法使用 LoadBalancer 类型的 Service 的。因为 Kubernetes 本身没有为裸机群集提供网络负载均衡器（类型为 LoadBalancer 的服务）的实现。如果你的 Kubernetes 集群没有在公有云的 IaaS 平台（GCP，AWS，Azure …）上运行，则 LoadBalancers 将在创建时无限期地保持 “挂起” 状态，也就是说只有公有云厂商自家的 Kubernetes 支持 LoadBalancer 。</p><p>为了从外部访问裸机 Kubernetes 群集，目前只能使用 <code>NodePort</code> 或 <code>Ingress</code> 的方法进行服务暴露。前者的缺点是每个暴露的服务需要占用所有节点的某个端口，后者的缺点是仅仅能支持 <code>HTTP</code> 协议。</p><h2 id="什么是-metallb">什么是 MetalLB</h2><p><code>MetalLB</code> 是一个负载均衡器，专门解决裸金属 Kubernetes 集群中无法使用 <code>LoadBalancer</code> 类型服务的痛点。<code>MetalLB</code> 使用标准化的路由协议，以便裸金属 Kubernetes 集群上的外部服务也尽可能地工作。即 MetalLB 能够帮助你在裸金属 Kubernetes 集群中创建 LoadBalancer 类型的 Kubernetes 服务，该项目发布于 2017 年底，当前处于 <code>Beta</code> 阶段。</p><blockquote><p>项目地址：<a href="https://github.com/danderson/metallb" target="_blank" rel="noopener">https://github.com/danderson/metallb</a></p></blockquote><h2 id="metallb-工作原理">MetalLB 工作原理</h2><p>MetalLB 会在 Kubernetes 内运行，监控服务对象的变化，一旦监测到有新的 LoadBalancer 服务运行，并且没有可申请的负载均衡器之后，就会完成地址分配和外部声明两部分的工作。</p><h3 id="地址分配">地址分配</h3><p>在云环境中，当你请求一个负载均衡器时，云平台会自动分配一个负载均衡器的 IP 地址给你，应用程序通过此 IP 来访问经过负载均衡处理的服务。</p><p>使用 MetalLB 时，MetalLB 会自己为用户的 LoadBalancer 类型 Service 分配 IP 地址，当然该 IP 地址不是凭空产生的，需要用户在配置中提供一个 IP 地址池，Metallb 将会在其中选取地址分配给服务。</p><h3 id="外部声明">外部声明</h3><p>MetalLB 将 IP 分配给某个服务后，它需要对外宣告此 IP 地址，并让外部主机可以路由到此 IP。</p><p>MetalLB 支持两种声明模式：Layer 2（ ARP / NDP ）模式或者 BGP 模式。</p><ol><li>Layer 2 模式</li></ol><p><img src="https://kubernetes.github.io/ingress-nginx/images/baremetal/metallb.jpg" alt=""></p><p>在任何以太网环境均可使用该模式。当在第二层工作时，将有一台机器获得 IP 地址（即服务的所有权）。MetalLB 使用标准的地址发现协议（对于 IPv4 是 ARP，对于 IPv6 是 NDP）宣告 IP 地址，使其在本地网路中可达。从 LAN 的角度来看，仅仅是某台机器多配置了一个 IP 地址。</p><p>Layer 2 模式下，每个 Service 会有集群中的一个 Node 来负责。服务的入口流量全部经由单个节点，然后该节点的 Kube-Proxy 会把流量再转发给服务的 Pods。也就是说，该模式下 MetalLB 并没有真正提供负载均衡器。尽管如此，MetalLB 提供了故障转移功能，如果持有 IP 的节点出现故障，则默认 10 秒后即发生故障转移，IP 会被分配给其它健康的节点。</p><p>Layer 2 模式的优缺点：</p><ul><li><p>Layer 2 模式更为通用，不需要用户有额外的设备；</p></li><li><p>Layer 2 模式下存在单点问题，服务的所有入口流量经由单点，其网络带宽可能成为瓶颈；</p></li><li><p>由于 Layer 2 模式需要 ARP/NDP 客户端配合，当故障转移发生时，MetalLB 会发送 ARP 包来宣告 MAC 地址和 IP 映射关系的变化，地址分配略为繁琐。</p></li></ul><ol start="2"><li>BGP 模式</li></ol><p>当在第三层工作时，集群中所有机器都和你控制的最接近的路由器建立 BGP 会话，此会话让路由器能学习到如何转发针对 K8S 服务 IP 的数据包。</p><p>通过使用 BGP，可以实现真正的跨多节点负载均衡（需要路由器支持 multipath），还可以基于 BGP 的策略机制实现细粒度的流量控制。</p><p>具体的负载均衡行为和路由器有关，可保证的共同行为是：每个连接（TCP 或 UDP 会话）的数据包总是路由到同一个节点上。</p><p>BGP 模式的优缺点：</p><ul><li><p>不能优雅处理故障转移，当持有服务的节点宕掉后，所有活动连接的客户端将收到 Connection reset by peer ；</p></li><li><p>BGP 路由器对数据包的源 IP、目的 IP、协议类型进行简单的哈希，并依据哈希值决定发给哪个 K8S 节点。问题是 K8S 节点集是不稳定的，一旦（参与 BGP）的节点宕掉，很大部分的活动连接都会因为 rehash 而坏掉。</p></li></ul><p>BGP 模式问题的缓和措施：</p><ul><li><p>将服务绑定到一部分固定的节点上，降低 rehash 的概率。</p></li><li><p>在流量低的时段改变服务的部署。</p></li><li><p>客户端添加透明重试逻辑，当发现连接 TCP 层错误时自动重试。</p></li></ul><a id="more"></a><h2 id="部署-metallb">部署 MetalLB</h2><h3 id="环境要求">环境要求</h3><p>根据部署模式不同，MetalLB 可能需要以下环境：</p><ol><li><p>一个 Kubernetes 集群，运行 Kubernetes 1.13.0 或更高版本。</p></li><li><p>Kubernetes 集群的网络配置可以与 MetalLB 共存。</p></li><li><p>有一些提供给 MetalLB 分发的 IPv4 地址。</p></li><li><p>根据部署模式，可能需要一个或多个 BGP 的路由器 。</p></li></ol><p>MetalLB 目前支持网络插件范围</p><table><thead><tr><th>网络插件</th><th>兼容性</th></tr></thead><tbody><tr><td>Calico</td><td>部分支持（有附加文档）</td></tr><tr><td>Flannel</td><td>支持</td></tr><tr><td>Kube-router</td><td>不支持</td></tr><tr><td>Romana</td><td>支持（有附加文档）</td></tr><tr><td>Weave Net</td><td>支持</td></tr></tbody></table><blockquote><p>从 Kubernetes 1.9 开始, Kube-Proxy 除了支持默认的 Iptables 模式之外，还支持更高效的 IPVS 模式。MetalLB 可以在Kubenetes 1.13 或更高版本的 Kube-Proxy 中使用 IPVS 模式。但是,它尚未明确测试，因此风险自负。具体内容可参考：<a href="https://github.com/google/metallb/issues/153" target="_blank" rel="noopener">https://github.com/google/metallb/issues/153</a></p></blockquote><h3 id="安装-metallb">安装 MetalLB</h3><p>安装 MetalLB 一共有两种方法：使用 Kubernetes YAML 文件或使用 Helm 包管理器。</p><h4 id="使用-yaml-文件部署">使用 YAML 文件部署</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 目前 MetalLB 最新版本为 0.8.1</span><br><span class="line">$ kubectl apply -f https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;google&#x2F;metallb&#x2F;v0.8.1&#x2F;manifests&#x2F;metallb.yaml</span><br></pre></td></tr></table></figure><p>部署完成后，将在 <code>metallb-system</code> 命名空间下将 MetalLB 部署到集群。YAML 文件中主要包含以下一些组件：</p><ol><li><p><code>metallb-system/controller</code>，这是处理 <code>IP</code> 地址分配的控制器。</p></li><li><p><code>metallb-system/speakerdaemonset</code> 这是支持你选择协议以使服务可达的组件。</p></li><li><p><code>Controller</code> 和 <code>Speaker</code> 的 <code>Service Accounts</code>，以及组件需要运行的 <code>RBAC</code> 权限。</p></li></ol><blockquote><p>通过 YAML 安装文件部署并不包含 MetalLB 配置文件，但 MetalLB 的组件仍能启动，但在你定义和部署 <code>configmap</code> 之前将保持空闲状态 。</p></blockquote><h4 id="使用-helm-部署">使用 Helm 部署</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --name metallb stable&#x2F;metallb</span><br></pre></td></tr></table></figure><p>如果你还不知道什么是 Helm，可以先参考 <a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486154&amp;idx=1&amp;sn=becd5dd0fadfe0b6072f5dfdc6fdf786&amp;chksm=eac52be3ddb2a2f555b8b1028db97aa3e92d0a4880b56f361e4b11cd252771147c44c08c8913&amp;token=1182029777&amp;lang=zh_CN#rd" target="_blank" rel="noopener">「Helm 入门指南」</a> 一文。</p><h2 id="配置-metallb">配置 MetalLB</h2><p>MetalLB 安装完成后，我们还需要根据具体的地址和通告方式配置名为 <code>metallb-system/config</code> 的 ConfigMap。Controller 会读取该 ConfigMap，并重新加载配置。</p><blockquote><p>通过 Helm 安装时，MetalLB 读取的 ConfigMap 名为 metallb-config 。</p></blockquote><h3 id="配置-metallb-为二层模式">配置 MetalLB 为二层模式</h3><p>第二层模式是最简单的配置方式：在许多情况下，您不需要任何特定于协议的配置，只需要 IP 地址。</p><p>第二层模式不要求将 IP 绑定到工作节点的网络接口。它的工作原理是直接响应本地网络上的 ARP 请求，将机器的 MAC 地址提供给客户端。</p><p>下面我们来看一个实际例子，我们将配置一个由 MetalLB 二层模式控制的外部 IP 段为 192.168.1.240 - 192.168.1.250。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ cat MetalLB-Layer2-Config.yaml</span><br><span class="line"></span><br><span class="line">kind: ConfigMap</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: config</span><br><span class="line">  namespace: metallb-system</span><br><span class="line">data:</span><br><span class="line">  config: |</span><br><span class="line">    address-pools:</span><br><span class="line">    - name: default</span><br><span class="line">      protocol: layer2</span><br><span class="line">      addresses:</span><br><span class="line">      - 192.168.0.10-192.168.0.100</span><br></pre></td></tr></table></figure><blockquote><p>注意：这里的 IP 地址范围需要跟集群实际情况相对应。</p></blockquote><p>首先，我们使用 <code>kubectl apply -f MetalLB-Layer2-Config.yaml</code> 命令使配置生效。如果你想看到详细配置更新过程，可以使用以下类似命令查看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs -f [metallb-controller-pod]</span><br><span class="line">或者　</span><br><span class="line">$ kubectl logs -l component&#x3D;speaker -n metallb-system</span><br></pre></td></tr></table></figure><p>接下来，我们来创建一个服务类型为 LoadBalancer 的 Nginx 服务来验证下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ cat nginx-test.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: apps&#x2F;v1beta2</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nginx</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nginx</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 80</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 80</span><br><span class="line">  selector:</span><br><span class="line">    app: nginx</span><br><span class="line">  type: LoadBalancer</span><br></pre></td></tr></table></figure><p>服务创建完成，运行 <code>kubectl apply -f nginx-test.yaml</code> 命令后，我们可以看到对应服务信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods nginx-68995d8957-bczhf -o wide</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE   IP            NODE       </span><br><span class="line">nginx-68995d8957-bczhf   2&#x2F;2     Running   0          19d   10.244.0.78   ubuntu-1  </span><br><span class="line"></span><br><span class="line">$ kubectl get svc</span><br><span class="line">NAME    TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE</span><br><span class="line">nginx   LoadBalancer   10.97.187.100   192.168.0.10   80:32353&#x2F;TCP   179m</span><br></pre></td></tr></table></figure><p>从输出结果，我们可以看到 LoadBalancer 类型的服务，并且分配的外部 IP 地址是地址池中的第一个 IP <code>192.168.0.10</code>。</p><p>最后，我们通过 <code>curl http://192.168.0.10</code> 命令来验证下，发现可以正常显示 Nginx 的欢迎信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;&#x2F;title&gt;</span><br><span class="line">................</span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure><p>至此，<code>MetalLB Layer 2</code> 模式的配置就结束了。</p><h3 id="配置-metallb-为-bgp-模式">配置 MetalLB 为 BGP 模式</h3><p>对于配置为具有一个 <code>BGP</code> 路由器和一个 IP 地址范围的 BGP 模式，你需要先准备好以下 4 条配置信息：</p><ol><li><p>MetalLB 应连接的路由器 IP 地址。</p></li><li><p>路由器的 AS 号。</p></li><li><p>MetalLB 应该使用的 AS 编号。</p></li><li><p>IP 地址范围，表示为 CIDR 前缀。</p></li></ol><p>由于这种配置方式需要具备 BGP 功能的硬件路由器支持，目前我们环境中不具备此等条件。这里就简单说下 MetalLB 对应的配置方式，具体内容就不展开讲解了。</p><p>由于前面已经安装了 MetalLB 的 <code>Controller</code> 和 <code>Speaker</code>，只是使用的是 Layer 2 模式。这里只需要改为 BGP 模式，我们修改 Configmap 中 Config 配置就可以了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 假如要为 MetalLB 提供范围 192.168.9.0&#x2F;24 和 AS 号 65009，并将其连接到 192.168.0.1 的 AS 号为 65000 的路由器。</span><br><span class="line">$ cat MetalLB-BGP-Config.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  namespace: metallb-system</span><br><span class="line">  name: config</span><br><span class="line">data:</span><br><span class="line">  config: |</span><br><span class="line">    peers:</span><br><span class="line">    - peer-address: 192.168.0.1</span><br><span class="line">      peer-asn: 65000</span><br><span class="line">      my-asn: 65009</span><br><span class="line">    address-pools:</span><br><span class="line">    - name: default</span><br><span class="line">      protocol: bgp</span><br><span class="line">      addresses:</span><br><span class="line">      #- 192.168.0.10-192.168.0.100</span><br><span class="line">      - 192.168.9.0&#x2F;24</span><br></pre></td></tr></table></figure><h2 id="总结">总结</h2><p>本文简单介绍了 MetalLB 的用途以及 MetalLB 的两种部署模式：Layer 2 模式和 BGP 模式。在实际应用中，如果条件满足，推荐使用 BGP 模式。</p><h2 id="参考资料">参考资料</h2><ol><li><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a></p></li><li><p><a href="https://blog.fleeto.us/post/intro-metallb/" target="_blank" rel="noopener">https://blog.fleeto.us/post/intro-metallb/</a></p></li><li><p><a href="https://ieevee.com/tech/2019/06/30/metallb.html" target="_blank" rel="noopener">https://ieevee.com/tech/2019/06/30/metallb.html</a></p></li><li><p><a href="https://blog.csdn.net/kunyus/article/details/88616653" target="_blank" rel="noopener">https://blog.csdn.net/kunyus/article/details/88616653</a></p></li><li><p><a href="https://vqiu.cn/metallb-si-you-ji-qun-loadbalancer/amp/" target="_blank" rel="noopener">https://vqiu.cn/metallb-si-you-ji-qun-loadbalancer/amp/</a></p></li><li><p><a href="https://sre.ink/metallb-kubernetes-loadbalancer-no-ipvs/" target="_blank" rel="noopener">https://sre.ink/metallb-kubernetes-loadbalancer-no-ipvs/</a></p></li><li><p><a href="https://blog.csdn.net/networken/article/details/85928369" target="_blank" rel="noopener">https://blog.csdn.net/networken/article/details/85928369</a></p></li><li><p><a href="https://blog.gmem.cc/external-lb-for-on-premise-k8s-cluster" target="_blank" rel="noopener">https://blog.gmem.cc/external-lb-for-on-premise-k8s-cluster</a></p></li><li><p><a href="https://leeif.me/2019/02/k8s-deploy-metallb-LoadBalancer.html" target="_blank" rel="noopener">https://leeif.me/2019/02/k8s-deploy-metallb-LoadBalancer.html</a></p></li><li><p><a href="https://mshk.top/2019/04/kubernetes-metallb-loadbalancer-nginx-ingress-controller/" target="_blank" rel="noopener">https://mshk.top/2019/04/kubernetes-metallb-loadbalancer-nginx-ingress-controller/</a></p></li></ol></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;私有云裸金属架构（这里是相对云上环境来说，不是说无操作系统）上部署的 Kubernetes 集群，通常是无法使用 LoadBalancer 类型的 Service 的。因为 Kubernetes 本身没有为裸机群集提供网络负载均衡器（类型为 LoadBalancer 的服务）的实现。如果你的 Kubernetes 集群没有在公有云的 IaaS 平台（GCP，AWS，Azure …）上运行，则 LoadBalancers 将在创建时无限期地保持 “挂起” 状态，也就是说只有公有云厂商自家的 Kubernetes 支持 LoadBalancer 。&lt;/p&gt;
&lt;p&gt;为了从外部访问裸机 Kubernetes 群集，目前只能使用 &lt;code&gt;NodePort&lt;/code&gt; 或 &lt;code&gt;Ingress&lt;/code&gt; 的方法进行服务暴露。前者的缺点是每个暴露的服务需要占用所有节点的某个端口，后者的缺点是仅仅能支持 &lt;code&gt;HTTP&lt;/code&gt; 协议。&lt;/p&gt;
&lt;h2 id=&quot;什么是-MetalLB&quot;&gt;什么是 MetalLB&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;MetalLB&lt;/code&gt; 是一个负载均衡器，专门解决裸金属 Kubernetes 集群中无法使用 &lt;code&gt;LoadBalancer&lt;/code&gt; 类型服务的痛点。&lt;code&gt;MetalLB&lt;/code&gt; 使用标准化的路由协议，以便裸金属 Kubernetes 集群上的外部服务也尽可能地工作。即 MetalLB 能够帮助你在裸金属 Kubernetes 集群中创建 LoadBalancer 类型的 Kubernetes 服务，该项目发布于 2017 年底，当前处于 &lt;code&gt;Beta&lt;/code&gt; 阶段。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/danderson/metallb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/danderson/metallb&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;MetalLB-工作原理&quot;&gt;MetalLB 工作原理&lt;/h2&gt;
&lt;p&gt;MetalLB 会在 Kubernetes 内运行，监控服务对象的变化，一旦监测到有新的 LoadBalancer 服务运行，并且没有可申请的负载均衡器之后，就会完成地址分配和外部声明两部分的工作。&lt;/p&gt;
&lt;h3 id=&quot;地址分配&quot;&gt;地址分配&lt;/h3&gt;
&lt;p&gt;在云环境中，当你请求一个负载均衡器时，云平台会自动分配一个负载均衡器的 IP 地址给你，应用程序通过此 IP 来访问经过负载均衡处理的服务。&lt;/p&gt;
&lt;p&gt;使用 MetalLB 时，MetalLB 会自己为用户的 LoadBalancer 类型 Service 分配 IP 地址，当然该 IP 地址不是凭空产生的，需要用户在配置中提供一个 IP 地址池，Metallb 将会在其中选取地址分配给服务。&lt;/p&gt;
&lt;h3 id=&quot;外部声明&quot;&gt;外部声明&lt;/h3&gt;
&lt;p&gt;MetalLB 将 IP 分配给某个服务后，它需要对外宣告此 IP 地址，并让外部主机可以路由到此 IP。&lt;/p&gt;
&lt;p&gt;MetalLB 支持两种声明模式：Layer 2（ ARP / NDP ）模式或者 BGP 模式。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Layer 2 模式&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://kubernetes.github.io/ingress-nginx/images/baremetal/metallb.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;在任何以太网环境均可使用该模式。当在第二层工作时，将有一台机器获得 IP 地址（即服务的所有权）。MetalLB 使用标准的地址发现协议（对于 IPv4 是 ARP，对于 IPv6 是 NDP）宣告 IP 地址，使其在本地网路中可达。从 LAN 的角度来看，仅仅是某台机器多配置了一个 IP 地址。&lt;/p&gt;
&lt;p&gt;Layer 2 模式下，每个 Service 会有集群中的一个 Node 来负责。服务的入口流量全部经由单个节点，然后该节点的 Kube-Proxy 会把流量再转发给服务的 Pods。也就是说，该模式下 MetalLB 并没有真正提供负载均衡器。尽管如此，MetalLB 提供了故障转移功能，如果持有 IP 的节点出现故障，则默认 10 秒后即发生故障转移，IP 会被分配给其它健康的节点。&lt;/p&gt;
&lt;p&gt;Layer 2 模式的优缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Layer 2 模式更为通用，不需要用户有额外的设备；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Layer 2 模式下存在单点问题，服务的所有入口流量经由单点，其网络带宽可能成为瓶颈；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;由于 Layer 2 模式需要 ARP/NDP 客户端配合，当故障转移发生时，MetalLB 会发送 ARP 包来宣告 MAC 地址和 IP 映射关系的变化，地址分配略为繁琐。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;BGP 模式&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当在第三层工作时，集群中所有机器都和你控制的最接近的路由器建立 BGP 会话，此会话让路由器能学习到如何转发针对 K8S 服务 IP 的数据包。&lt;/p&gt;
&lt;p&gt;通过使用 BGP，可以实现真正的跨多节点负载均衡（需要路由器支持 multipath），还可以基于 BGP 的策略机制实现细粒度的流量控制。&lt;/p&gt;
&lt;p&gt;具体的负载均衡行为和路由器有关，可保证的共同行为是：每个连接（TCP 或 UDP 会话）的数据包总是路由到同一个节点上。&lt;/p&gt;
&lt;p&gt;BGP 模式的优缺点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;不能优雅处理故障转移，当持有服务的节点宕掉后，所有活动连接的客户端将收到 Connection reset by peer ；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;BGP 路由器对数据包的源 IP、目的 IP、协议类型进行简单的哈希，并依据哈希值决定发给哪个 K8S 节点。问题是 K8S 节点集是不稳定的，一旦（参与 BGP）的节点宕掉，很大部分的活动连接都会因为 rehash 而坏掉。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;BGP 模式问题的缓和措施：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;将服务绑定到一部分固定的节点上，降低 rehash 的概率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在流量低的时段改变服务的部署。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;客户端添加透明重试逻辑，当发现连接 TCP 层错误时自动重试。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>再见 Docker，是时候拥抱下一代容器工具了</title>
    <link href="https://www.hi-linux.com/posts/62714.html"/>
    <id>https://www.hi-linux.com/posts/62714.html</id>
    <published>2020-05-08T01:00:00.000Z</published>
    <updated>2020-05-08T05:41:44.769Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2 id="什么是-linux-容器">什么是 Linux 容器？</h2><p><code>Linux</code> 容器是由 <code>Linux</code> 内核所提供的具有特定隔离功能的进程，<code>Linux</code> 容器技术能够让你对应用及其整个运行时环境（包括全部所需文件）一起进行打包或隔离。从而让你在不同环境（如开发、测试和生产等环境）之间轻松迁移应用的同时，还可保留应用的全部功能。</p><p><code>Linux</code> 容器还有利于明确划分职责范围，减少开发和运维团队间的冲突。这样，开发人员可以全心投入应用开发，而运维团队则可专注于基础架构维护。由于 <code>Linux</code> 容器基于开源技术构建，还将便于你在未来轻松采用各类更新、更强的技术产品。包括 <code>CRI-O</code>、<code>Kubernetes</code> 和 <code>Docker</code> 在内的容器技术，可帮助你的团队有效简化、加速和编排应用的开发与部署。</p><h2 id="什么是-docker">什么是 Docker？</h2><p><code>Docker</code> 是一个开源的应用容器引擎，属于 <code>Linux</code> 容器的一种封装，<code>Docker</code> 提供简单易用的容器使用接口，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 <code>Linux</code> 机器上。容器是完全使用沙箱机制，相互之间不会有任何接口。</p><p>Docker 是目前最流行的 <code>Linux</code> 容器解决方案，即使 <code>Docker</code> 是目前管理 <code>Linux</code> 容器的一个非常方便的工具，但它也有两个缺点：</p><ol><li><p><code>Docker</code> 需要在你的系统上运行一个守护进程。</p></li><li><p><code>Docker</code> 是以 <code>root</code> 身份在你的系统上运行该守护程序。</p></li></ol><p>这些缺点的存在可能有一定的安全隐患，为了解决这些问题，下一代容器化工具 <code>Podman</code> 出现了 。</p><h2 id="什么是-podman">什么是 Podman ？</h2><p><img src="https://static.oschina.net/uploads/img/201808/31192253_l67X.png" alt=""></p><p><code>Podman</code> 是一个开源的容器运行时项目，可在大多数 <code>Linux</code> 平台上使用。<code>Podman</code> 提供与 <code>Docker</code> 非常相似的功能。正如前面提到的那样，它不需要在你的系统上运行任何守护进程，并且它也可以在没有 <code>root</code> 权限的情况下运行。</p><p><code>Podman</code> 可以管理和运行任何符合 <code>OCI</code>（Open Container Initiative）规范的容器和容器镜像。<code>Podman</code> 提供了一个与 <code>Docker</code> 兼容的命令行前端来管理 <code>Docker</code> 镜像。</p><blockquote><ol><li><p>Podman 官网地址：<a href="https://podman.io/" target="_blank" rel="noopener">https://podman.io/</a></p></li><li><p>Podman 项目地址：<a href="https://github.com/containers/libpod" target="_blank" rel="noopener">https://github.com/containers/libpod</a></p></li></ol></blockquote><a id="more"></a><h2 id="安装-podman">安装 Podman</h2><p><code>Podman</code> 目前已支持大多数发行版本通过软件包来进行安装，下面我们来举几个常用发行版本的例子。</p><ul><li>Fedora / CentOS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum -y install podman</span><br></pre></td></tr></table></figure><ul><li>Ubuntu</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update -qq</span><br><span class="line">$ sudo apt-get install -qq -y software-properties-common uidmap</span><br><span class="line">$ sudo add-apt-repository -y ppa:projectatomic&#x2F;ppa</span><br><span class="line">$ sudo apt-get update -qq</span><br><span class="line">$ sudo apt-get -qq -y install podman</span><br></pre></td></tr></table></figure><ul><li>MacOS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew cask install podman</span><br></pre></td></tr></table></figure><ul><li>RHEL 7</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo subscription-manager repos --enable&#x3D;rhel-7-server-extras-rpms</span><br><span class="line">$ sudo yum -y install podman</span><br></pre></td></tr></table></figure><ul><li>Arch Linux</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo pacman -S podman</span><br></pre></td></tr></table></figure><p>更多系统的安装方法，可参考官方文档：<a href="https://github.com/containers/libpod/blob/master/install.md" target="_blank" rel="noopener">https://github.com/containers/libpod/blob/master/install.md</a></p><h2 id="使用-podman">使用 Podman</h2><p>使用 <code>Podman</code> 非常的简单，<code>Podman</code> 的指令跟 <code>Docker</code> 大多数都是相同的。下面我们来看几个常用的例子：</p><h3 id="运行一个容器">运行一个容器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ podman run -dt -p 8080:8080&#x2F;tcp  \</span><br><span class="line">-e HTTPD_VAR_RUN&#x3D;&#x2F;var&#x2F;run&#x2F;httpd  \</span><br><span class="line">-e HTTPD_MAIN_CONF_D_PATH&#x3D;&#x2F;etc&#x2F;httpd&#x2F;conf.d \</span><br><span class="line">-e HTTPD_MAIN_CONF_PATH&#x3D;&#x2F;etc&#x2F;httpd&#x2F;conf \</span><br><span class="line">-e HTTPD_CONTAINER_SCRIPTS_PATH&#x3D;&#x2F;usr&#x2F;share&#x2F;container-scripts&#x2F;httpd&#x2F; \</span><br><span class="line">registry.fedoraproject.org&#x2F;f27&#x2F;httpd &#x2F;usr&#x2F;bin&#x2F;run-httpd</span><br></pre></td></tr></table></figure><h3 id="列出运行的容器">列出运行的容器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ podman ps -a</span><br></pre></td></tr></table></figure><h3 id="分析一个运行的容器">分析一个运行的容器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ podman inspect -l | grep IPAddress\&quot;:</span><br><span class="line">&quot;SecondaryIPAddresses&quot;: null,</span><br><span class="line">&quot;IPAddress&quot;: &quot;&quot;,</span><br></pre></td></tr></table></figure><h3 id="查看一个运行中容器的日志">查看一个运行中容器的日志</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo podman logs --latest</span><br><span class="line">10.88.0.1 - - [07&#x2F;Feb&#x2F;2018:15:22:11 +0000] &quot;GET &#x2F; HTTP&#x2F;1.1&quot; 200 612 &quot;-&quot; &quot;curl&#x2F;7.55.1&quot; &quot;-&quot;</span><br><span class="line">10.88.0.1 - - [07&#x2F;Feb&#x2F;2018:15:22:30 +0000] &quot;GET &#x2F; HTTP&#x2F;1.1&quot; 200 612 &quot;-&quot; &quot;curl&#x2F;7.55.1&quot; &quot;-&quot;</span><br><span class="line">10.88.0.1 - - [07&#x2F;Feb&#x2F;2018:15:22:30 +0000] &quot;GET &#x2F; HTTP&#x2F;1.1&quot; 200 612 &quot;-&quot; &quot;curl&#x2F;7.55.1&quot; &quot;-&quot;</span><br><span class="line">10.88.0.1 - - [07&#x2F;Feb&#x2F;2018:15:22:31 +0000] &quot;GET &#x2F; HTTP&#x2F;1.1&quot; 200 612 &quot;-&quot; &quot;curl&#x2F;7.55.1&quot; &quot;-&quot;</span><br><span class="line">10.88.0.1 - - [07&#x2F;Feb&#x2F;2018:15:22:31 +0000] &quot;GET &#x2F; HTTP&#x2F;1.1&quot; 200 612 &quot;-&quot; &quot;curl&#x2F;7.55.1&quot; &quot;-&quot;</span><br></pre></td></tr></table></figure><h3 id="查看一个运行容器中的进程资源使用情况">查看一个运行容器中的进程资源使用情况</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo podman top &lt;container_id&gt;</span><br><span class="line">  UID   PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">    0 31873 31863  0 09:21 ?        00:00:00 nginx: master process nginx -g daemon off;</span><br><span class="line">  101 31889 31873  0 09:21 ?        00:00:00 nginx: worker process</span><br></pre></td></tr></table></figure><h3 id="停止一个运行中的容器">停止一个运行中的容器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo podman stop --latest</span><br></pre></td></tr></table></figure><h3 id="删除一个容器">删除一个容器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo podman rm --latest</span><br></pre></td></tr></table></figure><p>以上这些特性基本上都和 <code>Docker</code> 一样，<code>Podman</code> 除了兼容这些特性外，还支持了一些新的特性。</p><h3 id="给容器设置一个检查点">给容器设置一个检查点</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo podman container checkpoint &lt;container_id&gt;</span><br></pre></td></tr></table></figure><blockquote><p>需要 CRIU 3.11 以上版本支持，CRIU 项目地址：<a href="https://criu.org/" target="_blank" rel="noopener">https://criu.org/</a></p></blockquote><h3 id="根据检查点位置恢复容器">根据检查点位置恢复容器</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo podman container restore &lt;container_id&gt;</span><br></pre></td></tr></table></figure><h3 id="迁移容器">迁移容器</h3><p>Podman 支持将容器从一台机器迁移到另一台机器。</p><p>首先，在源机器上对容器设置检查点，并将容器打包到指定位置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo podman container checkpoint &lt;container_id&gt; -e &#x2F;tmp&#x2F;checkpoint.tar.gz</span><br><span class="line">$ scp &#x2F;tmp&#x2F;checkpoint.tar.gz &lt;destination_system&gt;:&#x2F;tmp</span><br></pre></td></tr></table></figure><p>其次，在目标机器上使用源机器上传输过来的打包文件对容器进行恢复。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo podman container restore -i &#x2F;tmp&#x2F;checkpoint.tar.gz</span><br></pre></td></tr></table></figure><h3 id="配置别名">配置别名</h3><p>如果习惯了使用 <code>Docker</code> 命令，可以直接给 <code>Podman</code> 配置一个别名来实现无缝转移。你只需要在 <code>.bashrc</code> 下加入以下行内容即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo &quot;alias docker&#x3D;podman&quot; &gt;&gt; .bashrc</span><br><span class="line">$ source .bashrc</span><br></pre></td></tr></table></figure><h3 id="podman-如何实现开机重启容器">Podman 如何实现开机重启容器</h3><p>由于 <code>Podman</code> 不再使用守护进程管理服务，所以不能通过守护进程去实现自动重启容器的功能。那如果要实现开机自动重启容器，又该如何实现呢？</p><p>其实方法很简单，现在大多数系统都已经采用 <code>Systemd</code> 作为守护进程管理工具。这里我们就可以使用 <code>Systemd</code> 来实现 <code>Podman</code> 开机重启容器，这里我们以启动一个 <code>Nginx</code> 容器为例子。</p><p>首先，我们先运行一个 <code>Nginx</code> 容器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo podman run -t -d -p 80:80 --name nginx nginx</span><br></pre></td></tr></table></figure><p>然后，在建立一个 <code>Systemd</code> 服务配置文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ vim &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;nginx_container.service</span><br><span class="line"></span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;Podman Nginx Service</span><br><span class="line">After&#x3D;network.target</span><br><span class="line">After&#x3D;network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;podman start -a nginx</span><br><span class="line">ExecStop&#x3D;&#x2F;usr&#x2F;bin&#x2F;podman stop -t 10 nginx</span><br><span class="line">Restart&#x3D;always</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure><p>接下来，启用这个 <code>Systemd</code> 服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl daemon-reload</span><br><span class="line">$ sudo systemctl enable nginx_container.service</span><br><span class="line">$ sudo systemctl start nginx_container.service</span><br></pre></td></tr></table></figure><p>服务启用成功后，我们可以通过 <code>systemctl status</code> 命令查看到这个服务的运行状况。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl status nginx_container.service</span><br><span class="line">● nginx_container.service - Podman Nginx Service</span><br><span class="line">   Loaded: loaded (&#x2F;etc&#x2F;systemd&#x2F;system&#x2F;nginx_container.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Sat 2019-08-20 20:59:26 UTC; 1min 41s ago</span><br><span class="line"> Main PID: 845 (podman)</span><br><span class="line">    Tasks: 16 (limit: 4915)</span><br><span class="line">   Memory: 37.6M</span><br><span class="line">   CGroup: &#x2F;system.slice&#x2F;nginx_container.service</span><br><span class="line">           └─845 &#x2F;usr&#x2F;bin&#x2F;podman start -a nginx</span><br><span class="line"></span><br><span class="line">Aug 20 20:59:26 Ubuntu-dev.novalocal systemd[1]: Started Podman Nginx Service.</span><br></pre></td></tr></table></figure><p>之后每次系统重启后 <code>Systemd</code> 都会自动启动这个服务所对应的容器。</p><h2 id="其它相关工具">其它相关工具</h2><p><code>Podman</code> 只是 <code>OCI</code> 容器生态系统计划中的一部分，主要专注于帮助用户维护和修改符合 <code>OCI</code> 规范容器镜像。其它的组件还有 <code>Buildah</code>、<code>Skopeo</code> 等。</p><h3 id="buildah">Buildah</h3><p><img src="https://camo.githubusercontent.com/843f7639202a27bf5b6abc2afcc405e82804156a/68747470733a2f2f63646e2e7261776769742e636f6d2f636f6e7461696e6572732f6275696c6461682f6d61737465722f6c6f676f732f6275696c6461682d6c6f676f5f6c617267652e706e67" alt=""></p><p>虽然 <code>Podman</code> 也可以支持用户构建 <code>Docker</code> 镜像，但是构建速度比较慢。并且默认情况下使用 <code>VFS</code> 存储驱动程序会消耗大量磁盘空间。</p><p><code>Buildah</code> 是一个专注于构建 <code>OCI</code> 容器镜像的工具，<code>Buildah</code> 构建速度非常快并使用覆盖存储驱动程序，可以节约大量的空间。</p><p><code>Buildah</code> 基于 <code>fork-exec</code> 模型，不以守护进程运行。<code>Buildah</code> 支持 <code>Dockerfile</code> 中的所有命令。你可以直接使用 <code>Dockerfiles</code> 来构建镜像，并且不需要任何 <code>root</code> 权限。 <code>Buildah</code> 也支持用自己的语法文件构建镜像，可以允许将其他脚本语言集成到构建过程中。</p><p>下面是一个使用 <code>Buidah</code> 自有语法构建的例子。</p><p><img src="https://i.loli.net/2019/08/23/coEdGDYeFlMquUT.png" alt=""></p><p><code>Buildah</code> 和 <code>Podman</code> 之间的一个主要区别是：<code>Podman</code> 用于运行和管理容器， 允许我们使用熟悉的容器 <code>CLI</code> 命令在生产环境中管理和维护这些镜像和容器，而 <code>Buildah</code> 主用于构建容器。</p><blockquote><p>项目地址：<a href="https://github.com/containers/buildah" target="_blank" rel="noopener">https://github.com/containers/buildah</a></p></blockquote><h3 id="skopeo">Skopeo</h3><p><img src="https://camo.githubusercontent.com/19ba0305d59474c3cada4b65d5812c8c4c59465c/68747470733a2f2f63646e2e7261776769742e636f6d2f636f6e7461696e6572732f736b6f70656f2f6d61737465722f646f63732f736b6f70656f2e737667" alt=""></p><p><code>Skopeo</code> 是一个镜像管理工具，允许我们通过 <code>Push</code>、<code>Pull</code>和复制镜像来处理 <code>Docker</code> 和符合 <code>OCI</code> 规范的镜像。</p><blockquote><p>项目地址：<a href="https://github.com/containers/skopeo" target="_blank" rel="noopener">https://github.com/containers/skopeo</a></p></blockquote><h2 id="延伸阅读">延伸阅读</h2><h3 id="什么是-oci">什么是 OCI？</h3><p><code>OCI</code> (Open Container Initiative)，是一个轻量级，开放的治理结构（项目）。在 <code>Linux</code> 基金会的支持下成立，致力于围绕容器格式和运行时创建开放的行业标准。</p><p><code>OCI</code> 项目由 <code>Docker</code>、<code>CoreOS</code> 和容器行业中的其它领导者在 2015 年 6 月的时候启动，<code>OCI</code> 的技术委员会成员包括 <code>Red Hat</code>、<code>Microsoft</code>、<code>Docker</code>、<code>Cruise</code>、<code>IBM</code>、<code>Google</code>、<code>Red Hat</code> 和 <code>SUSE</code> 等。</p><h3 id="什么是-cri">什么是 CRI？</h3><p><code>CRI</code>（Container Runtime Interface）是 <code>Kubernetes</code> v1.5 引入的容器运行时接口，它将 <code>Kubelet</code> 与容器运行时解耦，将原来完全面向 <code>Pod</code> 级别的内部接口拆分成面向 <code>Sandbox</code> 和 <code>Container</code> 的 <code>gRPC</code> 接口，并将镜像管理和容器管理分离到不同的服务。</p><p><img src="https://i.loli.net/2019/08/23/L8lM1KENGdxqmgn.png" alt=""></p><h3 id="什么是-cni">什么是 CNI？</h3><p><code>CNI</code>（Container Network Interface）是 <code>CNCF</code> 旗下的一个项目，是 <code>Google</code> 和 <code>CoreOS</code> 主导制定的容器网络标准。<code>CNI</code> 包含方法规范、参数规范等，是 <code>Linux</code> 容器网络配置的一组标准和库，用户可以根据这些标准和库来开发自己的容器网络插件。<code>CNI</code> 已经被 <code>Kubernetes</code>、<code>Mesos</code>、<code>Cloud Foundry</code>、<code>RKT</code> 等使用，同时 <code>Calico</code>、<code>Weave</code> 等项目都在为 CNI 提供插件。</p><p><img src="https://upload-images.jianshu.io/upload_images/5006907-be852310382f8119.png" alt=""></p><h2 id="总结">总结</h2><p>本文介绍三个了符合 <code>CRI</code> 标准的容器工具 <code>Podman</code>、 <code>Buildah</code> 和 <code>Skopeo</code>。这三个工具都是基于 <code>*nix</code> 传统的 <code>fork-exec</code> 模型，解决了由于 <code>Docker</code> 守护程序导致的启动和安全问题，提高了容器的性能和安全。</p><h2 id="参考文档">参考文档</h2><ol><li><a href="https://igene.tw/podman-intro" target="_blank" rel="noopener">https://igene.tw/podman-intro</a></li><li><a href="https://zhuanlan.zhihu.com/p/77373246" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/77373246</a></li><li><a href="https://zhuanlan.zhihu.com/p/47706426" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47706426</a></li><li><a href="https://xuanwo.io/2019/08/06/oci-intro/" target="_blank" rel="noopener">https://xuanwo.io/2019/08/06/oci-intro/</a></li><li><a href="https://www.jianshu.com/p/62e71584d1cb" target="_blank" rel="noopener">https://www.jianshu.com/p/62e71584d1cb</a></li><li><a href="https://kubernetes.feisky.xyz/cha-jian-kuo-zhan/cri" target="_blank" rel="noopener">https://kubernetes.feisky.xyz/cha-jian-kuo-zhan/cri</a></li><li><a href="https://blog.csdn.net/networken/article/details/98684527" target="_blank" rel="noopener">https://blog.csdn.net/networken/article/details/98684527</a></li><li><a href="https://www.zcfy.cc/article/demystifying-the-open-container-initiative-oci-specifications" target="_blank" rel="noopener">https://www.zcfy.cc/article/demystifying-the-open-container-initiative-oci-specifications</a></li></ol></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是-Linux-容器？&quot;&gt;什么是 Linux 容器？&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Linux&lt;/code&gt; 容器是由 &lt;code&gt;Linux&lt;/code&gt; 内核所提供的具有特定隔离功能的进程，&lt;code&gt;Linux&lt;/code&gt; 容器技术能够让你对应用及其整个运行时环境（包括全部所需文件）一起进行打包或隔离。从而让你在不同环境（如开发、测试和生产等环境）之间轻松迁移应用的同时，还可保留应用的全部功能。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Linux&lt;/code&gt; 容器还有利于明确划分职责范围，减少开发和运维团队间的冲突。这样，开发人员可以全心投入应用开发，而运维团队则可专注于基础架构维护。由于 &lt;code&gt;Linux&lt;/code&gt; 容器基于开源技术构建，还将便于你在未来轻松采用各类更新、更强的技术产品。包括 &lt;code&gt;CRI-O&lt;/code&gt;、&lt;code&gt;Kubernetes&lt;/code&gt; 和 &lt;code&gt;Docker&lt;/code&gt; 在内的容器技术，可帮助你的团队有效简化、加速和编排应用的开发与部署。&lt;/p&gt;
&lt;h2 id=&quot;什么是-Docker？&quot;&gt;什么是 Docker？&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Docker&lt;/code&gt; 是一个开源的应用容器引擎，属于 &lt;code&gt;Linux&lt;/code&gt; 容器的一种封装，&lt;code&gt;Docker&lt;/code&gt; 提供简单易用的容器使用接口，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 &lt;code&gt;Linux&lt;/code&gt; 机器上。容器是完全使用沙箱机制，相互之间不会有任何接口。&lt;/p&gt;
&lt;p&gt;Docker 是目前最流行的 &lt;code&gt;Linux&lt;/code&gt; 容器解决方案，即使 &lt;code&gt;Docker&lt;/code&gt; 是目前管理 &lt;code&gt;Linux&lt;/code&gt; 容器的一个非常方便的工具，但它也有两个缺点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Docker&lt;/code&gt; 需要在你的系统上运行一个守护进程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Docker&lt;/code&gt; 是以 &lt;code&gt;root&lt;/code&gt; 身份在你的系统上运行该守护程序。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些缺点的存在可能有一定的安全隐患，为了解决这些问题，下一代容器化工具 &lt;code&gt;Podman&lt;/code&gt; 出现了 。&lt;/p&gt;
&lt;h2 id=&quot;什么是-Podman-？&quot;&gt;什么是 Podman ？&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://static.oschina.net/uploads/img/201808/31192253_l67X.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Podman&lt;/code&gt; 是一个开源的容器运行时项目，可在大多数 &lt;code&gt;Linux&lt;/code&gt; 平台上使用。&lt;code&gt;Podman&lt;/code&gt; 提供与 &lt;code&gt;Docker&lt;/code&gt; 非常相似的功能。正如前面提到的那样，它不需要在你的系统上运行任何守护进程，并且它也可以在没有 &lt;code&gt;root&lt;/code&gt; 权限的情况下运行。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Podman&lt;/code&gt; 可以管理和运行任何符合 &lt;code&gt;OCI&lt;/code&gt;（Open Container Initiative）规范的容器和容器镜像。&lt;code&gt;Podman&lt;/code&gt; 提供了一个与 &lt;code&gt;Docker&lt;/code&gt; 兼容的命令行前端来管理 &lt;code&gt;Docker&lt;/code&gt; 镜像。&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Podman 官网地址：&lt;a href=&quot;https://podman.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://podman.io/&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Podman 项目地址：&lt;a href=&quot;https://github.com/containers/libpod&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/containers/libpod&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>看业界大神是如何进行独立思考的！做为技术人，你具备这样的能力吗？</title>
    <link href="https://www.hi-linux.com/posts/34120.html"/>
    <id>https://www.hi-linux.com/posts/34120.html</id>
    <published>2020-05-06T01:00:00.000Z</published>
    <updated>2020-05-06T04:39:24.993Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>这是一个非常复杂的世界，这个世界上有很多各式各样的观点和思维方式，作为一个程序员的我，也会有程序员的思维方式，程序员的思维方式更接近数学的思维方式，数学的思维方式让可以很容易地理清楚这个混乱的世界，其实，并不需要太复杂的数学逻辑，只需要使用一些简单的数学方法，就可以大幅提升自己的认识能力，所以，在这里，记录一篇我自己的思维方式，一方面给大家做个参考，另一方面也供更高阶的人给我进行指正。算是 “开源我的思维方式”，开放不仅仅是为了输出，更是为了看看有没有更好的方式。</p><p>我的思维方式中，使用数学逻辑的方式进行思考，通常来说，我会使用五步思考的方式：</p><h3 id="第一步信息数据可考证">第一步：信息数据可考证</h3><p>如果一个观点或是一个见解的数据是错误的，那么就会造成后面的观点全是错的，所以，首要的是要进行数据的查证或考证。一般来说，如果一篇文章的作者足够严谨的话，他的需要给他的数据建立相关的引用或是可以考证的方法方式。如果一篇文章中出现的是，“有关专家表明”、“美国科学家证明”、“经济学家指出”，但是没有任出处，也没有点明这个专家或是科学家的名字，或是，也没有说明或引用让读者可以自己去验证的方法。那么，其引用的话或是数据是无法考证的，如果是无法考证的，那么，这篇文章的水份就非常大了。一般来说，当我读到一篇文章中的东西没有可考证的来源或是方法时，通常来说，我就不会再读了，因为这篇文章的价值已经不大了，如果我关心这篇文章中的东西，我会改为自己去查找的方式，虽然变“重”了，但是很安全。（所以，像 Wikipedia 这样的网站是我经常去获得信息的地方，因为信息可以被考证是其基本价值观）</p><h3 id="第二步处理集合和其包含关系">第二步：处理集合和其包含关系</h3><p>这是一个非常简单的人人都会的数学逻辑。比如：哲学家是人，柏拉图是哲学家，所以，柏拉图是人。就是一个在包含关系下的推理。你不要小看这个简单的逻辑，其实很多人并不会很好的应用，相反，当感情支配了他们以后，他们会以点代面，以特例代替普遍性。比如，地图炮就是一种，他们看到了多个案例，他们就开始把这个案例上升上更大的范围，比如：河南人新疆人都是小偷，上海人都是小市民。日本人都是变态和反人类……等等。除了这些地图炮外，还有否定整个人的，比如一个人犯了个错或是性格上有缺陷，就会把整个人全盘否定掉，员工抢个月饼就上升到其价值观有问题……。在数学的逻辑包含中，超集的定义可以适用于子集，通过子集的特征可以对超集进行探索，但是没法定义超集。另外，集合的大小也是一个很重要的事，<a href="https://zh.wikipedia.org/wiki/%E5%80%96%E5%AD%98%E8%80%85%E5%81%8F%E5%B7%AE" target="_blank" rel="noopener">幸存者偏差</a>会是一个很容易让人掉下去的陷阱，因为可能会有很大的样本集可能在你的视线盲区。</p><a id="more"></a><h3 id="第三步处理逻辑因果关系">第三步：处理逻辑因果关系</h3><p>所谓因果关系，其实就是分辨充分条件、必要条件和充分必要条件，然后处理其中的逻辑是否有关联性，而且有非常强的因果关系。没有能力分辨充分必要条件处理因果关系是很多人的硬伤。就像我在《<a href="https://coolshell.cn/articles/19271.html" target="_blank" rel="noopener">努力就会成功</a>》中说的一样，“努力” 和 “成功”是否有因果关系？各种逻辑混淆、概念偷换、模糊因果、似是而非全是在这里。比如：掩耳盗铃、刻舟求剑就是因果关系混乱的表现。人们会经常地混淆两个看来一起发生，但是并没有关联在一起的事。因果关系是最容易被模糊和偷换的，比如：很多人都容易混淆“加班”就会有“产出”，混淆了“行动”就会有“结果”，混淆了“抵制”就会赢得“尊重”，混淆了“批评”等于“反对”……等等。除了这些以外，微信公众号里的很多时评文章，他们的文章中的结论和其论据是没有因果关系的，好多文章就是混淆、模糊、偷换……<strong>因果关系出问题的文章读多了是对大脑有损伤的，要尽量远离。</strong></p><h3 id="第四步找到靠谱的基准线">第四步：找到靠谱的基准线</h3><p>就像我们写代码一样，我们都是会去找一些最佳实践或是业内标准，原因是因为，这样的东西都是经过长时间被这个世界上很多人 Review 过的，是值得依赖和靠谱的，他们会考虑到很多你没有考虑过的问题。所以，你也会看到很多时评都会找欧美发达国家的作参考的做法，因为毕竟人家的文化是相对比较文明、科学、开放和先进的。找到世界或是国际的通行标准，会更容易让人进步。比如：以开放包容加强沟通的心态，就会比封闭抵制敌对的心态要好得多得多，智者建桥，愚者建墙。当然，我们也开始发现，有一些事上，有利于自己的就对标，不利于自己的就不对标，而且，除了好的事，不好的事也在找欧美作对标，于是开始 “多基准线” 和 “乱基准线”，这种方式需要我们小心分辨。</p><h3 id="第五步更为深入和高维的思考">第五步：更为深入和高维的思考</h3><p>如果一件事情只在表面上进行思考其实只是一种浅度思考，在 Amazon，线上系统出现故障的时候，需要写一个 Correction of Errors 的报告，其中需要Ask 5 Whys（参看 Wikipedia 的 <a href="https://en.wikipedia.org/wiki/Five_whys" target="_blank" rel="noopener">Five Whys 词条</a>），这种思考方式可以让你不断追问到深层次的本质问题，会让你自己做大量的调查和研究，让你不会成为一个只会在表面上进行思考的简单动物。比如：当你看到有出乎你意料的事件发生时（比如负面的暴力事件），你需要问一下，为什么会发生，原因是什么？然后罗列尽可能全的原因，再不断地追问并拷证下去（这跟写程序一样，需要从正向案例和负向案例进行考虑分析，才可能写出健壮性很强的代码），我们才会得出一个比较健壮的答案或结构。</p><p>需要注意的是，在上述的这五种思维方式下，你的思考是不可能快得起来的，这是一个 “慢思考”（注：如果读过《<a href="https://book.douban.com/subject/10785583/" target="_blank" rel="noopener">思考，快与慢</a>》这本书的人就知道我在说什么），独立思考是需要使用大脑中的“慢系统”，慢系统是反人性的，所以，能真正做到独立思考的人很少。更多的人的“独立思考”其实只不过是毫无章法的乱思考罢了。</p><p>通过上述的这五点，我相信你是很容易识别或是分辨出哪些信息是靠谱的，哪些信息是很扯的，甚至会改善你自己的言论和思考。但是，<strong>请注意，这些方法并不能让你获得真理或是真相。</strong></p><p>但是这也够了，一个人如果拥有了能够分辨是非的能力，也是很不错的了。虽然不知道事实是什么，但是你也不会盲从和偏信，从而不会被人煽动，而成为幕后黑手的的一只“肉鸡”。</p><p>多说两句，下面是一些我个人的一些实践：</p><ul><li><p>当新闻报道报道的不是客观事实，而是加入了很多观点，那么这篇新闻报道是不可信的。</p></li><li><p>对于评论性的文章，没有充足权威可信的论据时，不能完全相信。</p></li><li><p>不是当事人，不是见证人，还要装作自己是知情的……不知道这种人的自信怎么来的？</p></li><li><p>信息不公开的，并有意屏蔽信息的，不能作为可信的信息源。</p></li><li><p>当出现大是或是大非的事时，一定要非常小心，这个世界不存在完全的美和完全的丑，这样的观点通常来说都是危险的，此时要多看看不同角度的报道和评论，要多收集一些信息，还要多问问为什么。</p></li></ul><p>欢迎大家在评论区留言，告诉我一些你的实践和思维方式。</p><blockquote><p>本文转载自：「酷壳」，原文：<a href="https://url.cn/5MGMoTP%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://url.cn/5MGMoTP，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <code>editor@hi-linux.com</code> 。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是一个非常复杂的世界，这个世界上有很多各式各样的观点和思维方式，作为一个程序员的我，也会有程序员的思维方式，程序员的思维方式更接近数学的思维方式，数学的思维方式让可以很容易地理清楚这个混乱的世界，其实，并不需要太复杂的数学逻辑，只需要使用一些简单的数学方法，就可以大幅提升自己的认识能力，所以，在这里，记录一篇我自己的思维方式，一方面给大家做个参考，另一方面也供更高阶的人给我进行指正。算是 “开源我的思维方式”，开放不仅仅是为了输出，更是为了看看有没有更好的方式。&lt;/p&gt;
&lt;p&gt;我的思维方式中，使用数学逻辑的方式进行思考，通常来说，我会使用五步思考的方式：&lt;/p&gt;
&lt;h3 id=&quot;第一步：信息数据可考证&quot;&gt;第一步：信息数据可考证&lt;/h3&gt;
&lt;p&gt;如果一个观点或是一个见解的数据是错误的，那么就会造成后面的观点全是错的，所以，首要的是要进行数据的查证或考证。一般来说，如果一篇文章的作者足够严谨的话，他的需要给他的数据建立相关的引用或是可以考证的方法方式。如果一篇文章中出现的是，“有关专家表明”、“美国科学家证明”、“经济学家指出”，但是没有任出处，也没有点明这个专家或是科学家的名字，或是，也没有说明或引用让读者可以自己去验证的方法。那么，其引用的话或是数据是无法考证的，如果是无法考证的，那么，这篇文章的水份就非常大了。一般来说，当我读到一篇文章中的东西没有可考证的来源或是方法时，通常来说，我就不会再读了，因为这篇文章的价值已经不大了，如果我关心这篇文章中的东西，我会改为自己去查找的方式，虽然变“重”了，但是很安全。（所以，像 Wikipedia 这样的网站是我经常去获得信息的地方，因为信息可以被考证是其基本价值观）&lt;/p&gt;
&lt;h3 id=&quot;第二步：处理集合和其包含关系&quot;&gt;第二步：处理集合和其包含关系&lt;/h3&gt;
&lt;p&gt;这是一个非常简单的人人都会的数学逻辑。比如：哲学家是人，柏拉图是哲学家，所以，柏拉图是人。就是一个在包含关系下的推理。你不要小看这个简单的逻辑，其实很多人并不会很好的应用，相反，当感情支配了他们以后，他们会以点代面，以特例代替普遍性。比如，地图炮就是一种，他们看到了多个案例，他们就开始把这个案例上升上更大的范围，比如：河南人新疆人都是小偷，上海人都是小市民。日本人都是变态和反人类……等等。除了这些地图炮外，还有否定整个人的，比如一个人犯了个错或是性格上有缺陷，就会把整个人全盘否定掉，员工抢个月饼就上升到其价值观有问题……。在数学的逻辑包含中，超集的定义可以适用于子集，通过子集的特征可以对超集进行探索，但是没法定义超集。另外，集合的大小也是一个很重要的事，&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E5%80%96%E5%AD%98%E8%80%85%E5%81%8F%E5%B7%AE&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;幸存者偏差&lt;/a&gt;会是一个很容易让人掉下去的陷阱，因为可能会有很大的样本集可能在你的视线盲区。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="思想" scheme="https://www.hi-linux.com/tags/%E6%80%9D%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>推荐一款命令行下最快的文本搜索神器 RipGrep</title>
    <link href="https://www.hi-linux.com/posts/29245.html"/>
    <id>https://www.hi-linux.com/posts/29245.html</id>
    <published>2018-09-17T01:00:00.000Z</published>
    <updated>2018-09-17T02:18:53.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Ripgrep 是命令行下一个基于行的搜索工具，RipGrep 使用 Rust 开发，可以在多平台下运行，支持 Mac、Linux 和 Windows 等平台。RipGrep 与 The Silver Searcher、Ack 和 GNU Grep 的功能类似。</p><p>RipGrep 官方号称比其它类似工具在搜索速度上快上 N 倍，VSCode 也从 <a href="https://code.visualstudio.com/updates/v1_11#_text-search-improvements" target="_blank" rel="noopener">1.11 版本</a>开始默认将 RipGrep 做为其搜索工具，由此其功能强大可见一斑。</p><p>项目地址：<a href="https://github.com/BurntSushi/ripgrep" target="_blank" rel="noopener">https://github.com/BurntSushi/ripgrep</a></p><p><strong>Ripgrep 支持的一些特性</strong></p><ul><li>自动递归搜索 （grep 需要 -R）。</li><li>自动忽略 .gitignore 中的文件以及二进制文件和隐藏文件。</li><li>可以搜索指定文件类型，如：<code>rg -tpy foo</code> 则限定只搜索 Python 文件，<code>rg -Tjs foo</code> 则排除掉 JS 文件。</li><li>支持大部分 Grep 的 特性，例如：显示搜索结果的上下文、支持多个模式搜索、高亮匹配的搜索结果以及支持 Unicode 等。</li><li>支持各种文本编码格式，如：UTF-8、UTF-16、latin-1、GBK、EUC-JP、Shift_JIS 等。</li><li>支持搜索常见格式的压缩文件，如：gzip、xz、lzma、bzip2、lz4 等。</li><li>自动高亮匹配的结果。</li></ul><a id="more"></a><p><strong>Ripgrep 官方性能基准测试结果</strong></p><ul><li>搜索整个 Linux 内核源代码</li></ul><table><thead><tr><th>Tool</th><th>Command</th><th>Line count</th><th>Time</th></tr></thead><tbody><tr><td>ripgrep (Unicode)</td><td><code>rg -n -w '[A-Z]+_SUSPEND'</code></td><td>450</td><td><strong>0.106s</strong></td></tr><tr><td><a href="https://www.kernel.org/pub/software/scm/git/docs/git-grep.html" target="_blank" rel="noopener">git grep</a></td><td><code>LC_ALL=C git grep -E -n -w '[A-Z]+_SUSPEND'</code></td><td>450</td><td>0.553s</td></tr><tr><td><a href="https://github.com/ggreer/the_silver_searcher" target="_blank" rel="noopener">The Silver Searcher</a></td><td><code>ag -w '[A-Z]+_SUSPEND'</code></td><td>450</td><td>0.589s</td></tr><tr><td><a href="https://www.kernel.org/pub/software/scm/git/docs/git-grep.html" target="_blank" rel="noopener">git grep (Unicode)</a></td><td><code>LC_ALL=en_US.UTF-8 git grep -E -n -w '[A-Z]+_SUSPEND'</code></td><td>450</td><td>2.266s</td></tr><tr><td><a href="https://github.com/svent/sift" target="_blank" rel="noopener">sift</a></td><td><code>sift --git -n -w '[A-Z]+_SUSPEND'</code></td><td>450</td><td>3.505s</td></tr><tr><td><a href="https://github.com/petdance/ack2" target="_blank" rel="noopener">ack</a></td><td><code>ack -w '[A-Z]+_SUSPEND'</code></td><td>1878</td><td>6.823s</td></tr><tr><td><a href="https://github.com/monochromegane/the_platinum_searcher" target="_blank" rel="noopener">The Platinum Searcher</a></td><td><code>pt -w -e '[A-Z]+_SUSPEND'</code></td><td>450</td><td>14.208s</td></tr></tbody></table><table><thead><tr><th>Tool</th><th>Command</th><th>Line count</th><th>Time</th></tr></thead><tbody><tr><td>ripgrep</td><td><code>rg -L -u -tc -n -w '[A-Z]+_SUSPEND'</code></td><td>404</td><td><strong>0.079s</strong></td></tr><tr><td><a href="https://github.com/gvansickle/ucg" target="_blank" rel="noopener">ucg</a></td><td><code>ucg --type=cc -w '[A-Z]+_SUSPEND'</code></td><td>390</td><td>0.163s</td></tr><tr><td><a href="https://www.gnu.org/software/grep/" target="_blank" rel="noopener">GNU grep</a></td><td><code>egrep -R -n --include='*.c' --include='*.h' -w '[A-Z]+_SUSPEND'</code></td><td>404</td><td>0.611s</td></tr></tbody></table><ul><li>在单个大文件上对 Ripgrep 和 GNU Grep 进行比较，文件大小大约 9.3G。</li></ul><table><thead><tr><th>Tool</th><th>Command</th><th>Line count</th><th>Time</th></tr></thead><tbody><tr><td>ripgrep</td><td><code>rg -w 'Sherlock [A-Z]\w+'</code></td><td>5268</td><td><strong>2.108s</strong></td></tr><tr><td><a href="https://www.gnu.org/software/grep/" target="_blank" rel="noopener">GNU grep</a></td><td><code>LC_ALL=C egrep -w 'Sherlock [A-Z]\w+'</code></td><td>5268</td><td>7.014s</td></tr></tbody></table><p><strong>Ripgrep 效果图</strong></p><p><img src="https://www.hi-linux.com/img/linux/ripgrep.png" alt=""></p><h3 id="安装-ripgrep">安装 Ripgrep</h3><p>Ripgrep 具有良好跨平台特性，支持在 Linux、macOS、Windows 等多种平台下安装。官方也提供了各平台对应的二进制版本，下面我们以 Linux 平台为例使用二进制版本进行安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ wget  https://github.com/BurntSushi/ripgrep/releases/download/0.10.0/ripgrep-0.10.0-x86_64-unknown-linux-musl.tar.gz</span><br><span class="line">$ tar xzvf ripgrep-0.10.0-x86_64-unknown-linux-musl.tar.gz</span><br><span class="line">$ cp ripgrep-0.10.0-x86_64-unknown-linux-musl/rg  /usr/<span class="built_in">local</span>/bin/</span><br></pre></td></tr></table></figure><p>如果你使用其它平台，方法与其类似。你可根据实际情况在官方<a href="https://github.com/BurntSushi/ripgrep/releases" target="_blank" rel="noopener">下载页面</a>下载对应版本进行安装。当然官方也提供了其它多种多样的安装方式，具体可参考官方<a href="https://github.com/BurntSushi/ripgrep#installation" target="_blank" rel="noopener">安装文档</a>。</p><h3 id="ripgrep-语法格式">Ripgrep 语法格式</h3><ul><li>整体语法格式</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">USAGE:</span><br><span class="line"></span><br><span class="line">    rg [OPTIONS] PATTERN [PATH ...]</span><br><span class="line">    rg [OPTIONS] [-e PATTERN ...] [-f PATTERNFILE ...] [PATH ...]</span><br><span class="line">    rg [OPTIONS] --files [PATH ...]</span><br><span class="line">    rg [OPTIONS] --<span class="built_in">type</span>-list</span><br><span class="line">    <span class="built_in">command</span> | rg [OPTIONS] PATTERN</span><br><span class="line"></span><br><span class="line">ARGS:</span><br><span class="line">    &lt;PATTERN&gt;</span><br><span class="line">            A regular expression used <span class="keyword">for</span> searching. To match a pattern beginning with a</span><br><span class="line">            dash, use the -e/--regexp flag.</span><br><span class="line"></span><br><span class="line">            For example, to search <span class="keyword">for</span> the literal <span class="string">'-foo'</span>, you can use this flag:</span><br><span class="line"></span><br><span class="line">                rg -e -foo</span><br><span class="line"></span><br><span class="line">            You can also use the special <span class="string">'--'</span> delimiter to indicate that no more flags</span><br><span class="line">            will be provided. Namely, the following is equivalent to the above:</span><br><span class="line"></span><br><span class="line">                rg -- -foo</span><br><span class="line"></span><br><span class="line">    &lt;PATH&gt;...</span><br><span class="line">            A file or directory to search. Directories are searched recursively. Paths specified on</span><br><span class="line">            the <span class="built_in">command</span> line override glob and ignore rules.</span><br></pre></td></tr></table></figure><ul><li>支持的命令行选项</li></ul><p>这里我们把一些常用选项做下介绍。</p><table><thead><tr><th>选项</th><th>说明</th><th>备注</th></tr></thead><tbody><tr><td>-A, --after-context <code>&lt;NUM&gt;</code></td><td>显示匹配内容后的 <code>&lt;NUM&gt;</code> 行。</td><td>会覆盖 <code>--context</code> 选项。</td></tr><tr><td>-B, --before-context <code>&lt;NUM&gt;</code></td><td>显示匹配内容前的 <code>&lt;NUM&gt;</code> 行。</td><td>会覆盖 <code>--context</code> 选项。</td></tr><tr><td>-b, --byte-offset</td><td>显示匹配内容在文件中的字节偏移。</td><td>和 <code>-o</code> 一起使用时只打印偏移。</td></tr><tr><td>-s, --case-sensitive</td><td>启用大小写敏感。</td><td>会覆盖 <code>-i(--ignore case)</code> 和 <code>-S(--smart case)</code> 选项。</td></tr><tr><td>–color <code>&lt;WHEN&gt;</code></td><td>什么时候使用颜色，默认值为：auto。可选值有：never、auto、always、ansi。</td><td>如果 <code>--vimgre</code> 选项被使用，那么默认值是 never。</td></tr><tr><td>–column</td><td>显示匹配所在列数 (从 1 开始)。</td><td>如果不显示列号可用 <code>--no-column</code> 取消掉。</td></tr><tr><td>-C, --context <code>&lt;NUM&gt;</code></td><td>显示匹配内容的前面和后面的 <code>&lt;NUM&gt;</code> 行。</td><td>该选项会覆盖 <code>-B</code> 和 <code>-A</code> 选项。</td></tr><tr><td>–context-separator <code>&lt;SEPARATOR&gt;</code></td><td>在输出结果中分隔非连续的输出行 。</td><td>可以使用<code>\x7F</code> 或 <code>\t</code>，默认是 <code>--</code>。</td></tr><tr><td>-c, --count</td><td>只显示匹配结果的总行数。</td><td>如果只有一个文件给 Ripgrep，那么只打印匹配结果的总行数。可以用 <code>--with-filename</code> 来强制打印文件名，该选项会覆盖 <code>--count-matches</code> 选项。</td></tr><tr><td>–count-matches</td><td>只显示匹配结果的总次数。</td><td>可以用 <code>--with-filename</code> 来强制在只有一个文件时也输出文件名。</td></tr><tr><td>–debug</td><td>显示调试信息。</td><td></td></tr><tr><td>–dfa-size-limit <code>&lt;NUM+SUFFIX?&gt;</code></td><td>指定正则表达式 DFA 的上限，默认为 10M。</td><td>该选项允许接受与 <code>--max-filesize</code> 相同大小的后缀标志。</td></tr><tr><td>-E, --encoding <code>&lt;ENCODING&gt;</code></td><td>指定文本编码格式, 默认是 auto。</td><td>更多编码格式参考：<a href="https://encoding.spec.whatwg.org/#concept-encoding-get" target="_blank" rel="noopener">https://encoding.spec.whatwg.org/#concept-encoding-get</a></td></tr><tr><td>-f, --file <code>&lt;PATTERNFILE&gt;</code>…</td><td>从文件中读入搜索模式, 一行一个模式。</td><td>结合 <code>-e/--regexp</code> 参数可多个文件一起组合使用，所有组合会被匹配。</td></tr><tr><td>–files</td><td>打印所有将被搜索的文件路径。</td><td>以 <code>rg &lt;options&gt; --files [PATH...]</code> 方式使用，不能增加搜索模式。</td></tr><tr><td>-l, --files-with-matches</td><td>只打印有匹配的文件名。</td><td>该选项会覆盖 <code>--files-without-match</code>。</td></tr><tr><td>–files-without-match</td><td>只打印无匹配的文件名。</td><td>该选项会覆盖 <code>--file-with-matches</code>。</td></tr><tr><td>-F, --fixed-strings</td><td>把搜索模式当成常规文字而非正则表达式。</td><td>该选项可以用<code>--no-fixed-strings</code> 来禁止。</td></tr><tr><td>-L, --follow</td><td>该选项会递归搜索符号链接，默认是关闭的。</td><td>该选项可以用 <code>--no-follow</code> 选项来手动关闭。</td></tr><tr><td>-g, --glob <code>&lt;GLOB&gt;</code>…</td><td>包含或排除用于搜索匹配给定的文件和目录，可以用 ! 来取反。</td><td>该选项可以多次使用，会匹配 .gitignore 中的规则。</td></tr><tr><td>-h, --help</td><td>打印帮助信息。</td><td></td></tr><tr><td>–heading</td><td>打印文件名到匹配内容的上方而不是在同一行。</td><td>该选项是默认启用的，可以用 <code>--no-heading</code> 来关闭。</td></tr><tr><td>–hidden</td><td>启用搜索隐藏文件和文件夹。</td><td>默认情况下是忽略搜索隐藏文件和文件夹的, 可用 <code>--no-hidden</code> 来关闭。</td></tr><tr><td>–iglob <code>&lt;GLOB&gt;</code>…</td><td>作用同 <code>--glob</code>, 但这个选项大小写不敏感。</td><td></td></tr><tr><td>-i, --ignore-case</td><td>指定搜索模式中的大小写不敏感。</td><td>该选项会被 <code>-s/--case-sensitive</code> 或 <code>-S/--smart-case</code> 覆盖。</td></tr><tr><td>–ignore-file <code>&lt;PATH&gt;</code>…</td><td>指定搜索时需忽略的路径，格式同 <code>.gitignore</code>, 可同时指定多个。</td><td>如果存在多个 <code>--ignore-file</code> 标记时，后面优先级会更高。</td></tr><tr><td>-v, --invert-match</td><td>反向匹配，显示与给定模式不匹配的行。</td><td></td></tr><tr><td>-n, --line-number</td><td>显示匹配内容所在文件的行数，该选项默认是打开的。</td><td></td></tr><tr><td>-x, --line-regexp</td><td>只显示整行都匹配搜索模式的行。</td><td>该选项会覆盖 <code>--word-regexp</code>。</td></tr><tr><td>-M, --max-columns <code>&lt;NUM&gt;</code></td><td>不打印长于 <code>&lt;NUM&gt;</code> 中指定节字大小的匹配行内容，只显示该行的匹配数。</td><td></td></tr><tr><td>-m, --max-count <code>&lt;NUM&gt;</code></td><td>限制一个文件中最多 <code>&lt;NUM&gt;</code> 行被匹配。</td><td></td></tr><tr><td>–max-depth <code>&lt;NUM&gt;</code></td><td>限制文件夹递归搜索深度。</td><td>如：<code>rg --max-depth 0 dir/</code> 则表示不执行任何搜索。</td></tr><tr><td>–max-filesize <code>&lt;NUM+SUFFIX?&gt;</code></td><td>搜索时忽略大于 <code>&lt;NUM&gt;</code> byte 的文件。</td><td>SUFFIX 的单位可以是：K、M、G，默认是：byte。</td></tr><tr><td>–mmap</td><td>尽量使用 Memory Maps 进行搜索，这样速度会更快。该选项是默认行为。</td><td>如果使用 <code>--mmap</code> 搜索文件时 Ripgrep 发生意外中止，可使用 <code>--no-mmap</code> 选项关闭它。</td></tr><tr><td>–no-config</td><td>不读取 configuration 文件, 并忽略 RIPGREP_CONFIG_PATH 变量。</td><td></td></tr><tr><td>–no-filename</td><td>不要打印匹配文件的文件名。</td><td></td></tr><tr><td>–no-heading</td><td>不在每个匹配行上方打印文件名，而是在匹配行的同一行上打印。</td><td></td></tr><tr><td>–no-ignore</td><td>不读取忽略文件，如：.gitignore、.ignore 等。</td><td>该选项可以用 <code>--ignore</code> 关闭。</td></tr><tr><td>–no-ignore-global</td><td>不读取全局的 ignore 文件，比如: <code>$HOME/.config/git/ignore</code>。</td><td>该选项可以用 <code>--ignore-global</code> 关闭。</td></tr><tr><td>–no-ignore-messages</td><td>取消解析 .ignroe、.gitignore 文件中相关错误信息。</td><td>该选项可通过 <code>--ignore-messages</code> 关闭。</td></tr><tr><td>–no-ignore-parent</td><td>不读取父文件夹里的 .gitignore、.ignore 文件。</td><td>该选项可通过 <code>--ignore-parent</code> 关闭。</td></tr><tr><td>–no-ignore-vcs</td><td>不读取版本控制器中的 .ignore 文件。</td><td>该选项可通过 <code>--ignore-vcs</code> 关闭。</td></tr><tr><td>-N, --no-line-number</td><td>不打印匹配行数。</td><td></td></tr><tr><td>–no-messages</td><td>不打印打开和读取文件时相关错误信息。</td><td></td></tr><tr><td>-0, --null</td><td>在打印的文件路径后加一个 NUL 字符。</td><td>这对于结合 Xargs 使用时是非常有用的。</td></tr><tr><td>-o, --only-matching</td><td>只打印匹配的内容，而不是整行。</td><td></td></tr><tr><td>–passthru</td><td>同时打印文件中匹配和不匹配的行。</td><td></td></tr><tr><td>–path-separator <code>&lt;SEPARATOR&gt;</code></td><td>路径分隔符，在 Linux 上默认是 /，Windows 上默认是 \ 。</td><td></td></tr><tr><td>–pre <code>&lt;COMMAND&gt;</code></td><td>用 <code>&lt;COMMAND&gt;</code> 处理文件后，并将结果传递给 Ripgrep。</td><td>该选项存在一定的性能损耗。</td></tr><tr><td>-p, --pretty</td><td>该选项是 <code>--color always --heading --line-number</code> 的别名。</td><td></td></tr><tr><td>-q, --quiet</td><td>该选项不会打印到标准输出, 如果匹配发现时就停止搜索。</td><td>当 RipGrep 用于 exit 代码时该选项非常有用。</td></tr><tr><td>–regex-size-limit <code>&lt;NUM+SUFFIX?&gt;</code></td><td>设置已编译正则表达式的上限，默认限制为10M。</td><td></td></tr><tr><td>-e, --regexp <code>&lt;PATTERN&gt;</code>…</td><td>使用正则来匹配搜索条件。</td><td>该选项可以多次使用，可打印匹配任何模式的行。</td></tr><tr><td>-r, --replace <code>&lt;REPLACEMENT_TEXT&gt;</code></td><td>用相应文件内容代替匹配内容打印出来。</td><td></td></tr><tr><td>-z, --search-zip</td><td>在 gz、bz2、xz、lzma、lz4 文件类型中搜索。</td><td>该选项可通过 <code>--no-search-zip</code> 关闭。</td></tr><tr><td>-S, --smart-case</td><td>如果全小写，则大小写不敏感，否则大小写敏感。</td><td>该选项可通过 <code>-s/--case-sensitive</code> 和 <code>-i/--ignore-case</code> 来关闭。</td></tr><tr><td>–sort <code>&lt;SORTBY&gt;</code></td><td>将输出结果按升序进行排序，可排序类型有：path、modified、accessed、created 。</td><td></td></tr><tr><td>–sortr <code>&lt;SORTBY&gt;</code></td><td>将输出结果按降序进行排序，可排序类型有：path、modified、accessed、created 。</td><td></td></tr><tr><td>–stats</td><td>打印出统计结果。</td><td></td></tr><tr><td>-a, --text</td><td>搜索二进制文件。</td><td>该选项可通过 <code>--no-text</code> 关闭。</td></tr><tr><td>-j, --threads <code>&lt;NUM&gt;</code></td><td>搜索时要使用的线程数。</td><td></td></tr><tr><td>-t, --type <code>&lt;TYPE&gt;</code>…</td><td>只搜索指定的文件类型。</td><td>可以通过 <code>--type-list</code> 来列出支持的文件类型。</td></tr><tr><td>–type-add <code>&lt;TYPE_SPEC&gt;</code>…</td><td>添加一种文件类型。</td><td></td></tr><tr><td>–type-clear <code>&lt;TYPE&gt;</code>…</td><td>清除默认的文件类型。</td><td></td></tr><tr><td>–type-list</td><td>列出所有内置文件类型。</td><td></td></tr><tr><td>-T, --type-not <code>&lt;TYPE&gt;</code>…</td><td>不要搜索某种文件类型。</td><td></td></tr><tr><td>-u, --unrestricted</td><td><code>-u</code> 搜索.gitignore 里的文件, <code>-uu</code> 搜索隐藏文件，<code>-uuu</code> 搜索二进制文件。</td><td></td></tr><tr><td>-V, --version</td><td>打印版本信息。</td><td></td></tr><tr><td>–vimgrep</td><td>每一次匹配都单独打印一行，如果一行有多次匹配会打印成多行。</td><td></td></tr><tr><td>-H, --with-filename</td><td>打印匹配的文件路径，该选项默认打开。</td><td>该选项可通过 <code>--no-filename</code> 关闭。</td></tr><tr><td>-w, --word-regexp</td><td>把搜索参数作为单独单词匹配。</td><td>该选项会覆盖 <code>--line-regexp</code> 选项。</td></tr></tbody></table><p>更多命令行选项，可通过 <code>rg --help</code> 自行查看。</p><h3 id="ripgrep-使用实例">Ripgrep 使用实例</h3><h4 id="搜索指定文件中包含关键字的内容">搜索指定文件中包含关键字的内容</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ rg <span class="string">'github.com'</span>  README.md</span><br><span class="line">1:&lt;h1 align=<span class="string">"center"</span>&gt;&lt;a title=<span class="string">"New «NexT» 6.0.0 version [Reloaded]"</span> href=<span class="string">"https://github.com/theme-next/hexo-theme-next"</span>&gt;NexT&lt;/a&gt;&lt;/h1&gt;</span><br><span class="line">6:[![mnt-image]](https://github.com/theme-next/hexo-theme-next)</span><br><span class="line">21:More NexT examples [here](https://github.com/iissnan/hexo-theme-next/issues/119).</span><br><span class="line">41:   $ curl -s https://api.github.com/repos/iissnan/hexo-theme-next/releases/latest | grep tarball_url | cut -d <span class="string">'"'</span> -f 4 | wget -i - -O- | tar -zx -C themes/next --strip-components=1</span><br><span class="line">51:   $ curl -L https://api.github.com/repos/iissnan/hexo-theme-next/tarball/v5.1.2 | tar -zxv -C themes/next --strip-components=1</span><br><span class="line">57:   $ git <span class="built_in">clone</span> --branch v5.1.2 https://github.com/iissnan/hexo-theme-next themes/next</span><br><span class="line">67:   $ curl -L https://api.github.com/repos/iissnan/hexo-theme-next/tarball | tar -zxv -C themes/next --strip-components=1</span><br><span class="line">73:   $ git <span class="built_in">clone</span> https://github.com/iissnan/hexo-theme-next themes/next</span><br><span class="line">110:For those who also encounter **Error: Cannot find module <span class="string">'hexo-util'</span>** [issue](https://github.com/iissnan/hexo-theme-next/issues/1490), please check your NPM version.</span><br><span class="line">128:<span class="comment">### Theme configurations using Hexo data files ([#328](https://github.com/iissnan/hexo-theme-next/issues/328))</span></span><br><span class="line">282:NexT uses [Tomorrow Theme](https://github.com/chriskempson/tomorrow-theme) with 5 themes <span class="keyword">for</span> you to choose from.</span><br><span class="line">288:Head over to [Tomorrow Theme](https://github.com/chriskempson/tomorrow-theme) <span class="keyword">for</span> more details.</span><br><span class="line">367:[download-latest-url]: https://github.com/iissnan/hexo-theme-next/archive/master.zip</span><br><span class="line">368:[releases-latest-url]: https://github.com/iissnan/hexo-theme-next/releases/latest</span><br><span class="line">369:[releases-url]: https://github.com/iissnan/hexo-theme-next/releases</span><br><span class="line">370:[tags-url]: https://github.com/iissnan/hexo-theme-next/tags</span><br><span class="line">371:[commits-url]: https://github.com/iissnan/hexo-theme-next/commits/master</span><br></pre></td></tr></table></figure><h4 id="搜索指定文件中包含以关键字开头的单词的内容">搜索指定文件中包含以关键字开头的单词的内容</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ rg <span class="string">'lang\w+'</span> README.md</span><br><span class="line">154:<span class="comment">### Multiple languages support, including:</span></span><br><span class="line">168:Default language is English.</span><br><span class="line">171:language: en</span><br><span class="line">172:<span class="comment"># language: zh-Hans</span></span><br><span class="line">173:<span class="comment"># language: zh-hk</span></span><br><span class="line">174:<span class="comment"># language: zh-tw</span></span><br><span class="line">175:<span class="comment"># language: ru</span></span><br><span class="line">176:<span class="comment"># language: fr-FR</span></span><br><span class="line">177:<span class="comment"># language: de</span></span><br><span class="line">178:<span class="comment"># language: ja</span></span><br><span class="line">179:<span class="comment"># language: id</span></span><br><span class="line">180:<span class="comment"># language: pt</span></span><br><span class="line">181:<span class="comment"># language: pt-BR</span></span><br></pre></td></tr></table></figure><h4 id="搜索指定文件中包含以关键字开头的内容">搜索指定文件中包含以关键字开头的内容</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ rg <span class="string">'hexo\w*'</span> README.md</span><br><span class="line">1:&lt;h1 align=<span class="string">"center"</span>&gt;&lt;a title=<span class="string">"New «NexT» 6.0.0 version [Reloaded]"</span> href=<span class="string">"https://github.com/theme-next/hexo-theme-next"</span>&gt;NexT&lt;/a&gt;&lt;/h1&gt;</span><br><span class="line">3:&lt;p align=<span class="string">"center"</span>&gt;NexT is a high quality elegant &lt;a href=<span class="string">"http://hexo.io"</span>&gt;Hexo&lt;/a&gt; theme. It is crafted from scratch, with love.&lt;/p&gt;</span><br><span class="line">6:[![mnt-image]](https://github.com/theme-next/hexo-theme-next)</span><br><span class="line">9:[![hexo-image]][hexo-url]</span><br><span class="line">21:More NexT examples [here](https://github.com/iissnan/hexo-theme-next/issues/119).</span><br><span class="line">25:**1.** Change dir to **hexo root** directory. There must be `node_modules`, `<span class="built_in">source</span>`, `themes` and other directories:</span><br><span class="line">27:   $ <span class="built_in">cd</span> hexo</span><br><span class="line">41:   $ curl -s https://api.github.com/repos/iissnan/hexo-theme-next/releases/latest | grep tarball_url | cut -d <span class="string">'"'</span> -f 4 | wget -i - -O- | tar -zx -C themes/next --strip-components=1</span><br><span class="line">51:   $ curl -L https://api.github.com/repos/iissnan/hexo-theme-next/tarball/v5.1.2 | tar -zxv -C themes/next --strip-components=1</span><br><span class="line">57:   $ git <span class="built_in">clone</span> --branch v5.1.2 https://github.com/iissnan/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure><h4 id="搜索指定目录及子目中包含关键字的内容">搜索指定目录及子目中包含关键字的内容</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ rg <span class="string">'github.com'</span> ./</span><br><span class="line">./src/scrollspy.js</span><br><span class="line">6:* Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)</span><br><span class="line"></span><br><span class="line">./src/affix.js</span><br><span class="line">6: * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)</span><br><span class="line"></span><br><span class="line">./src/js.cookie.js</span><br><span class="line">3: * https://github.com/js-cookie/js-cookie</span><br></pre></td></tr></table></figure><h4 id="搜索以关键字为独立单词的内容">搜索以关键字为独立单词的内容</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ rg -w <span class="string">'github.com'</span> ./</span><br><span class="line">./bower.json</span><br><span class="line">36:    <span class="string">"url"</span> : <span class="string">"http://github.com/julianshapiro/velocity.git"</span></span><br><span class="line"></span><br><span class="line">./velocity.ui.js</span><br><span class="line">58:        var abortError = <span class="string">"Velocity UI Pack: You need to update Velocity (jquery.velocity.js) to a newer version. Visit http://github.com/julianshapiro/velocity."</span>;</span><br><span class="line"></span><br><span class="line">./velocity.js</span><br><span class="line">442:    /* IE detection. Gist: https://gist.github.com/julianshapiro/9098609 */</span><br><span class="line">463:    /* rAF shim. Gist: https://gist.github.com/julianshapiro/9497513 */</span><br><span class="line">472:            /* Technique by Erik Moller. MIT license: https://gist.github.com/paulirish/1579671 */</span><br><span class="line">480:    /* Array compacting. Copyright Lo-Dash. MIT License: https://github.com/lodash/lodash/blob/master/LICENSE.txt */</span><br><span class="line">522:        /* Copyright Martin Bohm. MIT License: https://gist.github.com/Tomalak/818a78a226a0738eaade */</span><br></pre></td></tr></table></figure><h4 id="搜索包含关键字内容的文件并且只打印文件名">搜索包含关键字内容的文件并且只打印文件名</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ rg -w <span class="string">'github.com'</span> ./ -l</span><br><span class="line">./velocity.js</span><br><span class="line">./bower.json</span><br><span class="line">./velocity.ui.js</span><br><span class="line">./velocity.ui.min.js</span><br></pre></td></tr></table></figure><h4 id="在指定文件类型格式为-js-的文件中搜索包含关键字的内容">在指定文件类型格式为 JS 的文件中搜索包含关键字的内容</h4><p>RipGrep 实现的方式存在多种多样，这里介绍比较常用的两种。</p><ul><li>第一种：使用 <code>--type</code> 选项指定文件类型。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ rg <span class="string">'function writeOnCanvas'</span> --<span class="built_in">type</span> js</span><br><span class="line"><span class="built_in">source</span>/lib/Han/dist/han.js</span><br><span class="line">1726:<span class="keyword">function</span> writeOnCanvas( text, font ) &#123;</span><br></pre></td></tr></table></figure><ul><li>第二种：使用 <code>--glob</code> 选项来通配需要的文件类型。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ rg  <span class="string">'function writeOnCanvas'</span> -g <span class="string">'*.js'</span></span><br><span class="line"><span class="built_in">source</span>/lib/Han/dist/han.js</span><br><span class="line">1726:<span class="keyword">function</span> writeOnCanvas( text, font ) &#123;</span><br></pre></td></tr></table></figure><p>如果要同时搜索多个文件类型可以写成下面这样。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ rg <span class="string">'Hanzi'</span> -g <span class="string">'*.&#123;js,css&#125;'</span></span><br><span class="line">han.min.js</span><br><span class="line">2:/*! Han.css: the CSS typography framework optimised <span class="keyword">for</span> Hanzi */</span><br><span class="line"></span><br><span class="line">han.js</span><br><span class="line">3: * Han.css: the CSS typography framework optimised <span class="keyword">for</span> Hanzi</span><br><span class="line">48:  // Address Hanzi and Western script mixed spacing</span><br><span class="line">426:    /* Hanzi and Western mixed spacing </span><br><span class="line"></span><br><span class="line">han.css</span><br><span class="line">4:/*! Han.css: the CSS typography framework optimised <span class="keyword">for</span> Hanzi */</span><br><span class="line"></span><br><span class="line">han.min.css</span><br><span class="line">4:/*! Han.css: the CSS typography framework optimised <span class="keyword">for</span> Hanzi */</span><br></pre></td></tr></table></figure><h4 id="在当前目下并且不包含文件类型格式为-css-的文件中搜索包含关键字的内容">在当前目下并且不包含文件类型格式为 CSS 的文件中搜索包含关键字的内容</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ rg <span class="string">'revertVowel'</span> --<span class="built_in">type</span>-not css</span><br><span class="line"><span class="built_in">source</span>/lib/Han/dist/han.min.js</span><br><span class="line">(this[<span class="string">"comb-liga-zhuyin"</span>]=O.substZhuyinCombLiga(this.context)),this&#125;,revertVowelCombLiga:<span class="function"><span class="title">function</span></span>()&#123;try&#123;this[<span class="string">"comb-liga-vowel"</span>].revert(<span class="string">"all"</span>)&#125;catch(a)&#123;&#125;<span class="built_in">return</span> this&#125;,revertVowelICombLiga:<span class="function"><span class="title">function</span></span>()&#123;try&#123;this[<span class="string">"comb-liga-vowel-i"</span>].revert(<span class="string">"all"</span>)&#125;catch(a)&#123;&#125;<span class="built_in">return</span> this&#125;,revertZhuyinCombLiga:<span class="function"><span class="title">function</span></span>()&#123;try&#123;this[<span class="string">"comb-liga-zhuyin"</span>].revert(<span class="string">"all"</span>)&#125;catch(a)&#123;&#125;<span class="built_in">return</span> this&#125;,revertCombLigaWithPUA:<span class="function"><span class="title">function</span></span>()&#123;try&#123;this[<span class="string">"comb-liga-vowel"</span>].revert(<span class="string">"all"</span>),this[<span class="string">"comb-liga-vowel-i"</span>].revert(<span class="string">"all"</span>),this[<span class="string">"comb-liga-zhuyin"</span>].revert(<span class="string">"all"</span>)&#125;catch(a)&#123;&#125;<span class="built_in">return</span> this&#125;,substInaccurateChar:<span class="function"><span class="title">function</span></span>()&#123;<span class="built_in">return</span> this[<span class="string">"inaccurate-char"</span>]=O.substInaccurateChar(this.context),this&#125;,revertInaccurateChar:<span class="function"><span class="title">function</span></span>()&#123;try&#123;this[<span class="string">"inaccurate-char"</span>].revert(<span class="string">"all"</span>)&#125;catch(a)&#123;&#125;<span class="built_in">return</span> this&#125;&#125;),a.addEventListener(<span class="string">"DOMContentLoaded"</span>,<span class="function"><span class="title">function</span></span>()&#123;var a;K.classList.contains(<span class="string">"han-init"</span>)?O.init():(a=J.querySelector(<span class="string">".han-init-context"</span>))&amp;&amp;(O.init=O(a).render())&#125;),(<span class="string">"undefined"</span>==typeof b||b===!1)&amp;&amp;(a.Han=O),O&#125;);</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span>/lib/Han/dist/han.js</span><br><span class="line">2939:  revertVowelCombLiga: <span class="function"><span class="title">function</span></span>() &#123;</span><br><span class="line">2946:  revertVowelICombLiga: <span class="function"><span class="title">function</span></span>() &#123;</span><br></pre></td></tr></table></figure><p>你也可以用下面的更简洁的写法来达到同样的效果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rg &#39;revertVowel&#39; -Tcss</span><br></pre></td></tr></table></figure><h4 id="使用正则表达式进行关键字搜索">使用正则表达式进行关键字搜索</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ rg -e <span class="string">"noConf.*lict"</span> ./</span><br><span class="line">./js.cookie.js</span><br><span class="line">21:api.noConflict = <span class="function"><span class="title">function</span></span> () &#123;</span><br><span class="line"></span><br><span class="line">./scrollspy.js</span><br><span class="line">166:  $.fn.scrollspy.noConflict = <span class="function"><span class="title">function</span></span> () &#123;</span><br><span class="line"></span><br><span class="line">./affix.js</span><br><span class="line">139:  $.fn.affix.noConflict = <span class="function"><span class="title">function</span></span> () &#123;</span><br></pre></td></tr></table></figure><h4 id="搜索匹配关键字的内容及显示其上下内容各两行">搜索匹配关键字的内容及显示其上下内容各两行</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$  rg -e <span class="string">"noConf.*lict"</span>  -C2</span><br><span class="line">js.cookie.js</span><br><span class="line">19-var OldCookies = window.Cookies;</span><br><span class="line">20-var api = window.Cookies = factory();</span><br><span class="line">21:api.noConflict = <span class="function"><span class="title">function</span></span> () &#123;</span><br><span class="line">22-window.Cookies = OldCookies;</span><br><span class="line">23-<span class="built_in">return</span> api;</span><br><span class="line"></span><br><span class="line">scrollspy.js</span><br><span class="line">164-  // =====================</span><br><span class="line">165-</span><br><span class="line">166:  $.fn.scrollspy.noConflict = <span class="function"><span class="title">function</span></span> () &#123;</span><br><span class="line">167-    $.fn.scrollspy = old</span><br><span class="line">168-    <span class="built_in">return</span> this</span><br><span class="line"></span><br><span class="line">affix.js</span><br><span class="line">137-  // =================</span><br><span class="line">138-</span><br><span class="line">139:  $.fn.affix.noConflict = <span class="function"><span class="title">function</span></span> () &#123;</span><br><span class="line">140-    $.fn.affix = old</span><br><span class="line">141-    <span class="built_in">return</span> this</span><br></pre></td></tr></table></figure><h4 id="搜索不包含关键字的内容">搜索不包含关键字的内容</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ rg -v <span class="string">"hexo"</span> merge-configs.js</span><br><span class="line">2:</span><br><span class="line">3:var merge = require(<span class="string">'./merge'</span>);</span><br><span class="line">4:</span><br><span class="line">5:/**</span><br><span class="line">8: */</span><br><span class="line">12:    <span class="keyword">if</span> ( data &amp;&amp; data.next ) &#123;</span><br><span class="line">13:      <span class="keyword">if</span> ( data.next.override ) &#123;</span><br><span class="line">15:      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">17:      &#125;</span><br><span class="line">18:    &#125;</span><br><span class="line">19:  &#125;</span><br><span class="line">20:&#125;);</span><br><span class="line">21:</span><br><span class="line">30:&#125;);</span><br></pre></td></tr></table></figure><h4 id="搜索关键字并只显示关键字部分的内容">搜索关键字并只显示关键字部分的内容</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ rg -e <span class="string">"hexo.*warn"</span> -o ./</span><br><span class="line">./tags/lazy-image.js</span><br><span class="line">12:hexo.log.warn</span><br><span class="line"></span><br><span class="line">./merge-configs.js</span><br><span class="line">23:hexo.log.warn</span><br><span class="line">24:hexo.log.warn</span><br><span class="line">25:hexo.log.warn</span><br><span class="line">26:hexo.log.warn</span><br><span class="line">27:hexo.log.warn</span><br><span class="line">28:hexo.log.warn</span><br><span class="line">29:hexo.log.warn</span><br><span class="line"></span><br><span class="line">./tags/button.js</span><br><span class="line">13:hexo.log.warn</span><br><span class="line"></span><br><span class="line">./tags/full-image.js</span><br><span class="line">12:hexo.log.warn</span><br></pre></td></tr></table></figure><h4 id="搜索关键字并忽略关键字大小写的内容">搜索关键字并忽略关键字大小写的内容</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ rg -ie &quot;Return.*&quot; merge.js</span><br><span class="line">103:var root &#x3D; freeGlobal || freeSelf || Function(&#39;return this&#39;)();</span><br><span class="line">120:    return freeProcess &amp;&amp; freeProcess.binding(&#39;util&#39;);</span><br><span class="line">134: * @returns &#123;Object&#125; Returns &#96;map&#96;.</span><br><span class="line">137:  &#x2F;&#x2F; Don&#39;t return &#96;map.set&#96; because it&#39;s not chainable in IE 11.</span><br><span class="line">139:  return map;</span><br><span class="line">148: * @returns &#123;Object&#125; Returns &#96;set&#96;.</span><br><span class="line">151:  &#x2F;&#x2F; Don&#39;t return &#96;set.add&#96; because it&#39;s not chainable in IE 11.</span><br></pre></td></tr></table></figure><h4 id="把关键字当成常量字符进行搜索">把关键字当成常量字符进行搜索</h4><p>关键字中包含 <code>.(){}*+</code> 类似字符时，不需要手动转义。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ rg -F <span class="string">"i++)"</span> ./</span><br><span class="line">./tags/exturl.js</span><br><span class="line">27:  <span class="keyword">for</span> (; i &lt; len; i++) &#123;</span><br><span class="line"></span><br><span class="line">./tags/group-pictures.js</span><br><span class="line">795:    <span class="keyword">for</span> (var i = 0; i &lt; rows; i++) &#123;</span><br><span class="line">805:    <span class="keyword">for</span> (var i = 0; i &lt; rows.length; i++) &#123;</span><br><span class="line">825:    <span class="keyword">for</span> (var i = 0; i &lt; pictures.length; i++) &#123;</span><br></pre></td></tr></table></figure><p>如果要搜索的字符是以 <code>-</code> 开头时，要用 <code>--</code> 来作为分隔符。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ rg -- -1 merge.js</span><br><span class="line">190:  var index &#x3D; -1,</span><br><span class="line">210:  var index &#x3D; -1,</span><br><span class="line">233:  var index &#x3D; -1,</span><br><span class="line">255:  var index &#x3D; -1,</span><br><span class="line">317:  var index &#x3D; -1,</span><br><span class="line">348:  var index &#x3D; -1,</span><br><span class="line">435:  var index &#x3D; -1,</span><br><span class="line">533:  var index &#x3D; -1,</span><br><span class="line">605:  return assocIndexOf(this.__data__, key) &gt; -1;</span><br><span class="line">645:  var index &#x3D; -1,</span><br><span class="line">889: * @returns &#123;number&#125; Returns the index of the matched value, else &#96;-1&#96;.</span><br><span class="line">898:  return -1;</span><br></pre></td></tr></table></figure><p>或者使用 <code>-e</code> 参数也可以达到类似目的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ rg -e <span class="string">"-1"</span> <span class="built_in">source</span>/js</span><br><span class="line"><span class="built_in">source</span>/js/src/js.cookie.js</span><br><span class="line">113:cookie = cookie.slice(1, -1);</span><br><span class="line">155:expires: -1</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span>/js/src/motion.js</span><br><span class="line">190:    cursor: -1,</span><br><span class="line">223:        getMistLineSettings(<span class="variable">$logoLineBottom</span>, <span class="string">'-100%'</span>)</span><br></pre></td></tr></table></figure><h4 id="打印当前目下所有将被搜索的文件列表">打印当前目下所有将被搜索的文件列表</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ rg --files</span><br><span class="line">merge.js</span><br><span class="line">merge-configs.js</span><br><span class="line">tags/lazy-image.js</span><br><span class="line">tags/center-quote.js</span><br><span class="line">tags/tabs.js</span><br><span class="line">tags/note.js</span><br><span class="line">tags/button.js</span><br><span class="line">tags/full-image.js</span><br><span class="line">tags/group-pictures.js</span><br><span class="line">tags/label.js</span><br><span class="line">tags/exturl.js</span><br></pre></td></tr></table></figure><h4 id="输出所有内置可识别文件类型">输出所有内置可识别文件类型</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ rg --type-list</span><br><span class="line">agda: *.agda, *.lagda</span><br><span class="line">aidl: *.aidl</span><br><span class="line">amake: *.bp, *.mk</span><br><span class="line">asciidoc: *.adoc, *.asc, *.asciidoc</span><br><span class="line">asm: *.S, *.asm, *.s</span><br><span class="line">ats: *.ats, *.dats, *.hats, *.sats</span><br><span class="line">avro: *.avdl, *.avpr, *.avsc</span><br><span class="line">awk: *.awk</span><br><span class="line">bazel: *.bzl, BUILD, WORKSPACE</span><br><span class="line">bitbake: *.bb, *.bbappend, *.bbclass, *.conf, *.inc</span><br><span class="line">bzip2: *.bz2</span><br><span class="line">c: *.H, *.c, *.cats, *.h</span><br><span class="line">cabal: *.cabal</span><br><span class="line">cbor: *.cbor</span><br><span class="line">ceylon: *.ceylon</span><br><span class="line">clojure: *.clj, *.cljc, *.cljs, *.cljx</span><br><span class="line">cmake: *.cmake, CMakeLists.txt</span><br><span class="line">coffeescript: *.coffee</span><br><span class="line">config: *.cfg, *.conf, *.config, *.ini</span><br><span class="line">cpp: *.C, *.H, *.cc, *.cpp, *.cxx, *.h, *.hh, *.hpp, *.hxx, *.inl</span><br><span class="line">creole: *.creole</span><br><span class="line">crystal: *.cr, Projectfile</span><br><span class="line">cs: *.cs</span><br><span class="line">......</span><br></pre></td></tr></table></figure><h4 id="自定义搜索文件类型">自定义搜索文件类型</h4><p>默认情况下，Ripgrep 附带了一堆预定义的类型。 通常，这些类型对应于众所周知的公共格式。 您也可以定义自己的类型，例如：您可能经常搜索 Web 类型文件，其中包含 Javascript、HTML 和 CSS 类型的文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ rg --type-add &#39;web:*.html&#39; --type-add &#39;web:*.css&#39; --type-add &#39;web:*.js&#39; -tweb display</span><br><span class="line">han.css</span><br><span class="line">28:  display: block;</span><br><span class="line">34:  display: inline-block;</span><br><span class="line">37:  display: none;</span><br><span class="line">45:  display: none;</span><br><span class="line">174:  display: table; &#x2F;* 1 *&#x2F;</span><br><span class="line"></span><br><span class="line">han.js</span><br><span class="line">442:    &#x2F;&#x2F; The feature displays the following characters</span><br><span class="line">446:    &#39;display-as&#39;: &#123;</span><br><span class="line">1732:  canvas.style.display &#x3D; &#39;none&#39;</span><br></pre></td></tr></table></figure><p>也可以直接简写成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rg --type-add &#39;web:*.&#123;html,css,js&#125;&#39; -tweb display</span><br></pre></td></tr></table></figure><p>不过有一点是要注意的，上面增加的 Web 类型文件是临时的，只对当前命令有效。如果需要长期使用自定义的类型，你可以新增一个别名来在每次运行 RipGrep 时自动增加对应的文件类型。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ alias rg&#x3D;&quot;rg --type-add &#39;web:*.&#123;html,css,js&#125;&#39;&quot;</span><br></pre></td></tr></table></figure><p>当然还有另一种方法来达到类似的目的，那就是使用 RipGrep 的配置文件，RipGrep 的配置文件默认为 <code>$HOME/.ripgreprc</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ cat <span class="variable">$HOME</span>/.ripgreprc</span><br><span class="line"><span class="comment"># Don't let ripgrep vomit really long lines to my terminal.</span></span><br><span class="line">--max-columns=150</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add my 'web' type.</span></span><br><span class="line">--<span class="built_in">type</span>-add</span><br><span class="line">web:*.&#123;html,css,js&#125;*</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using glob patterns to include/exclude files or folders</span></span><br><span class="line">--glob=!git/*</span><br><span class="line"></span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">--glob</span><br><span class="line">!git/*</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the colors.</span></span><br><span class="line">--colors=line:none</span><br><span class="line">--colors=line:style:bold</span><br><span class="line"></span><br><span class="line"><span class="comment"># Because who cares about case!?</span></span><br><span class="line">--smart-case</span><br></pre></td></tr></table></figure><h4 id="将包含关键字的内容在查找结果中进行替换">将包含关键字的内容在查找结果中进行替换</h4><p>RipGrep 提供了一个在查找过程中直接将关键字内容进行替换的功能，下面我们来看一个将关键字中部分内容的小写字母转大写字母的例子。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ rg browse README.md --replace Browse</span><br><span class="line">305:<span class="comment"># can be any image format supported by web Browsers (JPEG,PNG,GIF,SVG,..)</span></span><br><span class="line">322:![Browser-image]</span><br><span class="line">324:[![Browser Stack](.github/Browserstack_logo.png)](https://www.Browserstack.com/)</span><br><span class="line">325:&gt;**BrowserStack** is a cloud-based cross-Browser testing tool that enables developers to <span class="built_in">test</span> their websites across various Browsers on different operating systems and mobile devices, without requiring users to install virtual machines, devices or emulators.</span><br><span class="line">343:[Browser-image]: https://img.shields.io/badge/Browser-%20chrome%20%7C%20firefox%20%7C%20opera%20%7C%20safari%20%7C%20ie%20%3E%3D%209-lightgrey.svg</span><br><span class="line">344:[Browser-url]: https://www.Browserstack.com</span><br></pre></td></tr></table></figure><p>上面的结果实质上只是在标准输出中进行替换，并不会对实际文件进行修改。如果你需要对实际文件进行修改，你可以结合 Sed 命令来达到目的。</p><ul><li>如果你使用 GNU Sed (CentOS、Ubuntu 等各种 Linux 发行版)，可以使用以下命令。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rg browse --files-with-matches | xargs sed -i <span class="string">'s/browse/Browse/g'</span></span><br></pre></td></tr></table></figure><ul><li>如果您使用 BSD Sed（ macOS 和 FreeBSD），则必须将以上命令修改为以下命令。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rg browse --files-with-matches | xargs sed -i <span class="string">''</span> <span class="string">'s/browse/Browse/g'</span></span><br></pre></td></tr></table></figure><blockquote><p>BSD Sed 中的 -i 标志需要提供文件扩展名以对所有已修改的文件进行备份，这里指定空字符串可防止进行文件备份。</p></blockquote><ul><li>如果您的文件路径中包含空格，则需要使用 NUL 终结符分隔文件路径。这里就需要使用 <code>-0</code> 参数来告诉 Ripgrep 在每个路径之间输出 NUL 字节，并告诉 Xargs 读取由 NUL 字节分隔的路径。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ rg Browse --files-with-matches -0 | xargs -0 sed -i &#39;&#39; &#39;s&#x2F;Browse&#x2F;browse&#x2F;g&#39;</span><br></pre></td></tr></table></figure><h4 id="直接在压缩文件中搜索包含关键字的内容">直接在压缩文件中搜索包含关键字的内容</h4><p>Ripgrep 目前仅支持 gzip、bzip2、lzma、lz4 和 xz 这几种压缩格式，并且需要在系统上已安装相应的 gzip，bzip2 和 xz 工具包。也就是说，Ripgrep 是通过 Shelling 到另一个进程来进行解压缩的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rg -z license UNLICENSE.gz</span><br><span class="line">24:For more information, please refer to &lt;http://unlicense.org/&gt;</span><br></pre></td></tr></table></figure><blockquote><p>Ripgrep 目前不会搜索存档格式，因此会跳过 *.tar.gz 文件。</p></blockquote><h4 id="自动补全功能">自动补全功能</h4><p>RipGrep 提供的二进制包中默认提供了 SHELL 自动补全功能，只需根据不同 SHELL 放到对应目录即可使用了。</p><ul><li>Bash</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mv rg.bash <span class="variable">$XDG_CONFIG_HOME</span>/bash_completion/</span><br><span class="line">或者</span><br><span class="line">$ mv rg.bash /etc/bash_completion.d/</span><br></pre></td></tr></table></figure><ul><li>ZSH</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mv _rg <span class="variable">$fpath</span>/</span><br></pre></td></tr></table></figure><ul><li>Fish</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mv rg.fish <span class="variable">$HOME</span>/.config/fish/completions/</span><br></pre></td></tr></table></figure><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/ROI5tMv" target="_blank" rel="noopener">http://t.cn/ROI5tMv</a><br><a href="http://t.cn/Rs3caWW" target="_blank" rel="noopener">http://t.cn/Rs3caWW</a><br><a href="http://t.cn/Rs3F6Ty" target="_blank" rel="noopener">http://t.cn/Rs3F6Ty</a></p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ripgrep 是命令行下一个基于行的搜索工具，RipGrep 使用 Rust 开发，可以在多平台下运行，支持 Mac、Linux 和 Windows 等平台。RipGrep 与 The Silver Searcher、Ack 和 GNU Grep 的功能类似。&lt;/p&gt;
&lt;p&gt;RipGrep 官方号称比其它类似工具在搜索速度上快上 N 倍，VSCode 也从 &lt;a href=&quot;https://code.visualstudio.com/updates/v1_11#_text-search-improvements&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;1.11 版本&lt;/a&gt;开始默认将 RipGrep 做为其搜索工具，由此其功能强大可见一斑。&lt;/p&gt;
&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/BurntSushi/ripgrep&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/BurntSushi/ripgrep&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ripgrep 支持的一些特性&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自动递归搜索 （grep 需要 -R）。&lt;/li&gt;
&lt;li&gt;自动忽略 .gitignore 中的文件以及二进制文件和隐藏文件。&lt;/li&gt;
&lt;li&gt;可以搜索指定文件类型，如：&lt;code&gt;rg -tpy foo&lt;/code&gt; 则限定只搜索 Python 文件，&lt;code&gt;rg -Tjs foo&lt;/code&gt; 则排除掉 JS 文件。&lt;/li&gt;
&lt;li&gt;支持大部分 Grep 的 特性，例如：显示搜索结果的上下文、支持多个模式搜索、高亮匹配的搜索结果以及支持 Unicode 等。&lt;/li&gt;
&lt;li&gt;支持各种文本编码格式，如：UTF-8、UTF-16、latin-1、GBK、EUC-JP、Shift_JIS 等。&lt;/li&gt;
&lt;li&gt;支持搜索常见格式的压缩文件，如：gzip、xz、lzma、bzip2、lz4 等。&lt;/li&gt;
&lt;li&gt;自动高亮匹配的结果。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
  </entry>
  
  <entry>
    <title>手把手教你打造高效的 Kubernetes 命令行终端</title>
    <link href="https://www.hi-linux.com/posts/44953.html"/>
    <id>https://www.hi-linux.com/posts/44953.html</id>
    <published>2018-09-13T01:00:00.000Z</published>
    <updated>2018-09-13T02:34:56.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Kubernetes 作为云原生时代的操作系统，熟悉和使用它是每名用户的必备技能。本文将介绍一些提高操作 Kubernetes 效率的技巧以及如何打造一个高效的 Kubernetes 命令行终端的方法。</p><h3 id="kubectl-自动补全">Kubectl 自动补全</h3><p>Kubectl 这个命令行工具非常重要，与之相关的命令也很多。我们也记不住那么多的命令，而且也会经常写错，所以命令行自动补全是很有必要的。Kubectl 工具本身就支持自动补全，只需简单设置一下即可。</p><ul><li>Bash 用户</li></ul><p>大多数用户的 Shell 使用的是 Bash，Linux 系统可以通过下面的命令来设置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"source &lt;(kubectl completion bash)"</span> &gt;&gt; ~/.bashrc</span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><p>如果发现不能自动补全，可以尝试安装 <code>bash-completion</code> 然后刷新即可！</p><ul><li>ZSH 用户</li></ul><p>如果你使用的 Shell 是 ZSH，可以通过下面的命令来设置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"source &lt;(kubectl completion zsh)"</span> &gt;&gt; ~/.zshrc</span><br><span class="line">$ <span class="built_in">source</span> ~/.zshrc</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="交互式-kubernetes-客户端">交互式 Kubernetes 客户端</h3><h4 id="kube-prompt">Kube-prompt</h4><p>Kube-prompt 可以让你在命令行下接受与 Kubectl 相同的命令，并且不需要提供 Kubectl前缀。Kube-prompt 还提了交互式会话下的命令提示、自动补全等功能。</p><p>项目地址：<a href="https://github.com/c-bata/kube-prompt" target="_blank" rel="noopener">https://github.com/c-bata/kube-prompt</a></p><p><strong>安装 Kube-prompt</strong></p><p>Kube-prompt 使用 Go 语言开发，天生良好的跨平台性。安装起来非常简单，只需下载各平台对应的二进制版本就可以开箱即用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Linux</span><br><span class="line">$ wget https:&#x2F;&#x2F;github.com&#x2F;c-bata&#x2F;kube-prompt&#x2F;releases&#x2F;download&#x2F;v1.0.3&#x2F;kube-prompt_v1.0.3_linux_amd64.zip</span><br><span class="line">$ unzip kube-prompt_v1.0.3_linux_amd64.zip</span><br><span class="line"></span><br><span class="line"># macOS (darwin)</span><br><span class="line">$ wget https:&#x2F;&#x2F;github.com&#x2F;c-bata&#x2F;kube-prompt&#x2F;releases&#x2F;download&#x2F;v1.0.3&#x2F;kube-prompt_v1.0.3_darwin_amd64.zip</span><br><span class="line">$ unzip kube-prompt_v1.0.3_darwin_amd64.zip</span><br><span class="line"></span><br><span class="line"># 给 kube-prompt 加上执行权限并移动常用的可搜索路径。</span><br><span class="line">$ chmod +x kube-prompt</span><br><span class="line">$ sudo mv .&#x2F;kube-prompt &#x2F;usr&#x2F;local&#x2F;bin&#x2F;kube-prompt</span><br></pre></td></tr></table></figure><p><strong>Kube-prompt 使用效果图</strong></p><p><img src="https://www.hi-linux.com/img/linux/kube-prompt.gif" alt=""></p><h4 id="kube-shell">Kube-shell</h4><p>Kube-shell 可以为 Kubectl 提供自动的命令提示和补全，Kube-shell 与 Kube-prompt 的使用方法类似。</p><p>项目地址：<a href="https://github.com/cloudnativelabs/kube-shell" target="_blank" rel="noopener">https://github.com/cloudnativelabs/kube-shell</a></p><p><strong>Kube-shell 特性</strong></p><ul><li>命令提示，给出命令的使用说明。</li><li>自动补全，列出可选命令并可以通过 TAB 键自动补全，支持模糊搜索。</li><li>支持语法高亮。</li><li>使用 TAB 键可以列出可选的对象。</li><li>支持 VIM 模式。</li></ul><p><strong>安装 Kube-shell</strong></p><p>Kube-shell 安装非常的简单，使用 PIP 就可以一键安装了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install kube-shell</span><br></pre></td></tr></table></figure><blockquote><p>注：kube-shell 必须用 root 用户运行。如果是普通用户运行会报 <code>/bin/sh: 1: kubectl: not found</code> 错误。即使使用了 <code>sudo  kube-shell</code> 同样也是不行的。</p></blockquote><p><strong>Kube-shell 使用效果图</strong></p><p><img src="https://www.hi-linux.com/img/linux/kube-shell.gif" alt=""></p><blockquote><p>注：Kube-prompt 和 Kube-shell 我都使用过，更推荐 Kube-prompt。Go 的原生性更好一些，并且 Kube-prompt 也不需要 root 权限。</p></blockquote><h3 id="kubectl-aliases">Kubectl Aliases</h3><p>Kubectl Aliases 是一个通过编程方式生成的 Kubectl 别名脚本。如果你需要频繁地使用 Kubectl 和 Kubernetes API 进行交互，使用别名将会为你节省大量的时间。</p><p>项目地址: <a href="https://github.com/ahmetb/kubectl-aliases" target="_blank" rel="noopener">https://github.com/ahmetb/kubectl-aliases</a></p><p><strong>安装 Kubectl Aliases</strong></p><p>Kubectl Aliases 就只是一个 SHELL 脚本，你只需直接下载 <code>.kubectl_aliases</code> 文件并将其保存在 <code>$HOME</code> 目录中，然后在 <code>SHELL</code> 配置文件中调用即可。</p><ul><li>下载脚本</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> <span class="variable">$HOME</span></span><br><span class="line">$ wget https://raw.githubusercontent.com/ahmetb/kubectl-alias/master/.kubectl_aliases</span><br></pre></td></tr></table></figure><ul><li>配置 SHELL</li></ul><p>Bash 用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/.bashrc</span><br><span class="line"></span><br><span class="line">[ -f ~/.kubectl_aliases ] &amp;&amp; <span class="built_in">source</span> ~/.kubectl_aliases</span><br></pre></td></tr></table></figure><p>ZSH 用户</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/.zshrc</span><br><span class="line"></span><br><span class="line">[ -f ~/.kubectl_aliases ] &amp;&amp; <span class="built_in">source</span> ~/.kubectl_aliases</span><br></pre></td></tr></table></figure><p>如果你想在运行之前打印完整的 Kubectl 命令，可以加上以下行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> <span class="function"><span class="title">kubectl</span></span>() &#123; <span class="built_in">echo</span> <span class="string">"+ kubectl <span class="variable">$@</span>"</span>; <span class="built_in">command</span> kubectl <span class="variable">$@</span>; &#125;</span><br></pre></td></tr></table></figure><p><strong>Kubectl 别名生成规则</strong></p><p><img src="https://www.hi-linux.com/img/linux/kubectl-alias.jpeg" alt=""></p><p><strong>Kubectl 别名使用示例</strong></p><ul><li>简单别名示例</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kd → kubectl describe</span><br></pre></td></tr></table></figure><ul><li>高级别名示例</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kgdepallw → kubectl get deployment –all-namespaces –watch</span><br></pre></td></tr></table></figure><h3 id="kubeval">Kubeval</h3><p>如果你手动写 Kubernetes manifest 文件，检查 manifest 文件的语法是否有误是很困难的，特别是当你有多个不同版本的 Kubernetes 集群时，确认配置文件语法是否正确更是难上加难。</p><p>Kubeval 是一个用于校验 Kubernetes YAML 或 JSON 配置文件的工具，支持多个 Kubernetes 版本，可以帮助我们解决不少的麻烦。</p><p>项目地址：<a href="https://github.com/garethr/kubeval" target="_blank" rel="noopener">https://github.com/garethr/kubeval</a></p><p><strong>Kubeval 安装</strong></p><p>Kubeval 同样是一款使用 Go 语言开发，天生良好的跨平台性。安装起来非常简单，只需下载各平台对应的二进制版本就可以开箱即用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Linux</span><br><span class="line">$ wget https:&#x2F;&#x2F;github.com&#x2F;garethr&#x2F;kubeval&#x2F;releases&#x2F;download&#x2F;0.7.1&#x2F;kubeval-linux-amd64.tar.gz</span><br><span class="line">$ tar xf kubeval-linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line"># macOS (darwin)</span><br><span class="line">$ wget https:&#x2F;&#x2F;github.com&#x2F;garethr&#x2F;kubeval&#x2F;releases&#x2F;download&#x2F;0.7.1&#x2F;kubeval-darwin-amd64.tar.gz</span><br><span class="line">$ tar xf kubeval-darwin-amd64.tar.gz</span><br><span class="line"></span><br><span class="line"># 给 kubeval 加上执行权限并移动常用的可搜索路径。</span><br><span class="line">$ chmod +x kubeval</span><br><span class="line">$ sudo mv kubeval &#x2F;usr&#x2F;local&#x2F;bin</span><br></pre></td></tr></table></figure><p><strong>Kubeval 使用示例</strong></p><ul><li>Kubernetes manifest 文件正常的情况</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubeval nginx-deployment.yaml</span><br><span class="line">The document nginx-deployment.yaml contains a valid Deployment</span><br></pre></td></tr></table></figure><ul><li>Kubernetes manifest 文件不正常的情况</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubeval nginx.yaml</span><br><span class="line"></span><br><span class="line">The document nginx.yaml contains an invalid Deployment</span><br><span class="line">---&gt; spec.replicas: Invalid type. Expected: integer, given: string</span><br></pre></td></tr></table></figure><h3 id="其它一些实用工具">其它一些实用工具</h3><p>下面这几个工具也挺不错的，使用起来都很简单。就不展开讲了，如果有兴趣可以去看下官方文档具体的使用方法。</p><ul><li>Kube-ps1</li></ul><p>该工具主要作用为命令行终端增加一个提示符。</p><p>项目地址：<a href="https://github.com/jonmosco/kube-ps1" target="_blank" rel="noopener">https://github.com/jonmosco/kube-ps1</a></p><p>Kube-ps1 使用效果图</p><p><img src="https://www.hi-linux.com/img/linux/kube-ps1.gif" alt=""></p><ul><li>Kubectx</li></ul><p>该工具主要作用是快速在多个 Kubernetes 集群中切换。</p><p>项目地址：<a href="https://github.com/ahmetb/kubectx" target="_blank" rel="noopener">https://github.com/ahmetb/kubectx</a></p><p>Kubectx 使用效果图</p><p><img src="https://www.hi-linux.com/img/linux/kubectx-demo.gif" alt=""></p><ul><li>Kubens</li></ul><p>该工具可以帮助您快速的在 Kubernetes 的多个命名空间之间切换。</p><p>项目地址：<a href="https://github.com/ahmetb/kubectx" target="_blank" rel="noopener">https://github.com/ahmetb/kubectx</a></p><p>Kubens 使用效果图</p><p><img src="https://www.hi-linux.com/img/linux/kubens-demo.gif" alt=""></p><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RD6vxGf" target="_blank" rel="noopener">http://t.cn/RD6vxGf</a><br><a href="http://t.cn/RD6vbc1" target="_blank" rel="noopener">http://t.cn/RD6vbc1</a></p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 作为云原生时代的操作系统，熟悉和使用它是每名用户的必备技能。本文将介绍一些提高操作 Kubernetes 效率的技巧以及如何打造一个高效的 Kubernetes 命令行终端的方法。&lt;/p&gt;
&lt;h3 id=&quot;Kubectl-自动补全&quot;&gt;Kubectl 自动补全&lt;/h3&gt;
&lt;p&gt;Kubectl 这个命令行工具非常重要，与之相关的命令也很多。我们也记不住那么多的命令，而且也会经常写错，所以命令行自动补全是很有必要的。Kubectl 工具本身就支持自动补全，只需简单设置一下即可。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Bash 用户&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;大多数用户的 Shell 使用的是 Bash，Linux 系统可以通过下面的命令来设置：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;source &amp;lt;(kubectl completion bash)&quot;&lt;/span&gt; &amp;gt;&amp;gt; ~/.bashrc&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;source&lt;/span&gt; ~/.bashrc&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果发现不能自动补全，可以尝试安装 &lt;code&gt;bash-completion&lt;/code&gt; 然后刷新即可！&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ZSH 用户&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果你使用的 Shell 是 ZSH，可以通过下面的命令来设置：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;string&quot;&gt;&quot;source &amp;lt;(kubectl completion zsh)&quot;&lt;/span&gt; &amp;gt;&amp;gt; ~/.zshrc&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;source&lt;/span&gt; ~/.zshrc&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>使用 TC 和 Netem 模拟网络异常</title>
    <link href="https://www.hi-linux.com/posts/35699.html"/>
    <id>https://www.hi-linux.com/posts/35699.html</id>
    <published>2018-09-10T01:00:00.000Z</published>
    <updated>2018-09-10T04:00:22.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>在某些情况下，我们需要模拟网络很差的状态来测试软件能够正常工作，比如网络延迟、丢包、乱序、重复等。Linux 系统下强大的流量控制工具 TC 能很轻松地完成这个需求，TC 命令行工具是 <code>IProute2</code> 软件包中的软件，可以根据系统版本自行安装。</p><p>这篇文章介绍的功能主要是通过 <code>Netem</code> 这个内核模块来实现的。<code>Netem</code> 是 <code>Network Emulator</code> 的缩写，关于更多功能以及参数的详细解释可以参阅 <code>TC-Netem</code> 的 Man Page。</p><blockquote><p>Netem 与 TC 简要说明</p><p>Netem 是 Linux 2.6 及以上内核版本提供的一个网络模拟功能模块。该功能模块可以用来在性能良好的局域网中，模拟出复杂的互联网传输性能。例如:低带宽、传输延迟、丢包等等情况。使用 Linux 2.6 (或以上) 版本内核的很多 Linux 发行版都默认开启了该内核模块，比如：Fedora、Ubuntu、Redhat、OpenSuse、CentOS、Debian 等等。</p><p>TC 是 Linux 系统中的一个用户态工具，全名为 Traffic Control (流量控制)。TC 可以用来控制 Netem 模块的工作模式，也就是说如果想使用 Netem 需要至少两个条件，一是内核中的 Netem 模块被启用，另一个是要有对应的用户态工具 TC 。</p></blockquote><p>TC 能做的事情很多，除了本文介绍的还有带宽控制、优先级控制等等。这些功能是通过类似 Netem 的内核模块实现的。</p><a id="more"></a><h3 id="网络状况模拟">网络状况模拟</h3><p>网络状况欠佳从用户角度来说就是下载东西慢（网页一直加载、视频卡顿、图片加载很久等），从网络报文角度来看却有很多情况：比如：延迟（某个机器发送报文很慢）、丢包（发送的报文在网络中丢失需要一直重传）、乱序（报文顺序错乱，需要大量计算时间来重新排序）、重复（报文有大量重复，导致网络拥堵）、错误（接收到的报文有误只能丢弃重传）等。</p><p>对于这些情况，都可以用 Netem 来模拟。需要注意的是，Netem 是直接作用于指定网卡上的，也就是说所有从该网卡发送出去的包都会收到配置参数的影响，所以最好搭建临时的虚拟机进行测试。</p><p>在下面的例子中 <code>add</code> 表示为指定网卡添加 Netem 配置，<code>change</code> 表示修改已经存在的 Netem 配置到新的值，<code>replace</code> 表示替换已经存在的 Netem 配置的值。如果要删除网卡上的 Netem 配置可以使用 <code>del</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc del dev enp0s5 root</span><br></pre></td></tr></table></figure><h4 id="1-模拟延迟传输">1. 模拟延迟传输</h4><p>最简单的例子是所有的报文延迟 100ms 发送：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc add dev enp0s5 root netem delay 100ms</span><br></pre></td></tr></table></figure><p>如果你想在一个局域网里模拟远距离传输的延迟可以用这个方法，比如实际用户访问网站延迟为 101 ms，而你测试环境网络交互只需要 1ms，那么只要添加 100ms 额外延迟就行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc replace dev enp0s5 root netem delay 100ms</span><br><span class="line">$ ping dev-node-02</span><br><span class="line">PING dev-node-02 (192.168.100.212) 56(84) bytes of data.</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;102 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;2 ttl&#x3D;64 time&#x3D;100 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;3 ttl&#x3D;64 time&#x3D;100 ms</span><br><span class="line">^C</span><br><span class="line">--- dev-node-02 ping statistics ---</span><br><span class="line">4 packets transmitted, 4 received, 0% packet loss, time 3000ms</span><br><span class="line">rtt min&#x2F;avg&#x2F;max&#x2F;mdev &#x3D; 100.293&#x2F;101.053&#x2F;102.795&#x2F;1.061 ms</span><br></pre></td></tr></table></figure><p>如果在网络中看到非常稳定的时延，很可能是某个地方加了定时器，因为网络线路很复杂，传输过程一定会有变化。因此实际情况网络延迟一定会有变化的，<code>Netem</code> 也考虑到这一点，提供了额外的参数来控制延迟的时间分布。完整的参数列表为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DELAY :&#x3D; delay TIME [ JITTER [ CORRELATION ]]]</span><br><span class="line">    [ distribution &#123; uniform | normal | pareto |  paretonormal &#125; ]</span><br></pre></td></tr></table></figure><p>除了延迟时间 <code>TIME</code> 之外，还有三个可选参数：</p><ul><li><code>JITTER</code>：抖动，增加一个随机时间长度，让延迟时间出现在某个范围。</li><li><code>CORRELATION</code>：相关，下一个报文延迟时间和上一个报文的相关系数。</li><li><code>distribution</code>：分布，延迟的分布模式。可以选择的值有 <code>uniform</code>、<code>normal</code>、<code>pareto</code> 和 <code>paretonormal</code>。</li></ul><p>先说说 <code>JITTER</code>，如果设置为 <code>20ms</code>，那么报文延迟的时间在 100ms  ± 20ms 之间，具体值随机选择：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc replace dev enp0s5 root netem delay 100ms 20ms</span><br><span class="line">$ ping dev-node-02</span><br><span class="line">PING dev-node-02 (192.168.100.212) 56(84) bytes of data.</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;108 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;2 ttl&#x3D;64 time&#x3D;107 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;3 ttl&#x3D;64 time&#x3D;92 ms</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p><code>CORRELATION</code> 指相关性，因为网络状况是平滑变化的，短时间里相邻报文的延迟应该是近似的而不是完全随机的。这个值是个百分比，如果为 <code>100%</code>，就退化到固定延迟的情况；如果是 <code>0%</code> 则退化到随机延迟的情况。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc replace dev enp0s5 root netem delay 100ms 20ms 50%</span><br><span class="line">$ ping dev-node-02</span><br><span class="line">PING dev-node-02 (192.168.100.212) 56(84) bytes of data.</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;104 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;2 ttl&#x3D;64 time&#x3D;109 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;5 ttl&#x3D;64 time&#x3D;101 ms</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>报文的分布和很多现实事件一样都满足某种统计规律，比如最常用的正态分布。因此为了更逼近现实情况，可以使用 <code>distribution</code> 参数来限制它的延迟分布模型。比如让报文延迟时间满足正态分布：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc replace dev enp0s5 root netem delay 100ms 20ms distribution normal</span><br><span class="line">$ ping dev-node-02</span><br><span class="line">PING dev-node-02 (192.168.100.212) 56(84) bytes of data.</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;82.0 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;2 ttl&#x3D;64 time&#x3D;82.3 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;3 ttl&#x3D;64 time&#x3D;98.1 ms</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>这样的话，大部分的延迟会在平均值的一定范围内，而很少接近出现最大值和最小值的延迟。</p><p>其他分布方法包括：<a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)" target="_blank" rel="noopener">uniform</a>、<a href="https://en.wikipedia.org/wiki/Pareto_distribution" target="_blank" rel="noopener">pareto</a> 和 <code>paretonormal</code>，这些分布方法感兴趣的读者可以自行了解。对于大多数情况，随机在某个时间范围里延迟就能满足需求的。</p><h4 id="2-模拟丢包率">2. 模拟丢包率</h4><p>另一个常见的网络异常是因为丢包，丢包会导致重传，从而增加网络链路的流量和延迟。Netem 的 <code>loss</code> 参数可以模拟丢包率，比如发送的报文有 50% 的丢包率（为了容易用 ping 看出来，所以这个数字我选的很大，实际情况丢包率可能比这个小很多，比如 <code>0.5%</code>）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc change dev enp0s5 root netem loss 50%</span><br><span class="line">$ ping dev-node-02</span><br><span class="line">PING dev-node-02 (192.168.100.212) 56(84) bytes of data.</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;1 ttl&#x3D;64 time&#x3D;0.290 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;4 ttl&#x3D;64 time&#x3D;0.308 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;5 ttl&#x3D;64 time&#x3D;0.221 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;8 ttl&#x3D;64 time&#x3D;0.371 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;9 ttl&#x3D;64 time&#x3D;0.315 ms</span><br></pre></td></tr></table></figure><p>可以从 <code>icmp_seq</code> 序号看出来大约有一半的报文丢掉了，和延迟类似丢包率也可以增加一个相关系数，表示后一个报文丢包概率和它前一个报文的相关性。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc change dev enp0s5 root netem loss 0.3% 25%</span><br></pre></td></tr></table></figure><p>这个命令表示，丢包率是 0.3%，并且当前报文丢弃的可能性和前一个报文有 25% 相关。默认的丢包模型为随机，loss 也支持 <code>state</code>（4-state Markov 模型） 和 <code>gemodel</code>（Gilbert-Elliot 丢包模型） 两种模型的丢包，因为两者都相对复杂，这里就不再详细介绍了。</p><p>需要注意的是，丢包信息会发送到上层协议。如果是 TCP 协议，那么 TCP 会进行重传，所以对应用来说看不到丢包。这时候要模拟丢包，需要把 loss 配置到网桥或者路由设备上。</p><h4 id="3-模拟包重复">3. 模拟包重复</h4><p>报文重复和丢包的参数类似，就是重复率和相关性两个参数，比如随机产生 50% 重复的包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc change dev enp0s5 root netem duplicate 50%</span><br><span class="line">$ ping dev-node-02</span><br><span class="line">PING dev-node-02 (192.168.100.212) 56(84) bytes of data.</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;2 ttl&#x3D;64 time&#x3D;0.284 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;3 ttl&#x3D;64 time&#x3D;0.420 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;3 ttl&#x3D;64 time&#x3D;0.447 ms (DUP!)</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;4 ttl&#x3D;64 time&#x3D;0.437 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;4 ttl&#x3D;64 time&#x3D;0.515 ms (DUP!)</span><br><span class="line">......</span><br></pre></td></tr></table></figure><h4 id="4-模拟包损坏">4. 模拟包损坏</h4><p>报文损坏和报文重复的参数也类似，比如随机产生 2% 损坏的报文（在报文的随机位置造成一个比特的错误）。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc change dev enp0s5 root netem corrupt 2%</span><br><span class="line">$ ping dev-node-02</span><br><span class="line">......</span><br><span class="line">PING dev-node-02 (192.168.100.212) 56(84) bytes of data.</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;3 ttl&#x3D;64 time&#x3D;0.362 ms</span><br><span class="line">Warning: time of day goes back (-4611686018427387574us), taking countermeasures.</span><br><span class="line">Warning: time of day goes back (-4611686018427387454us), taking countermeasures.</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;4 ttl&#x3D;64 time&#x3D;0.000 ms</span><br><span class="line">wrong data byte #53 should be 0x35 but was 0xb5</span><br><span class="line">#1610 11 12 13 14 15 16 17 18 19 1a 1b 1c 1d 1e 1f 20 21 22 23 24 25 26 27 28 29 2a 2b 2c 2d 2e 2f</span><br><span class="line">#4830 31 32 33 34 b5 36 37</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;5 ttl&#x3D;64 time&#x3D;0.476 ms</span><br><span class="line">......</span><br></pre></td></tr></table></figure><h4 id="5-模拟包乱序">5. 模拟包乱序</h4><p>网络传输并不能保证顺序，传输层 TCP 会对报文进行重组保证顺序，所以报文乱序对应用的影响比上面的几种问题要小。</p><p>报文乱序和前面的参数不太一样，因为上面的报文问题都是独立的。针对单个报文做操作就行，而乱序则牵涉到多个报文的重组。模拟报乱序一定会用到延迟（因为模拟乱序的本质就是把一些包延迟发送），Netem 有两种方法可以做。</p><p>第一种是固定的每隔一定数量的报文就乱序一次。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 每 5 个报文（第 5、10、15…报文）会正常发送，其他的报文延迟 100ms。</span><br><span class="line">$ tc qdisc change dev enp0s5 root netem reorder 50% gap 3 delay 100ms</span><br><span class="line">$ ping -i 0.05 dev-node-02</span><br><span class="line">PING dev-node-02 (192.168.100.212) 56(84) bytes of data.</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;9 ttl&#x3D;64 time&#x3D;2.55 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;8 ttl&#x3D;64 time&#x3D;100 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;10 ttl&#x3D;64 time&#x3D;100 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;11 ttl&#x3D;64 time&#x3D;100 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;13 ttl&#x3D;64 time&#x3D;0.245 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;12 ttl&#x3D;64 time&#x3D;102 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;14 ttl&#x3D;64 time&#x3D;1.00 ms</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>要想看到 ping 报文的乱序，我们要保证发送报文的间隔小于报文的延迟时间 <code>100ms</code>，这里用 <code>-i 0.05</code> 把发送间隔设置为 <code>50ms</code>。</p><p>第二种方法的乱序是相对随机的，使用概率来选择乱序的报文。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc change dev enp0s5 root netem reorder 50% 15% delay 300ms</span><br><span class="line">$ ping -i 0.05 dev-node-02</span><br><span class="line">PING dev-node-02 (192.168.100.212) 56(84) bytes of data.</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;4 ttl&#x3D;64 time&#x3D;0.423 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;7 ttl&#x3D;64 time&#x3D;0.250 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;2 ttl&#x3D;64 time&#x3D;301 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;3 ttl&#x3D;64 time&#x3D;301 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;9 ttl&#x3D;64 time&#x3D;0.238 ms</span><br><span class="line">64 bytes from dev-node-02 (192.168.100.212): icmp_seq&#x3D;5 ttl&#x3D;64 time&#x3D;301 ms</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>50% 的报文会正常发送，其他报文（1-50%）延迟 300ms 发送，这里选择的延迟很大是为了能够明显看出来乱序的结果。</p><h4 id="6-其它技巧">6. 其它技巧</h4><ul><li>查看已经配置的网络条件</li></ul><p>该命令将查看并显示 enp0s5 网卡的相关传输配置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tc qdisc show dev enp0s5</span><br></pre></td></tr></table></figure><h3 id="推荐两个工具">推荐两个工具</h3><p>Netem 在 TC 中算是比较简单的模块，如果要实现流量控制或者精细化的过滤需要更复杂的配置。这里推荐两个小工具，它们共同的特点是用法简单，能满足特定的需求，而不用自己去倒腾 TC 的命令。</p><h4 id="1-wondershaper">1. Wondershaper</h4><p>项目地址：<a href="https://github.com/magnific0/wondershaper" target="_blank" rel="noopener">https://github.com/magnific0/wondershaper</a></p><p>Netem 只能模拟网络状况，不能控制带宽，<a href="https://www.hecticgeek.com/2012/02/simple-traffic-shaping-ubuntu-linux/" target="_blank" rel="noopener">Wondershaper</a> 则能完美解决这个问题。Wondershaper 实际上是一个 SHELL 脚本，它使用 TC 来进行流量速率调整，使用 QoS 来处理特定的网络接口。外发流量通过放在不同优先级的队列中，来达到限制传出流量速率的目的；而传入流量通过丢包的方式来达到速率限制的目的。</p><h5 id="安装-wondershaper">安装 Wondershaper</h5><ul><li>在 Ubuntu / Debian 下安装 Wondershaper</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install wondershaper</span><br></pre></td></tr></table></figure><ul><li>在 Fdora / CentOS / RHEL 中安装 Wondershaper</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 需启用 EPEL 仓库</span><br><span class="line">$ sudo yum install wondershaper</span><br></pre></td></tr></table></figure><h5 id="使用-wondershaper">使用 Wondershaper</h5><p>Wondershaper 的使用非常简单，只有三个参数：网卡名、下行限速、上行限速。比如要设置网卡下载速度为 200kb/s，上传速度为 <code>150kb/s</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo wondershaper enp0s5 200 150</span><br></pre></td></tr></table></figure><p>如果你要将速率限制消除，可以通过运行下面的命令来达到目的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo wondershaper clear enp0s5</span><br></pre></td></tr></table></figure><h4 id="2-comcast">2. Comcast</h4><p>项目地址：<a href="https://github.com/tylertreat/comcast" target="_blank" rel="noopener">https://github.com/tylertreat/comcast</a></p><p><a href="https://github.com/tylertreat/comcast" target="_blank" rel="noopener">Comcast</a> 是一个跨平台的网络模拟工具，旨在其他平台（OSX、Windows、BSD）也能提供类似网络模拟的功能。</p><p>它的使用也相对简单：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ comcast --device&#x3D;enp0s5 --latency&#x3D;250 \</span><br><span class="line">    --target-bw&#x3D;1000 \</span><br><span class="line">    --default-bw&#x3D;1000000 \</span><br><span class="line">    --packet-loss&#x3D;10% \</span><br><span class="line">    --target-addr&#x3D;8.8.8.8,10.0.0.0&#x2F;24 \</span><br><span class="line">    --target-proto&#x3D;tcp,udp,icmp \</span><br><span class="line">    --target-port&#x3D;80,22,1000:2000</span><br></pre></td></tr></table></figure><ul><li><code>--device</code> 说明要控制的网卡为 <code>enp0s5</code>。</li><li><code>--latency</code> 指定 250ms 的延迟。</li><li><code>--target-bw</code>指定目标带宽。</li><li><code>--default-bw</code> 指定默认带宽。</li><li><code>--packet-loss</code> 指定丢包率。</li><li><code>--target-addr</code>、<code>--target-proto</code>、<code>--target-port</code> 参数指定在满足这些条件的报文上实施上面的配置。</li></ul><h3 id="总结">总结</h3><p>可以看出，TC 的 Netem 模块主要用来模拟各种网络的异常状况，本身并没有提供宽带限制的功能，而且一旦在网卡上配置了 Netem，该网卡上所有的报文都会受影响，如果想精细地控制部分报文，需要用到 TC 的 <a href="http://lartc.org/howto/lartc.qdisc.filters.html" target="_blank" rel="noopener">filter</a> 功能。</p><h3 id="参考资料">参考资料</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RsUAV3y" target="_blank" rel="noopener">http://t.cn/RsUAV3y</a><br><a href="http://t.cn/RsUwmQX" target="_blank" rel="noopener">http://t.cn/RsUwmQX</a><br><a href="http://t.cn/RsUVqk0" target="_blank" rel="noopener">http://t.cn/RsUVqk0</a></p><blockquote><p>本文在 「使用 tc netem 模拟网络异常」的基础上整理和修改，原文地址：<a href="http://t.cn/RsUwmQX%E3%80%82" target="_blank" rel="noopener">http://t.cn/RsUwmQX。</a></p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在某些情况下，我们需要模拟网络很差的状态来测试软件能够正常工作，比如网络延迟、丢包、乱序、重复等。Linux 系统下强大的流量控制工具 TC 能很轻松地完成这个需求，TC 命令行工具是 &lt;code&gt;IProute2&lt;/code&gt; 软件包中的软件，可以根据系统版本自行安装。&lt;/p&gt;
&lt;p&gt;这篇文章介绍的功能主要是通过 &lt;code&gt;Netem&lt;/code&gt; 这个内核模块来实现的。&lt;code&gt;Netem&lt;/code&gt; 是 &lt;code&gt;Network Emulator&lt;/code&gt; 的缩写，关于更多功能以及参数的详细解释可以参阅 &lt;code&gt;TC-Netem&lt;/code&gt; 的 Man Page。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Netem 与 TC 简要说明&lt;/p&gt;
&lt;p&gt;Netem 是 Linux 2.6 及以上内核版本提供的一个网络模拟功能模块。该功能模块可以用来在性能良好的局域网中，模拟出复杂的互联网传输性能。例如:低带宽、传输延迟、丢包等等情况。使用 Linux 2.6 (或以上) 版本内核的很多 Linux 发行版都默认开启了该内核模块，比如：Fedora、Ubuntu、Redhat、OpenSuse、CentOS、Debian 等等。&lt;/p&gt;
&lt;p&gt;TC 是 Linux 系统中的一个用户态工具，全名为 Traffic Control (流量控制)。TC 可以用来控制 Netem 模块的工作模式，也就是说如果想使用 Netem 需要至少两个条件，一是内核中的 Netem 模块被启用，另一个是要有对应的用户态工具 TC 。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;TC 能做的事情很多，除了本文介绍的还有带宽控制、优先级控制等等。这些功能是通过类似 Netem 的内核模块实现的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
  </entry>
  
  <entry>
    <title>关于 Kubernetes Master 高可用的一些策略</title>
    <link href="https://www.hi-linux.com/posts/2897.html"/>
    <id>https://www.hi-linux.com/posts/2897.html</id>
    <published>2018-09-05T01:00:00.000Z</published>
    <updated>2018-09-05T08:36:24.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p><a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a> 高可用也许是完成了初步的技术评估，打算将生产环境迁移进 Kubernetes 集群之前普遍面临的问题。 为了减少因为服务器当机引起的业务中断，生产环境中的业务系统往往已经做好了高可用，而当引入 Kubernetes 这一套新的集群管理系统之后，服务器不再是单一的个体，位于中央位置的 Kubernetes Master 一旦中断服务，将导致所有 Node 节点均不可控，有可能造成严重的事故。</p><p>总体来讲这是一个<a href="https://kubernetes.io/docs/admin/high-availability/" target="_blank" rel="noopener">被多次讨论</a>，但暂时<a href="https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/" target="_blank" rel="noopener">没有形成统一解决方案</a>的话题。今天主要介绍一些 Kubernetes Master 高可用的策略，供大家参考。</p><h3 id="一个小目标">一个小目标</h3><p>高可用是复杂的系统工程。出于篇幅的考虑以及能力的限制，今天我们先关注一个小目标：所有的 Kubernetes Master 服务器没有单点故障，任何一台服务器当机均不影响 Kubernetes 的正常工作。</p><p>实现这一目标带来的直接收益是我们可以在不影响业务正常运行的前提下实现所有服务器的滚动升级，有助于完成系统组件升级以及安全补丁的下发。</p><p>为了实现没有单点故障的目标，需要为以下几个组件建立高可用方案：</p><ul><li><a href="https://github.com/coreos/etcd" target="_blank" rel="noopener">etcd</a></li><li><a href="https://kubernetes.io/docs/admin/kube-apiserver/" target="_blank" rel="noopener">kube-apiserver</a></li><li><a href="https://kubernetes.io/docs/admin/kube-controller-manager/" target="_blank" rel="noopener">kube-controller-manager</a> 与 <a href="https://kubernetes.io/docs/admin/kube-scheduler/" target="_blank" rel="noopener">kube-scheduler</a></li><li><a href="https://github.com/kubernetes/dns" target="_blank" rel="noopener">kube-dns</a></li></ul><p>这些组件的关系可参考下面这张集群架构示意图。</p><p><img src="https://www.hi-linux.com/img/linux/kubernetes-architecture.png" alt=""></p><p>下面为大家逐个详细介绍各个组件的高可用策略。</p><a id="more"></a><h3 id="etcd高可用">etcd高可用</h3><p>etcd 是 Kubernetes 当中唯一带状态的服务，也是高可用的难点。Kubernetes 选用 etcd 作为它的后端数据存储仓库正是看重了其使用分布式架构，没有单点故障的特性。</p><p>虽然单节点的 etcd 也可以正常运行。但是推荐的部署方案均是采用 3 个或者 5 个节点组成 etcd 集群，供 Kubernetes 使用。</p><p>大家常使用的 <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/" target="_blank" rel="noopener">kubeadm</a> 工具默认是在一个单节点上启动 etcd 以及所有的 Master 组件。虽然使用起来非常方便，但是要用到生产环境还是要注意这个节点当机的风险。</p><p>etcd 的高可用基本有三种思路：</p><p>一是使用独立的 etcd 集群，使用 3 台或者 5 台服务器只运行 etcd，独立维护和升级。甚至可以使用 CoreOS 的 update-engine 和 locksmith 让服务器完全自主的完成升级。这个 etcd 集群将作为基石用于构建整个集群。采用这项策略的主要动机是 etcd 集群的节点增减都需要显式的通知集群，保证 etcd 集群节点稳定可以更方便的用程序完成集群滚动升级，减轻维护负担。</p><p>二是在 Kubernetes Master 上用 static pod 的形式来运行 etcd，并将多台 Kubernetes Master 上的 etcd 组成集群。 在这一模式下，各个服务器的 etcd 实例被注册进了 Kubernetes 当中，虽然无法直接使用 kubectl 来管理这部分实例，但是监控以及日志搜集组件均可正常工作。在这一模式运行下的 etcd 可管理性更强。</p><p>三是使用 CoreOS 提出的 <a href="https://github.com/kubernetes-incubator/bootkube/issues/31" target="_blank" rel="noopener">self-hosted etcd</a> 方案，将本应在底层为 Kubernetes 提供服务的 etcd 运行在 Kubernetes 之上。 实现 Kubernetes 对自身依赖组件的管理。在这一模式下的 etcd 集群可以直接使用 <a href="https://github.com/coreos/etcd-operator" target="_blank" rel="noopener">etcd-operator</a> 来自动化运维，最符合 Kubernetes 的使用习惯。</p><p>这三种思路均可以实现 etcd 高可用的目标，但是在选择过程中却要根据实际情况做出一些判断。简单来讲预算充足但保守的项目选方案一， 想一步到位并愿意承担一定风险的项目选方案三。折中一点选方案二。各个方案的优劣以及做选择过程中的取舍在这里就不详细展开了，对这块有疑问的朋友可以私下联系交流。</p><h3 id="kube-apiserver-高可用">kube-apiserver 高可用</h3><p>apiserver 本身是一个无状态服务，要实现其高可用相对要容易一些，难点在于如何将运行在多台服务器上的 apiserver 用一个统一的外部入口暴露给所有 Node 节点。</p><p>说是难点，其实对于这种无状态服务的高可用，我们在设计业务系统的高可用方案时已经有了相当多的经验积累。需要注意的是 apiserver 所使用的 SSL 证书要包含外部入口的地址，不然 Node 节点无法正常访问 apiserver。</p><p>apiserver 的高可用也有三种基本思路：</p><p>一是使用外部负载均衡器，不管是使用公有云提供的负载均衡器服务或是在私有云中使用 LVS 或者 HaProxy 自建负载均衡器都可以归到这一类。 负载均衡器是非常成熟的方案，在这里略过不做过多介绍。如何保证负载均衡器的高可用，则是选择这一方案需要考虑的新问题。</p><p>二是在网络层做负载均衡。比如在 Master 节点上用 BGP 做 ECMP，或者在 Node 节点上用 iptables 做 NAT 都可以实现。采用这一方案不需要额外的外部服务，但是对网络配置有一定的要求。</p><p>三是在 Node 节点上使用反向代理对多个 Master 做负载均衡。这一方案同样不需要依赖外部的组件，但是当 Master 节点有增减时，如何动态配置 Node 节点上的负载均衡器成为了另外一个需要解决的问题。</p><p>从目前各个集群管理工具的选择来看，这三种模式都有被使用，目前还没有明确的推荐方案产生。建议在公有云上的集群多考虑第一种模式，在私有云环境中由于维护额外的负载均衡器也是一项负担，建议考虑第二种或是第三种方案。</p><h3 id="kube-controller-manager-与-kube-scheduler-高可用">kube-controller-manager 与 kube-scheduler 高可用</h3><p>这两项服务是 Master 节点的一部分，他们的高可用相对容易，仅需要运行多份实例即可。这些实例会通过向 apiserver 中的 Endpoint 加锁的方式来进行 leader election， 当目前拿到 leader 的实例无法正常工作时，别的实例会拿到锁，变为新的 leader。</p><p>目前在多个 Master 节点上采用 static pod 模式部署这两项服务的方案比较常见，激进一点也可以采用 self-hosted 的模式，在 Kubernetes 之上用 DaemonSet 或者 Deployment 来部署。</p><h3 id="kube-dns-高可用">Kube-dns 高可用</h3><p>严格来说 kube-dns 并不算是 Master 组件的一部分，因为它是可以跑在 Node 节点上，并用 Service 向集群内部提供服务的。但在实际环境中，由于默认配置只运行了一份 kube-dns 实例，在其升级或是所在节点当机时，会出现集群内部 dns 服务不可用的情况，严重时会影响到线上服务的正常运行。</p><p>为了避免故障，请将 kube-dns 的 replicas 值设为 2 或者更多，并用 anti-affinity 将他们部署在不同的 Node 节点上。这项操作比较容易被疏忽，直到出现故障时才发现原来是 kube-dns 只运行了一份实例导致的故障。</p><h3 id="总结">总结</h3><p>上面介绍了 Kubernetes Master 各个组件高可用可以采用的策略。其中 etcd 和 kube-apiserver 的高可用是整个方案的重点。由于存在多种高可用方案，集群管理员应当根据集群所处环境以及其他限制条件选择适合的方案。</p><p>这种没有绝对的通用方案，需要集群建设者根据不同的现状在多个方案中做选择的情况在 Kubernetes 集群建设过程中频频出现， 也是整个建设过程中最有挑战的一部分。容器网络方案的选型作为 Kubernetes 建设过程中需要面对的另外一个大问题也属于这种情况，今后有机会再来分享这个话题。</p><p>在实际建设过程中，在完成了上述四个组件的高可用之后，最好采取实际关机检验的方式来验证高可用方案的可靠性，并根据检验的结果不断调整和优化整个方案。</p><p>此外将高可用方案与系统自动化升级方案结合在一起考虑，实现高可用下的系统自动升级，将大大减轻集群的日常运维负担，值得投入精力去研究。</p><p>虽然本篇主要在讲 Kubernetes Master 高可用的方案，但需要指出的是高可用也并不是必须的，为了实现高可用所付出的代价并不低， 需要有相应的收益来平衡。对于大量的小规模集群来说，业务系统并没有实现高可用，贸然去做集群的高可用收益有限。这时采用单 Master 节点的方案，做好 etcd 的数据备份，不失为理性的选择。</p><blockquote><p>来源：极术<br>原文：<a href="http://t.cn/Rs72Axn" target="_blank" rel="noopener">http://t.cn/Rs72Axn</a><br>题图：来自谷歌图片搜索<br>版权：本文版权归原作者所有</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://kubernetes.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Kubernetes&lt;/a&gt; 高可用也许是完成了初步的技术评估，打算将生产环境迁移进 Kubernetes 集群之前普遍面临的问题。 为了减少因为服务器当机引起的业务中断，生产环境中的业务系统往往已经做好了高可用，而当引入 Kubernetes 这一套新的集群管理系统之后，服务器不再是单一的个体，位于中央位置的 Kubernetes Master 一旦中断服务，将导致所有 Node 节点均不可控，有可能造成严重的事故。&lt;/p&gt;
&lt;p&gt;总体来讲这是一个&lt;a href=&quot;https://kubernetes.io/docs/admin/high-availability/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;被多次讨论&lt;/a&gt;，但暂时&lt;a href=&quot;https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;没有形成统一解决方案&lt;/a&gt;的话题。今天主要介绍一些 Kubernetes Master 高可用的策略，供大家参考。&lt;/p&gt;
&lt;h3 id=&quot;一个小目标&quot;&gt;一个小目标&lt;/h3&gt;
&lt;p&gt;高可用是复杂的系统工程。出于篇幅的考虑以及能力的限制，今天我们先关注一个小目标：所有的 Kubernetes Master 服务器没有单点故障，任何一台服务器当机均不影响 Kubernetes 的正常工作。&lt;/p&gt;
&lt;p&gt;实现这一目标带来的直接收益是我们可以在不影响业务正常运行的前提下实现所有服务器的滚动升级，有助于完成系统组件升级以及安全补丁的下发。&lt;/p&gt;
&lt;p&gt;为了实现没有单点故障的目标，需要为以下几个组件建立高可用方案：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/coreos/etcd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;etcd&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/admin/kube-apiserver/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;kube-apiserver&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/admin/kube-controller-manager/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;kube-controller-manager&lt;/a&gt; 与 &lt;a href=&quot;https://kubernetes.io/docs/admin/kube-scheduler/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;kube-scheduler&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/kubernetes/dns&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;kube-dns&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些组件的关系可参考下面这张集群架构示意图。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/kubernetes-architecture.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;下面为大家逐个详细介绍各个组件的高可用策略。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Docker 最佳实践之多阶段构建</title>
    <link href="https://www.hi-linux.com/posts/55545.html"/>
    <id>https://www.hi-linux.com/posts/55545.html</id>
    <published>2018-09-03T01:00:00.000Z</published>
    <updated>2018-09-03T02:28:12.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Docker 目前在容器市场可以说是占领了大部分市场，Docker 掀起了容器革命，同时也改变了现代化云平台的构建方式。尽管 Docker 很强大，但使用过程当中也遇到了一些问题。比如:我想要构建一个编译型语言镜像，需要先在一个 Dockerfile 中编译，然后再使用另外一个 Dockerfile 把编译好的文件放到镜像中。这样无形当中就增大了 CI/CD 的复杂度。</p><blockquote><p>Docker 多阶段构建是 17.05 以后引入的新特性，旨在解决编译和构建复杂的问题。因此要使用多阶段构建特性必须使用高于或等于 17.05 的 Docker。</p></blockquote><h3 id="多阶段构建出现之前">多阶段构建出现之前</h3><ul><li>构建镜像最具挑战性的一点是使镜像大小尽可能的小。Dockerfile 中的每条指令都为镜像添加了一个镜像层，您需要记住在移动到下一个镜像层之前清理任何不需要的组件。</li><li>为了编写一个真正高效的 Dockerfile，传统上需要使用 Shell 技巧和其它逻辑来保持镜像层尽可能小，并确保每个镜像层都具有前一层所需的组件而不是其它任何东西。</li></ul><p>很多时候我们用一个 Dockerfile 维护开发环境（包含构建应用程序所需的所有内容），用另一个 Dockerfile 维护生产环境（只包含您的应用程序以及运行它所需的内容）。实际上同时维护两个 Dockerfiles 并不是一种理想的构建模式，这种模式被称为建造者模式。</p><p>我们来看一个例子，这是一个 Dockerfile.build 和 Dockerfile 的例子，它遵循上面的模式。</p><p><strong>Dockerfile.build</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.7.3</span><br><span class="line">WORKDIR &#x2F;go&#x2F;src&#x2F;github.com&#x2F;alexellis&#x2F;href-counter&#x2F;</span><br><span class="line">COPY app.go .</span><br><span class="line">RUN go get -d -v golang.org&#x2F;x&#x2F;net&#x2F;html \</span><br><span class="line">  &amp;&amp; CGO_ENABLED&#x3D;0 GOOS&#x3D;linux go build -a -installsuffix cgo -o app .</span><br></pre></td></tr></table></figure><p>请注意，此示例使用 Bash <code>&amp;&amp;</code> 运算符人为合并两个 RUN 命令，以避免在 Image 中创建出其它层。这种方法很容易出错并且难以维护。</p><a id="more"></a><p><strong>Dockerfile</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest  </span><br><span class="line">RUN apk --no-cache add ca-certificates</span><br><span class="line">WORKDIR &#x2F;root&#x2F;</span><br><span class="line">COPY app .</span><br><span class="line">CMD [&quot;.&#x2F;app&quot;]</span><br></pre></td></tr></table></figure><p><strong><a href="http://build.sh" target="_blank" rel="noopener">build.sh</a></strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/sh</span></span><br><span class="line"><span class="built_in">echo</span> Building alexellis2/href-counter:build</span><br><span class="line">docker build --build-arg https_proxy=<span class="variable">$https_proxy</span> --build-arg http_proxy=<span class="variable">$http_proxy</span> \  </span><br><span class="line">    -t alexellis2/href-counter:build . -f Dockerfile.build</span><br><span class="line">docker container create --name extract alexellis2/href-counter:build  </span><br><span class="line">docker container cp extract:/go/src/github.com/alexellis/href-counter/app ./app  </span><br><span class="line">docker container rm -f extract</span><br><span class="line"><span class="built_in">echo</span> Building alexellis2/href-counter:latest</span><br><span class="line">docker build --no-cache -t alexellis2/href-counter:latest .</span><br><span class="line">rm ./app</span><br></pre></td></tr></table></figure><p>当您运行 <code>build.sh</code> 脚本时，它首先会构建第一个 Image 并从创建容器中复制编译好的程序到本地。然后在第二个 Image 中将构建好的程序运行起来。</p><h3 id="使用多阶段构建">使用多阶段构建</h3><p>从上面的过程中可以看到过程是非常复杂且容易出错的，多阶段构建的出现就大大简化了这种情况！</p><p>对于多阶段构建，您可以在 Dockerfile 中使用多个 FROM 语句。每个 FROM 指令可以使用不同的基础镜像，并且每个 FROM 指令都会开始一个新的构建阶段。您可以选择性地将各构建阶段中的内容从一个阶段复制到另一个阶段，从而在最终 Image 中只留下您想要的内容。</p><p>为了说明这是如何工作的，我们调整上述示例中的 Dockerfile 使用多阶段方式来构建。</p><p><strong>Dockerfile</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.7.3</span><br><span class="line">WORKDIR &#x2F;go&#x2F;src&#x2F;github.com&#x2F;alexellis&#x2F;href-counter&#x2F;</span><br><span class="line">RUN go get -d -v golang.org&#x2F;x&#x2F;net&#x2F;html  </span><br><span class="line">COPY app.go .</span><br><span class="line">RUN CGO_ENABLED&#x3D;0 GOOS&#x3D;linux go build -a -installsuffix cgo -o app .</span><br><span class="line"></span><br><span class="line">FROM alpine:latest  </span><br><span class="line">RUN apk --no-cache add ca-certificates</span><br><span class="line">WORKDIR &#x2F;root&#x2F;</span><br><span class="line">COPY --from&#x3D;0 &#x2F;go&#x2F;src&#x2F;github.com&#x2F;alexellis&#x2F;href-counter&#x2F;app .</span><br><span class="line">CMD [&quot;.&#x2F;app&quot;]</span><br></pre></td></tr></table></figure><p>可以看到您只需要单个 Dockerfile 并且不需要任何的单独构建脚本，就可以构建出上面相同的 Image。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker build -t app:latest .</span><br></pre></td></tr></table></figure><p>通过 <code>docker build</code> 构建后，最终结果是产生与之前相同大小的 Image，但复杂性显著降低。您不需要创建任何中间 Image，也不需要将任何编译结果临时提取到本地系统。</p><p>哪它是如何工作的呢？关键就在 <code>COPY --from=0</code> 这个指令上。Dockerfile 中第二个 FROM 指令以 alpine:latest 为基础镜像开始了一个新的构建阶段，并通过 <code>COPY --from=0</code> 仅将前一阶段的构建文件复制到此阶段。前一构建阶段中产生的 Go SDK 和任何中间层都会在此阶段中被舍弃，而不是保存在最终 Image 中。</p><h3 id="为多构建阶段命名">为多构建阶段命名</h3><p>默认情况下，构建阶段是未命名的。您可以通过一个整数值来引用它们，默认是从第 0 个 FROM 指令开始的。 为了方便管理，您也可以通过向 FROM 指令添加 as NAME 来命名您的各个构建阶段。下面的示例就通过命名各个构建阶段并在 COPY 指令中使用名称来访问指定的构建阶段。</p><p>这样做的好处就是即使稍后重新排序 Dockerfile 中的指令，COPY 指令一样能找到对应的构建阶段。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">FROM golang:1.7.3 as builder</span><br><span class="line">WORKDIR &#x2F;go&#x2F;src&#x2F;github.com&#x2F;alexellis&#x2F;href-counter&#x2F;</span><br><span class="line">RUN go get -d -v golang.org&#x2F;x&#x2F;net&#x2F;html  </span><br><span class="line">COPY app.go    .</span><br><span class="line">RUN CGO_ENABLED&#x3D;0 GOOS&#x3D;linux go build -a -installsuffix cgo -o app .</span><br><span class="line"></span><br><span class="line">FROM alpine:latest  </span><br><span class="line">RUN apk --no-cache add ca-certificates</span><br><span class="line">WORKDIR &#x2F;root&#x2F;</span><br><span class="line">COPY --from&#x3D;builder &#x2F;go&#x2F;src&#x2F;github.com&#x2F;alexellis&#x2F;href-counter&#x2F;app .</span><br><span class="line">CMD [&quot;.&#x2F;app&quot;]</span><br></pre></td></tr></table></figure><h3 id="停在特定的构建阶段">停在特定的构建阶段</h3><p>构建镜像时，不一定需要构建整个 Dockerfile 中每个阶段，您也可以指定需要构建的阶段。比如：您只构建 Dockerfile 中名为 builder 的阶段：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker build --target builder -t alexellis2&#x2F;href-counter:latest .</span><br></pre></td></tr></table></figure><p>此功能适合以下场景：</p><ul><li>调试特定的构建阶段。</li><li>在 Debug 阶段，启用所有程序调试模式或调试工具，而在生产阶段尽量精简。</li><li>在 Testing 阶段，您的应用程序使用测试数据，但在生产阶段则使用生产数据。</li></ul><h3 id="使用外部镜像作为构建阶段">使用外部镜像作为构建阶段</h3><p>使用多阶段构建时，您不仅可以从 Dockerfile 中创建的镜像中进行复制。您还可以使用 <code>COPY --from</code> 指令从单独的 Image 中复制，支持使用本地 Image 名称、本地或 Docker 注册中心可用的标记或标记 ID。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">COPY --from&#x3D;nginx:latest &#x2F;etc&#x2F;nginx&#x2F;nginx.conf &#x2F;nginx.conf</span><br></pre></td></tr></table></figure><blockquote><p>本文在 「Docker 最佳实践之多阶段构建 」的基础上整理和修改，原文地址：<a href="http://t.cn/RFx6ML2" target="_blank" rel="noopener">http://t.cn/RFx6ML2</a> 。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Docker 目前在容器市场可以说是占领了大部分市场，Docker 掀起了容器革命，同时也改变了现代化云平台的构建方式。尽管 Docker 很强大，但使用过程当中也遇到了一些问题。比如:我想要构建一个编译型语言镜像，需要先在一个 Dockerfile 中编译，然后再使用另外一个 Dockerfile 把编译好的文件放到镜像中。这样无形当中就增大了 CI/CD 的复杂度。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Docker 多阶段构建是 17.05 以后引入的新特性，旨在解决编译和构建复杂的问题。因此要使用多阶段构建特性必须使用高于或等于 17.05 的 Docker。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;多阶段构建出现之前&quot;&gt;多阶段构建出现之前&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;构建镜像最具挑战性的一点是使镜像大小尽可能的小。Dockerfile 中的每条指令都为镜像添加了一个镜像层，您需要记住在移动到下一个镜像层之前清理任何不需要的组件。&lt;/li&gt;
&lt;li&gt;为了编写一个真正高效的 Dockerfile，传统上需要使用 Shell 技巧和其它逻辑来保持镜像层尽可能小，并确保每个镜像层都具有前一层所需的组件而不是其它任何东西。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;很多时候我们用一个 Dockerfile 维护开发环境（包含构建应用程序所需的所有内容），用另一个 Dockerfile 维护生产环境（只包含您的应用程序以及运行它所需的内容）。实际上同时维护两个 Dockerfiles 并不是一种理想的构建模式，这种模式被称为建造者模式。&lt;/p&gt;
&lt;p&gt;我们来看一个例子，这是一个 Dockerfile.build 和 Dockerfile 的例子，它遵循上面的模式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dockerfile.build&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;FROM golang:1.7.3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;WORKDIR &amp;#x2F;go&amp;#x2F;src&amp;#x2F;github.com&amp;#x2F;alexellis&amp;#x2F;href-counter&amp;#x2F;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;COPY app.go .&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;RUN go get -d -v golang.org&amp;#x2F;x&amp;#x2F;net&amp;#x2F;html \&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;  &amp;amp;&amp;amp; CGO_ENABLED&amp;#x3D;0 GOOS&amp;#x3D;linux go build -a -installsuffix cgo -o app .&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;请注意，此示例使用 Bash &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; 运算符人为合并两个 RUN 命令，以避免在 Image 中创建出其它层。这种方法很容易出错并且难以维护。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Docker" scheme="https://www.hi-linux.com/categories/docker/"/>
    
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>初识 Istio</title>
    <link href="https://www.hi-linux.com/posts/28535.html"/>
    <id>https://www.hi-linux.com/posts/28535.html</id>
    <published>2018-08-28T01:00:00.000Z</published>
    <updated>2018-08-28T08:59:51.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>如果你比较关注新兴技术的话，那么很可能在不同的地方听说过 Istio，并且知道它和 Service Mesh 有着牵扯。这篇文章可以作为了解 Istio 的入门介绍。通过本文你可以了解什么是 Istio，Istio 为什么最近这么火，以及 Istio 能够我们带来什么好处。</p><h3 id="什么是-istio">什么是 Istio？</h3><p>官方对 Istio 的介绍浓缩成了一句话：</p><blockquote><p>An open platform to connect, secure, control and observe services.</p></blockquote><p>翻译过来，就是&quot;连接、安全加固、控制和观察服务的开放平台&quot;。开放平台就是指它本身是开源的，服务对应的是微服务，也可以粗略地理解为单个应用。 中间的四个动词就是 Istio 的主要功能，官方也各有一句话的说明。这里再阐释一下：</p><ul><li>连接（Connect）：智能控制服务之间的调用流量，能够实现灰度升级、AB 测试和红黑部署等功能。</li><li>安全加固（Secure）：自动为服务之间的调用提供认证、授权和加密。</li><li>控制（Control）：应用用户定义的 Policy，保证资源在消费者中公平分配。</li><li>观察（Observe）：查看服务运行期间的各种数据，比如日志、监控和 Tracing，了解服务的运行情况。</li></ul><p><img src="https://www.hi-linux.com/img/linux/istio1.jpg" alt=""></p><a id="more"></a><p>虽然听起来非常高级，功能非常强大，但是一股脑出现这么多名词，还都是非常虚的概念，说了跟没说一样。要想理解上面这几句话的含义，我们还是从头说起，先聊聊 Service Mesh。</p><blockquote><p>NOTE：其实 Istio 的源头是微服务，但这又是一个比较大的话题，目前可以参考网络上各种文章。如果有机会，我们再来聊聊微服务。</p></blockquote><h3 id="什么是-service-mesh">什么是 Service Mesh</h3><p>一般介绍 Service Mesh 的文章都会从网络层的又一个抽象说起，把 Service Mesh 看做建立在 TCP 层之上的微服务层。我这次换个思路，从 Service Mesh 的技术根基——网络代理来分析。</p><p>说起网络代理，如果对软件架构比较熟悉的会想到 Nginx 等反向代理软件。其实网络代理的范围比较广，可以肯定的说，有网络访问的地方就会有代理的存在。</p><p>Wikipedia 对代理的定义如下：</p><blockquote><p>In computer networks, a proxy server is a server (a computer system or an application) that acts as an intermediary for requests from clients seeking resources from other servers.</p></blockquote><p>NOTE：代理可以是嵌套的，也就是说通信双方 A、B 中间可以多层代理，而这些代理的存在有可能对 A、B 是透明的。</p><p>简单来说，网络代理可以简单类比成现实生活中的中介，本来需要通信的双方因为各种原因在中间再加上一道关卡。本来双方就能完成的通信，为何非要多此一举呢？那是因为代理可以为整个通信带来更多的功能，比如：</p><ul><li>拦截：代理可以选择性拦截传输的网络流量，比如：一些公司限制员工在上班的时候不能访问某些游戏或者电商网站，数据中心中拒绝恶意访问网关等。</li><li>统计：既然所有的流量都经过代理，那么代理也可以用来统计网络中的数据信息，比如了解哪些人在访问哪些网站，通信的应答延迟等。</li><li>缓存：如果通信双方比较远，访问比较慢，那么代理可以把最近访问的数据缓存在本地，后面的访问不用访问后端来做到加速。CDN 就是这个功能的典型场景。</li><li>分发：如果某个通信方有多个服务器后端，代理可以根据某些规则来选择如何把流量发送给多个服务器，也就是我们常说的负载均衡功能。比如著名的 Nginx 软件。</li><li>跳板：如果 A、B 双方因为某些原因不能直接访问，而代理可以和双方通信，那么通过代理，双方可以绕过原来的限制进行通信。这应该是国内网民比较熟悉的场景。</li><li>注入：既然代理可以看到流量，那么它也可以修改网络流量，可以自动在收到的流量中添加一些数据，比如有些宽带提供商的弹窗广告。</li><li>……</li></ul><p><img src="https://www.hi-linux.com/img/linux/istio2.jpg" alt=""></p><p>不是要讲 Service Mesh 吗？为什么扯了一堆代理的事情？因为 Service Mesh 可以看做是传统代理的升级版，用来解决现在微服务框架中出现的问题，可以把 Service Mesh 看做是分布式的微服务代理。</p><p>在传统模式下，代理一般是集中式的单独的服务器，所有的请求都要先通过代理，然后再流入转发到实际的后端。而在 Service Mesh 中，代理变成了分布式的，它常驻在了应用的身边（最常见的就是 Kubernetes Sidecar 模式，每一个应用的 Pod 中都运行着一个代理，负责流量相关的事情）。这样的话，应用所有的流量都被代理接管，那么这个代理就能做到上面提到的所有可能的事情，从而带来无限的想象力。</p><p><img src="https://www.hi-linux.com/img/linux/istio3.jpg" alt=""></p><p>此外，原来的代理都是基于网络流量的，一般都是工作在 IP 或者 TCP 层，很少关心具体的应用逻辑。但是 Service Mesh 中，代理会知道整个集群的所有应用信息，并且额外添加了热更新、注入服务发现、降级熔断、认证授权、超时重试、日志监控等功能，让这些通用的功能不必每个应用都自己实现，放在代理中即可。换句话说，Service Mesh 中的代理对微服务中的应用做了定制化的改进！</p><p><img src="https://www.hi-linux.com/img/linux/istio4.jpg" alt=""></p><p>就这样，借着微服务和容器化的东风，传统的代理摇身一变，成了如今炙手可热的 Service Mesh。应用微服务之后，每个单独的微服务都会有很多副本，而且可能会有多个版本，这么多微服务之间的相互调用和管理非常复杂，但是有了 Service Mesh，我们可以把这块内容统一在代理层。</p><p><img src="https://www.hi-linux.com/img/linux/istio5.gif" alt=""></p><p>有了看起来四通八达的分布式代理，我们还需要对这些代理进行统一的管理。手动更新每个代理的配置，对代理进行升级或者维护是个不可持续的事情，在前面的基础上，在加上一个控制中心，一个完整的 Service Mesh 就成了。管理员只需要根据控制中心的 API 来配置整个集群的应用流量、安全规则即可，代理会自动和控制中心打交道根据用户的期望改变自己的行为。</p><p><img src="https://www.hi-linux.com/img/linux/istio6.jpg" alt=""></p><blockquote><p>NOTE：所以你也可以理解 Service Mesh 中的代理会抢了 Nginx 的生意，这也是为了 Nginx 也要开始做 NginMesh 的原因。</p></blockquote><h3 id="再来看-istio">再来看 Istio</h3><p>了解了 Service Mesh 的概念，我们再来看 Istio ，也许就会清楚很多。首先来看 Istio 官方给出的架构图：</p><p><img src="https://www.hi-linux.com/img/linux/istio7.jpg" alt=""></p><p>可以看到，Istio 就是我们上述提到的 Service Mesh 架构的一种实现，服务之间的通信（比如这里的 Service A 访问 Service B）会通过代理（默认是 Envoy）来进行，而且中间的网络协议支持 HTTP/1.1、HTTP/2、gRPC 或者 TCP，可以说覆盖了主流的通信协议。控制中心做了进一步的细分，分成了 Pilot、Mixer、和 Citadel，它们的各自功能如下：</p><ul><li>Pilot：为 Envoy 提供了服务发现，流量管理和智能路由（AB测试、金丝雀发布等），以及错误处理（超时、重试、熔断）功能。用户通过 Pilot 的 API 管理网络相关的资源对象，Pilot 会根据用户的配置和服务的信息把网络流量管理变成 Envoy 能识别的格式分发到各个 Sidecar 代理中。</li><li>Mixer：为整个集群执行访问控制（哪些用户可以访问哪些服务）和 Policy 管理（Rate Limit，Quota 等），并且收集代理观察到的服务之间的流量统计数据。</li><li>Citadel：为服务之间提供认证和证书管理，可以让服务自动升级成 TLS 协议。</li></ul><p>代理会和控制中心通信，一方面可以获取需要的服务之间的信息，另一方面也可以汇报服务调用的 Metrics 数据。知道 Istio 的核心架构，再来看看它的功能描述就非常容易理解了。</p><ul><li>连接：控制中心可以从集群中获取所有服务的信息，并分发给代理，这样代理就能根据用户的期望来完成服务之间的通信（自动地服务发现、负载均衡、流量控制等）。</li><li>安全加固：因为所有的流量都是通过代理的，那么代理接收到不加密的网络流量之后，可以自动做一次封装，把它升级成安全的加密流量。</li><li>控制：用户可以配置各种规则（比如 RBAC 授权、白名单、Rate Limit 或者 Quota 等），当代理发现服务之间的访问不符合这些规则，就直接拒绝掉。</li><li>观察：所有的流量都经过代理，因此代理对整个集群的访问情况知道得一清二楚，它把这些数据上报到控制中心，那么管理员就能观察到整个集群的流量情况了。</li></ul><h3 id="istio-解决什么问题">Istio 解决什么问题</h3><p>虽然看起来非常炫酷，功能也很强大，但是一个架构和产品出来都是要解决具体的问题。所以这部分我们来看看微服务架构中的难题以及 Istio 给出的答案。</p><p>首先，原来的单个应用拆分成了许多分散的微服务，它们之间相互调用才能完成一个任务，而一旦某个过程出错（组件越多，出错的概率也就越大），就非常难以排查。</p><p>用户请求出现问题无外乎两个问题：错误和响应慢。如果请求错误，那么我们需要知道那个步骤出错了，这么多的微服务之间的调用怎么确定哪个有调用成功？哪个没有调用成功呢？如果是请求响应太慢，我们就需要知道到底哪些地方比较慢？整个链路的调用各阶段耗时是多少？哪些调用是并发执行的，哪些是串行的？这些问题需要我们能非常清楚整个集群的调用以及流量情况。</p><p><img src="https://www.hi-linux.com/img/linux/istio8.jpg" alt=""></p><p>此外，微服务拆分成这么多组件，如果单个组件出错的概率不变，那么整体有地方出错的概率就会增大。服务调用的时候如果没有错误处理机制，那么会导致非常多的问题。比如如果应用没有配置超时参数，或者配置的超时参数不对，则会导致请求的调用链超时叠加，对于用户来说就是请求卡住了；如果没有重试机制，那么因为各种原因导致的偶发故障也会导致直接返回错误给用户，造成不好的用户体验；此外，如果某些节点异常（比如网络中断，或者负载很高），也会导致应用整体的响应时间变成，集群服务应该能自动避开这些节点上的应用；最后，应用也是会出现 Bug 的，各种 Bug 会导致某些应用不可访问。这些问题需要每个应用能及时发现问题，并做好对应的处理措施。</p><p><img src="https://www.hi-linux.com/img/linux/istio9.jpg" alt=""></p><p>应用数量的增多，对于日常的应用发布来说也是个难题。应用的发布需要非常谨慎，如果应用都是一次性升级的，出现错误会导致整个线上应用不可用，影响范围太大；而且，很多情况我们需要同时存在不同的版本，使用 AB 测试验证哪个版本更好；如果版本升级改动了 API，并且互相有依赖，那么我们还希望能自动地控制发布期间不同版本访问不同的地址。这些问题都需要智能的流量控制机制。</p><p><img src="https://www.hi-linux.com/img/linux/istio10.jpg" alt=""></p><p>为了保证整个系统的安全性，每个应用都需要实现一套相似的认证、授权、HTTPS、限流等功能。一方面大多数的程序员都安全相关的功能并不擅长或者感兴趣，另外这些完全相似的内容每次都要实现一遍是非常冗余的。这个问题需要一个能自动管理安全相关内容的系统。</p><p><img src="https://www.hi-linux.com/img/linux/istio11.jpg" alt=""></p><p>上面提到的这些问题是不是非常熟悉？它们就是 istio 尝试解决的问题，如果把上面的问题和 istio 提供的功能做个映射，你会发现它们是非常匹配，毕竟 istio 就是为了解决微服务的这些问题才出现的。</p><p><img src="https://www.hi-linux.com/img/linux/istio12.gif" alt=""></p><h3 id="用什么姿势接入-istio">用什么姿势接入 Istio？</h3><p>虽然 Istio 能解决那么多的问题，但是引入 Istio 并不是没有代价的。最大的问题是 Istio 的复杂性，强大的功能也意味着 Istio 的概念和组件非常多，要想理解和掌握 Istio ，并成功在生产环境中部署需要非常详细的规划。一般情况下，集群管理团队需要对 Kubernetes 非常熟悉，了解常用的使用模式，然后采用逐步演进的方式把 Istio 的功能分批掌控下来。</p><p>第一步，自然是在测试环境搭建一套 Istio 的集群，理解所有的核心概念和组件。了解 Istio 提供的接口和资源，知道它们的用处，思考如何应用到自己的场景中，然后是熟悉 Istio 的源代码，跟进社区的 Issues，了解目前还存在的 Issues 和 Bug，思考如何规避或者修复。这一步是基础，需要积累到 Istio 安装部署、核心概念、功能和缺陷相关的知识，为后面做好准备。</p><p>第二步，可以考虑接入 Istio 的观察性功能，包括 Logging、Tracing、Metrics 数据。应用部署到集群中，选择性地（一般是流量比较小，影响范围不大的应用）为一些应用开启 Istio 自动注入功能，接管应用的流量，并安装 Prometheus 和 Zipkin 等监控组件，收集系统所有的监控数据。这一步可以试探性地了解 Istio 对应用的性能影响，同时建立服务的性能测试基准，发现服务的性能瓶颈，帮助快速定位应用可能出现的问题。此时，这些功能可以是对应用开发者透明的，只需要集群管理员感知，这样可以减少可能带来的风险。</p><p>第三步，为应用配置 Timeout 超时参数、自动重试、熔断和降级等功能，增加服务的容错性。这样可以避免某些应用错误进行这些配置导致问题的出现，这一步完成后需要通知所有的应用开发者删除掉在应用代码中对应的处理逻辑。这一步需要开发者和集群管理员同时参与。</p><p>第四步，和 Ingress、Helm、应用上架等相关组件和流程对接，使用 Istio 接管应用的升级发布流程。让开发者可以配置应用灰度发布升级的策略，支持应用的蓝绿发布、金丝雀发布以及 AB 测试。</p><p>第五步，接入安全功能。配置应用的 TLS 互信，添加 RBAC 授权，设置应用的流量限制，提升整个集群的安全性。因为安全的问题配置比较繁琐，而且优先级一般会比功能性相关的特性要低，所以这里放在了最后。</p><p>当然这个步骤只是一个参考，每个公司需要根据自己的情况、人力、时间和节奏来调整，找到适合自己的方案。</p><h3 id="总结">总结</h3><p>Istio 的架构在数据中心和集群管理中非常常见，每个 Agent 分布在各个节点上（可以是服务器、虚拟机、Pod、容器）负责接收指令并执行，以及汇报信息；控制中心负责汇聚整个集群的信息，并提供 API 让用户对集群进行管理。Kubernetes 也是类似的架构，SDN（Software Defined Network） 也是如此。相信以后会有更多类似架构的出现，这是因为数据中心要管理的节点越来越多，我们需要把任务执行分布到各节点（Agent 负责的功能），同时也需要对整个集群进行管理和控制（Control Plane 的功能），完全去中心化的架构是无法满足后面这个要求的。</p><p>Istio 的出现为负责的微服务架构减轻了很多的负担，开发者不用关心服务调用的超时、重试、Rate Limit 的实现，服务之间的安全、授权也自动得到了保证；集群管理员也能够很方便地发布应用（AB 测试和灰度发布），并且能清楚看到整个集群的运行情况。</p><p>但是这并不表明有了 Istio 就可以高枕无忧了，Istio 只是把原来分散在应用内部的复杂性统一抽象出来放到了统一的地方，并没有让原来的复杂消失不见。因此我们需要维护 Istio 整个集群，而 Istio 的架构比较复杂，尤其是它一般还需要架在 Kubernetes 之上，这两个系统都比较复杂，而且它们的稳定性和性能会影响到整个集群。因此再采用 Isito 之前，必须做好清楚的规划，权衡它带来的好处是否远大于额外维护它的花费，需要有相关的人才对整个网络、Kubernetes 和 Istio 都比较了解才行。</p><h3 id="参考资料">参考资料</h3><p><a href="https://istio.io/docs/concepts/what-is-istio/" target="_blank" rel="noopener">Istio / What is Istio?</a>：Istio 官网上对 Istio 进行介绍的文档<br><a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html" target="_blank" rel="noopener">Pattern: Service Mesh</a>：Service Mesh Pattern 详解的文章</p><blockquote><p>来源：Cizixs Writes Here<br>原文：<a href="http://t.cn/RkksFOW" target="_blank" rel="noopener">http://t.cn/RkksFOW</a><br>题图：来自谷歌图片搜索<br>版权：本文版权归原作者所有</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果你比较关注新兴技术的话，那么很可能在不同的地方听说过 Istio，并且知道它和 Service Mesh 有着牵扯。这篇文章可以作为了解 Istio 的入门介绍。通过本文你可以了解什么是 Istio，Istio 为什么最近这么火，以及 Istio 能够我们带来什么好处。&lt;/p&gt;
&lt;h3 id=&quot;什么是-Istio？&quot;&gt;什么是 Istio？&lt;/h3&gt;
&lt;p&gt;官方对 Istio 的介绍浓缩成了一句话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An open platform to connect, secure, control and observe services.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;翻译过来，就是&amp;quot;连接、安全加固、控制和观察服务的开放平台&amp;quot;。开放平台就是指它本身是开源的，服务对应的是微服务，也可以粗略地理解为单个应用。 中间的四个动词就是 Istio 的主要功能，官方也各有一句话的说明。这里再阐释一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;连接（Connect）：智能控制服务之间的调用流量，能够实现灰度升级、AB 测试和红黑部署等功能。&lt;/li&gt;
&lt;li&gt;安全加固（Secure）：自动为服务之间的调用提供认证、授权和加密。&lt;/li&gt;
&lt;li&gt;控制（Control）：应用用户定义的 Policy，保证资源在消费者中公平分配。&lt;/li&gt;
&lt;li&gt;观察（Observe）：查看服务运行期间的各种数据，比如日志、监控和 Tracing，了解服务的运行情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/istio1.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>使用 IPVS 实现 Kubernetes 入口流量负载均衡</title>
    <link href="https://www.hi-linux.com/posts/29792.html"/>
    <id>https://www.hi-linux.com/posts/29792.html</id>
    <published>2018-08-27T01:00:00.000Z</published>
    <updated>2018-08-27T06:17:59.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>新搭建的 Kubernetes 集群如何承接外部访问的流量，是刚上手 Kubernetes 时常常会遇到的问题。 在公有云上，官方给出了比较直接的答案，使用 LoadBalancer 类型的 Service，利用公有云提供的负载均衡服务来承接流量，同时在多台服务器之间进行负载均衡。</p><p>而在私有环境中，如何正确的将外部流量引入到集群内部，却暂时没有标准的做法。 本文将介绍一种基于 IPVS 来承接流量并实现负载均衡的方法，供大家参考。在阅读本文前建议先了解文中相关基础知识点，推荐阅读下「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486130&amp;idx=1&amp;sn=41ee30f02113dac86398653f542a3c70&amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3&amp;token=1694324409&amp;lang=zh_CN#rd" target="_blank" rel="noopener">浅析从外部访问 Kubernetes 集群中应用的几种方式</a>」一文。</p><h3 id="ipvs">IPVS</h3><p>IPVS 是 LVS 项目的一部分，是一款运行在 Linux kernel 当中的 4 层负载均衡器，性能异常优秀。 根据<a href="https://www.lvtao.net/server/taobao-linux-kernel.html" target="_blank" rel="noopener">这篇文章</a>的介绍，使用调优后的内核，可以轻松处理每秒 10 万次以上的转发请求。目前在中大型互联网项目中，IPVS 被广泛的使用，用于承接网站入口处的流量。</p><h3 id="kubernetes-service">Kubernetes Service</h3><p><a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486130&amp;idx=1&amp;sn=41ee30f02113dac86398653f542a3c70&amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3&amp;token=1694324409&amp;lang=zh_CN#rd" target="_blank" rel="noopener">Service</a> 是 Kubernetes 的基础概念之一，它将一组 Pod 抽象成为一项服务，统一的对外提供服务，在各个 Pod 之间实现负载均衡。 Service 有多种类型，最基本的 ClusterIP 类型解决了集群内部访问服务的需求，NodePort 类型通过 Node 节点的端口暴露服务， 再配合上 LoadBalancer 类型所定义的负载均衡器，实现了流量经过前端负载均衡器分发到各个 Node 节点暴露出的端口， 再通过 IPtables进行一次负载均衡，最终分发到实际的 Pod 上这个过程。</p><p>在 Service 的 Spec 中，externalIPs 字段平常鲜有人提到，当把 IP 地址填入这个字段后，Kube-Proxy 会增加对应的 IPtables 规则，当有以对应 IP 为目标的流量发送到 Node节点时，IPtables将进行 NAT，将流量转发到对应的服务上。一般情况下，很少会遇到服务器接受非自身绑定 IP 流量的情况，所以 externalIPs 不常被使用，但配合网络层的其他工具，它可以实现给 Service 绑定外部 IP 的效果。</p><p>今天我们将使用 externalIPs 配合 IPVS 的 DR(Direct Routing )模式实现将外部流量引入到集群内部，同时实现负载均衡。</p><a id="more"></a><h3 id="环境搭建">环境搭建</h3><p>为了演示，我们搭建了 4 台服务器组成的集群。一台服务器运行 IPVS，扮演负载均衡器的作用。一台服务器运行 Kubernetes Master 组件，其他两台服务器作为 Node 加入到 Kubernetes 集群当中。搭建过程这里不详细介绍，大家可以参考相关文档，比如：「<a href="https://k8s-install.opsnull.com/" target="_blank" rel="noopener">和我一步步部署 kubernetes 集群</a>」。</p><p>所有服务器在 172.17.8.0/24 这个网段中，服务的 VIP 我们设定为 172.17.8.201。整体架构如下图所示：</p><p><img src="https://www.hi-linux.com/img/linux/ipvs-kubernetes.png" alt=""></p><p>接下来让我们来配置 IPVS 和 Kubernetes。</p><h4 id="使用-externalips-暴露-kubernetes-service">使用 externalIPs 暴露 Kubernetes Service</h4><p>首先在集群内部运行 2 个 nginx Pod 用作演示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run nginx --image&#x3D;nginx --replicas&#x3D;2</span><br></pre></td></tr></table></figure><p>再将它暴露为 Service，同时设定 externalIPs 字段</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl expose deployment nginx --port 80 --external-ip 172.17.8.201</span><br></pre></td></tr></table></figure><p>查看 IPtables 配置，确认对应的 IPtables 规则已经被加入。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo iptables -t nat -L KUBE-SERVICES -n</span><br><span class="line">Chain KUBE-SERVICES (2 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SVC-4N57TFCL4MD7ZTDA  tcp  --  0.0.0.0&#x2F;0            10.3.0.156           &#x2F;* default&#x2F;nginx: cluster IP *&#x2F; tcp dpt:80</span><br><span class="line">KUBE-MARK-MASQ  tcp  --  0.0.0.0&#x2F;0            172.17.8.201         &#x2F;* default&#x2F;nginx: external IP *&#x2F; tcp dpt:80</span><br><span class="line">KUBE-SVC-4N57TFCL4MD7ZTDA  tcp  --  0.0.0.0&#x2F;0            172.17.8.201         &#x2F;* default&#x2F;nginx: external IP *&#x2F; tcp dpt:80 PHYSDEV match ! --physdev-is-in ADDRTYPE match src-type !LOCAL</span><br><span class="line">KUBE-SVC-4N57TFCL4MD7ZTDA  tcp  --  0.0.0.0&#x2F;0            172.17.8.201         &#x2F;* default&#x2F;nginx: external IP *&#x2F; tcp dpt:80 ADDRTYPE match dst-type LOCAL</span><br><span class="line">KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  0.0.0.0&#x2F;0            10.3.0.1             &#x2F;* default&#x2F;kubernetes:https cluster IP *&#x2F; tcp dpt:443</span><br><span class="line">KUBE-NODEPORTS  all  --  0.0.0.0&#x2F;0            0.0.0.0&#x2F;0            &#x2F;* kubernetes service nodeports; NOTE: this must be the last rule in this chain *&#x2F; ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure><h4 id="配置-ipvs-实现流量转发">配置 IPVS 实现流量转发</h4><p>首先在 IPVS 服务器上，打开 ipv4_forward。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sysctl -w net.ipv4.ip_forward&#x3D;1</span><br></pre></td></tr></table></figure><p>接下来加载 IPVS 内核模块。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo modprobe ip_vs</span><br></pre></td></tr></table></figure><p>将 VIP 绑定在网卡上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ifconfig eth0:0 172.17.8.201 netmask 255.255.255.0 broadcast 172.17.8.255</span><br></pre></td></tr></table></figure><p>再使用 ipvsadm 来配置 IPVS，这里我们直接使用 Docker 镜像，避免和特定发行版绑定。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --privileged -it --rm --net host luizbafilho&#x2F;ipvsadm</span><br><span class="line">&#x2F; # ipvsadm</span><br><span class="line">IP Virtual Server version 1.2.1 (size&#x3D;4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">&#x2F; # ipvsadm -A -t 172.17.8.201:80</span><br><span class="line">&#x2F; # ipvsadm -a -t 172.17.8.201:80 -r 172.17.8.11:80 -g</span><br><span class="line">&#x2F; # ipvsadm -a -t 172.17.8.201:80 -r 172.17.8.12:80 -g</span><br><span class="line">&#x2F; # ipvsadm</span><br><span class="line">IP Virtual Server version 1.2.1 (size&#x3D;4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  172.17.8.201:http wlc</span><br><span class="line">  -&gt; 172.17.8.11:http             Route   1      0          0</span><br><span class="line">  -&gt; 172.17.8.12:http             Route   1      0          0</span><br></pre></td></tr></table></figure><p>可以看到，我们成功建立了从 VIP 到后端服务器的转发。</p><h4 id="验证转发效果">验证转发效果</h4><p>首先使用 curl 来测试是否能够正常访问 Nginx 服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ curl http:&#x2F;&#x2F;172.17.8.201</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;&#x2F;title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;&#x2F;style&gt;</span><br><span class="line">&lt;&#x2F;head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;&#x2F;p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;nginx.org&#x2F;&quot;&gt;nginx.org&lt;&#x2F;a&gt;.&lt;br&#x2F;&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href&#x3D;&quot;http:&#x2F;&#x2F;nginx.com&#x2F;&quot;&gt;nginx.com&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;</span><br><span class="line">&lt;&#x2F;body&gt;</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure><p>接下来在 172.17.8.11 上抓包来确认 IPVS 的工作情况。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tcpdump -i any port 80</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes</span><br><span class="line">04:09:07.503858 IP 172.17.8.1.51921 &gt; 172.17.8.201.http: Flags [S], seq 2747628840, win 65535, options [mss 1460,nop,wscale 5,nop,nop,TS val 1332071005 ecr 0,sackOK,eol], length 0</span><br><span class="line">04:09:07.504241 IP 10.2.0.1.51921 &gt; 10.2.0.3.http: Flags [S], seq 2747628840, win 65535, options [mss 1460,nop,wscale 5,nop,nop,TS val 1332071005 ecr 0,sackOK,eol], length 0</span><br><span class="line">04:09:07.504498 IP 10.2.0.1.51921 &gt; 10.2.0.3.http: Flags [S], seq 2747628840, win 65535, options [mss 1460,nop,wscale 5,nop,nop,TS val 1332071005 ecr 0,sackOK,eol], length 0</span><br><span class="line">04:09:07.504827 IP 10.2.0.3.http &gt; 10.2.0.1.51921: Flags [S.], seq 3762638044, ack 2747628841, win 28960, options [mss 1460,sackOK,TS val 153786592 ecr 1332071005,nop,wscale 7], length 0</span><br><span class="line">04:09:07.504827 IP 10.2.0.3.http &gt; 172.17.8.1.51921: Flags [S.], seq 3762638044, ack 2747628841, win 28960, options [mss 1460,sackOK,TS val 153786592 ecr 1332071005,nop,wscale 7], length 0</span><br><span class="line">04:09:07.504888 IP 172.17.8.201.http &gt; 172.17.8.1.51921: Flags [S.], seq 3762638044, ack 2747628841, win 28960, options [mss 1460,sackOK,TS val 153786592 ecr 1332071005,nop,wscale 7], length 0</span><br><span class="line">04:09:07.505599 IP 172.17.8.1.51921 &gt; 172.17.8.201.http: Flags [.], ack 1, win 4117, options [nop,nop,TS val 1332071007 ecr 153786592], length 0</span><br></pre></td></tr></table></figure><p>可以看到，由客户端 172.17.8.1 发送给 172.17.8.201 的封包，经过 IPVS 的中转发送给了 172.17.8.11 这台服务器，并经过 NAT 后发送给了 10.2.0.3 这个 Pod。返回的封包不经过 IPVS 服务器直接从 172.17.8.11 发送给了 172.17.8.1。 说明 IPVS 的 DR 模式工作正常。重复多次测试可以看到流量分别从 172.17.8.11 和 172.17.8.12 进入，再分发给不同的 Pod，说明负载均衡工作正常。</p><p>与传统的 IPVS DR 模式配置不同的是，我们并未在承接流量的服务器上执行绑定 VIP，再关闭 ARP 的操作。 那是因为对 VIP 的处理直接发生在 IPtables上，我们无需在服务器上运行程序来承接流量，IPtables 会将流量转发到对应的 Pod 上。</p><p>使用这种方法来承接流量，仅需要配置 externalIPs 为 VIP 即可，无需对服务器做任何特殊的设置，使用起来相当方便。</p><h3 id="总结">总结</h3><p>在本文中演示了使用 IPVS 配合 externalIPs 实现将外部流量导入到 Kubernetes 集群中，并实现负载均衡的方法。 希望可以帮助大家理解 IPVS 和 externalIPs 的工作原理，以便在恰当的场景下合理使用这两项技术解决问题。 实际部署时，还需要考虑后台服务器可用性检查，IPVS 节点主从备份，水平扩展等问题。在这里就不详细介绍了。</p><p>在 Kubernetes 中还有许多与 externalIPs 类似的非常用功能，有些甚至是使用 Annotation 来进行配置，将来有机会再进一步分享。</p><blockquote><p>来源：极术<br>原文：<a href="http://t.cn/RX8vzvC" target="_blank" rel="noopener">http://t.cn/RX8vzvC</a><br>题图：来自谷歌图片搜索<br>版权：本文版权归原作者所有</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;新搭建的 Kubernetes 集群如何承接外部访问的流量，是刚上手 Kubernetes 时常常会遇到的问题。 在公有云上，官方给出了比较直接的答案，使用 LoadBalancer 类型的 Service，利用公有云提供的负载均衡服务来承接流量，同时在多台服务器之间进行负载均衡。&lt;/p&gt;
&lt;p&gt;而在私有环境中，如何正确的将外部流量引入到集群内部，却暂时没有标准的做法。 本文将介绍一种基于 IPVS 来承接流量并实现负载均衡的方法，供大家参考。在阅读本文前建议先了解文中相关基础知识点，推荐阅读下「&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247486130&amp;amp;idx=1&amp;amp;sn=41ee30f02113dac86398653f542a3c70&amp;amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3&amp;amp;token=1694324409&amp;amp;lang=zh_CN#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;浅析从外部访问 Kubernetes 集群中应用的几种方式&lt;/a&gt;」一文。&lt;/p&gt;
&lt;h3 id=&quot;IPVS&quot;&gt;IPVS&lt;/h3&gt;
&lt;p&gt;IPVS 是 LVS 项目的一部分，是一款运行在 Linux kernel 当中的 4 层负载均衡器，性能异常优秀。 根据&lt;a href=&quot;https://www.lvtao.net/server/taobao-linux-kernel.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这篇文章&lt;/a&gt;的介绍，使用调优后的内核，可以轻松处理每秒 10 万次以上的转发请求。目前在中大型互联网项目中，IPVS 被广泛的使用，用于承接网站入口处的流量。&lt;/p&gt;
&lt;h3 id=&quot;Kubernetes-Service&quot;&gt;Kubernetes Service&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247486130&amp;amp;idx=1&amp;amp;sn=41ee30f02113dac86398653f542a3c70&amp;amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3&amp;amp;token=1694324409&amp;amp;lang=zh_CN#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Service&lt;/a&gt; 是 Kubernetes 的基础概念之一，它将一组 Pod 抽象成为一项服务，统一的对外提供服务，在各个 Pod 之间实现负载均衡。 Service 有多种类型，最基本的 ClusterIP 类型解决了集群内部访问服务的需求，NodePort 类型通过 Node 节点的端口暴露服务， 再配合上 LoadBalancer 类型所定义的负载均衡器，实现了流量经过前端负载均衡器分发到各个 Node 节点暴露出的端口， 再通过 IPtables进行一次负载均衡，最终分发到实际的 Pod 上这个过程。&lt;/p&gt;
&lt;p&gt;在 Service 的 Spec 中，externalIPs 字段平常鲜有人提到，当把 IP 地址填入这个字段后，Kube-Proxy 会增加对应的 IPtables 规则，当有以对应 IP 为目标的流量发送到 Node节点时，IPtables将进行 NAT，将流量转发到对应的服务上。一般情况下，很少会遇到服务器接受非自身绑定 IP 流量的情况，所以 externalIPs 不常被使用，但配合网络层的其他工具，它可以实现给 Service 绑定外部 IP 的效果。&lt;/p&gt;
&lt;p&gt;今天我们将使用 externalIPs 配合 IPVS 的 DR(Direct Routing )模式实现将外部流量引入到集群内部，同时实现负载均衡。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>浅谈 Kubernetes 数据持久化方案</title>
    <link href="https://www.hi-linux.com/posts/14136.html"/>
    <id>https://www.hi-linux.com/posts/14136.html</id>
    <published>2018-08-25T01:00:00.000Z</published>
    <updated>2018-08-24T06:10:17.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h3 id="kubernetes-volume-相关概念">Kubernetes Volume 相关概念</h3><p>缺省情况下，一个运行中的容器对文件系统的写入都是发生在其分层文件系统的可写层。一旦容器运行结束，所有写入都会被丢弃。如果数据需要长期存储，那就需要对容器数据做持久化支持。</p><p>Kubernetes 和 Docker 类似，也是通过 Volume 的方式提供对存储的支持。Volume 被定义在 Pod 上，可以被 Pod 里的多个容器挂载到相同或不同的路径下。Kubernetes 中 Volume 的 概念与Docker 中的 Volume 类似，但不完全相同。具体区别如下：</p><ul><li>Kubernetes 中的 Volume 与 Pod 的生命周期相同，但与容器的生命周期不相关。当容器终止或重启时，Volume 中的数据也不会丢失。</li><li>当 Pod 被删除时，Volume 才会被清理。并且数据是否丢失取决于 Volume 的具体类型，比如：emptyDir 类型的 Volume 数据会丢失，而 PV 类型的数据则不会丢失。</li></ul><p>Volume 的核心是目录，可以通过 Pod 中的容器来访问。该目录是如何形成的、支持该目录的介质以及其内容取决于所使用的特定卷类型。要使用 Volume，需要为 Pod 指定为 Volume (<code>spec.volumes</code> 字段) 以及将它挂载到容器的位置 (<code>spec.containers.volumeMounts</code> 字段)。Kubernetes 支持多种类型的卷，一个 Pod 可以同时使用多种类型的 Volume。</p><p>容器中的进程看到的是由其 Docker 镜像和 Volume 组成的文件系统视图。 Docker 镜像位于文件系统层次结构的根目录，任何 Volume 都被挂载在镜像的指定路径中。Volume 无法挂载到其他 Volume 上或与其他 Volume 的硬连接。Pod 中的每个容器都必须独立指定每个 Volume 的挂载位置。</p><p>Kubernetes 目前支持多种 Volume 类型，大致如下：</p><ul><li>awsElasticBlockStore</li><li>azureDisk</li><li>azureFile</li><li>cephfs</li><li>csi</li><li>downwardAPI</li><li>emptyDir</li><li>fc (fibre channel)</li><li>flocker</li><li>gcePersistentDisk</li><li>gitRepo</li><li>glusterfs</li><li>hostPath</li><li>iscsi</li><li>local</li><li>nfs</li><li>persistentVolumeClaim</li><li>projected</li><li>portworxVolume</li><li>quobyte</li><li>rbd</li><li>scaleIO</li><li>secret</li><li>storageos</li><li>vsphereVolume</li></ul><blockquote><p>注：这些 Volume 并非全部都是持久化的，比如: emptyDir、secret、gitRepo 等，就会随着 Pod 的消亡而消失。</p></blockquote><a id="more"></a><h3 id="kubernetes-非持久化存储方式">Kubernetes 非持久化存储方式</h3><p>下面我们对一些常见的 Volume 做一个基本的介绍。</p><h4 id="emptrydir">emptryDir</h4><p>emptryDir，顾名思义是一个空目录，它的生命周期和所属的 Pod 是完全一致的。emptyDir 类型的 Volume 在 Pod 分配到 Node 上时会被创建，Kubernetes 会在 Node 上自动分配一个目录，因此无需指定 Node 宿主机上对应的目录文件。这个目录的初始内容为空，当 Pod 从 Node 上移除（Pod 被删除或者 Pod 发生迁移）时，emptyDir 中的数据会被永久删除。</p><p>emptyDir Volume 主要用于某些应用程序无需永久保存的临时目录，在多个容器之间共享数据等。缺省情况下，emptryDir 是使用主机磁盘进行存储的。你也可以使用其它介质作为存储，比如：网络存储、内存等。设置 <code>emptyDir.medium</code> 字段的值为 Memory 就可以使用内存进行存储，使用内存做为存储可以提高整体速度，但是要注意一旦机器重启，内容就会被清空，并且也会受到容器内存的限制。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-pd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">gcr.io/google_containers/test-webserver</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">test-container</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/cache</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">cache-volume</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cache-volume</span></span><br><span class="line">    <span class="attr">emptyDir:</span> <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><h4 id="hostpath">hostPath</h4><p>hostPath 类型的 Volume 允许用户挂载 Node 宿主机上的文件或目录到 Pod 中。大多数 Pod 都用不到这种 Volume，其缺点比较明显，比如：</p><ul><li>由于每个节点上的文件都不同，具有相同配置（例如：从 podTemplate 创建的）的 Pod 在不同节点上的行为可能会有所不同。</li><li>在底层主机上创建的文件或目录只能由 root 写入。您需要在特权容器中以 root 身份运行进程，或修改主机上的文件权限才可以写入 hostPath 卷。</li></ul><p>当然，存在即合理。这种类型的 Volume 主要用在以下场景中：</p><ul><li>运行中的容器需要访问 Docker 内部的容器，使用 /var/lib/docker 来做为 hostPath 让容器内应用可以直接访问 Docker 的文件系统。</li><li>在容器中运行 cAdvisor，使用 /dev/cgroups 来做为 hostPath。</li><li>和 DaemonSet 搭配使用，用来操作主机文件。例如：日志采集方案 FLK 中的 FluentD 就采用这种方式来加载主机的容器日志目录，达到收集本主机所有日志的目的。</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">test-pd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">k8s.gcr.io/test-webserver</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">test-container</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/test-pd</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">test-volume</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-volume</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="comment"># directory location on host</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/data</span></span><br><span class="line">      <span class="comment"># this field is optional</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">Directory</span></span><br></pre></td></tr></table></figure><h3 id="kubernetes-持久化存储方式">Kubernetes 持久化存储方式</h3><p>Kubernetes 目前可以使用 PersistentVolume、PersistentVolumeClaim、StorageClass 三种 API 资源来进行持久化存储，下面分别介绍下各种资源的概念。</p><h4 id="pv">PV</h4><p>PV 的全称是：PersistentVolume（持久化卷）。PersistentVolume 是 Volume 的一种类型，是对底层的共享存储的一种抽象。PV 由集群管理员进行创建和配置，就像节点 (Node) 是集群中的资源一样，PV 也是集群资源的一种。PV 包含存储类型、存储大小和访问模式。PV 的生命周期独立于 Pod，例如：当使用它的 Pod 销毁时对 PV 没有影响。</p><p>PersistentVolume 通过插件机制实现与共享存储的对接。Kubernetes 目前支持以下插件类型：</p><ul><li>GCEPersistentDisk</li><li>AWSElasticBlockStore</li><li>AzureFile</li><li>AzureDisk</li><li>FC (Fibre Channel)</li><li>FlexVolume</li><li>Flocker</li><li>NFS</li><li>iSCSI</li><li>RBD (Ceph Block Device)</li><li>CephFS</li><li>Cinder (OpenStack block storage)</li><li>Glusterfs</li><li>VsphereVolume</li><li>Quobyte Volumes</li><li>HostPath</li><li>VMware Photon</li><li>Portworx Volumes</li><li>ScaleIO Volumes</li><li>StorageOS</li></ul><h4 id="pvc">PVC</h4><p>PVC 的全称是：PersistentVolumeClaim（持久化卷声明），PVC 是用户对存储资源的一种请求。PVC 和 Pod 比较类似，Pod 消耗的是节点资源，PVC 消耗的是 PV 资源。Pod 可以请求 CPU 和内存，而 PVC 可以请求特定的存储空间和访问模式。对于真正使用存储的用户不需要关心底层的存储实现细节，只需要直接使用 PVC 即可。</p><h4 id="storageclass">StorageClass</h4><p>由于不同的应用程序对于存储性能的要求也不尽相同，比如：读写速度、并发性能、存储大小等。如果只能通过 PVC 对 PV 进行静态申请，显然这并不能满足任何应用对于存储的各种需求。为了解决这一问题，Kubernetes 引入了一个新的资源对象：StorageClass，通过 StorageClass 的定义，集群管理员可以先将存储资源定义为不同类型的资源，比如快速存储、慢速存储等。</p><p>当用户通过 PVC 对存储资源进行申请时，StorageClass 会使用 Provisioner（不同 Volume 对应不同的 Provisioner）来自动创建用户所需 PV。这样应用就可以随时申请到合适的存储资源，而不用担心集群管理员没有事先分配好需要的 PV。</p><ul><li>自动创建的 PV 以 <code>${namespace}-${pvcName}-${pvName}</code> 这样的命名格式创建在后端存储服务器上的共享数据目录中。</li><li>自动创建的 PV 被回收后会以 <code>archieved-${namespace}-${pvcName}-${pvName}</code> 这样的命名格式存在后端存储服务器上。</li></ul><h3 id="kubernetes-访问存储资源的方式">Kubernetes 访问存储资源的方式</h3><p>Kubernetes 目前可以使用三种方式来访问存储资源。</p><ul><li>直接访问</li></ul><p>该种方式移植性比较差，可扩展能力差。把 Volume 的基本信息完全暴露给用户，有安全隐患。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-pv01.png" alt=""></p><ul><li>静态 PV</li></ul><p>集群管理员提前手动创建一些 PV。它们带有可供集群用户使用的实际存储的细节，之后便可用于 PVC 消费。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-pv02.png" alt=""></p><blockquote><p>注：这种方式请求的 PVC 必须要与管理员创建的 PV 保持一致，如：存储大小和访问模式，否则不能将 PVC 绑定到 PV 上。</p></blockquote><ul><li>动态 PV</li></ul><p>当集群管理员创建的静态 PV 都不匹配用户的 PVC 时，PVC 请求存储类 StorageClass，StorageClass 动态的为 PVC 创建所需的 PV。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-pv03.png" alt=""></p><blockquote><p>注：此功能需要基于 StorageClass。集群管理员必须先创建并配置好请求的 StorageClass，只有请求的 StorageClass 存在的情况下才能进行动态的创建。</p></blockquote><h3 id="使用-pv-进行持久化存储实例">使用 PV 进行持久化存储实例</h3><p>这里我们将介绍如何使用 PV 资源进行数据持久化，这也是本文的重点内容。我们将以 NFS 做为后端存储结合 PV 为例，讲解 Kubernetes 如何实现数据持久化。</p><h4 id="部署-nfs-服务器">部署 NFS 服务器</h4><h5 id="安装-nfs-服务端">安装 NFS 服务端</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Ubuntu &#x2F; Debian</span><br><span class="line">$ sudo apt install nfs-kernel-server</span><br></pre></td></tr></table></figure><h5 id="新建数据目录和设置目录权限">新建数据目录和设置目录权限</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir -p &#x2F;data&#x2F;kubernetes&#x2F;</span><br><span class="line">$ sudo chmod 755 &#x2F;data&#x2F;kubernetes&#x2F;</span><br></pre></td></tr></table></figure><h5 id="配置-nfs-服务端">配置 NFS 服务端</h5><p>NFS 的默认配置文件是 <code>/etc/exports</code> ，在该配置文件中添加下面的配置信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim &#x2F;etc&#x2F;exports</span><br><span class="line">&#x2F;data&#x2F;kubernetes  *(rw,sync,no_root_squash)</span><br></pre></td></tr></table></figure><p>配置文件说明：</p><ul><li>/data/kubernetes  设置共享的数据的目录。</li><li><code>*</code> 表示任何人都有权限连接，当然也可以设置成是一个网段、一个 IP、或者是域名。</li><li>rw 设置共享目录的读写权限。</li><li>sync 表示文件同时写入硬盘和内存。</li><li>no_root_squash 当登录 NFS 主机使用共享目录的使用者是 root 时，其权限将被转换成为匿名使用者，通常它的 UID 与 GID，都会变成 nobody 身份。</li></ul><h5 id="启动-nfs-服务端">启动 NFS 服务端</h5><ul><li>启动 NFS 服务端</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl restart nfs-kernel-server</span><br></pre></td></tr></table></figure><ul><li>验证 NFS 服务端是否正常启动</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo rpcinfo -p|grep nfs</span><br><span class="line">100003    3   tcp   2049  nfs</span><br><span class="line">100003    4   tcp   2049  nfs</span><br><span class="line">100003    3   udp   2049  nfs</span><br></pre></td></tr></table></figure><ul><li>查看具体目录挂载权限</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat &#x2F;var&#x2F;lib&#x2F;nfs&#x2F;etab</span><br><span class="line">&#x2F;data&#x2F;kubernetes*(rw,sync,wdelay,hide,nocrossmnt,secure,no_root_squash,no_all_squash,no_subtree_check,secure_locks,acl,no_pnfs,anonuid&#x3D;65534,anongid&#x3D;65534,sec&#x3D;sys,rw,secure,no_root_squash,no_all_squash)</span><br></pre></td></tr></table></figure><p>如果以上步骤都正常的话，到这里 NFS 服务端就已经正常安装完成。</p><h5 id="安装-nfs-客户端">安装 NFS 客户端</h5><ul><li>安装 NFS 客户端</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install nfs-common</span><br></pre></td></tr></table></figure><blockquote><p>注：所有 Node 宿主机都需要安装 NFS 客户端。</p></blockquote><ul><li>验证 RPC 服务状态</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl status rpcbind.service</span><br><span class="line">● rpcbind.service - RPC bind portmap service</span><br><span class="line">   Loaded: loaded (&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;rpcbind.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Tue 2018-08-07 09:54:29 CST; 49s ago</span><br><span class="line">     Docs: man:rpcbind(8)</span><br><span class="line"> Main PID: 17501 (rpcbind)</span><br><span class="line">    Tasks: 1 (limit: 2313)</span><br><span class="line">   CGroup: &#x2F;system.slice&#x2F;rpcbind.service</span><br><span class="line">           └─17501 &#x2F;sbin&#x2F;rpcbind -f -w</span><br></pre></td></tr></table></figure><ul><li>检查 NFS 服务端可用的共享目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo showmount -e 192.168.100.213</span><br><span class="line">Export list for 192.168.100.213:</span><br><span class="line">&#x2F;data&#x2F;kubernetes *</span><br></pre></td></tr></table></figure><ul><li>挂载 NFS 共享目录到本地</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir -p &#x2F;data&#x2F;kubernetes&#x2F;</span><br><span class="line">$ sudo mount -t nfs 192.168.100.213:&#x2F;data&#x2F;kubernetes&#x2F; &#x2F;data&#x2F;kubernetes&#x2F;</span><br></pre></td></tr></table></figure><ul><li>验证 NFS 客户端</li></ul><p>挂载成功后，在客户端上面的目录中新建一个文件，然后检查在 NFS 服务端的共享目录下是否也会出现该文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 在 NFS 客户端新建</span><br><span class="line">$ sudo touch &#x2F;data&#x2F;kubernetes&#x2F;test.txt</span><br><span class="line"></span><br><span class="line"># 在 NFS 服务端查看</span><br><span class="line">$ sudo ls -ls &#x2F;data&#x2F;kubernetes&#x2F;</span><br><span class="line">total 0</span><br><span class="line">0 -rw-r--r-- 1 root root 0 Aug  7 09:59 test.txt</span><br></pre></td></tr></table></figure><h4 id="实现静态-pv">实现静态 PV</h4><h5 id="新建-pv-资源">新建 PV 资源</h5><p>完成上面的共享存储后，我们就可以来使用 PV 和 PVC 来管理和使用这些共享存储。PV 作为存储资源主要包括存储能力、访问模式、存储类型、回收策略等关键信息。</p><p>下面我们来新建一个 PV 对象并使用 NFS 做为后端存储类型，该 PV 包括 1G 的存储空间、访问模式为 ReadWriteOnce、回收策略为 Recyle。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">pv1-nfs.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span>  <span class="string">pv1-nfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">1Gi</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line">  <span class="attr">nfs:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/data/kubernetes</span></span><br><span class="line">    <span class="attr">server:</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.213</span></span><br></pre></td></tr></table></figure><blockquote><p>注：Kubernetes 支持的 PV 类型有很多，比如常见的 Ceph、GlusterFs、NFS 等。更多的支持类型可以查看<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_blank" rel="noopener">官方文档</a>。</p></blockquote><p>我们先使用 Kubectl 创建该 PV 资源。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f pv1-nfs.yaml</span><br><span class="line">persistentvolume &quot;pv1-nfs&quot; created</span><br></pre></td></tr></table></figure><p>从下面的结果，我们可以看到 pv1-nfs 已经创建成功。状态是 Available，这表示 pv1-nfs 准备就绪，可以被 PVC 申请。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Available                                      46s</span><br></pre></td></tr></table></figure><p>我们对上面的 PV 属性来做一个简单的解读。</p><p><strong>Capacity（存储能力）</strong></p><p>一般来说，一个 PV 对象都要指定一个存储能力，通过 PV 的 Capacity 属性来设置。这里的 storage=1Gi 表示设置存储空间的大小。</p><p><strong>AccessModes（访问模式）</strong></p><p>AccessModes 是用来对 PV 进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：</p><ul><li>ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载。</li><li>ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载。</li><li>ReadWriteMany（RWX）：读写权限，可以被多个节点挂载。</li></ul><blockquote><p>注：一些 PV 可能支持多种访问模式，但是在挂载的时候只能使用一种访问模式，多种访问模式是不会生效的。</p></blockquote><p>下图是一些常用的 Volume 插件支持的访问模式：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-pv04.png" alt=""></p><p><strong>PersistentVolumeReclaimPolicy（回收策略）</strong></p><p>当前 PV 设置的回收策略，我们这里指定的 PV 的回收策略为 Recycle。目前 PV 支持的策略有三种：</p><ul><li>Retain（保留）- 保留数据，需要管理员手工清理数据。</li><li>Recycle（回收）- 清除 PV 中的数据，效果相当于执行 <code>rm -rf /thevoluem/*</code> 。</li><li>Delete（删除）- 与 PV 相连的后端存储完成 Volume 的删除操作，这种方式常见于云服务商的存储服务，比如 ASW EBS。</li></ul><blockquote><p>注：目前只有 NFS 和 HostPath 两种类型支持回收策略。设置为 Retain 这种策略会更加保险一些。</p></blockquote><p><strong>状态</strong></p><p>一个 PV 的生命周期中，可能会处于 4 种不同的阶段。</p><ul><li>Available（可用）：表示可用状态，还未被任何 PVC 绑定。</li><li>Bound（已绑定）：表示 PV 已经被 PVC 绑定。</li><li>Released（已释放）：PVC 被删除，但是资源还未被集群重新声明。</li><li>Failed（失败）： 表示该 PV 的自动回收失败。</li></ul><h5 id="新建-pvc-资源">新建 PVC 资源</h5><p>我们平时真正使用的资源其实是 PVC，就类似于我们的服务是通过 Pod 来运行的，而不是 Node，只是 Pod 跑在 Node 上而已。</p><p>首先，我们新建一个数据卷声明，向 PV 请求 1Gi 的存储容量。其访问模式设置为 ReadWriteOnce。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">pvc1-nfs.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc1-nfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure><p>在新建 PVC 之前，我们可以看下之前创建的 PV 的状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Available                                      28m</span><br></pre></td></tr></table></figure><p>我们可以看到当前 pv1-nfs 是在 Available 的一个状态，所以这个时候我们的 PVC 可以和这个 PV 进行绑定。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f pvc1-nfs.yaml</span><br><span class="line">persistentvolumeclaim &quot;pvc1-nfs&quot; created</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1-nfs   Bound     pv1-nfs   1Gi        RWO                           34s</span><br></pre></td></tr></table></figure><p>从上面的结果可以看到 pvc1-nfs 创建成功了，并且状态是 Bound 状态。这个时候我们再看下 PV 的状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM              STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Bound     default&#x2F;pvc1-nfs                            31m</span><br></pre></td></tr></table></figure><p>同样我们可以看到 PV 也是 Bound 状态，对应的声明是 default/pvc1-nfs，表示 default 命名空间下面的 pvc1-nfs，表示我们刚刚新建的 pvc1-nfs 和 pv1-nfs 绑定成功。</p><p>PV 和 PVC 的绑定是系统自动完成的，不需要显示指定要绑定的 PV。系统会根据 PVC 中定义的要求去查找处于 Available 状态的 PV。</p><ul><li>如果找到合适的 PV 就完成绑定。</li><li>如果没有找到合适的 PV 那么 PVC 就会一直处于 Pending 状态，直到找到合适的 PV 完成绑定为止。</li></ul><p>下面我们来看一个例子，这里声明一个 PVC 的对象，它要求 PV 的访问模式是 ReadWriteOnce、存储容量 2Gi 和标签值为 app=nfs。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">pvc2-nfs.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc2-nfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nfs</span></span><br></pre></td></tr></table></figure><p>我们先查看下当前系统的所有 PV 资源。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM              STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Bound     default&#x2F;pvc1-nfs                            52m</span><br></pre></td></tr></table></figure><p>从结果可以看到，目前所有 PV 都是 Bound 状态，并没有 Available 状态的 PV。所以我们现在用上面新建的 PVC 是无法匹配到合适的 PV 的。我们来创建 PVC 看看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f pvc2-nfs.yaml</span><br><span class="line">persistentvolumeclaim &quot;pvc2-nfs&quot; created</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1-nfs   Bound     pv1-nfs   1Gi        RWO                           28m</span><br><span class="line">pvc2-nfs   Pending                                                      16s</span><br></pre></td></tr></table></figure><p>从结果我们可以看到 pvc2-nfs 当前就是 Pending 状态，因为并没有合适的 PV 给这个 PVC 使用。现在我们来新建一个合适该 PVC 使用的 PV。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">pv2-nfs.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv2-nfs</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line">  <span class="attr">nfs:</span></span><br><span class="line">    <span class="attr">server:</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.213</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/data/kubernetes</span></span><br></pre></td></tr></table></figure><p>使用 Kubectl 创建该 PV。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f pv2-nfs.yaml</span><br><span class="line">persistentvolume &quot;pv2-nfs&quot; created</span><br><span class="line"></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM              STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Bound     default&#x2F;pvc1-nfs                            1h</span><br><span class="line">pv2-nfs   2Gi        RWO            Recycle          Bound     default&#x2F;pvc2-nfs                            18s</span><br></pre></td></tr></table></figure><p>创建完 pv2-nfs 后，从上面的结果你会发现该 PV 已经是 Bound 状态了。其对应的 PVC 是 default/pvc2-nfs，这就证明 pvc2-nfs 终于找到合适的 PV 且完成了绑定。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1-nfs   Bound     pv1-nfs   1Gi        RWO                           36m</span><br><span class="line">pvc2-nfs   Bound     pv2-nfs   2Gi        RWO                           9m</span><br></pre></td></tr></table></figure><blockquote><p>注：如果 PVC 申请的容量大小小于 PV 提供的大小，PV 同样会分配该 PV 所有容量给 PVC，如果 PVC 申请的容量大小大于 PV 提供的大小，此次申请就会绑定失败。</p></blockquote><h5 id="使用-pvc-资源">使用 PVC 资源</h5><p>这里我们已经完成了 PV 和 PVC 创建，现在我们就可以使用这个 PVC 了。这里我们使用 Nginx 的镜像来创建一个 Deployment，将容器的 <code>/usr/share/nginx/html</code> 目录通过 Volume 挂载到名为 pvc2-nfs 的 PVC 上，并通过 NodePort 类型的 Service 来暴露服务。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">nfs-pvc-deploy.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-pvc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nfs-pvc</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">web</span></span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">www</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">www</span></span><br><span class="line">        <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">          <span class="attr">claimName:</span> <span class="string">pvc2-nfs</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-pvc</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nfs-pvc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="string">web</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nfs-pvc</span></span><br></pre></td></tr></table></figure><p>使用 Kubectl 创建这个 Deployment。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nfs-pvc-deploy.yaml</span><br><span class="line">deployment.extensions &quot;nfs-pvc&quot; created</span><br><span class="line">service &quot;nfs-pvc&quot; created</span><br><span class="line"></span><br><span class="line">$ kubectl get pods -o wide|grep nfs-pvc</span><br><span class="line">nfs-pvc-789788587b-ctp58                        1&#x2F;1       Running     0          4m        172.30.24.6   dev-node-02</span><br><span class="line">nfs-pvc-789788587b-q294p                        1&#x2F;1       Running     0          4m        172.30.92.6   dev-node-03</span><br><span class="line">nfs-pvc-789788587b-rtl5s                        1&#x2F;1       Running     0          4m        172.30.87.9   dev-node-01</span><br><span class="line"></span><br><span class="line">$ kubectl get svc|grep nfs-pvc</span><br><span class="line">nfs-pvc                         NodePort       10.254.5.24      &lt;none&gt;                                            80:8682&#x2F;TCP                5m</span><br></pre></td></tr></table></figure><p>通过 NodePort 访问该服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ curl -I http:&#x2F;&#x2F;192.168.100.211:8682</span><br><span class="line">HTTP&#x2F;1.1 403 Forbidden</span><br><span class="line">Server: nginx&#x2F;1.7.9</span><br><span class="line">Date: Tue, 07 Aug 2018 03:42:30 GMT</span><br><span class="line">Content-Type: text&#x2F;html</span><br><span class="line">Content-Length: 168</span><br><span class="line">Connection: keep-alive</span><br></pre></td></tr></table></figure><p>我们可以看到 Nginx 返回了 403，这是因为我们用 NFS 中的共享目录做为 Nginx 的默认站点目录，目前这个 NFS 共享目录中没有可用的 index.html 文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls  &#x2F;data&#x2F;kubernetes&#x2F;</span><br><span class="line">test.txt</span><br></pre></td></tr></table></figure><p>在 NFS 服务端共享目录下新建一个 index.html 的文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sh -c &quot;echo &#39;&lt;h1&gt;Hello Kubernetes~&lt;&#x2F;h1&gt;&#39; &gt; &#x2F;data&#x2F;kubernetes&#x2F;index.html&quot;</span><br><span class="line">$ ls &#x2F;data&#x2F;kubernetes&#x2F;</span><br><span class="line">index.html  test.txt</span><br></pre></td></tr></table></figure><p>再次通过 NodePort 访问该服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl  http:&#x2F;&#x2F;192.168.100.211:8682</span><br><span class="line">&lt;h1&gt;Hello Kubernetes~&lt;&#x2F;h1&gt;</span><br></pre></td></tr></table></figure><h6 id="使用-subpath-对同一个-pv-进行隔离">使用 subPath 对同一个 PV 进行隔离</h6><p>从上面的例子中，我们可以看到容器中的数据是直接放到共享数据目录根目录下的。如果有多个容器都使用一个 PVC 的话，这样就很容易造成文件冲突。Pod 中 <code>volumeMounts.subPath</code> 属性可用于指定引用卷内的路径，只需设置该属性就可以解决该问题。</p><p>修改刚才创建 Deployment 的 YAML 文件，增加 subPath 行。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">nfs-pvc-deploy.yaml</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">volumeMounts:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">www</span></span><br><span class="line">  <span class="attr">subPath:</span> <span class="string">nginx-pvc-test</span></span><br><span class="line">  <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>更改完 YAML 文件后，重新更新下 Deployment 即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f nfs-pvc-deploy.yaml</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">deployment.extensions &quot;nfs-pvc&quot; configured</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">service &quot;nfs-pvc&quot; configured</span><br></pre></td></tr></table></figure><p>更新完后，NFS 的数据共享目录下就会自动新增一个同 subPath 名字一样的目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls  &#x2F;data&#x2F;kubernetes&#x2F;</span><br><span class="line">index.html      nginx-pvc-test&#x2F; test.txt</span><br></pre></td></tr></table></figure><p>同样 nginx-pvc-test 目录下默认是空的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ls &#x2F;data&#x2F;kubernetes&#x2F;nginx-pvc-test&#x2F;</span><br></pre></td></tr></table></figure><p>新增一个 index 文件后访问该服务，一切安好。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sh -c &quot;echo &#39;&lt;h1&gt;Hello Kubernetes~&lt;&#x2F;h1&gt;&#39; &gt; &#x2F;data&#x2F;kubernetes&#x2F;nginx-pvc-test&#x2F;index.html&quot;</span><br><span class="line">$ curl  http:&#x2F;&#x2F;192.168.100.211:8682</span><br><span class="line">&lt;h1&gt;Hello Kubernetes~&lt;&#x2F;h1&gt;</span><br></pre></td></tr></table></figure><h5 id="验证-pvc-中的数据持久化">验证 PVC 中的数据持久化</h5><p>上面我们已经成功的在 Pod 中使用了 PVC 来做为存储，现在我们来验证下数据是否会丢失。我们分两种情况来验证：一种是直接删除 Deployment 和 Service，另一种是先删除 PVC 后再删除 Deployment 和 Service。</p><h6 id="直接删除-deployment-和-service">直接删除 Deployment 和 Service</h6><p>在这种情况下数据会永久保存下来，删除 Deployment 和 Service 不会对数据造成任何影响。</p><ul><li>删除 Deployment 和 Service</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete -f nfs-pvc-deploy.yaml</span><br><span class="line">service &quot;nfs-pvc&quot; deleted</span><br><span class="line">deployment.extensions &quot;nfs-pvc&quot; deleted</span><br></pre></td></tr></table></figure><ul><li>查看数据共享目录下面的数据</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls &#x2F;data&#x2F;kubernetes&#x2F;nginx-pvc-test&#x2F;</span><br><span class="line">index.html</span><br></pre></td></tr></table></figure><h6 id="先删除-pvc-后再删除-deployment-和-service">先删除 PVC 后再删除 Deployment 和 Service</h6><ul><li>删除 PVC</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete pvc pvc2-nfs</span><br><span class="line">persistentvolumeclaim &quot;pvc2-nfs&quot; deleted</span><br></pre></td></tr></table></figure><p>我们可以看到 PVC 状态已经变成了 Terminating，但是现在数据共享目录中的文件和服务都是可以正常访问的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS        VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1-nfs   Bound         pv1-nfs   1Gi        RWO                           3h</span><br><span class="line">pvc2-nfs   Terminating   pv2-nfs   2Gi        RWO                           6m</span><br><span class="line"></span><br><span class="line">$ ls &#x2F;data&#x2F;kubernetes&#x2F;nginx-pvc-test&#x2F;</span><br><span class="line">index.html</span><br><span class="line"></span><br><span class="line">$ curl  http:&#x2F;&#x2F;192.168.100.211:8928</span><br><span class="line">&lt;h1&gt;Hello Kubernetes~&lt;&#x2F;h1&gt;</span><br></pre></td></tr></table></figure><p>这是因为还有 Pod 正在使用 pvc2-nfs 这个 PVC，那么对应的资源依然可用。如果无 Pod 继续使用 pvc2-nfs 这个 PVC，则相应 PVC 对应的资源就会被收回。</p><ul><li>删除 Deployment 和 Service</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete -f nfs-pvc-deploy.yaml</span><br><span class="line">deployment.extensions &quot;nfs-pvc&quot; deleted</span><br><span class="line">service &quot;nfs-pvc&quot; deleted</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1-nfs   Bound     pv1-nfs   1Gi        RWO                           3h</span><br><span class="line"></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM              STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Bound       default&#x2F;pvc1-nfs                            4h</span><br><span class="line">pv2-nfs   2Gi        RWO            Recycle          Available                                               25m</span><br><span class="line"></span><br><span class="line">$ ls &#x2F;data&#x2F;kubernetes&#x2F;</span><br></pre></td></tr></table></figure><p>从上面的结果我们可以看到 pvc2-nfs 这个 PVC 已经不存在了，pv2-nfs 这个 PV 的状态也变成 Available 了。由于我们设置的 PV 的回收策略是 Recycle，我们可以发现 NFS 的共享数据目录下面的数据也没了，这是因为我们把 PVC 给删除掉后回收了数据。</p><h4 id="使用-storageclass-实现动态-pv">使用 StorageClass 实现动态 PV</h4><p>上面的例子中我们学习了静态 PV 和 PVC 的使用方法，所谓静态 PV 就是我要使用的一个 PVC 的话就必须手动去创建一个 PV。</p><p>这种方式在很多使用场景下使用起来都不灵活，需要依赖集群管理员事先完成 PV 的建立。特别是对于 StatefulSet 类型的应用，简单的使用静态的 PV 就不是很合适了。这种情况下我们就需要用到动态 PV，动态 PV 的实现需要用到 StorageClass。</p><h5 id="创建-provisioner">创建 Provisioner</h5><p>要使用 StorageClass，我们就得安装对应的自动配置程序。比如：我们这里存储后端使用的是 NFS，那么我们就需要使用到一个对应的自动配置程序。支持 NFS 的自动配置程序就是 <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client" target="_blank" rel="noopener">nfs-client</a>，我们把它称作 Provisioner。这个程序可以使用我们已经配置好的 NFS 服务器，来自动创建持久卷，也就是自动帮我们创建 PV。</p><ul><li>以 Deployment 方式部署一个 Provisioner</li></ul><p>根据实际情况将下面的环境变量 <code>NFS_SERVER</code>、<code>NFS_PATH</code> 和 NFS 相关配置替换成你的对应的值。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">nfs-client.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">strategy:</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">Recreate</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">quay.io/external_storage/nfs-client-provisioner:latest</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-root</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/persistentvolumes</span></span><br><span class="line">          <span class="attr">env:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PROVISIONER_NAME</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">fuseim.pri/ifs</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NFS_SERVER</span></span><br><span class="line">              <span class="attr">value:</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.213</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NFS_PATH</span></span><br><span class="line">              <span class="attr">value:</span> <span class="string">/data/kubernetes</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nfs-client-root</span></span><br><span class="line">          <span class="attr">nfs:</span></span><br><span class="line">            <span class="attr">server:</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.213</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/data/kubernetes</span></span><br></pre></td></tr></table></figure><p>使用 Kubectl 命令建立这个 Deployment</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nfs-client.yaml</span><br><span class="line">deployment.extensions &quot;nfs-client-provisioner&quot; created</span><br></pre></td></tr></table></figure><ul><li>给 nfs-client-provisioner 创建 ServiceAccount</li></ul><p>从 Kubernetes 1.6 版本开始，API Server 启用了 RBAC 授权。Provisioner 要想在 Kubernetes 中创建对应的 PV 资源，就得有对应的权限。</p><p>这里我们新建一个名为 nfs-client-provisioner 的 ServiceAccount 并绑定在一个名为 nfs-client-provisioner-runner 的 ClusterRole 上。该 ClusterRole 包含对 PersistentVolumes 的增、删、改、查等权限。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">$ vim nfs-client-sa.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumes&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumeclaims&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;storage.k8s.io&quot;]</span><br><span class="line">    resources: [&quot;storageclasses&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;events&quot;]</span><br><span class="line">    verbs: [&quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    namespace: default</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>使用 Kubectl 命令建立这个 ServiceAccount。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nfs-client-sa.yaml</span><br><span class="line">serviceaccount &quot;nfs-client-provisioner&quot; created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io &quot;nfs-client-provisioner-runner&quot; created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io &quot;run-nfs-client-provisioner&quot; created</span><br></pre></td></tr></table></figure><ul><li>创建 StorageClass 对象</li></ul><p>这里我们创建了一个名为 course-nfs-storage 的 StorageClass 对象，注意下面的 Provisioner 对应的值一定要和上面的 Deployment下面 PROVISIONER_NAME 这个环境变量的值一样。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ vim nfs-client-class.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: storage.k8s.io&#x2F;v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: course-nfs-storage</span><br><span class="line">provisioner: fuseim.pri&#x2F;ifs # or choose another name, must match deployment&#39;s env PROVISIONER_NAME&#39;</span><br></pre></td></tr></table></figure><p>使用 Kubectl 命令建立这个 StorageClass。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nfs-client-class.yaml</span><br><span class="line">storageclass.storage.k8s.io &quot;course-nfs-storage&quot; created</span><br></pre></td></tr></table></figure><p>以上都创建完成后查看下相关资源的状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods|grep nfs-client</span><br><span class="line">NAME                                            READY     STATUS      RESTARTS   AGE</span><br><span class="line">nfs-client-provisioner-9d94b899c-nn4c7          1&#x2F;1       Running     0          1m</span><br><span class="line"></span><br><span class="line">$ kubectl get storageclass</span><br><span class="line">NAME                 PROVISIONER      AGE</span><br><span class="line">course-nfs-storage   fuseim.pri&#x2F;ifs   1m</span><br></pre></td></tr></table></figure><h5 id="手动创建的一个-pvc-对象">手动创建的一个 PVC 对象</h5><ul><li>新建一个 PVC 对象</li></ul><p>我们这里就来建立一个能使用 StorageClass 资源对象来动态建立 PV 的 PVC，要创建使用 StorageClass 资源对象的 PVC 有以下两种方法。</p><p>方法一：在这个 PVC 对象中添加一个 Annotations 属性来声明 StorageClass 对象的标识。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 这里我们声明了一个 PVC 对象，采用 ReadWriteMany 的访问模式并向 PV 请求 100Mi 的空间。</span><br><span class="line">$ vim test-pvc.yaml</span><br><span class="line"></span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pvc</span><br><span class="line">  annotations:</span><br><span class="line">    volume.beta.kubernetes.io&#x2F;storage-class: &quot;course-nfs-storage&quot;</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 100Mi</span><br></pre></td></tr></table></figure><p>方法二：把名为 course-nfs-storage 的 StorageClass 设置为 Kubernetes 的默认后端存储。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl patch storageclass course-nfs-storage -p &#39;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io&#x2F;is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&#39;</span><br><span class="line">storageclass.storage.k8s.io &quot;course-nfs-storage&quot; patched</span><br></pre></td></tr></table></figure><p>上面这两种方法都是可以的，为了不影响系统的默认行为，这里我们采用第一种方法，直接使用 YAML 文件创建即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f test-pvc.yaml</span><br><span class="line">persistentvolumeclaim &quot;test-pvc&quot; created</span><br></pre></td></tr></table></figure><p>创建完成后，我们来看看对应的资源是否创建成功。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE</span><br><span class="line">pvc1-nfs   Bound     pv1-nfs                                    1Gi        RWO                                 4h</span><br><span class="line">test-pvc   Bound     pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79   100Mi      RWX            course-nfs-storage   41s</span><br><span class="line"></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM              STORAGECLASS         REASON    AGE</span><br><span class="line">pv1-nfs                                    1Gi        RWO            Recycle          Bound       default&#x2F;pvc1-nfs                                  5h</span><br><span class="line">pv2-nfs                                    2Gi        RWO            Recycle          Available                                                     1h</span><br><span class="line">pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79   100Mi      RWX            Delete           Bound       default&#x2F;test-pvc   course-nfs-storage             2m</span><br></pre></td></tr></table></figure><p>从上面的结果我们可以看到一个名为 test-pvc 的 PVC 对象创建成功并且状态已经是 Bound 了。对应也自动创建了一个名为 pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79 的 PV 对象，其访问模式是 RWX，回收策略是 Delete。STORAGECLASS 栏中的值也正是我们创建的 StorageClass 对象 course-nfs-storage。</p><ul><li>测试</li></ul><p>我们用一个简单的示例来测试下用 StorageClass 方式声明的 PVC 对象是否能正常存储。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ vim test-pod.yaml</span><br><span class="line"></span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: test-pod</span><br><span class="line">    image: busybox</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    command:</span><br><span class="line">    - &quot;&#x2F;bin&#x2F;sh&quot;</span><br><span class="line">    args:</span><br><span class="line">    - &quot;-c&quot;</span><br><span class="line">    - &quot;touch &#x2F;mnt&#x2F;SUCCESS &amp;&amp; exit 0 || exit 1&quot;</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: nfs-pvc</span><br><span class="line">      mountPath: &quot;&#x2F;mnt&quot;</span><br><span class="line">  restartPolicy: &quot;Never&quot;</span><br><span class="line">  volumes:</span><br><span class="line">  - name: nfs-pvc</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: test-pvc</span><br></pre></td></tr></table></figure><p>上面这个 Pod 的作用非常简单，就是在一个 busybox 容器里的 /mnt 目录下面新建一个 SUCCESS 的文件，而 /mnt 目录是挂载到 test-pvc 这个资源对象上的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f test-pod.yaml</span><br><span class="line">pod &quot;test-pod&quot; created</span><br></pre></td></tr></table></figure><p>完成 Pod 创建后，我们可以在 NFS 服务器的共享数据目录下面查看数据是否存在。我们可以看到下面有一个名字很长的文件夹，这个文件夹的命名方式是：<code>${namespace}-${pvcName}-${pvName}</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls &#x2F;data&#x2F;kubernetes&#x2F;</span><br><span class="line">default-test-pvc-pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79</span><br></pre></td></tr></table></figure><p>再看下这个文件夹下面的文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls &#x2F;data&#x2F;kubernetes&#x2F;default-test-pvc-pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79&#x2F;</span><br><span class="line">SUCCESS</span><br></pre></td></tr></table></figure><p>我们看到下面有一个 SUCCESS 的文件，说明 PV 对应的存储里可以成功写入文件。</p><h5 id="自动创建的一个-pvc-对象">自动创建的一个 PVC 对象</h5><p>在上面的演示过程中，我们可以看到是手动创建的一个 PVC 对象，而在实际使用中更多使用 StorageClass 的是 StatefulSet 类型的服务。</p><p>StatefulSet 类型的服务是可以通过一个 volumeClaimTemplates 属性来直接使用 StorageClass。volumeClaimTemplates 其实就是一个 PVC 对象的模板，类似于 StatefulSet 下面的 template，而这种模板可以动态的去创建相应的 PVC 对象。</p><ul><li>创建一个 StatefulSet 对象</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$ vim test-statefulset-nfs.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1beta1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-web</span><br><span class="line">spec:</span><br><span class="line">  serviceName: &quot;nginx&quot;</span><br><span class="line">  replicas: 3</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-web</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 10</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: web</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: www</span><br><span class="line">          mountPath: &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      name: www</span><br><span class="line">      annotations:</span><br><span class="line">        volume.beta.kubernetes.io&#x2F;storage-class: course-nfs-storage</span><br><span class="line">    spec:</span><br><span class="line">      accessModes: [ &quot;ReadWriteOnce&quot; ]</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 1Gi</span><br></pre></td></tr></table></figure><p>使用 Kubectl 命令建立这个 StatefulSet 对象。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f test-statefulset-nfs.yaml</span><br><span class="line">statefulset.apps &quot;nfs-web&quot; created</span><br></pre></td></tr></table></figure><ul><li>检查相应资源对像是否已完成创建</li></ul><p>创建完成后可以看到上面 StatefulSet 对象中定义的 3 个 Pod 已经运行成功。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods</span><br><span class="line">NAME                                            READY     STATUS      RESTARTS   AGE</span><br><span class="line">nfs-web-0                                       1&#x2F;1       Running     0          19s</span><br><span class="line">nfs-web-1                                       1&#x2F;1       Running     0          16s</span><br><span class="line">nfs-web-2                                       1&#x2F;1       Running     0          6s</span><br></pre></td></tr></table></figure><p>再查看下 PVC 和 PV 对象。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pvc</span><br><span class="line">NAME            STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE</span><br><span class="line">www-nfs-web-0   Bound     pvc-16ba792f-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            course-nfs-storage   1m</span><br><span class="line">www-nfs-web-1   Bound     pvc-18c631d4-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            course-nfs-storage   1m</span><br><span class="line">www-nfs-web-2   Bound     pvc-1ed50c38-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            course-nfs-storage   1m</span><br><span class="line"></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                   STORAGECLASS         REASON    AGE</span><br><span class="line">pvc-16ba792f-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            Delete           Bound       default&#x2F;www-nfs-web-0   course-nfs-storage             3m</span><br><span class="line">pvc-18c631d4-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            Delete           Bound       default&#x2F;www-nfs-web-1   course-nfs-storage             3m</span><br><span class="line">pvc-1ed50c38-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            Delete           Bound       default&#x2F;www-nfs-web-2   course-nfs-storage             3m</span><br></pre></td></tr></table></figure><p>我们可以看到生成了 3 个 PVC 对象，名称由模板名称加上 Pod 的名称组合而成，而这 3 个 PVC 对象也都是绑定状态。</p><ul><li>检查 NFS 服务器上面的是否生成相应的数据目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ls  &#x2F;data&#x2F;kubernetes&#x2F; -l</span><br><span class="line">total 16</span><br><span class="line">drwxrwxrwx 2 root root 4096 Aug  7 15:32 default-test-pvc-pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79</span><br><span class="line">drwxrwxrwx 2 root root 4096 Aug  7 15:40 default-www-nfs-web-0-pvc-16ba792f-9a15-11e8-9a96-001c42c61a79</span><br><span class="line">drwxrwxrwx 2 root root 4096 Aug  7 15:40 default-www-nfs-web-1-pvc-18c631d4-9a15-11e8-9a96-001c42c61a79</span><br><span class="line">drwxrwxrwx 2 root root 4096 Aug  7 15:40 default-www-nfs-web-2-pvc-1ed50c38-9a15-11e8-9a96-001c42c61a79</span><br></pre></td></tr></table></figure><h4 id="部署一个使用-storageclass-的应用">部署一个使用 StorageClass 的应用</h4><p>上面的例子中都是简单的运行了一个 Nginx 来演示功能，接下来我们用 Helm 来部署一个具体的应用看看效果。如果你对 Helm 还不够了解，可以先读读 「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486154&amp;idx=1&amp;sn=becd5dd0fadfe0b6072f5dfdc6fdf786&amp;chksm=eac52be3ddb2a2f555b8b1028db97aa3e92d0a4880b56f361e4b11cd252771147c44c08c8913&amp;mpshare=1&amp;scene=23&amp;srcid=0809XT1uzvaUqkaWiouHAUv4%23rd" target="_blank" rel="noopener">Helm 入门指南</a>」一文。</p><p>这里我们同样以部署 DokuWiki 的为例。在「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486201&amp;idx=1&amp;sn=8ba9e1ce4ce1cd3e62472db8b946daf9&amp;chksm=eac52bd0ddb2a2c6c579d0e083eb180d1a57c6ad968f1c6dd0f5b74f7b0702e77d85325a5128&amp;mpshare=1&amp;scene=23&amp;srcid=08211PurH1JwOAE6ueZYuZVt%23rd" target="_blank" rel="noopener">利用 Helm 快速部署 Ingress</a>」一文中我们在部署时关闭了 PersistentVolume。现在我们就演示加上 PersistentVolume 的效果。</p><p>DokuWiki 默认是启用 Persistence 特性的，这里主要通过 <code>persistence.apache.storageClass</code>、<code>persistence.apache.size</code> 和 <code>persistence.dokuwiki.storageClass</code>、<code>persistence.dokuwiki.size</code> 几个参数来设置 Apache 和 DokuWiki 两个应用对应的 storageClass 名称和存储大小 。</p><ul><li>使用 helm install 进行一键部署</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">$ cd &#x2F;home&#x2F;k8s&#x2F;charts&#x2F;stable</span><br><span class="line">$ helm install --name dokuwiki --set &quot;ingress.enabled&#x3D;true,ingress.hosts[0].name&#x3D;wiki.hi-linux.com,persistence.apache.storageClass&#x3D;course-nfs-storage,persistence.apache.size&#x3D;500Mi,persistence.dokuwiki.storageClass&#x3D;course-nfs-storage,persistence.dokuwiki.size&#x3D;500Mi&quot;  dokuwiki</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Secret</span><br><span class="line">NAME               TYPE    DATA  AGE</span><br><span class="line">dokuwiki-dokuwiki  Opaque  1     6m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;PersistentVolumeClaim</span><br><span class="line">NAME                        STATUS  VOLUME                                    CAPACITY  ACCESS MODES  STORAGECLASS        AGE</span><br><span class="line">dokuwiki-dokuwiki-apache    Bound   pvc-1bfd0981-9af0-11e8-9a96-001c42c61a79  500Mi     RWO           course-nfs-storage  6m</span><br><span class="line">dokuwiki-dokuwiki-dokuwiki  Bound   pvc-1bffad3d-9af0-11e8-9a96-001c42c61a79  500Mi     RWO           course-nfs-storage  6m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME               TYPE          CLUSTER-IP     EXTERNAL-IP  PORT(S)                   AGE</span><br><span class="line">dokuwiki-dokuwiki  LoadBalancer  10.254.95.241  &lt;pending&gt;    80:8592&#x2F;TCP,443:8883&#x2F;TCP  6m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;Deployment</span><br><span class="line">NAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">dokuwiki-dokuwiki  1        1        1           1          6m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;Ingress</span><br><span class="line">NAME               HOSTS              ADDRESS  PORTS  AGE</span><br><span class="line">dokuwiki-dokuwiki  wiki.hi-linux.com  80       6m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                               READY  STATUS   RESTARTS  AGE</span><br><span class="line">dokuwiki-dokuwiki-bf9fb965c-d9x2w  1&#x2F;1    Running  1         6m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line"></span><br><span class="line">** Please be patient while the chart is being deployed **</span><br><span class="line"></span><br><span class="line">1. Get the DokuWiki URL indicated on the Ingress Rule and associate it to your cluster external IP:</span><br><span class="line"></span><br><span class="line">   export CLUSTER_IP&#x3D;$(minikube ip) # On Minikube. Use: &#96;kubectl cluster-info&#96; on others K8s clusters</span><br><span class="line">   export HOSTNAME&#x3D;$(kubectl get ingress --namespace default dokuwiki-dokuwiki -o jsonpath&#x3D;&#39;&#123;.spec.rules[0].host&#125;&#39;)</span><br><span class="line">   echo &quot;Dokuwiki URL: http:&#x2F;&#x2F;$HOSTNAME&#x2F;&quot;</span><br><span class="line">   echo &quot;$CLUSTER_IP  $HOSTNAME&quot; | sudo tee -a &#x2F;etc&#x2F;hosts</span><br><span class="line"></span><br><span class="line">2. Login with the following credentials</span><br><span class="line"></span><br><span class="line">  echo Username: user</span><br><span class="line">  echo Password: $(kubectl get secret --namespace default dokuwiki-dokuwiki -o jsonpath&#x3D;&quot;&#123;.data.dokuwiki-password&#125;&quot; | base64 --decode)</span><br></pre></td></tr></table></figure><ul><li>查看部署完成后状态</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm list</span><br><span class="line">NAME         REVISIONUPDATED                 STATUS  CHART              NAMESPACE</span><br><span class="line">dokuwiki     1       Wed Aug  8 17:47:48 2018DEPLOYEDdokuwiki-2.0.3     default</span><br></pre></td></tr></table></figure><ul><li>在后端存储上查看对应的数据目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 应用对应的数据目录已经自动创建</span><br><span class="line">$ ls &#x2F;data&#x2F;kubernetes&#x2F;*dokuwiki* -ld</span><br><span class="line">drwxrwxrwx 3 root   root   4096 Aug  8 17:52 &#x2F;data&#x2F;kubernetes&#x2F;default-dokuwiki-dokuwiki-apache-pvc-1bfd0981-9af0-11e8-9a96-001c42c61a79</span><br><span class="line">drwxrwxrwx 5 daemon daemon 4096 Aug  8 17:53 &#x2F;data&#x2F;kubernetes&#x2F;default-dokuwiki-dokuwiki-dokuwiki-pvc-1bffad3d-9af0-11e8-9a96-001c42c61a79</span><br><span class="line"></span><br><span class="line"># 查看应用对应的数据目录下文件</span><br><span class="line">$ ls  &#x2F;data&#x2F;kubernetes&#x2F;default-dokuwiki-dokuwiki-dokuwiki-pvc-1bffad3d-9af0-11e8-9a96-001c42c61a79&#x2F; -l</span><br><span class="line">total 12</span><br><span class="line">drwxr-xr-x  2 daemon daemon 4096 Aug  8 17:49 conf</span><br><span class="line">drwxr-xr-x 12 daemon daemon 4096 Aug  8 17:48 data</span><br><span class="line">drwxr-xr-x  5 daemon daemon 4096 Aug  8 17:48 lib</span><br><span class="line"></span><br><span class="line">$ ls  &#x2F;data&#x2F;kubernetes&#x2F;default-dokuwiki-dokuwiki-apache-pvc-1bfd0981-9af0-11e8-9a96-001c42c61a79&#x2F;conf&#x2F;</span><br><span class="line">bitnami  deflate.conf  extra  httpd.conf  magic  mime.types  original  vhosts</span><br></pre></td></tr></table></figure><ul><li>访问 Web</li></ul><p>根据提示生成相应的登陆用户名和密码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ echo Username: user</span><br><span class="line">Username: user</span><br><span class="line"></span><br><span class="line">$ echo Password: $(kubectl get secret --namespace default dokuwiki-dokuwiki -o jsonpath&#x3D;&quot;&#123;.data.dokuwiki-password&#125;&quot; | base64 --decode)</span><br><span class="line">Password: e2GrABBkwF</span><br></pre></td></tr></table></figure><p>通过浏览器访问该应用。效果图如下：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-ingress01.png" alt=""></p><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RmDscuQ" target="_blank" rel="noopener">http://t.cn/RmDscuQ</a><br><a href="http://t.cn/RDqXk2U" target="_blank" rel="noopener">http://t.cn/RDqXk2U</a><br><a href="http://t.cn/RDqX1qi" target="_blank" rel="noopener">http://t.cn/RDqX1qi</a><br><a href="http://t.cn/RDqT4Xw" target="_blank" rel="noopener">http://t.cn/RDqT4Xw</a><br><a href="http://t.cn/RmDscuQ" target="_blank" rel="noopener">http://t.cn/RmDscuQ</a><br><a href="http://t.cn/RDqg4D0" target="_blank" rel="noopener">http://t.cn/RDqg4D0</a><br><a href="http://t.cn/RDqkyoC" target="_blank" rel="noopener">http://t.cn/RDqkyoC</a><br><a href="http://t.cn/RDVE0bW" target="_blank" rel="noopener">http://t.cn/RDVE0bW</a><br><a href="http://t.cn/R6GaBUK" target="_blank" rel="noopener">http://t.cn/R6GaBUK</a></p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Kubernetes-Volume-相关概念&quot;&gt;Kubernetes Volume 相关概念&lt;/h3&gt;
&lt;p&gt;缺省情况下，一个运行中的容器对文件系统的写入都是发生在其分层文件系统的可写层。一旦容器运行结束，所有写入都会被丢弃。如果数据需要长期存储，那就需要对容器数据做持久化支持。&lt;/p&gt;
&lt;p&gt;Kubernetes 和 Docker 类似，也是通过 Volume 的方式提供对存储的支持。Volume 被定义在 Pod 上，可以被 Pod 里的多个容器挂载到相同或不同的路径下。Kubernetes 中 Volume 的 概念与Docker 中的 Volume 类似，但不完全相同。具体区别如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 中的 Volume 与 Pod 的生命周期相同，但与容器的生命周期不相关。当容器终止或重启时，Volume 中的数据也不会丢失。&lt;/li&gt;
&lt;li&gt;当 Pod 被删除时，Volume 才会被清理。并且数据是否丢失取决于 Volume 的具体类型，比如：emptyDir 类型的 Volume 数据会丢失，而 PV 类型的数据则不会丢失。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Volume 的核心是目录，可以通过 Pod 中的容器来访问。该目录是如何形成的、支持该目录的介质以及其内容取决于所使用的特定卷类型。要使用 Volume，需要为 Pod 指定为 Volume (&lt;code&gt;spec.volumes&lt;/code&gt; 字段) 以及将它挂载到容器的位置 (&lt;code&gt;spec.containers.volumeMounts&lt;/code&gt; 字段)。Kubernetes 支持多种类型的卷，一个 Pod 可以同时使用多种类型的 Volume。&lt;/p&gt;
&lt;p&gt;容器中的进程看到的是由其 Docker 镜像和 Volume 组成的文件系统视图。 Docker 镜像位于文件系统层次结构的根目录，任何 Volume 都被挂载在镜像的指定路径中。Volume 无法挂载到其他 Volume 上或与其他 Volume 的硬连接。Pod 中的每个容器都必须独立指定每个 Volume 的挂载位置。&lt;/p&gt;
&lt;p&gt;Kubernetes 目前支持多种 Volume 类型，大致如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;awsElasticBlockStore&lt;/li&gt;
&lt;li&gt;azureDisk&lt;/li&gt;
&lt;li&gt;azureFile&lt;/li&gt;
&lt;li&gt;cephfs&lt;/li&gt;
&lt;li&gt;csi&lt;/li&gt;
&lt;li&gt;downwardAPI&lt;/li&gt;
&lt;li&gt;emptyDir&lt;/li&gt;
&lt;li&gt;fc (fibre channel)&lt;/li&gt;
&lt;li&gt;flocker&lt;/li&gt;
&lt;li&gt;gcePersistentDisk&lt;/li&gt;
&lt;li&gt;gitRepo&lt;/li&gt;
&lt;li&gt;glusterfs&lt;/li&gt;
&lt;li&gt;hostPath&lt;/li&gt;
&lt;li&gt;iscsi&lt;/li&gt;
&lt;li&gt;local&lt;/li&gt;
&lt;li&gt;nfs&lt;/li&gt;
&lt;li&gt;persistentVolumeClaim&lt;/li&gt;
&lt;li&gt;projected&lt;/li&gt;
&lt;li&gt;portworxVolume&lt;/li&gt;
&lt;li&gt;quobyte&lt;/li&gt;
&lt;li&gt;rbd&lt;/li&gt;
&lt;li&gt;scaleIO&lt;/li&gt;
&lt;li&gt;secret&lt;/li&gt;
&lt;li&gt;storageos&lt;/li&gt;
&lt;li&gt;vsphereVolume&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;注：这些 Volume 并非全部都是持久化的，比如: emptyDir、secret、gitRepo 等，就会随着 Pod 的消亡而消失。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>浅谈 DDoS 攻击与防御</title>
    <link href="https://www.hi-linux.com/posts/50873.html"/>
    <id>https://www.hi-linux.com/posts/50873.html</id>
    <published>2018-08-20T01:00:00.000Z</published>
    <updated>2018-08-24T06:09:58.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h3 id="什么是-ddos">什么是 DDoS</h3><p>DDoS 是英文 Distributed Denial of Service 的缩写，中文译作分布式拒绝服务。那什么又是拒绝服务（Denial of Service）呢？凡是能导致合法用户不能够正常访问网络服务的行为都算是拒绝服务攻击。也就是说拒绝服务攻击的目的非常明确，就是要阻止合法用户对正常网络资源的访问，从而达成攻击者不可告人的目的。</p><p>分布式拒绝服务攻击一旦被实施，攻击网络包就会从很多 DoS 攻击源犹如洪水般涌向受害主机。从而把合法用户的网络请求淹没，导致合法用户无法正常访问服务器的网络资源。</p><h3 id="ddos-攻击方式分类">DDoS 攻击方式分类</h3><ul><li>反射型</li></ul><p>一般而言，我们会根据针对的协议类型和攻击方式的不同，把 DDoS 分成 SYN Flood、ACK Flood、Connection Flood、UDP Flood、NTP Flood、SSDP Flood、DNS Flood、HTTP Flood、ICMP Flood、CC 等各类攻击类型。</p><p>每一种攻击类型都有其特点，而反射型的 DDoS 攻击是一种新的变种。攻击者并不直接攻击目标服务的 IP，而是利用互联网的某些特殊服务开放的服务器，通过伪造被攻击者的 IP 地址向有开放服务的服务器发送构造的请求报文，该服务器会将数倍于请求报文的回复数据发送到被攻击 IP，从而对后者间接形成 DDoS 攻击。</p><p>如下图所示，这里的攻击者（Attacker，实际情况中更多的会利用傀儡机进行攻击）不直接把攻击包发给受害者，而是冒充受害者给放大器（Amplifiers）发包，然后通过放大器再反射给受害者。</p><a id="more"></a><p><img src="https://www.hi-linux.com/img/linux/ddos1.png" alt=""></p><p>在反射型攻击中，攻击者利用了网络协议的缺陷或者漏洞进行 IP 欺骗，主要是因为很多协议（例如 ICMP、UDP 等）对源 IP 不进行认证。同时，要达到更好的攻击效果，黑客一般会选择具有放大效果的协议服务进行攻击。</p><p>总结一下就是利用 IP 欺骗进行反射和放大，从而达到四两拨千斤的效果。目前常见的反射攻击有：DNS 反射攻击、NTP 反射攻击、SSDP 反射攻击等。</p><blockquote><p>注：将源地址设为假的无法回应，即为 SYN Flood 攻击。制造流量和攻击目标收到的流量为 1:1，回报率低。</p></blockquote><ul><li>流量放大型</li></ul><p>通过递归等手法将攻击流量放大的攻击类型，比如：以反射型中常见的 SSDP 协议为例，攻击者将 Search type 设置为 ALL。搜索所有可用的设备和服务，这种递归效果产生的放大倍数是非常大的，攻击者只需要以较小的伪造源地址的查询流量就可以制造出几十甚至上百倍的应答流量发送至目标。</p><ul><li>混合型</li></ul><p>在实际情况中，攻击者只求达到打垮对方的目的。发展到现在，高级攻击者已经不倾向使用单一的攻击手段。而是根据目标系统的具体环境灵动组合，发动多种攻击手段。</p><p>比如：TCP 和 UDP、网络层和应用层攻击同时进行，这样的攻击既具备了海量的流量，又利用了协议、系统的缺陷，尽其所能地展开攻势。对于被攻击目标来说，需要面对不同协议、不同资源的分布式的攻击，分析、响应和处理的成本就会大大增加。</p><p><img src="https://www.hi-linux.com/img/linux/ddos2.png" alt=""></p><ul><li>脉冲波型</li></ul><p>这是一种新型的 DDoS 攻击方法，给某些 DDoS 攻击解决方案带来了问题，因为它允许攻击者攻击以前认为是安全的服务器。之所以将这种新技术命名为脉冲波，是由于其攻击流量展现出来的图形看起来很像不连贯的重复的脉冲状。这类攻击通常呈现一个有上有下的斜三角形的形状，这个过程体现了攻击者正在慢慢地组装机器人并将目标对准待攻击的目标。</p><p><img src="https://www.hi-linux.com/img/linux/ddos3.png" alt=""></p><p>一次新的脉冲波攻击从零开始，在很短的时间跨度内达到最大值，然后归零，再回到最大值，如此循环重复，中间的时间间隔很短。脉冲波型 DDoS 相对难以防御，因为其攻击方式避开了触发自动化的防御机制。</p><p><img src="https://www.hi-linux.com/img/linux/ddos4.png" alt=""></p><ul><li>链路泛洪</li></ul><p>随着 DDoS 攻击技术的发展，又出现了一种新型的攻击方式 Link Flooding Attack，这种方式不直接攻击目标而是以堵塞目标网络的上一级链路为目的。对于使用了 IP Anycast 的企业网络来说，常规的 DDoS 攻击流量会被分摊到不同地址的基础设施，这样能有效缓解大流量攻击。所以攻击者发明了一种新方法，攻击至目标网络 traceroute 的倒数第二跳，即上联路由，致使链路拥塞。</p><p><img src="https://www.hi-linux.com/img/linux/ddos5.png" alt=""></p><h3 id="常见-ddos-攻击方法">常见 DDoS 攻击方法</h3><p>DDoS 攻击从层次上可分为网络层攻击与应用层攻击，从攻击手法上可分为快型流量攻击与慢型流量攻击，但其原理都是造成资源过载，导致服务不可用。</p><h4 id="网络层-ddos-攻击">网络层 DDoS 攻击</h4><p>网络层 DDoS 攻击常见手段有：SYN Flood、ACK Flood、Connection Flood、UDP Flood、ICMP Flood、TCP Flood、Proxy Flood 等。</p><ul><li>SYN Flood 攻击</li></ul><p>SYN Flood 攻击是一种利用 TCP 协议缺陷，发送大量伪造的 TCP 连接请求，从而使得被攻击方资源耗尽（CPU 满负载或内存不足）的攻击方式。建立 TCP连接，需要三次握手（客户端发送 SYN 报文、服务端收到请求并返回报文表示接受、客户端也返回确认，完成连接）。</p><p>SYN Flood 就是用户向服务器发送报文后突然死机或掉线，那么服务器在发出应答报文后就无法收到客户端的确认报文（第三次握手无法完成），这时服务器端一般会重试并等待一段时间（至少 30s）后再丢弃这个未完成的连接。</p><p>一个用户出现异常导致服务器的一个线程等待一会儿并不是大问题，但恶意攻击者大量模拟（构造源 IP 去发送 SYN 包）这种情况，服务器端为了维护数以万计的半连接而消耗非常多的资源，结果往往是无暇理睬正常客户的请求，甚至崩溃。从正常客户的角度看来，网站失去了响应，无法访问。</p><p><img src="https://www.hi-linux.com/img/linux/ddos6.png" alt=""></p><ul><li>ACK Flood</li></ul><p>ACK Flood 攻击是在 TCP 连接建立之后进行的。所有数据传输的 TCP 报文都是带有 ACK 标志位的，主机在接收到一个带有 ACK 标志位的数据包的时候，需要检查该数据包所表示的连接四元组是否存在。如果存在则检查该数据包所表示的状态是否合法，然后再向应用层传递该数据包。如果在检查中发现该数据包不合法（例如：该数据包所指向的目的端口在本机并未开放），则主机操作系统协议栈会回应 RST 包告诉对方此端口不存在。</p><p>这里，服务器要做两个动作：查表、回应 ACK/RST。对比主机以及防火墙在接收到 ACK 报文和 SYN 报文时所做动作的复杂程度，显然 ACK 报文带来的负载要小得多。这种攻击方式显然没有 SYN Flood 给服务器带来的冲击大，因此攻击者一定要用大流量 ACK 小包冲击才会对服务器造成影响。所以在实际环境中，只有当攻击程序每秒钟发送 ACK 报文的速率达到一定的程度，才能使主机和防火墙的负载有大的变化。</p><p>当发包速率很大的时候，主机操作系统将耗费大量的精力接收报文、判断状态，同时要主动回应 RST 报文，正常的数据包就可能无法得到及时的处理。这时候客户端的表现就是访问页面反应很慢，丢包率较高。但是状态检测的防火墙通过判断 ACK 报文的状态是否合法，借助其强大的硬件能力可以较为有效的过滤攻击报文。当然如果攻击流量非常大，由于需要维护很大的连接状态表同时要检查数量巨大的 ACK 报文的状态，防火墙也会不堪重负导致网络瘫痪。</p><p>目前 ACK Flood 并没有成为攻击的主流，而通常是与其他攻击方式组合在一起使用。</p><p><img src="https://www.hi-linux.com/img/linux/ddos7.png" alt=""></p><ul><li>Connection Flood</li></ul><p>Connection Flood 是典型的并且非常有效的利用小流量冲击大带宽网络服务的攻击方式。这种攻击的原理是利用真实的 IP 地址向服务器发起大量的连接，并且建立连接之后很长时间不释放。长期占用服务器的资源，造成服务器上残余连接 (WAIT 状态) 过多，效率降低，甚至资源耗尽，无法响应其它客户所发起的连接。</p><p>其中一种攻击方法是每秒钟向服务器发起大量的连接请求，这类似于固定源 IP 的 SYN Flood 攻击，不同的是采用了真实的源 IP 地址。通常这可以在防火墙上限制每个源 IP 地址每秒钟的连接数来达到防护目的。</p><p>但现在已有工具采用慢速连接的方式，也即几秒钟才和服务器建立一个连接，连接建立成功之后并不释放并定时发送垃圾数据包给服务器使连接得以长时间保持。这样一个 IP 地址就可以和服务器建立成百上千的连接，而服务器可以承受的连接数是有限的，这就达到了拒绝服务的效果。</p><p><img src="https://www.hi-linux.com/img/linux/ddos8.png" alt=""></p><ul><li>UDP Flood 攻击</li></ul><p>由于 UDP 是一种无连接的协议，因此攻击者可以伪造大量的源 IP 地址去发送 UDP 包，此种攻击属于大流量攻击。正常应用情况下，UDP 包双向流量会基本相等，因此在消耗对方资源的时候也在消耗自己的资源。</p><ul><li>ICMP Flood 攻击</li></ul><p>此攻击属于大流量攻击，其原理就是不断发送不正常的 ICMP 包（所谓不正常就是 ICMP 包内容很大），导致目标带宽被占用。但其本身资源也会被消耗，并且目前很多服务器都是禁 ping 的（在防火墙里可以屏蔽 ICMP 包），因此这种方式已经落伍。</p><ul><li>Smurf 攻击</li></ul><p>这种攻击类似于 ICMP Flood 攻击，但它能巧妙地修改进程。Smurf 攻击通过使用将回复地址设置成受害网络的广播地址的 ICMP 应答请求数据包，来淹没受害主机。最终导致该网络的所有主机都对此 ICMP 应答请求做出答复，导致网络阻塞。更加复杂的 Smurf 将源地址改为第三方的受害者，最终导致第三方崩溃。</p><h4 id="应用层-ddos-攻击">应用层 DDoS 攻击</h4><p>应用层 DDoS 攻击不是发生在网络层，是发生在 TCP 建立握手成功之后，应用程序处理请求的时候。常见的有：CC 攻击、DNS Flood、慢速连接攻击等。</p><ul><li>CC 攻击</li></ul><p>CC 攻击（Challenge Collapsar）是 DDoS 攻击的一种，其前身名为 Fatboy 攻击，也是一种常见的网站攻击方法。CC 攻击还有一段比较有趣的历史，Collapsar 是绿盟科技的一款防御 DDoS 攻击的产品品牌，Collapasar 在对抗拒绝服务攻击的领域内具有比较高的影响力和口碑。然而黑客为了挑衅，研发了一款 Challenge Collapasar 工具简称 CC，表示要向 Collapasar 发起挑战。</p><p>CC 攻击的原理就是借助代理服务器针对目标系统的消耗资源比较大的页面不断发起正常的请求，造成对方服务器资源耗尽，一直到宕机崩溃。因此在发送 CC 攻击前，我们需要寻找加载比较慢，消耗资源比较多的网页。比如：需要查询数据库的页面、读写硬盘的文件等。相比其它的 DDoS 攻击 CC 更有技术含量一些，这种攻击你见不到真实源 IP。见不到特别大的异常流量，但造成服务器无法进行正常连接。</p><p><img src="https://www.hi-linux.com/img/linux/ddos9.png" alt=""></p><ul><li>Slowloris 攻击</li></ul><p>Slowloris 是一种慢速连接攻击，Slowloris 是利用 Web Server 的漏洞或设计缺陷，直接造成拒绝服务。其原理是：以极低的速度往服务器发送 HTTP 请求，Apache 等中间件默认会设置最大并发链接数，而这种攻击就是会持续保持连接，导致服务器链接饱和不可用。Slowloris 有点类似于 SYN Flood 攻击，只不过 Slowloris 是基于 HTTP 协议。</p><p>Slowloris  PoC</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 构造以下畸形 HTTP 请求包</span><br><span class="line">GET &#x2F; HTTP&#x2F;1.1\r\n</span><br><span class="line">Host: Victim host\r\n</span><br><span class="line">User-Agent: Mozilla&#x2F;4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident&#x2F;4.0; .NET CLR 1.1.4322; .NET CLR 2.0.503l3; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; MSOffice 12)\r\n</span><br><span class="line">Content-Length: 42\r\n</span><br></pre></td></tr></table></figure><p>完整的 HTTP 请求头结尾应该是两次的 <code>\r\n\r\n</code>，这里少了一次，因此服务器将会一直等待。</p><ul><li>Slow Attack</li></ul><p>Slow Attack 也是一种慢速 DoS 攻击，它通过消耗服务器的系统资源和连接数，导致 Web 服务器无法正常工作。常见的攻击方式包括 Slow Header、Slow Body 和 Slow Read。</p><ol><li>Slow Header：正常的 HTTP Header 以两个 CLRF 结尾，通过发送只包含一个 CLRF 的畸形 Header 请求来占用 Web 服务器连接，从而达到消耗掉服务器所有可用的连接数。最终造成 Web 服务器资源饱和并拒绝新的服务。</li><li>Slow Read：向服务器请求很大的文件，然后通过设置 TCP 滑动窗口较小值，导致服务器以极慢的速度传输文件。这样，就会占用服务器大量的内存，从而造成拒绝服务。</li><li>Slow Body：在向服务器发送 HTTP Post 包时，指定一个非常大的 Content-Length 值，然后以极低的速度发包并保持连接不断，最终导致服务器连接饱和不可用。</li></ol><p>Kali Linux 提供的专用测试工具 SlowHTTPTest 能够实现以上三种 Slow Attack 方式。</p><ul><li>JavaScript DDoS</li></ul><p>基于 JavaScript 的 DDoS 攻击利用的工具是普通网民的上网终端，这也意味着只要装有浏览器的电脑，都能被用作为 DDoS 攻击者的工具。当被操纵的浏览器数量达到一定程度时，这种 DDoS 攻击方式将会带来巨大的破坏性。</p><p>攻击者会在海量访问的网页中嵌入指向攻击目标网站的恶意 JavaScript 代码，当互联网用户访问该网页时，则流量被指向攻击目标网站。比较典型攻击事件：GitHub DDoS 攻击。</p><ul><li>ReDoS 攻击</li></ul><p>ReDoS (Regular expression Denial of Service)， 中文译作正则表达式拒绝服务攻击。开发人员使用了正则表达式来对用户输入的数据进行有效性校验，当编写校验的正则表达式存在缺陷或者不严谨时，攻击者可以构造特殊的字符串来大量消耗服务器的系统资源，从而造成服务器的服务中断或停止。 更详细介绍可参考：「<a href="https://cloud.tencent.com/developer/article/1041326" target="_blank" rel="noopener">浅析 ReDoS 的原理与实践</a>」一文。</p><ul><li>DNS Query Flood</li></ul><p>DNS 作为互联网的核心服务之一，自然也是 DDoS 攻击的一大主要目标。DNS Query Flood 采用的方法是操纵大量傀儡机器，向目标服务器发送大量的域名解析请求。服务器在接收到域名解析请求时，首先会在服务器上查找是否有对应的缓存，若查找不到且该域名无法直接解析时，便向其上层 DNS 服务器递归查询域名信息。</p><p>通常，攻击者请求解析的域名是随机生成或者是网络上根本不存在的域名。由于在本地域名服务器无法查到对应的结果，本地域名服务器必须使用递归查询向上层域名服务器提交解析请求，引起连锁反应。解析过程给本地域名服务器带来一定的负载，每秒钟域名解析请求超过一定的数量就会造成域名服务器解析域名超时。</p><p><img src="https://www.hi-linux.com/img/linux/ddos10.png" alt=""></p><p>根据微软的统计数据，一台 DNS 服务器所能承受的动态域名查询的上限是每秒钟 9000 个请求。而一台 P3 的 PC 机上可以轻易地构造出每秒钟几万个域名解析请求，足以使一台硬件配置极高的 DNS 服务器瘫痪，由此可见 DNS 服务器的脆弱性。</p><h4 id="无线-ddos-攻击">无线 DDoS 攻击</h4><ul><li>Auth Flood 攻击</li></ul><p>Auth Flood 攻击，即身份验证洪水攻击。该攻击目标主要针对那些处于通过验证和 AP 建立关联的关联客户端，攻击者将向 AP 发送大量伪造的身份验证请求帧（伪造的身份验证服务和状态代码），当收到大量伪造的身份验证请求超过所能承受的能力时，AP将断开其他无线服务连接。</p><ul><li>Deauth Flood 攻击</li></ul><p>Deauth Flood 攻击即为取消验证洪水攻击，它旨在通过欺骗从 AP 到客户端单播地址的取消身份验证帧来将客户端转为未关联 / 未认证的状态。对于目前的工具来说，这种形式的攻击在打断客户无线服务方面非常有效和快捷。一般来说，在攻击者发送另一个取消身份验证帧之前，客户端会重新关联和认证以再次获取服务。攻击者反复欺骗取消身份验证帧就能使所有客户端持续拒绝服务。</p><ul><li>Association Flood 攻击</li></ul><p>Association Flood 攻击即为关联洪水攻击。在无线路由器或者接入点内置一个列表即为连接状态表，里面可显示出所有与该 AP 建立连接的无线客户端状态。它试图通过利用大量模仿和伪造的无线客户端关联来填充 AP 的客户端关联表，从而达到淹没 AP 的目的。</p><p>由于开放身份验证（空身份验证）允许任何客户端通过身份验证后关联。利用这种漏洞的攻击者可以通过创建多个到达已连接或已关联的客户端来模仿很多客户端，从而淹没目标 AP 的客户端关联表。</p><ul><li>Disassociation Flood 攻击</li></ul><p>Disassociation Flood 攻击即为取消关联洪水攻击，和 Deauth Flood 攻击表现方式很相似。它通过欺骗从 AP 到客户端的取消关联帧来强制客户端成为未关联 / 未认证的状态。一般来说，在攻击者发送另一个取消关联帧之前，客户端会重新关联以再次获取服务。攻击者反复欺骗取消关联帧就能使客户端持续拒绝服务。</p><p>Disassociation Broadcast 攻击和 Disassociation Flood 攻击原理基本一致，只是在发送程度及使用工具上有所区别。前者很多时候用于配合进行无线中间人攻击，而后者常用于目标确定的点对点无线 DoS，比如：破坏或干扰指定机构或部门的无线接入点等。</p><ul><li>RF Jamming 攻击</li></ul><p>RF Jamming 攻击即为 RF 干扰攻击。该攻击是通过发出干扰射频达到破坏正常无线通信的目的。而前面几种攻击主要是基于无线通信过程及协议的。RF 为射频，主要包括无线信号发射机及收信机等。</p><h3 id="ddos-攻击现象判定方法">DDoS 攻击现象判定方法</h3><ul><li>SYN 类攻击判断</li></ul><ol><li>服务器 CPU 占用率很高。</li><li>出现大量的 SYN_RECEIVED 的网络连接状态。</li><li>网络恢复后，服务器负载瞬时变高。网络断开后瞬时负载下将。</li></ol><ul><li>UDP 类攻击判断</li></ul><ol><li>服务器 CPU 占用率很高。</li><li>网卡每秒接受大量的数据包。</li><li>网络 TCP 状态信息正常。</li></ol><ul><li>CC 类攻击判断</li></ul><ol><li>服务器 CPU 占用率很高。</li><li>Web 服务器出现类似 Service Unavailable 提示。</li><li>出现大量的 ESTABLISHED 的网络连接状态且单个 IP 高达几十个甚至上百个连接。</li><li>用户无法正常访问网站页面或打开过程非常缓慢，软重启后短期内恢复正常，几分钟后又无法访问。</li></ol><h3 id="ddos-攻击防御方法">DDoS 攻击防御方法</h3><ul><li>网络层 DDoS 防御</li></ul><ol><li>限制单 IP 请求频率。</li><li>网络架构上做好优化，采用负载均衡分流。</li><li>防火墙等安全设备上设置禁止 ICMP 包等。</li><li>通过 DDoS 硬件防火墙的数据包规则过滤、数据流指纹检测过滤、及数据包内容定制过滤等技术对异常流量进行清洗过滤。</li><li>采用 ISP 近源清洗，使用电信运营商提供的近源清洗和流量压制，避免全站服务对所有用户彻底无法访问。这是对超过自身带宽储备和自身 DDoS 防御能力之外超大流量的补充性缓解措施。</li></ol><ul><li>应用层 DDoS 防御</li></ul><ol><li>优化操作系统的 TCP/IP 栈。</li><li>应用服务器严格限制单个 IP 允许的连接数和 CPU 使用时间。</li><li>编写代码时，尽量实现优化并合理使用缓存技术。尽量让网站静态化，减少不必要的动态查询。网站静态化不仅能大大提高抗攻击能力，而且还给骇客入侵带来不少麻烦，至少到现在为止关于 HTML 的溢出还没出现。</li><li>增加 WAF（Web Application Firewall）设备，WAF 的中文名称叫做 Web 应用防火墙。Web 应用防火墙是通过执行一系列针对 HTTP / HTTPS 的安全策略来专门为 Web 应用提供保护的一款产品。</li><li>使用 CDN / 云清洗，在攻击发生时，进行云清洗。通常云清洗厂商策略有以下几步：预先设置好网站的 CNAME，将域名指向云清洗厂商的 DNS 服务器；在一般情况下，云清洗厂商的 DNS 仍将穿透 CDN 的回源的请求指向源站，在检测到攻击发生时，域名指向自己的清洗集群，然后再将清洗后的流量回源。</li><li>CDN 仅对 Web 类服务有效，针对游戏类 TCP 直连的服务无效。这时可以使用 DNS 引流 + ADS (Anti-DDoS System) 设备来清洗，还有在客户端和服务端通信协议做处理（如：封包加标签，依赖信息对称等）。</li></ol><p>DDoS 攻击究其本质其实是无法彻底防御的，我们能做得就是不断优化自身的网络和服务架构，来提高对 DDoS 的防御能力。</p><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RrSkw6a" target="_blank" rel="noopener">http://t.cn/RrSkw6a</a><br><a href="http://t.cn/RrSkNKe" target="_blank" rel="noopener">http://t.cn/RrSkNKe</a><br><a href="http://t.cn/RrSFJ1B" target="_blank" rel="noopener">http://t.cn/RrSFJ1B</a><br><a href="http://t.cn/RrovtI3" target="_blank" rel="noopener">http://t.cn/RrovtI3</a><br><a href="http://t.cn/RrKGEIb" target="_blank" rel="noopener">http://t.cn/RrKGEIb</a><br><a href="http://t.cn/RCwYkYf" target="_blank" rel="noopener">http://t.cn/RCwYkYf</a><br><a href="http://t.cn/RrKIAlN" target="_blank" rel="noopener">http://t.cn/RrKIAlN</a><br><a href="http://t.cn/RrKQ8j8" target="_blank" rel="noopener">http://t.cn/RrKQ8j8</a><br><a href="http://t.cn/RcCzPCO" target="_blank" rel="noopener">http://t.cn/RcCzPCO</a></p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;什么是-DDoS&quot;&gt;什么是 DDoS&lt;/h3&gt;
&lt;p&gt;DDoS 是英文 Distributed Denial of Service 的缩写，中文译作分布式拒绝服务。那什么又是拒绝服务（Denial of Service）呢？凡是能导致合法用户不能够正常访问网络服务的行为都算是拒绝服务攻击。也就是说拒绝服务攻击的目的非常明确，就是要阻止合法用户对正常网络资源的访问，从而达成攻击者不可告人的目的。&lt;/p&gt;
&lt;p&gt;分布式拒绝服务攻击一旦被实施，攻击网络包就会从很多 DoS 攻击源犹如洪水般涌向受害主机。从而把合法用户的网络请求淹没，导致合法用户无法正常访问服务器的网络资源。&lt;/p&gt;
&lt;h3 id=&quot;DDoS-攻击方式分类&quot;&gt;DDoS 攻击方式分类&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;反射型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一般而言，我们会根据针对的协议类型和攻击方式的不同，把 DDoS 分成 SYN Flood、ACK Flood、Connection Flood、UDP Flood、NTP Flood、SSDP Flood、DNS Flood、HTTP Flood、ICMP Flood、CC 等各类攻击类型。&lt;/p&gt;
&lt;p&gt;每一种攻击类型都有其特点，而反射型的 DDoS 攻击是一种新的变种。攻击者并不直接攻击目标服务的 IP，而是利用互联网的某些特殊服务开放的服务器，通过伪造被攻击者的 IP 地址向有开放服务的服务器发送构造的请求报文，该服务器会将数倍于请求报文的回复数据发送到被攻击 IP，从而对后者间接形成 DDoS 攻击。&lt;/p&gt;
&lt;p&gt;如下图所示，这里的攻击者（Attacker，实际情况中更多的会利用傀儡机进行攻击）不直接把攻击包发给受害者，而是冒充受害者给放大器（Amplifiers）发包，然后通过放大器再反射给受害者。&lt;/p&gt;
    
    </summary>
    
    
      <category term="DDoS" scheme="https://www.hi-linux.com/categories/DDoS/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="DDoS" scheme="https://www.hi-linux.com/tags/DDoS/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 资源管理概述</title>
    <link href="https://www.hi-linux.com/posts/65337.html"/>
    <id>https://www.hi-linux.com/posts/65337.html</id>
    <published>2018-08-16T01:00:00.000Z</published>
    <updated>2018-08-24T05:54:08.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Kubernetes 从创建之初的核心模块之一就是资源调度。想要在生产环境使用好 Kubernetes，必须对它的资源模型以及资源管理非常了解。这篇文章算是对散布在网络上的 Kubernetes 资源管理内容的一个总结。干货文章，强列推荐一读。</p><h3 id="kubernetes-资源简介">Kubernetes 资源简介</h3><h4 id="什么是资源">什么是资源？</h4><p>在 Kubernetes 中，有两个基础但是非常重要的概念：Node 和 Pod。Node 翻译成节点，是对集群资源的抽象；Pod 是对容器的封装，是应用运行的实体。Node 提供资源，而 Pod 使用资源，这里的资源分为计算（CPU、Memory、GPU）、存储（Disk、SSD）、网络（Network Bandwidth、IP、Ports）。这些资源提供了应用运行的基础，正确理解这些资源以及集群调度如何使用这些资源，对于大规模的 Kubernetes 集群来说至关重要，不仅能保证应用的稳定性，也可以提高资源的利用率。</p><p>在这篇文章，我们主要介绍 CPU 和内存这两个重要的资源，它们虽然都属于计算资源，但也有所差距。CPU 可分配的是使用时间，也就是操作系统管理的时间片，每个进程在一定的时间片里运行自己的任务（另外一种方式是绑核，也就是把 CPU 完全分配给某个 Pod 使用，但这种方式不够灵活会造成严重的资源浪费，Kubernetes 中并没有提供）；而对于内存，系统提供的是内存大小。</p><p>CPU 的使用时间是可压缩的，换句话说它本身无状态，申请资源很快，也能快速正常回收；而内存大小是不可压缩的，因为它是有状态的（内存里面保存的数据），申请资源很慢（需要计算和分配内存块的空间），并且回收可能失败（被占用的内存一般不可回收）。</p><p>把资源分成<strong>可压缩</strong>和<strong>不可压缩</strong>，是因为在资源不足的时候，它们的表现很不一样。对于不可压缩资源，如果资源不足，也就无法继续申请资源（内存用完就是用完了），并且会导致 Pod 的运行产生无法预测的错误（应用申请内存失败会导致一系列问题）；而对于可压缩资源，比如 CPU 时间片，即使 Pod 使用的 CPU 资源很多，CPU 使用也可以按照权重分配给所有 Pod 使用，虽然每个人使用的时间片减少，但不会影响程序的逻辑。</p><a id="more"></a><p>在 Kubernetes 集群管理中，有一个非常核心的功能：就是为 Pod 选择一个主机运行。调度必须满足一定的条件，其中最基本的是主机上要有足够的资源给 Pod 使用。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-res1.png" alt="Kubernetes scheduler architecture"></p><p>资源除了和调度相关之外，还和很多事情紧密相连，这正是这篇文章要解释的。</p><h4 id="kubernetes-资源的表示">Kubernetes 资源的表示</h4><p>用户在 Pod 中可以配置要使用的资源总量，Kubernetes 根据配置的资源数进行调度和运行。目前主要可以配置的资源是 CPU 和 Memory，对应的配置字段是 <code>spec.containers[].resource.limits/request.cpu/memory</code>。</p><p>需要注意的是，用户是对每个容器配置 Request 值，所有容器的资源请求之和就是 Pod 的资源请求总量，而我们一般会说 Pod 的资源请求和 Limits。</p><p><code>Limits</code> 和 <code>Requests</code> 的区别我们下面会提到，这里先说说比较容易理解的 CPU 和 Memory。</p><p><code>CPU</code> 一般用核数来标识，一核 CPU 相对于物理服务器的一个超线程核，也就是操作系统 <code>/proc/cpuinfo</code> 中列出来的核数。因为对资源进行了池化和虚拟化，因此 Kubernetes 允许配置非整数个的核数，比如 <code>0.5</code> 是合法的，它标识应用可以使用半个 CPU 核的计算量。CPU 的请求有两种方式，一种是刚提到的 <code>0.5</code>，<code>1</code> 这种直接用数字标识 CPU 核心数；另外一种表示是 <code>500m</code>，它等价于 <code>0.5</code>，也就是说 <code>1 Core = 1000m</code>。</p><p>内存比较容易理解，是通过字节大小指定的。如果直接一个数字，后面没有任何单位，表示这么多字节的内存；数字后面还可以跟着单位， 支持的单位有 <code>E</code>、<code>P</code>、<code>T</code>、<code>G</code>、<code>M</code>、<code>K</code>，前者分别是后者的 <code>1000</code> 倍大小的关系，此外还支持  <code>Ei</code>、<code>Pi</code>、<code>Ti</code>、<code>Gi</code>、<code>Mi</code>、<code>Ki</code>，其对应的倍数关系是 <code>2^10 = 1024</code>。比如要使用 100M 内存的话，直接写成 <code>100Mi</code>即可。</p><h4 id="节点可用资源">节点可用资源</h4><p>理想情况下，我们希望节点上所有的资源都可以分配给 Pod 使用，但实际上节点上除了运行 Pods 之外，还会运行其他的很多进程：系统相关的进程（比如 SSHD、Udev等），以及 Kubernetes 集群的组件（Kubelet、Docker等）。我们在分配资源的时候，需要给这些进程预留一些资源，剩下的才能给 Pod 使用。预留的资源可以通过下面的参数控制：</p><ul><li><code>--kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi]</code>：控制预留给 Kubernetes 集群组件的 CPU、Memory 和存储资源。</li><li><code>--system-reserved=[cpu=100mi][,][memory=100Mi][,][ephemeral-storage=1Gi]</code>：预留给系统的 CPU、Memory 和存储资源。</li></ul><p>这两块预留之后的资源才是 Pod 真正能使用的，不过考虑到 Eviction 机制（下面的章节会提到），Kubelet 会保证节点上的资源使用率不会真正到 100%，因此 Pod 的实际可使用资源会稍微再少一点。主机上的资源逻辑分配图如下所示：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-res2.png" alt="Kubernetes reserved resource"></p><blockquote><p>NOTE：需要注意的是，Allocatable 不是指当前机器上可以分配的资源，而是指能分配给 Pod 使用的资源总量，一旦 Kubelet 启动这个值是不会变化的。</p></blockquote><p>Allocatable 的值可以在 Node 对象的 <code>status</code> 字段中读取，比如下面这样：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">allocatable:</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">"2"</span></span><br><span class="line">    <span class="attr">ephemeral-storage:</span> <span class="string">"35730597829"</span></span><br><span class="line">    <span class="attr">hugepages-2Mi:</span> <span class="string">"0"</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">3779348Ki</span></span><br><span class="line">    <span class="attr">Pods:</span> <span class="string">"110"</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">"2"</span></span><br><span class="line">    <span class="attr">ephemeral-storage:</span> <span class="string">38770180Ki</span></span><br><span class="line">    <span class="attr">hugepages-2Mi:</span> <span class="string">"0"</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">3881748Ki</span></span><br><span class="line">    <span class="attr">Pods:</span> <span class="string">"110"</span></span><br></pre></td></tr></table></figure><h3 id="kubernetes-资源对象">Kubernetes 资源对象</h3><p>在这部分，我们来介绍 Kubernetes 中提供的让我们管理 Pod 资源的原生对象。</p><h4 id="请求requests和上限limits">请求（Requests）和上限（Limits）</h4><p>前面说过用户在创建 Pod 的时候，可以指定每个容器的 Requests 和 Limits 两个字段，下面是一个实例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="attr">requests:</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">"64Mi"</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">"250m"</span></span><br><span class="line">  <span class="attr">limits:</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">"128Mi"</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">"500m"</span></span><br></pre></td></tr></table></figure><p><code>Requests</code> 是容器请求要使用的资源，Kubernetes 会保证 Pod 能使用到这么多的资源。请求的资源是调度的依据，只有当节点上的可用资源大于 Pod 请求的各种资源时，调度器才会把 Pod 调度到该节点上（如果 CPU 资源足够，内存资源不足，调度器也不会选择该节点）。</p><p>需要注意的是，调度器只关心节点上可分配的资源，以及节点上所有 Pods 请求的资源，而<strong>不关心</strong>节点资源的实际使用情况，换句话说，如果节点上的 Pods 申请的资源已经把节点上的资源用满，即使它们的使用率非常低，比如说 CPU 和内存使用率都低于 10%，调度器也不会继续调度 Pod 上去。</p><p><code>Limits</code> 是 Pod 能使用的资源上限，是实际配置到内核 cgroups 里面的配置数据。对于内存来说，会直接转换成 <code>docker run</code> 命令行的 <code>--memory</code> 大小，最终会配置到 cgroups 对应任务的 <code>/sys/fs/cgroup/memory/……/memory.limit_in_bytes</code> 文件中。</p><blockquote><p>NOTE：如果 Limit 没有配置，则表明没有资源的上限，只要节点上有对应的资源，Pod 就可以使用。</p></blockquote><p>使用 Requests 和 Limits 概念，我们能分配更多的 Pod，提升整体的资源使用率。但是这个体系有个非常重要的问题需要考虑，那就是<strong>怎么去准确地评估 Pod 的资源 Requests</strong>？如果评估地过低，会导致应用不稳定；如果过高，则会导致使用率降低。这个问题需要开发者和系统管理员共同讨论和定义。</p><h4 id="limit-range默认资源配置">Limit Range（默认资源配置)</h4><p>为每个 Pod 都手动配置这些参数是挺麻烦的事情，Kubernetes 提供了 <code>LimitRange</code> 资源，可以让我们配置某个 Namespace 默认的 Request 和 Limit 值，比如下面的实例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">"v1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"LimitRange"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">you-shall-have-limits</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">limits:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">"Container"</span></span><br><span class="line">      <span class="attr">max:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">"2"</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">"1Gi"</span></span><br><span class="line">      <span class="attr">min:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">"100m"</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">"4Mi"</span></span><br><span class="line">      <span class="attr">default:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">"500m"</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">"200Mi"</span></span><br><span class="line">      <span class="attr">defaultRequest:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">"200m"</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">"100Mi"</span></span><br></pre></td></tr></table></figure><p>如果对应 Namespace 创建的 Pod 没有写资源的 Requests 和 Limits 字段，那么它会自动拥有下面的配置信息：</p><ul><li>内存请求是 100Mi，上限是 200Mi</li><li>CPU 请求是 200m，上限是 500m</li></ul><p>当然，如果 Pod 自己配置了对应的参数，Kubernetes 会使用 Pod 中的配置。使用 LimitRange 能够让 Namespace 中的 Pod 资源规范化，便于统一的资源管理。</p><h4 id="资源配额resource-quota">资源配额（Resource Quota）</h4><p>前面讲到的资源管理和调度可以认为 Kubernetes 把这个集群的资源整合起来，组成一个资源池，每个应用（Pod）会自动从整个池中分配资源来使用。默认情况下只要集群还有可用的资源，应用就能使用，并没有限制。Kubernetes 本身考虑到了多用户和多租户的场景，提出了 Namespace 的概念来对集群做一个简单的隔离。</p><p>基于 Namespace，Kubernetes 还能够对资源进行隔离和限制，这就是 Resource Quota 的概念，翻译成资源配额，它限制了某个 Namespace 可以使用的资源总额度。这里的资源包括 CPU、Memory 的总量，也包括 Kubernetes 自身对象（比如 Pod、Services 等）的数量。通过 Resource Quota，Kubernetes 可以防止某个 Namespace 下的用户不加限制地使用超过期望的资源，比如说不对资源进行评估就大量申请 16核 CPU 32G 内存的 Pod。</p><p>下面是一个资源配额的实例，它限制了 Namespace 只能使用 20 核 CPU 和 1G 内存，并且能创建 10 个 Pod、20 个 RC、5 个 Service，可能适用于某个测试场景。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceQuota</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">quota</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">hard:</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">"20"</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">    <span class="attr">Pods:</span> <span class="string">"10"</span></span><br><span class="line">    <span class="attr">replicationcontrollers:</span> <span class="string">"20"</span></span><br><span class="line">    <span class="attr">resourcequotas:</span> <span class="string">"1"</span></span><br><span class="line">    <span class="attr">services:</span> <span class="string">"5"</span></span><br></pre></td></tr></table></figure><p>Resource Quota 能够配置的选项还很多，比如 GPU、存储、Configmaps、PersistentVolumeClaims 等等，更多信息可以参考官方文档。</p><p>Resource Quota 要解决的问题和使用都相对独立和简单，但是它也有一个限制：那就是它不能根据集群资源动态伸缩。一旦配置之后，Resource Quota 就不会改变，即使集群增加了节点，整体资源增多也没有用。Kubernetes 现在没有解决这个问题，但是用户可以通过编写一个 Controller 的方式来自己实现。</p><h3 id="应用优先级">应用优先级</h3><h4 id="qos服务质量">QoS（服务质量）</h4><p>Requests 和 Limits 的配置除了表明资源情况和限制资源使用之外，还有一个隐藏的作用：它决定了 Pod 的 QoS 等级。</p><p>上一节我们提到了一个细节：如果 Pod 没有配置 Limits ，那么它可以使用节点上任意多的可用资源。这类 Pod 能灵活使用资源，但这也导致它不稳定且危险，对于这类 Pod 我们一定要在它占用过多资源导致节点资源紧张时处理掉。优先处理这类 Pod，而不是处理资源使用处于自己请求范围内的 Pod 是非常合理的想法，而这就是 Pod QoS 的含义：根据 Pod 的资源请求把 Pod 分成不同的重要性等级。</p><p>Kubernetes 把 Pod 分成了三个 QoS 等级：</p><ul><li><strong>Guaranteed</strong>：优先级最高，可以考虑数据库应用或者一些重要的业务应用。除非 Pods 使用超过了它们的 Limits，或者节点的内存压力很大而且没有 QoS 更低的 Pod，否则不会被杀死。</li><li><strong>Burstable</strong>：这种类型的 Pod 可以多于自己请求的资源（上限由 Limit 指定，如果 Limit 没有配置，则可以使用主机的任意可用资源），但是重要性认为比较低，可以是一般性的应用或者批处理任务。</li><li><strong>Best Effort</strong>：优先级最低，集群不知道 Pod 的资源请求情况，调度不考虑资源，可以运行到任意节点上（从资源角度来说），可以是一些临时性的不重要应用。Pod 可以使用节点上任何可用资源，但在资源不足时也会被优先杀死。</li></ul><p>Pod 的 Requests 和 Limits 是如何对应到这三个 QoS 等级上的，可以用下面一张表格概括：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-res3.png" alt="Pod QuS mapping"></p><p>看到这里，你也许看出来一个问题了：<strong>如果不配置 Requests 和 Limits，Pod 的 QoS 竟然是最低的</strong>。没错，所以推荐大家理解 QoS 的概念，并且按照需求<strong>一定要给 Pod 配置 Requests 和 Limits 参数</strong>，不仅可以让调度更准确，也能让系统更加稳定。</p><blockquote><p>NOTE：按照现在的方法根据 Pod 请求的资源进行配置不够灵活和直观，更理想的情况是用户可以直接配置 Pod 的 QoS，而不用关心具体的资源申请和上限值。但 Kubernetes 目前还没有这方面的打算。</p></blockquote><p>Pod 的 QoS 还决定了容器的 OOM（Out-Of-Memory）值，它们对应的关系如下：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-res4.png" alt="Pod QoS oom score"></p><p>可以看到，QoS 越高的 Pod OOM 值越低，也就越不容易被系统杀死。对于 Bustable Pod，它的值是根据 Request 和节点内存总量共同决定的:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oomScoreAdjust := <span class="number">1000</span> - (<span class="number">1000</span>*memoryRequest)/memoryCapacity</span><br></pre></td></tr></table></figure><p>其中 <code>memoryRequest</code> 是 Pod 申请的资源，<code>memoryCapacity</code> 是节点的内存总量。可以看到，申请的内存越多，OOM 值越低，也就越不容易被杀死。</p><p>QoS 的作用会在后面介绍 Eviction 的时候详细讲解。</p><h4 id="pod-优先级priority">Pod 优先级（Priority）</h4><p>除了 QoS，Kubernetes 还允许我们自定义 Pod 的优先级，比如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PriorityClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">high-priority</span></span><br><span class="line"><span class="attr">value:</span> <span class="number">1000000</span></span><br><span class="line"><span class="attr">globalDefault:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">"This priority class should be used for XYZ service Pods only."</span></span><br></pre></td></tr></table></figure><p>优先级的使用也比较简单，只需要在 <code>Pod.spec.PriorityClassName</code> 指定要使用的优先级名字，即可以设置当前 Pod 的优先级为对应的值。</p><p>Pod 的优先级在调度的时候会使用到。首先，待调度的 Pod 都在同一个队列中，启用了 Pod priority 之后，调度器会根据优先级的大小，把优先级高的 Pod 放在前面，提前调度。</p><p>另外，如果在调度的时候，发现某个 Pod 因为资源不足无法找到合适的节点，调度器会尝试 Preempt 的逻辑。简单来说，调度器会试图找到这样一个节点：找到它上面优先级低于当前要调度 Pod 的所有 Pod，如果杀死它们，能腾足够的资源，调度器会执行删除操作，把 Pod 调度到节点上。更多内容可以参考：<a href="https://Kubernetes.io/docs/concepts/configuration/Pod-priority-preemption/" target="_blank" rel="noopener">Pod Priority and Preemption - Kubernetes</a></p><h3 id="驱逐eviction">驱逐（Eviction）</h3><p>至此，我们讲述的都是理想情况下 Kubernetes 的工作状况，我们假设资源完全够用，而且应用也都是在使用规定范围内的资源。</p><p>但现实不会如此简单，在管理集群的时候我们常常会遇到资源不足的情况，在这种情况下我们要<strong>保证整个集群可用</strong>，并且尽可能<strong>减少应用的损失</strong>。保证集群可用比较容易理解，首先要保证系统层面的核心进程正常，其次要保证 Kubernetes 本身组件进程不出问题；但是如何量化应用的损失呢？首先能想到的是如果要杀死 Pod，要尽量减少总数。另外一个就和 Pod 的优先级相关了，那就是尽量杀死不那么重要的应用，让重要的应用不受影响。</p><p>Pod 的驱逐是在 Kubelet 中实现的，因为 Kubelet 能动态地感知到节点上资源使用率实时的变化情况。其核心的逻辑是：Kubelet 实时监控节点上各种资源的使用情况，一旦发现某个不可压缩资源出现要耗尽的情况，就会主动终止节点上的 Pod，让节点能够正常运行。被终止的 Pod 所有容器会停止，状态会被设置为 Failed。</p><h4 id="驱逐触发条件">驱逐触发条件</h4><p>那么哪些资源不足会导致 Kubelet 执行驱逐程序呢？目前主要有三种情况：实际内存不足、节点文件系统的可用空间（文件系统剩余大小和 Inode 数量）不足、以及镜像文件系统的可用空间（包括文件系统剩余大小和 Inode 数量）不足。</p><p>下面这图是具体的触发条件：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-res5.png" alt="eviction conddition"></p><p>有了数据的来源，另外一个问题是触发的时机，也就是到什么程度需要触发驱逐程序？Kubernetes 运行用户自己配置，并且支持两种模式：按照百分比和按照绝对数量。比如对于一个 32G 内存的节点当可用内存少于 10% 时启动驱逐程序，可以配置 <code>memory.available&lt;10%</code>或者 <code>memory.available&lt;3.2Gi</code>。</p><blockquote><p>NOTE：默认情况下，Kubelet 的驱逐规则是 <code>memory.available&lt;100Mi</code>，对于生产环境这个配置是不可接受的，所以一定要根据实际情况进行修改。</p></blockquote><h4 id="软驱逐soft-eviction和硬驱逐hard-eviction">软驱逐（Soft Eviction）和硬驱逐（Hard Eviction）</h4><p>因为驱逐 Pod 是具有毁坏性的行为，因此必须要谨慎。有时候内存使用率增高只是暂时性的，有可能 20s 内就能恢复，这时候启动驱逐程序意义不大，而且可能会导致应用的不稳定，我们要考虑到这种情况应该如何处理；另外需要注意的是，如果内存使用率过高，比如高于 95%（或者 90%，取决于主机内存大小和应用对稳定性的要求），那么我们不应该再多做评估和考虑，而是赶紧启动驱逐程序，因为这种情况再花费时间去判断可能会导致内存继续增长，系统完全崩溃。</p><p>为了解决这个问题，Kubernetes 引入了 Soft Eviction 和 Hard Eviction 的概念。</p><p><strong>软驱逐</strong>可以在资源紧缺情况并没有哪些严重的时候触发，比如内存使用率为 85%，软驱逐还需要配置一个时间指定软驱逐条件持续多久才触发，也就是说 Kubelet 在发现资源使用率达到设定的阈值之后，并不会立即触发驱逐程序，而是继续观察一段时间，如果资源使用率高于阈值的情况持续一定时间，才开始驱逐。并且驱逐 Pod 的时候，会遵循 Grace Period ，等待 Pod 处理完清理逻辑。和软驱逐相关的启动参数是：</p><ul><li><code>--eviction-soft</code>：软驱逐触发条件，比如 <code>memory.available&lt;1Gi</code>。</li><li><code>--eviction-sfot-grace-period</code>：触发条件持续多久才开始驱逐，比如 <code>memory.available=2m30s</code>。</li><li><code>--eviction-max-Pod-grace-period</code>：Kill Pod 时等待 Grace Period 的时间让 Pod 做一些清理工作，如果到时间还没有结束就做 Kill。</li></ul><p>前面两个参数必须同时配置，软驱逐才能正常工作；后一个参数会和 Pod 本身配置的 Grace Period 比较，选择较小的一个生效。</p><p><strong>硬驱逐</strong>更加直接干脆，Kubelet 发现节点达到配置的硬驱逐阈值后，立即开始驱逐程序，并且不会遵循 Grace Period，也就是说立即强制杀死 Pod。对应的配置参数只有一个 <code>--evictio-hard</code>，可以选择上面表格中的任意条件搭配。</p><p>设置这两种驱逐程序是为了平衡节点稳定性和对 Pod 的影响，软驱逐照顾到了 Pod 的优雅退出，减少驱逐对 Pod 的影响；而硬驱逐则照顾到节点的稳定性，防止资源的快速消耗导致节点不可用。</p><p>软驱逐和硬驱逐可以单独配置，不过还是推荐两者都进行配置，一起使用。</p><h4 id="驱逐哪些-pods">驱逐哪些 Pods？</h4><p>上面我们已经整体介绍了 Kubelet 驱逐 Pod 的逻辑和过程，那这里就牵涉到一个具体的问题：<strong>要驱逐哪些 Pod</strong>？驱逐的重要原则是尽量减少对应用程序的影响。</p><p>如果是存储资源不足，Kubelet 会根据情况清理状态为 Dead 的 Pod 和它的所有容器，以及清理所有没有使用的镜像。如果上述清理并没有让节点回归正常，Kubelet 就开始清理 Pod。</p><p>一个节点上会运行多个 Pod，驱逐所有的 Pods 显然是不必要的，因此要做出一个抉择：在节点上运行的所有 Pod 中选择一部分来驱逐。虽然这些 Pod 乍看起来没有区别，但是它们的地位是不一样的，正如乔治·奥威尔在《动物庄园》的那句话：</p><blockquote><p>所有动物生而平等，但有些动物比其他动物更平等。</p></blockquote><p>Pod 也是不平等的，有些 Pod 要比其他 Pod 更重要。只管来说，系统组件的 Pod 要比普通的 Pod 更重要，另外运行数据库的 Pod 自然要比运行一个无状态应用的 Pod 更重要。Kubernetes 又是怎么决定 Pod 的优先级的呢？这个问题的答案就藏在我们之前已经介绍过的内容里：Pod Requests 和 Limits、优先级（Priority），以及 Pod 实际的资源使用。</p><p>简单来说，Kubelet 会根据以下内容对 Pod 进行排序：Pod 是否使用了超过请求的紧张资源、Pod 的优先级、然后是使用的紧缺资源和请求的紧张资源之间的比例。具体来说，Kubelet 会按照如下的顺序驱逐 Pod：</p><ul><li>使用的紧张资源超过请求数量的 <code>BestEffort</code> 和 <code>Burstable</code> Pod，这些 Pod 内部又会按照优先级和使用比例进行排序。</li><li>紧张资源使用量低于 Requests 的 <code>Burstable</code> 和 <code>Guaranteed</code> 的 Pod 后面才会驱逐，只有当系统组件（Kubelet、Docker、Journald 等）内存不够，并且没有上面 QoS 比较低的 Pod 时才会做。执行的时候还会根据 Priority 排序，优先选择优先级低的 Pod。</li></ul><h4 id="防止波动">防止波动</h4><p>这里的波动有两种情况，我们先说说第一种。驱逐条件出发后，如果 Kubelet 驱逐一部分 Pod，让资源使用率低于阈值就停止，那么很可能过一段时间资源使用率又会达到阈值，从而再次出发驱逐，如此循环往复……为了处理这种问题，我们可以使用 <code>--eviction-minimum-reclaim</code>解决，这个参数配置每次驱逐至少清理出来多少资源才会停止。</p><p>另外一个波动情况是这样的：Pod 被驱逐之后并不会从此消失不见，常见的情况是 Kubernetes 会自动生成一个新的 Pod 来取代，并经过调度选择一个节点继续运行。如果不做额外处理，有理由相信 Pod 选择原来节点的可能性比较大（因为调度逻辑没变，而它上次调度选择的就是该节点），之所以说可能而不是绝对会再次选择该节点，是因为集群 Pod 的运行和分布和上次调度时极有可能发生了变化。</p><p>无论如何，如果被驱逐的 Pod 再次调度到原来的节点，很可能会再次触发驱逐程序，然后 Pod 再次被调度到当前节点，循环往复…… 这种事情当然是我们不愿意看到的，虽然看似复杂，但这个问题解决起来非常简单：驱逐发生后，Kubelet 更新节点状态，调度器感知到这一情况，暂时不往该节点调度 Pod 即可。<code>--eviction-pressure-transition-period</code> 参数可以指定 Kubelet 多久才上报节点的状态，因为默认的上报状态周期比较短，频繁更改节点状态会导致驱逐波动。</p><p>做一个总结，下面是一个使用了上面多种参数的驱逐配置实例（你应该能看懂它们是什么意思了）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">–eviction-soft=memory.available&lt;80%,nodefs.available&lt;2Gi \</span><br><span class="line">–eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s \</span><br><span class="line">–eviction-max-Pod-grace-period=120 \</span><br><span class="line">–eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi \</span><br><span class="line">–eviction-pressure-transition-period=30s \</span><br><span class="line">--eviction-minimum-reclaim=<span class="string">"memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi"</span></span><br></pre></td></tr></table></figure><h3 id="碎片整理和重调度">碎片整理和重调度</h3><p>Kubernetes 的调度器在为 Pod 选择运行节点的时候，只会考虑到调度那个时间点集群的状态，经过一系列的算法选择一个<strong>当时最合适</strong>的节点。但是集群的状态是不断变化的，用户创建的 Pod 也是动态的，随着时间变化，原来调度到某个节点上的 Pod 现在看来可能有更好的节点可以选择。比如考虑到下面这些情况：</p><ul><li>调度 Pod 的条件已经不再满足，比如节点的 Taints 和 Labels 发生了变化。</li><li>新节点加入了集群。如果默认配置了把 Pod 打散，那么应该有一些 Pod 最好运行在新节点上。</li><li>节点的使用率不均匀。调度后，有些节点的分配率和使用率比较高，另外一些比较低。</li><li>节点上有资源碎片。有些节点调度之后还剩余部分资源，但是又低于任何 Pod 的请求资源；或者 Memory 资源已经用完，但是 CPU 还有挺多没有使用。</li></ul><p>想要解决上述的这些问题，都需要把 Pod 重新进行调度（把 Pod 从当前节点移动到另外一个节点）。但是默认情况下，一旦 Pod 被调度到节点上，除非给杀死否则不会移动到另外一个节点的。</p><p>为此 Kubernetes 社区孵化了一个称为  <a href="https://github.com/Kubernetes-incubator/descheduler" target="_blank" rel="noopener"><code>Descheduler</code></a> 的项目，专门用来做重调度。重调度的逻辑很简单：找到上面几种情况中已经不是最优的 Pod，把它们驱逐掉（Eviction）。</p><p>目前，Descheduler 不会决定驱逐的 Pod 应该调度到哪台机器，而是<strong>假定默认的调度器会做出正确的调度抉择</strong>。也就是说，之所以 Pod 目前不合适，不是因为调度器的算法有问题，而是因为集群的情况发生了变化。如果让调度器重新选择，调度器现在会把 Pod 放到合适的节点上。这种做法让 Descheduler 逻辑比较简单，而且避免了调度逻辑出现在两个组件中。</p><p>Descheduler 执行的逻辑是可以配置的，目前有几种场景：</p><ul><li><code>RemoveDuplicates</code>：RS、Deployment 中的 Pod 不能同时出现在一台机器上。</li><li><code>LowNodeUtilization</code>：找到资源使用率比较低的 Node，然后驱逐其他资源使用率比较高节点上的 Pod，期望调度器能够重新调度让资源更均衡。</li><li><code>RemovePodsViolatingInterPodAntiAffinity</code>：找到已经违反 Pod Anti Affinity 规则的 Pods 进行驱逐，可能是因为反亲和是后面加上去的。</li><li><code>RemovePodsViolatingNodeAffinity</code>：找到违反 Node Affinity 规则的 Pods 进行驱逐，可能是因为 Node 后面修改了 Label。</li></ul><p>当然，为了保证应用的稳定性，Descheduler 并不会随意地驱逐 Pod，还是会尊重 Pod 运行的规则，包括 Pod 的优先级（不会驱逐 Critical Pod，并且按照优先级顺序进行驱逐）和 PDB（如果违反了 PDB，则不会进行驱逐），并且不会驱逐没有 Deployment、RS、Jobs 的 Pod 不会驱逐，Daemonset Pod 不会驱逐，有 Local storage 的 Pod 也不会驱逐。</p><p>Descheduler 不是一个常驻的任务，每次执行完之后会退出，因此推荐使用 CronJob 来运行。</p><p>总的来说，Descheduler 是对原生调度器的补充，用来解决原生调度器的调度决策随着时间会变得失效，或者不够优化的缺陷。</p><h3 id="资源动态调整">资源动态调整</h3><p>动态调整的思路：应用的实际流量会不断变化，因此使用率也是不断变化的，为了应对应用流量的变化，我们应用能够自动调整应用的资源。比如在线商品应用在促销的时候访问量会增加，我们应该自动增加 Pod 运算能力来应对；当促销结束后，有需要自动降低 Pod 的运算能力防止浪费。</p><p>运算能力的增减有两种方式：改变单个 Pod 的资源，以及增减 Pod 的数量。这两种方式对应了 Kubernetes 的 HPA 和 VPA。</p><h4 id="horizontal-pod-autoscaling横向-pod-自动扩展">Horizontal Pod AutoScaling（横向 Pod 自动扩展）</h4><p><img src="https://www.hi-linux.com/img/linux/k8s-res6.png" alt="kubernetes HPA"></p><p>横向 Pod 自动扩展的思路是这样的：Kubernetes 会运行一个 Controller，周期性地监听 Pod 的资源使用情况，当高于设定的阈值时，会自动增加 Pod 的数量；当低于某个阈值时，会自动减少 Pod 的数量。自然，这里的阈值以及 Pod 的上限和下限的数量都是需要用户配置的。</p><p>上面这句话隐藏了一个重要的信息：HPA 只能和 RC、Deployment、RS 这些可以动态修改 Replicas 的对象一起使用，而无法用于单个 Pod、Daemonset（因为它控制的 Pod 数量不能随便修改）等对象。</p><p>目前官方的监控数据来源是 Metrics Server 项目，可以配置的资源只有 CPU，但是用户可以使用自定义的监控数据（比如：Prometheus）。其他资源（比如：Memory）的 HPA 支持也已经在路上了。</p><h4 id="vertical-pod-autoscaling">Vertical Pod AutoScaling</h4><p>和 HPA 的思路相似，只不过 VPA 调整的是单个 Pod 的 Request 值（包括 CPU 和 Memory）。VPA 包括三个组件：</p><ul><li>Recommander：消费 Metrics Server 或者其他监控组件的数据，然后计算 Pod 的资源推荐值。</li><li>Updater：找到被 VPA 接管的 Pod 中和计算出来的推荐值差距过大的，对其做 Update 操作（目前是 Evict，新建的 Pod 在下面 Admission Controller 中会使用推荐的资源值作为 Request）。</li><li>Admission Controller：新建的 Pod 会经过该 Admission  Controller，如果 Pod 是被 VPA 接管的，会使用 Recommander 计算出来的推荐值。</li></ul><p>可以看到，这三个组件的功能是互相补充的，共同实现了动态修改 Pod 请求资源的功能。相对于 HPA，目前 VPA 还处于 Alpha，并且还没有合并到官方的 Kubernetes Release 中，后续的接口和功能很可能会发生变化。</p><h4 id="cluster-auto-scaler">Cluster Auto Scaler</h4><p>随着业务的发展，应用会逐渐增多，每个应用使用的资源也会增加，总会出现集群资源不足的情况。为了动态地应对这一状况，我们还需要 CLuster Auto Scaler，能够根据整个集群的资源使用情况来增减节点。</p><p>对于公有云来说，Cluster Auto Scaler 就是监控这个集群因为资源不足而 Pending 的 Pod，根据用户配置的阈值调用公有云的接口来申请创建机器或者销毁机器。对于私有云，则需要对接内部的管理平台。</p><p>目前 HPA 和 VPA 不兼容，只能选择一个使用，否则两者会相互干扰。而且 VPA 的调整需要重启 Pod，这是因为 Pod 资源的修改是比较大的变化，需要重新走一下 Apiserver、调度的流程，保证整个系统没有问题。目前社区也有计划在做原地升级，也就是说不通过杀死 Pod 再调度新 Pod 的方式，而是直接修改原有 Pod 来更新。</p><p>理论上 HPA 和 VPA 是可以共同工作的，HPA 负责瓶颈资源，VPA 负责其他资源。比如对于 CPU 密集型的应用，使用 HPA  监听 CPU 使用率来调整 Pods 个数，然后用 VPA 监听其他资源（Memory、IO）来动态扩展这些资源的 Request 大小即可。当然这只是理想情况，</p><h3 id="总结">总结</h3><p>从前面介绍的各种 Kubernetes 调度和资源管理方案可以看出来，提高应用的资源使用率、保证应用的正常运行、维护调度和集群的公平性是件非常复杂的事情，Kubernetes 并没有<em>完美</em>的方法，而是对各种可能的问题不断提出一些针对性的方案。</p><p>集群的资源使用并不是静态的，而是随着时间不断变化的，目前 Kubernetes 的调度决策都是基于调度时集群的一个静态资源切片进行的，动态地资源调整是通过 Kubelet 的驱逐程序进行的，HPA 和 VPA 等方案也不断提出，相信后面会不断完善这方面的功能，让 Kubernetes 更加智能。</p><p>资源管理和调度、应用优先级、监控、镜像中心等很多东西相关，是个非常复杂的领域。在具体的实施和操作的过程中，常常要考虑到企业内部的具体情况和需求，做出针对性的调整，并且需要开发者、系统管理员、SRE、监控团队等不同小组一起合作。但是这种付出从整体来看是值得的，提升资源的利用率能有效地节约企业的成本，也能让应用更好地发挥出作用。</p><h3 id="参考文档">参考文档</h3><p>Kubernetes 官方文档：</p><ul><li><a href="https://Kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/" target="_blank" rel="noopener">Managing Compute Resources for Containers</a>：如何为 Pod 配置 cpu 和 memory 资源</li><li><a href="https://Kubernetes.io/docs/tasks/configure-Pod-container/quality-service-Pod/" target="_blank" rel="noopener">Configure Quality of Service for Pods - Kubernetes</a>：Pod QoS 的定义和配置规则</li><li><a href="https://Kubernetes.io/docs/tasks/administer-cluster/out-of-resource/" target="_blank" rel="noopener">Configure Out Of Resource Handling - Kubernetes</a>：配置资源不足时 Kubernetes 的 处理方式，也就是 eviction</li><li><a href="https://Kubernetes.io/docs/concepts/policy/resource-quotas/" target="_blank" rel="noopener">Kubernetes 官方文档：Resource Quota</a>：为 namespace 配置 quota</li><li><a href="https://github.com/Kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md" target="_blank" rel="noopener">community/resource-qos.md at master · Kubernetes/community · GitHub</a>：QoS 设计文档</li><li><a href="https://Kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/" target="_blank" rel="noopener">Reserve Compute Resources for System Daemons</a>：如何在节点上预留资源</li><li><a href="https://github.com/Kubernetes-incubator/descheduler" target="_blank" rel="noopener">GitHub - Kubernetes-incubator/descheduler: Descheduler for Kubernetes</a>：descheduler 重调度官方 repo</li><li><a href="https://Kubernetes.io/docs/tasks/run-application/horizontal-Pod-autoscale/" target="_blank" rel="noopener">Horizontal Pod Autoscaler - Kubernetes</a>：Kubernetes HPA 介绍文档</li><li><a href="https://github.com/Kubernetes/community/blob/master/contributors/design-proposals/autoscaling/vertical-Pod-autoscaler.md" target="_blank" rel="noopener">community/vertical-Pod-autoscaler.md at master · Kubernetes/community · GitHub</a>: Kubernetes VPA 设计文档</li></ul><p>其他文档：</p><ul><li><a href="https://speakerdeck.com/thockin/everything-you-ever-wanted-to-know-about-resource-scheduling-dot-dot-dot-almost" target="_blank" rel="noopener">Everything You Ever Wanted To Know About Resource Scheduling… Almost - Speaker Deck</a>: Tim Hockin 在 kubecon 上介绍的 Kubernetes 资源管理理念，强烈推荐</li><li><a href="https://mp.weixin.qq.com/s/hyPNOcR3Nhy9bAiDhXUP7A" target="_blank" rel="noopener">聊聊Kubernetes计算资源模型（上）——资源抽象、计量与调度</a></li><li><a href="https://www.atatech.org/articles/99071" target="_blank" rel="noopener">【Sigma敏捷版系列文章】Kubernetes应用驱逐分析</a></li><li><a href="https://www.atatech.org/articles/99071" target="_blank" rel="noopener">Kubernetes 应用驱逐分析</a></li><li><a href="http://www.noqcks.io/notes/2018/02/03/understanding-Kubernetes-resources/" target="_blank" rel="noopener">Understanding Kubernetes Resources | Benji Visser</a>：介绍了 Kubernetes 的资源模型</li><li><a href="https://cloud.tencent.com/developer/article/1004976" target="_blank" rel="noopener">Kubernetes 资源分配之 Request 和 Limit 解析 - 云+社区 - 腾讯云</a>：用图表的方式解释了 requests 和 limits 的含义，以及在提高资源使用率方面的作用</li><li><a href="https://my.oschina.net/HardySimpson/blog/1359276" target="_blank" rel="noopener">Kubernetes中容器资源控制的那些事儿</a>：这篇文章介绍了 Kubernetes Pod 中 cpu 和 memory 的 request 和 limits 是如何最终变成 cgroups 配置的</li><li><a href="https://medium.com/@Rancher_Labs/the-three-pillars-of-Kubernetes-container-orchestration-247f42115a4a" target="_blank" rel="noopener">The Three Pillars of Kubernetes Container Orchestration</a>：Kubernetes 调度、资源管理和服务介绍</li><li><a href="https://www.slideshare.net/AmazonWebServices/dem19-advanced-auto-scaling-and-deployment-tools-for-Kubernetes-and-ecs" target="_blank" rel="noopener">DEM19 Advanced Auto Scaling and Deployment Tools for Kubernetes and E…</a></li><li><a href="https://my.oschina.net/jxcdwangtao/blog/837875" target="_blank" rel="noopener">Kubernetes Resource QoS机制解读</a></li><li><a href="https://www.jianshu.com/p/a5a7b3fb6806" target="_blank" rel="noopener">深入解析 Kubernetes 资源管理，容器云牛人有话说 - 简书</a></li></ul><blockquote><p>来源：Cizixs Writes Here<br>原文：<a href="http://t.cn/ReTAKl3" target="_blank" rel="noopener">http://t.cn/ReTAKl3</a><br>题图：来自谷歌图片搜索<br>版权：本文版权归原作者所有</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 从创建之初的核心模块之一就是资源调度。想要在生产环境使用好 Kubernetes，必须对它的资源模型以及资源管理非常了解。这篇文章算是对散布在网络上的 Kubernetes 资源管理内容的一个总结。干货文章，强列推荐一读。&lt;/p&gt;
&lt;h3 id=&quot;Kubernetes-资源简介&quot;&gt;Kubernetes 资源简介&lt;/h3&gt;
&lt;h4 id=&quot;什么是资源？&quot;&gt;什么是资源？&lt;/h4&gt;
&lt;p&gt;在 Kubernetes 中，有两个基础但是非常重要的概念：Node 和 Pod。Node 翻译成节点，是对集群资源的抽象；Pod 是对容器的封装，是应用运行的实体。Node 提供资源，而 Pod 使用资源，这里的资源分为计算（CPU、Memory、GPU）、存储（Disk、SSD）、网络（Network Bandwidth、IP、Ports）。这些资源提供了应用运行的基础，正确理解这些资源以及集群调度如何使用这些资源，对于大规模的 Kubernetes 集群来说至关重要，不仅能保证应用的稳定性，也可以提高资源的利用率。&lt;/p&gt;
&lt;p&gt;在这篇文章，我们主要介绍 CPU 和内存这两个重要的资源，它们虽然都属于计算资源，但也有所差距。CPU 可分配的是使用时间，也就是操作系统管理的时间片，每个进程在一定的时间片里运行自己的任务（另外一种方式是绑核，也就是把 CPU 完全分配给某个 Pod 使用，但这种方式不够灵活会造成严重的资源浪费，Kubernetes 中并没有提供）；而对于内存，系统提供的是内存大小。&lt;/p&gt;
&lt;p&gt;CPU 的使用时间是可压缩的，换句话说它本身无状态，申请资源很快，也能快速正常回收；而内存大小是不可压缩的，因为它是有状态的（内存里面保存的数据），申请资源很慢（需要计算和分配内存块的空间），并且回收可能失败（被占用的内存一般不可回收）。&lt;/p&gt;
&lt;p&gt;把资源分成&lt;strong&gt;可压缩&lt;/strong&gt;和&lt;strong&gt;不可压缩&lt;/strong&gt;，是因为在资源不足的时候，它们的表现很不一样。对于不可压缩资源，如果资源不足，也就无法继续申请资源（内存用完就是用完了），并且会导致 Pod 的运行产生无法预测的错误（应用申请内存失败会导致一系列问题）；而对于可压缩资源，比如 CPU 时间片，即使 Pod 使用的 CPU 资源很多，CPU 使用也可以按照权重分配给所有 Pod 使用，虽然每个人使用的时间片减少，但不会影响程序的逻辑。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>利用 Helm 快速部署 Ingress</title>
    <link href="https://www.hi-linux.com/posts/35116.html"/>
    <id>https://www.hi-linux.com/posts/35116.html</id>
    <published>2018-08-16T01:00:00.000Z</published>
    <updated>2018-08-24T06:03:49.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Ingress 是一种 Kubernetes 资源，也是将 Kubernetes 集群内服务暴露到外部的一种方式。本文将讲一讲如何用 Helm 在 Kubernetes 集群中部署 Ingress，并部署两个应用来演示 Ingress 的具体使用。</p><p>阅读本文前你需要先掌握 Helm 和一些 Kubernetes 服务暴露的相关知识点，如果你还不了解可以先读一读我之前写的 「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486154&amp;idx=1&amp;sn=becd5dd0fadfe0b6072f5dfdc6fdf786&amp;chksm=eac52be3ddb2a2f555b8b1028db97aa3e92d0a4880b56f361e4b11cd252771147c44c08c8913#rd" target="_blank" rel="noopener">Helm 入门指南</a>」和「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486130&amp;idx=1&amp;sn=41ee30f02113dac86398653f542a3c70&amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3#rd" target="_blank" rel="noopener">浅析从外部访问 Kubernetes 集群中应用的几种方式</a>」这两篇文章。</p><h3 id="部署-ingress-controller">部署 Ingress Controller</h3><p><code>Ingress</code> 只是一个统称，其由 <code>Ingress</code> 和 <code>Ingress Controller</code> 两部分组成。<code>Ingress</code> 用作将原来需要手动配置的规则抽象成一个 Ingress 对象，使用 YAML 格式的文件来创建和管理。<code>Ingress Controller</code> 用作通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化。</p><p>目前可用的 Ingress Controller 类型有很多，比如：Nginx、HAProxy、Traefik 等，我们将演示如何部署一个基于 Nginx 的 Ingress Controller。</p><p>这里我们使用 Helm 来部署，在开始部署前，请确认您已经安装和配置好 Helm 相关环境。</p><h4 id="查找软件仓库中是否有-nginx-ingress-包">查找软件仓库中是否有 Nginx Ingress 包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm search nginx-ingress</span><br><span class="line">NAME                CHART VERSIONAPP VERSIONDESCRIPTION</span><br><span class="line">stable&#x2F;nginx-ingress0.9.5        0.10.2     An nginx Ingress controller that uses ConfigMap...</span><br></pre></td></tr></table></figure><blockquote><p>注：这里我们使用的是在阿里云 Helm 镜像仓库。如果你还不知道如何增加三方仓库，可先阅读 「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486154&amp;idx=1&amp;sn=becd5dd0fadfe0b6072f5dfdc6fdf786&amp;chksm=eac52be3ddb2a2f555b8b1028db97aa3e92d0a4880b56f361e4b11cd252771147c44c08c8913#rd" target="_blank" rel="noopener">Helm 入门指南</a>」一文。</p></blockquote><p>阿里云 Helm 镜像仓库里的 nginx-ingress 软件包已经将要用到的相关容器镜像地址改成了国内可访问的地址。安装时需要用到的容器镜像有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">repository: registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;nginx-ingress-controller:0.10.2</span><br><span class="line">repository: registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;defaultbackend:1.3</span><br><span class="line">repository: sophos&#x2F;nginx-vts-exporter:0.6</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="使用-helm-部署-nginx-ingress-controller">使用 Helm 部署 Nginx Ingress Controller</h4><p>Ingress Controller 本身对外暴露的方式有几种，比如：hostNetwork、externalIP 等。这里我们采用 externalIP 的方式进行，如果你要使用 hostNetwork 方式，可以使用 <code>controller.hostNetwork=true</code> 参数进行设置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"># 启用 RBAC 支持</span><br><span class="line">$ helm install --name nginx-ingress --set &quot;rbac.create&#x3D;true,controller.service.externalIPs[0]&#x3D;192.168.100.211,controller.service.externalIPs[1]&#x3D;192.168.100.212,controller.service.externalIPs[2]&#x3D;192.168.100.213&quot; stable&#x2F;nginx-ingress</span><br><span class="line"></span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;ConfigMap</span><br><span class="line">NAME                      DATA  AGE</span><br><span class="line">nginx-ingress-controller  1     2m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;ClusterRole</span><br><span class="line">NAME           AGE</span><br><span class="line">nginx-ingress  2m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;ClusterRoleBinding</span><br><span class="line">NAME           AGE</span><br><span class="line">nginx-ingress  2m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;RoleBinding</span><br><span class="line">NAME           AGE</span><br><span class="line">nginx-ingress  2m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;ServiceAccount</span><br><span class="line">NAME           SECRETS  AGE</span><br><span class="line">nginx-ingress  1        2m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;Role</span><br><span class="line">NAME           AGE</span><br><span class="line">nginx-ingress  2m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME                           TYPE          CLUSTER-IP      EXTERNAL-IP                                      PORT(S)                   AGE</span><br><span class="line">nginx-ingress-controller       LoadBalancer  10.254.84.72    192.168.100.211,192.168.100.212,192.168.100.213  80:8410&#x2F;TCP,443:8948&#x2F;TCP  2m</span><br><span class="line">nginx-ingress-default-backend  ClusterIP     10.254.206.175  &lt;none&gt;                                           80&#x2F;TCP                    2m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;Deployment</span><br><span class="line">NAME                           DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">nginx-ingress-controller       1        1        1           1          2m</span><br><span class="line">nginx-ingress-default-backend  1        1        1           1          2m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;PodDisruptionBudget</span><br><span class="line">NAME                           MIN AVAILABLE  MAX UNAVAILABLE  ALLOWED DISRUPTIONS  AGE</span><br><span class="line">nginx-ingress-controller       1              N&#x2F;A              0                    2m</span><br><span class="line">nginx-ingress-default-backend  1              N&#x2F;A              0                    2m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                                           READY  STATUS   RESTARTS  AGE</span><br><span class="line">nginx-ingress-controller-665cd897fc-n5rq4      1&#x2F;1    Running  0         2m</span><br><span class="line">nginx-ingress-default-backend-df594cfc6-pbcxk  1&#x2F;1    Running  0         2m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">The nginx-ingress controller has been installed.</span><br><span class="line">It may take a few minutes for the LoadBalancer IP to be available.</span><br><span class="line">You can watch the status by running &#39;kubectl --namespace default get services -o wide -w nginx-ingress-controller&#39;</span><br><span class="line"></span><br><span class="line">An example Ingress that makes use of the controller:</span><br><span class="line"></span><br><span class="line">  apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">  kind: Ingress</span><br><span class="line">  metadata:</span><br><span class="line">    annotations:</span><br><span class="line">      kubernetes.io&#x2F;ingress.class: nginx</span><br><span class="line">    name: example</span><br><span class="line">    namespace: foo</span><br><span class="line">  spec:</span><br><span class="line">    rules:</span><br><span class="line">      - host: www.example.com</span><br><span class="line">        http:</span><br><span class="line">          paths:</span><br><span class="line">            - backend:</span><br><span class="line">                serviceName: exampleService</span><br><span class="line">                servicePort: 80</span><br><span class="line">              path: &#x2F;</span><br><span class="line">    # This section is only required if TLS is to be enabled for the Ingress</span><br><span class="line">    tls:</span><br><span class="line">        - hosts:</span><br><span class="line">            - www.example.com</span><br><span class="line">          secretName: example-tls</span><br><span class="line"></span><br><span class="line">If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:</span><br><span class="line"></span><br><span class="line">  apiVersion: v1</span><br><span class="line">  kind: Secret</span><br><span class="line">  metadata:</span><br><span class="line">    name: example-tls</span><br><span class="line">    namespace: foo</span><br><span class="line">  data:</span><br><span class="line">    tls.crt: &lt;base64 encoded cert&gt;</span><br><span class="line">    tls.key: &lt;base64 encoded key&gt;</span><br><span class="line">  type: kubernetes.io&#x2F;tls</span><br></pre></td></tr></table></figure><p>部署完成后我们可以看到 Kubernetes 服务中增加了 <code>nginx-ingress-controller</code> 和 <code>nginx-ingress-default-backend</code> 两个服务。<code>nginx-ingress-controller</code> 为 <code>Ingress Controller</code>，主要做为一个七层的负载均衡器来提供 HTTP 路由、粘性会话、SSL 终止、SSL直通、TCP 和 UDP 负载平衡等功能。<code>nginx-ingress-default-backend</code> 为默认的后端，当集群外部的请求通过 Ingress 进入到集群内部时，如果无法负载到相应后端的 Service 上时，这种未知的请求将会被负载到这个默认的后端上。</p><p>由于我们采用了 externalIP 方式对外暴露服务， 所以 <code>nginx-ingress-controller</code> 会在 192.168.100.211、192.168.100.212、192.168.100.213 三台节点宿主机上的 暴露 80/443 端口。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc</span><br><span class="line">NAME                            TYPE           CLUSTER-IP       EXTERNAL-IP                                       PORT(S)                    AGE</span><br><span class="line">kubernetes                      ClusterIP      10.254.0.1       &lt;none&gt;                                            443&#x2F;TCP                    18d</span><br><span class="line">nginx-ingress-controller        LoadBalancer   10.254.84.72     192.168.100.211,192.168.100.212,192.168.100.213   80:8410&#x2F;TCP,443:8948&#x2F;TCP   46s</span><br><span class="line">nginx-ingress-default-backend   ClusterIP      10.254.206.175   &lt;none&gt;                                            80&#x2F;TCP                     46s</span><br></pre></td></tr></table></figure><h4 id="访问-nginx-ingress-controller">访问 Nginx Ingress Controller</h4><p>我们可以使用以下命令来获取 Nginx 的 HTTP 和 HTTPS 地址。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl --namespace default get services -o wide -w nginx-ingress-controller</span><br><span class="line">NAME                       TYPE           CLUSTER-IP     EXTERNAL-IP                                       PORT(S)                    AGE       SELECTOR</span><br><span class="line">nginx-ingress-controller   LoadBalancer   10.254.84.72   192.168.100.211,192.168.100.212,192.168.100.213   80:8410&#x2F;TCP,443:8948&#x2F;TCP   4h        app&#x3D;nginx-ingress,component&#x3D;controller,release&#x3D;nginx-ingress</span><br></pre></td></tr></table></figure><p>因为我们还没有在 Kubernetes 集群中创建 Ingress资源，所以直接对 ExternalIP 的请求被负载到了 nginx-ingress-default-backend 上。nginx-ingress-default-backend 默认提供了两个 URL 进行访问，其中的 /healthz 用作健康检查返回 200，而 / 返回 404 错误。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># 返回 200</span><br><span class="line">$ curl -I  http:&#x2F;&#x2F;192.168.100.212&#x2F;healthz&#x2F;</span><br><span class="line">HTTP&#x2F;1.1 200 OK</span><br><span class="line">Server: nginx&#x2F;1.13.8</span><br><span class="line">Date: Tue, 24 Jul 2018 06:25:58 GMT</span><br><span class="line">Content-Type: text&#x2F;html</span><br><span class="line">Content-Length: 0</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Strict-Transport-Security: max-age&#x3D;15724800; includeSubDomains;</span><br><span class="line"></span><br><span class="line"># 返回 404</span><br><span class="line">$ curl -I  http:&#x2F;&#x2F;192.168.100.212&#x2F;</span><br><span class="line">HTTP&#x2F;1.1 404 Not Found</span><br><span class="line">Server: nginx&#x2F;1.13.8</span><br><span class="line">Date: Tue, 24 Jul 2018 06:26:39 GMT</span><br><span class="line">Content-Type: text&#x2F;plain; charset&#x3D;utf-8</span><br><span class="line">Content-Length: 21</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Strict-Transport-Security: max-age&#x3D;15724800; includeSubDomains;</span><br><span class="line"></span><br><span class="line"># 返回 200</span><br><span class="line">$ curl -I --insecure https:&#x2F;&#x2F;192.168.100.212&#x2F;healthz&#x2F;</span><br><span class="line">HTTP&#x2F;2 200</span><br><span class="line">server: nginx&#x2F;1.13.8</span><br><span class="line">date: Tue, 24 Jul 2018 06:27:41 GMT</span><br><span class="line">content-type: text&#x2F;html</span><br><span class="line">content-length: 0</span><br><span class="line">strict-transport-security: max-age&#x3D;15724800; includeSubDomains;</span><br><span class="line"></span><br><span class="line"># 返回 404</span><br><span class="line">$ curl -I --insecure https:&#x2F;&#x2F;192.168.100.212&#x2F;</span><br><span class="line">HTTP&#x2F;2 404</span><br><span class="line">server: nginx&#x2F;1.13.8</span><br><span class="line">date: Tue, 24 Jul 2018 06:28:25 GMT</span><br><span class="line">content-type: text&#x2F;plain; charset&#x3D;utf-8</span><br><span class="line">content-length: 21</span><br><span class="line">strict-transport-security: max-age&#x3D;15724800; includeSubDomains;</span><br></pre></td></tr></table></figure><p>在几台节点宿主机上查看，我们可以看到 ExternalIP 的 Service 是通过 Kube-Proxy对外暴露的，这里的 192.168.100.211、192.168.100.212、192.168.100.213 是三个内网 IP。 实际生产应用中是需要通过边缘路由器或全局统一接入层的负载均衡器将到达公网 IP 的外网流量转发到这几个内网 IP 上，外部用户再通过域名访问集群中以 Ingress 暴露的所有服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo netstat -tlunp|grep kube-proxy|grep -E &#39;80|443&#39;</span><br><span class="line">tcp        0      0 192.168.100.211:80      0.0.0.0:*               LISTEN      714&#x2F;kube-proxy</span><br><span class="line">tcp        0      0 192.168.100.211:443     0.0.0.0:*               LISTEN      714&#x2F;kube-proxy</span><br><span class="line"></span><br><span class="line">$ sudo netstat -tlunp|grep kube-proxy|grep -E &#39;80|443&#39;</span><br><span class="line">tcp        0      0 192.168.100.212:80      0.0.0.0:*               LISTEN      690&#x2F;kube-proxy</span><br><span class="line">tcp        0      0 192.168.100.212:443     0.0.0.0:*               LISTEN      690&#x2F;kube-proxy</span><br><span class="line"></span><br><span class="line">$ sudo netstat -tlunp|grep kube-proxy|grep -E &#39;80|443&#39;</span><br><span class="line">tcp        0      0 192.168.100.213:80      0.0.0.0:*               LISTEN      748&#x2F;kube-proxy</span><br><span class="line">tcp        0      0 192.168.100.213:443     0.0.0.0:*               LISTEN      748&#x2F;kube-proxy</span><br></pre></td></tr></table></figure><h4 id="卸载-nginx-ingress-controller">卸载 Nginx Ingress Controller</h4><p>使用 Helm 卸载 Nginx Ingress Controller 非常的简单，只需下面一条指令就搞定了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm delete --purge nginx-ingress</span><br></pre></td></tr></table></figure><p>使用 <code>--purge</code> 参数可以彻底删除 Release 不留下任何记录，否则下一次部署的时候不能使用重名的 Release。</p><h3 id="部署-ingress">部署 Ingress</h3><p>接下来，我们通过 Helm 以 Ingress 方式在 Kubernetes 集群中部署两个应用。由于测试环境没有使用 PersistentVolume（持久卷，简称 PV），下面两个例子中都暂时将其关闭。有关于 PersistentVolume 的知识点，我将在后面的文章来讲一讲，敬请期待。</p><h4 id="部署-dokuwiki">部署 DokuWiki</h4><p>DokuWiki 是一个针对小公司文件需求而开发的 Wiki 引擎，用 PHP 语言开发。DokuWiki 基于文本存储，不需要数据库。因为 DokuWiki 非常的轻量，所以我选择它来做演示。</p><p>这里我们使用 Helm 官方仓库里的 Chart 包来进行，因为阿里云镜像仓库中的很多 Chart 都不是最新的版本并且不支持以 Ingress 方式部署。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 从 Helm 官方 Chart 仓库迁出所有软件包</span><br><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;helm&#x2F;charts.git</span><br></pre></td></tr></table></figure><p>使用 <code>helm install</code> 进行一键部署，并通过 <code>ingress.enabled=true</code> 和 <code>ingress.hosts[0].name=wiki.hi-linux.com</code> 参数启用 Ingress 特性和设置对应的主机名。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">$ cd &#x2F;home&#x2F;k8s&#x2F;charts&#x2F;stable</span><br><span class="line">$ helm install --name dokuwiki --set &quot;ingress.enabled&#x3D;true,ingress.hosts[0].name&#x3D;wiki.hi-linux.com,persistence.enabled&#x3D;false&quot; dokuwiki</span><br><span class="line"></span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;Ingress</span><br><span class="line">NAME               HOSTS              ADDRESS  PORTS  AGE</span><br><span class="line">dokuwiki-dokuwiki  wiki.hi-linux.com  80       53s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                                READY  STATUS   RESTARTS  AGE</span><br><span class="line">dokuwiki-dokuwiki-747b45cddb-qt8l2  1&#x2F;1    Running  0         53s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Secret</span><br><span class="line">NAME               TYPE    DATA  AGE</span><br><span class="line">dokuwiki-dokuwiki  Opaque  1     54s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME               TYPE          CLUSTER-IP      EXTERNAL-IP  PORT(S)                   AGE</span><br><span class="line">dokuwiki-dokuwiki  LoadBalancer  10.254.235.137  &lt;pending&gt;    80:8430&#x2F;TCP,443:8848&#x2F;TCP  54s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;Deployment</span><br><span class="line">NAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">dokuwiki-dokuwiki  1        1        1           1          54s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line"></span><br><span class="line">** Please be patient while the chart is being deployed **</span><br><span class="line"></span><br><span class="line">1. Get the DokuWiki URL indicated on the Ingress Rule and associate it to your cluster external IP:</span><br><span class="line"></span><br><span class="line">   export CLUSTER_IP&#x3D;$(minikube ip) # On Minikube. Use: &#96;kubectl cluster-info&#96; on others K8s clusters</span><br><span class="line">   export HOSTNAME&#x3D;$(kubectl get ingress --namespace default dokuwiki-dokuwiki -o jsonpath&#x3D;&#39;&#123;.spec.rules[0].host&#125;&#39;)</span><br><span class="line">   echo &quot;Dokuwiki URL: http:&#x2F;&#x2F;$HOSTNAME&#x2F;&quot;</span><br><span class="line">   echo &quot;$CLUSTER_IP  $HOSTNAME&quot; | sudo tee -a &#x2F;etc&#x2F;hosts</span><br><span class="line"></span><br><span class="line">2. Login with the following credentials</span><br><span class="line"></span><br><span class="line">  echo Username: user</span><br><span class="line">  echo Password: $(kubectl get secret --namespace default dokuwiki-dokuwiki -o jsonpath&#x3D;&quot;&#123;.data.dokuwiki-password&#125;&quot; | base64 --decode)</span><br></pre></td></tr></table></figure><p>部署完成后，根据提示生成相应的登陆用户名和密码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ echo Username: user</span><br><span class="line">Username: user</span><br><span class="line"></span><br><span class="line">$ echo Password: $(kubectl get secret --namespace default dokuwiki-dokuwiki -o jsonpath&#x3D;&quot;&#123;.data.dokuwiki-password&#125;&quot; | base64 --decode)</span><br><span class="line">Password: e2GrABBkwF</span><br></pre></td></tr></table></figure><p>测试从各节点的宿主机 IP 访问应用，这里我们直接使用 Curl 命令进行访问。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">$ curl -I  http:&#x2F;&#x2F;wiki.hi-linux.com&#x2F;doku.php -x 192.168.100.211:80</span><br><span class="line">HTTP&#x2F;1.1 200 OK</span><br><span class="line">Server: nginx&#x2F;1.13.8</span><br><span class="line">Date: Wed, 25 Jul 2018 05:16:02 GMT</span><br><span class="line">Content-Type: text&#x2F;html; charset&#x3D;utf-8</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Vary: Accept-Encoding</span><br><span class="line">X-Powered-By: PHP&#x2F;7.0.31</span><br><span class="line">Vary: Cookie,Accept-Encoding</span><br><span class="line">Set-Cookie: DokuWiki&#x3D;k2clt6f2qe472ehsq6tcmh6v20; path&#x3D;&#x2F;; HttpOnly</span><br><span class="line">Expires: Thu, 19 Nov 1981 08:52:00 GMT</span><br><span class="line">Cache-Control: no-store, no-cache, must-revalidate</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Set-Cookie: DW68700bfd16c2027de7de74a5a8202a6f&#x3D;deleted; expires&#x3D;Thu, 01-Jan-1970 00:00:01 GMT; Max-Age&#x3D;0; path&#x3D;&#x2F;; HttpOnly</span><br><span class="line">X-UA-Compatible: IE&#x3D;edge,chrome&#x3D;1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ curl -I  http:&#x2F;&#x2F;wiki.hi-linux.com&#x2F;doku.php -x 192.168.100.212:80</span><br><span class="line">HTTP&#x2F;1.1 200 OK</span><br><span class="line">Server: nginx&#x2F;1.13.8</span><br><span class="line">Date: Wed, 25 Jul 2018 05:18:13 GMT</span><br><span class="line">Content-Type: text&#x2F;html; charset&#x3D;utf-8</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Vary: Accept-Encoding</span><br><span class="line">X-Powered-By: PHP&#x2F;7.0.31</span><br><span class="line">Vary: Cookie,Accept-Encoding</span><br><span class="line">Set-Cookie: DokuWiki&#x3D;ork8sv8qpurteblasuq3eb3nt2; path&#x3D;&#x2F;; HttpOnly</span><br><span class="line">Expires: Thu, 19 Nov 1981 08:52:00 GMT</span><br><span class="line">Cache-Control: no-store, no-cache, must-revalidate</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Set-Cookie: DW68700bfd16c2027de7de74a5a8202a6f&#x3D;deleted; expires&#x3D;Thu, 01-Jan-1970 00:00:01 GMT; Max-Age&#x3D;0; path&#x3D;&#x2F;; HttpOnly</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ curl -I  http:&#x2F;&#x2F;wiki.hi-linux.com&#x2F;doku.php -x 192.168.100.213:80</span><br><span class="line">HTTP&#x2F;1.1 200 OK</span><br><span class="line">Server: nginx&#x2F;1.13.8</span><br><span class="line">Date: Wed, 25 Jul 2018 05:18:30 GMT</span><br><span class="line">Content-Type: text&#x2F;html; charset&#x3D;utf-8</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Vary: Accept-Encoding</span><br><span class="line">X-Powered-By: PHP&#x2F;7.0.31</span><br><span class="line">Vary: Cookie,Accept-Encoding</span><br><span class="line">Set-Cookie: DokuWiki&#x3D;6ulgtsddqq3rlo0mriavj64jc4; path&#x3D;&#x2F;; HttpOnly</span><br><span class="line">Expires: Thu, 19 Nov 1981 08:52:00 GMT</span><br><span class="line">Cache-Control: no-store, no-cache, must-revalidate</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Set-Cookie: DW68700bfd16c2027de7de74a5a8202a6f&#x3D;deleted; expires&#x3D;Thu, 01-Jan-1970 00:00:01 GMT; Max-Age&#x3D;0; path&#x3D;&#x2F;; HttpOnly</span><br><span class="line">X-UA-Compatible: IE&#x3D;edge,chrome&#x3D;1</span><br></pre></td></tr></table></figure><p>Curl 用法很多，你也可以使用下面方式来达到相同的效果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -H &quot;Host:wiki.hi-linux.com&quot;  &quot;http:&#x2F;&#x2F;192.168.100.211&#x2F;doku.php&quot;</span><br></pre></td></tr></table></figure><p>当然你也可以在本地 hosts 文件中对 IP 和域名进行绑定后，通过浏览器访问该应用。效果图如下：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-ingress01.png" alt=""></p><h4 id="部署-minio">部署 Minio</h4><p>Minio 是一个基于 Apache License v2.0 开源协议的对象存储服务，Minio 使用 Go 语言开发，具有良好的跨平台性，同样是一个非常轻量的服务。它兼容亚马逊 S3 云存储服务接口，非常适合于存储大容量非结构化的数据，例如：图片、视频、日志文件、备份数据和容器/虚拟机镜像等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 从 Helm 官方 Chart 仓库迁出所有软件包</span><br><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;helm&#x2F;charts.git</span><br></pre></td></tr></table></figure><p>如果你需要修改主机名，请修改 values.yaml 文件中的 hosts 的值。我想你一定觉得很奇怪，为什么在这个例子我没用使用传递参数的方式来动态修改模板中对应的值？真相只有一个，哪就是我没有找到能成功修改模板中对应的变量，惊不惊喜，意不意外呢？哈哈哈。如果你知道可以留言告诉我哟！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cd &#x2F;home&#x2F;k8s&#x2F;charts&#x2F;stable</span><br><span class="line">$ cat minio&#x2F;values.yaml</span><br><span class="line">  hosts:</span><br><span class="line">    - minio.hi-linux.com</span><br><span class="line">    #- chart-example.local</span><br></pre></td></tr></table></figure><p>使用 <code>helm install</code> 进行一键部署，并通过 <code>ingress.enabled=true</code> 参数启用 Ingress 特性。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --name minio  --set &quot;ingress.enabled&#x3D;true,persistence.enabled&#x3D;false&quot; minio</span><br><span class="line"></span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;Ingress</span><br><span class="line">NAME   HOSTS               ADDRESS  PORTS  AGE</span><br><span class="line">minio  minio.hi-linux.com  80       1m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                   READY  STATUS   RESTARTS  AGE</span><br><span class="line">minio-7c7cf49d4-gqf8p  1&#x2F;1    Running  0         1m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Secret</span><br><span class="line">NAME   TYPE    DATA  AGE</span><br><span class="line">minio  Opaque  2     1m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;ConfigMap</span><br><span class="line">NAME   DATA  AGE</span><br><span class="line">minio  2     1m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME   TYPE       CLUSTER-IP  EXTERNAL-IP  PORT(S)   AGE</span><br><span class="line">minio  ClusterIP  None        &lt;none&gt;       9000&#x2F;TCP  1m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta2&#x2F;Deployment</span><br><span class="line">NAME   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">minio  1        1        1           1          1m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line"></span><br><span class="line">Minio can be accessed via port 9000 on the following DNS name from within your cluster:</span><br><span class="line">minio-svc.default.svc.cluster.local</span><br><span class="line"></span><br><span class="line">To access Minio from localhost, run the below commands:</span><br><span class="line"></span><br><span class="line">  1. export POD_NAME&#x3D;$(kubectl get pods --namespace default -l &quot;release&#x3D;minio&quot; -o jsonpath&#x3D;&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line"></span><br><span class="line">  2. kubectl port-forward $POD_NAME 9000 --namespace default</span><br><span class="line"></span><br><span class="line">Read more about port forwarding here: http:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;user-guide&#x2F;kubectl&#x2F;kubectl_port-forward&#x2F;</span><br><span class="line"></span><br><span class="line">You can now access Minio server on http:&#x2F;&#x2F;localhost:9000. Follow the below steps to connect to Minio server with mc client:</span><br><span class="line"></span><br><span class="line">  1. Download the Minio mc client - https:&#x2F;&#x2F;docs.minio.io&#x2F;docs&#x2F;minio-client-quickstart-guide</span><br><span class="line"></span><br><span class="line">  2. mc config host add minio-local http:&#x2F;&#x2F;localhost:9000 AKIAIOSFODNN7EXAMPLE wJalrXUtnFEMI&#x2F;K7MDENG&#x2F;bPxRfiCYEXAMPLEKEY S3v4</span><br><span class="line"></span><br><span class="line">  3. mc ls minio-local</span><br><span class="line"></span><br><span class="line">Alternately, you can use your browser or the Minio SDK to access the server - https:&#x2F;&#x2F;docs.minio.io&#x2F;categories&#x2F;17</span><br></pre></td></tr></table></figure><p>部署完成后，我们在本地 hosts 文件中对 IP 和域名进行绑定，并通过浏览器访问该应用。效果图如下：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-ingress03.png" alt=""></p><blockquote><p>登陆用户名和密码在部署完成后的提示信息中。</p></blockquote><p>最后我们在 Kubernetes 上来查看下部署成功后的 Ingress 信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get ingress</span><br><span class="line">NAME                HOSTS                ADDRESS   PORTS     AGE</span><br><span class="line">dokuwiki-dokuwiki   wiki.hi-linux.com              80        44m</span><br><span class="line">minio               minio.hi-linux.com             80        50s</span><br><span class="line">&#96;</span><br></pre></td></tr></table></figure><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/Revoeu7" target="_blank" rel="noopener">http://t.cn/Revoeu7</a><br><a href="http://t.cn/ReZkSqf" target="_blank" rel="noopener">http://t.cn/ReZkSqf</a></p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ingress 是一种 Kubernetes 资源，也是将 Kubernetes 集群内服务暴露到外部的一种方式。本文将讲一讲如何用 Helm 在 Kubernetes 集群中部署 Ingress，并部署两个应用来演示 Ingress 的具体使用。&lt;/p&gt;
&lt;p&gt;阅读本文前你需要先掌握 Helm 和一些 Kubernetes 服务暴露的相关知识点，如果你还不了解可以先读一读我之前写的 「&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247486154&amp;amp;idx=1&amp;amp;sn=becd5dd0fadfe0b6072f5dfdc6fdf786&amp;amp;chksm=eac52be3ddb2a2f555b8b1028db97aa3e92d0a4880b56f361e4b11cd252771147c44c08c8913#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Helm 入门指南&lt;/a&gt;」和「&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247486130&amp;amp;idx=1&amp;amp;sn=41ee30f02113dac86398653f542a3c70&amp;amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;浅析从外部访问 Kubernetes 集群中应用的几种方式&lt;/a&gt;」这两篇文章。&lt;/p&gt;
&lt;h3 id=&quot;部署-Ingress-Controller&quot;&gt;部署 Ingress Controller&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Ingress&lt;/code&gt; 只是一个统称，其由 &lt;code&gt;Ingress&lt;/code&gt; 和 &lt;code&gt;Ingress Controller&lt;/code&gt; 两部分组成。&lt;code&gt;Ingress&lt;/code&gt; 用作将原来需要手动配置的规则抽象成一个 Ingress 对象，使用 YAML 格式的文件来创建和管理。&lt;code&gt;Ingress Controller&lt;/code&gt; 用作通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化。&lt;/p&gt;
&lt;p&gt;目前可用的 Ingress Controller 类型有很多，比如：Nginx、HAProxy、Traefik 等，我们将演示如何部署一个基于 Nginx 的 Ingress Controller。&lt;/p&gt;
&lt;p&gt;这里我们使用 Helm 来部署，在开始部署前，请确认您已经安装和配置好 Helm 相关环境。&lt;/p&gt;
&lt;h4 id=&quot;查找软件仓库中是否有-Nginx-Ingress-包&quot;&gt;查找软件仓库中是否有 Nginx Ingress 包&lt;/h4&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ helm search nginx-ingress&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;NAME                	CHART VERSION	APP VERSION	DESCRIPTION&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;stable&amp;#x2F;nginx-ingress	0.9.5        	0.10.2     	An nginx Ingress controller that uses ConfigMap...&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;注：这里我们使用的是在阿里云 Helm 镜像仓库。如果你还不知道如何增加三方仓库，可先阅读 「&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247486154&amp;amp;idx=1&amp;amp;sn=becd5dd0fadfe0b6072f5dfdc6fdf786&amp;amp;chksm=eac52be3ddb2a2f555b8b1028db97aa3e92d0a4880b56f361e4b11cd252771147c44c08c8913#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Helm 入门指南&lt;/a&gt;」一文。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;阿里云 Helm 镜像仓库里的 nginx-ingress 软件包已经将要用到的相关容器镜像地址改成了国内可访问的地址。安装时需要用到的容器镜像有：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;repository: registry.cn-hangzhou.aliyuncs.com&amp;#x2F;google_containers&amp;#x2F;nginx-ingress-controller:0.10.2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;repository: registry.cn-hangzhou.aliyuncs.com&amp;#x2F;google_containers&amp;#x2F;defaultbackend:1.3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;repository: sophos&amp;#x2F;nginx-vts-exporter:0.6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>推荐一款比 Find 快 10 倍的搜索工具 FD</title>
    <link href="https://www.hi-linux.com/posts/15017.html"/>
    <id>https://www.hi-linux.com/posts/15017.html</id>
    <published>2018-08-13T01:00:00.000Z</published>
    <updated>2018-08-24T06:07:47.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>fd 是基于 Rust 开发的一个速度超快的命令行搜索工具，fd 旨在成为 Linux / Unix 下 find 命令的替代品。</p><p>fd 虽然不能提供现在 find 命令所有的强大功能，但它也提供了足够强大的功能来满足你日常需要。比如：简洁的语法、彩色的终端输出、超快的查询速度、智能大小写、支持正则表达式以及可并行执行命令等特性。</p><p>项目地址：<a href="https://github.com/sharkdp/fd" target="_blank" rel="noopener">https://github.com/sharkdp/fd</a></p><h3 id="安装-fd">安装 fd</h3><p>fd 具有良好跨平台特性，支持在 Linux、macOS、Windows 等多种平台下安装。下面我们介绍下几个比较常用平台的安装方法：</p><ul><li>Ubuntu / Debain</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ wget https:&#x2F;&#x2F;github.com&#x2F;sharkdp&#x2F;fd&#x2F;releases&#x2F;download&#x2F;v7.0.0&#x2F;fd_7.0.0_amd64.deb</span><br><span class="line">$ sudo dpkg -i fd_7.0.0_amd64.deb</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>Fedora</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ dnf install fd-find</span><br></pre></td></tr></table></figure><ul><li>macOS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install fd</span><br></pre></td></tr></table></figure><ul><li>Windows</li></ul><p><a href="http://scoop.sh/" target="_blank" rel="noopener">Scoop</a> 和 <a href="https://chocolatey.org/" target="_blank" rel="noopener">Chocolatey</a> 都是 Windows 下的包管理系统，其具体使用方法都可参考其官网。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 通过 Scoop 安装</span><br><span class="line">$ scoop install fd</span><br><span class="line"></span><br><span class="line"># 通过 Chocolatey 安装</span><br><span class="line">$ choco install fd</span><br></pre></td></tr></table></figure><p>更多系统的安装方法可参考<a href="https://github.com/sharkdp/fd" target="_blank" rel="noopener">官方文档</a>。</p><h3 id="fd-命令行选项">fd 命令行选项</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">USAGE:</span><br><span class="line">    fd [FLAGS&#x2F;OPTIONS] [&lt;pattern&gt;] [&lt;path&gt;...]</span><br><span class="line"></span><br><span class="line">FLAGS:</span><br><span class="line">    -H, --hidden            Search hidden files and directories</span><br><span class="line">    -I, --no-ignore         Do not respect .(git|fd)ignore files</span><br><span class="line">        --no-ignore-vcs     Do not respect .gitignore files</span><br><span class="line">    -s, --case-sensitive    Case-sensitive search (default: smart case)</span><br><span class="line">    -i, --ignore-case       Case-insensitive search (default: smart case)</span><br><span class="line">    -F, --fixed-strings     Treat the pattern as a literal string</span><br><span class="line">    -a, --absolute-path     Show absolute instead of relative paths</span><br><span class="line">    -L, --follow            Follow symbolic links</span><br><span class="line">    -p, --full-path         Search full path (default: file-&#x2F;dirname only)</span><br><span class="line">    -0, --print0            Separate results by the null character</span><br><span class="line">    -h, --help              Prints help information</span><br><span class="line">    -V, --version           Prints version information</span><br><span class="line"></span><br><span class="line">OPTIONS:</span><br><span class="line">    -d, --max-depth &lt;depth&gt;        Set maximum search depth (default: none)</span><br><span class="line">    -t, --type &lt;filetype&gt;...       Filter by type: file (f), directory (d), symlink (l),</span><br><span class="line">                                   executable (x)</span><br><span class="line">    -e, --extension &lt;ext&gt;...       Filter by file extension</span><br><span class="line">    -x, --exec &lt;cmd&gt;               Execute a command for each search result</span><br><span class="line">    -E, --exclude &lt;pattern&gt;...     Exclude entries that match the given glob pattern</span><br><span class="line">        --ignore-file &lt;path&gt;...    Add a custom ignore-file in .gitignore format</span><br><span class="line">    -c, --color &lt;when&gt;             When to use colors: never, *auto*, always</span><br><span class="line">    -j, --threads &lt;num&gt;            Set number of threads to use for searching &amp; executing</span><br><span class="line"></span><br><span class="line">ARGS:</span><br><span class="line">    &lt;pattern&gt;    the search pattern, a regular expression (optional)</span><br><span class="line">    &lt;path&gt;...    the root directory for the filesystem search (optional)</span><br></pre></td></tr></table></figure><h3 id="fd-使用实例">fd 使用实例</h3><h4 id="简单搜索">简单搜索</h4><p>fd 只需带上一个需要查找的参数就可以执行最简单的搜索，该参数就是你要搜索的任何东西。例如：你想要找一个包含 pace 关键字的文件名或目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ fd pace</span><br><span class="line">source&#x2F;lib&#x2F;Han&#x2F;dist&#x2F;font&#x2F;han-space.otf</span><br><span class="line">source&#x2F;lib&#x2F;Han&#x2F;dist&#x2F;font&#x2F;han-space.woff</span><br><span class="line">source&#x2F;lib&#x2F;pace</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-barber-shop.min.css</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-big-counter.min.css</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-bounce.min.css</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-center-atom.min.css</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-center-circle.min.css</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-center-radar.min.css</span><br></pre></td></tr></table></figure><blockquote><p>注：fd 默认是不区分大小写和支持模糊查询的。</p></blockquote><h4 id="按指定类型进行搜索">按指定类型进行搜索</h4><p>默认情况下，fd 会搜索所有符合条件的结果。如果你想指定搜索的类型可以使用 <code>-t</code> 参数，fd 目前支持四种类型：<code>f</code>、<code>d</code>、<code>l</code>、<code>x</code>，分别表示：文件、目录、符号链接、可执行文件。下面我们来看几个实际的例子：</p><ul><li>只搜索包含 pace 关键字的文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ fd -tf pace</span><br><span class="line">source&#x2F;lib&#x2F;Han&#x2F;dist&#x2F;font&#x2F;han-space.otf</span><br><span class="line">source&#x2F;lib&#x2F;Han&#x2F;dist&#x2F;font&#x2F;han-space.woff</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-barber-shop.min.css</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-big-counter.min.css</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-bounce.min.css</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-center-atom.min.css</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-center-circle.min.css</span><br><span class="line">source&#x2F;lib&#x2F;pace&#x2F;pace-theme-center-radar.min.css</span><br></pre></td></tr></table></figure><ul><li>只搜索包含 pace 关键字的目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ fd -td pace</span><br><span class="line">source&#x2F;lib&#x2F;pace</span><br></pre></td></tr></table></figure><h4 id="搜索指定目录">搜索指定目录</h4><p>fd 默认会在当前目录和其下所有子目录中搜索，如果你想搜索指定的目录就需要在第二个参数中指定。例如：要在指定的 <code>/etc</code> 目录中搜索包含 passwd 关键字的文件或目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ fd passwd &#x2F;etc</span><br><span class="line">&#x2F;etc&#x2F;master.passwd</span><br><span class="line">&#x2F;etc&#x2F;pam.d&#x2F;chkpasswd</span><br><span class="line">&#x2F;etc&#x2F;pam.d&#x2F;passwd</span><br><span class="line">&#x2F;etc&#x2F;passwd</span><br></pre></td></tr></table></figure><h4 id="通过正则表达式搜索">通过正则表达式搜索</h4><ul><li>搜索当前目录下以 head 开头并以 swig 结尾的文件。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ fd &#39;^head.*swig$&#39;</span><br><span class="line">layout&#x2F;_custom&#x2F;header.swig</span><br><span class="line">layout&#x2F;_partials&#x2F;head.swig</span><br><span class="line">layout&#x2F;_partials&#x2F;header.swig</span><br></pre></td></tr></table></figure><ul><li>搜索当前目录下文件名包含字母且文件名后缀为 PNG 的文件。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ fd &#39;[a-z]\.png$&#39;</span><br><span class="line">source&#x2F;images&#x2F;apple-touch-icon-next.png</span><br><span class="line">source&#x2F;images&#x2F;searchicon.png</span><br><span class="line">source&#x2F;lib&#x2F;fancybox&#x2F;source&#x2F;fancybox_overlay.png</span><br><span class="line">source&#x2F;lib&#x2F;fancybox&#x2F;source&#x2F;fancybox_sprite.png</span><br><span class="line">source&#x2F;lib&#x2F;fancybox&#x2F;source&#x2F;helpers&#x2F;fancybox_buttons.png</span><br></pre></td></tr></table></figure><h4 id="其它技巧">其它技巧</h4><ul><li>搜索隐藏文件</li></ul><p>fd 支持隐藏文件搜索，如果你需要搜索隐藏文件可以加上 <code>-H</code> 参数。例如：在当前目录下搜索关键字为 zshrc 的隐藏文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ fd -H zshrc</span><br><span class="line">.zshrc</span><br></pre></td></tr></table></figure><ul><li>搜索指定扩展名的文件</li></ul><p>在当前目录下搜索文件扩展名为 md 的文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ fd -e md</span><br><span class="line">README.cn.md</span><br><span class="line">README.md</span><br><span class="line">source&#x2F;lib&#x2F;fastclick&#x2F;README.md</span><br><span class="line">source&#x2F;lib&#x2F;jquery_lazyload&#x2F;CONTRIBUTING.md</span><br><span class="line">source&#x2F;lib&#x2F;jquery_lazyload&#x2F;README.md</span><br></pre></td></tr></table></figure><p>在当前目录下搜索文件名包含 reademe 且扩展名为 md 的文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ fd -e md readme</span><br><span class="line">README.cn.md</span><br><span class="line">README.md</span><br><span class="line">source&#x2F;lib&#x2F;fastclick&#x2F;README.md</span><br><span class="line">source&#x2F;lib&#x2F;jquery_lazyload&#x2F;README.md</span><br></pre></td></tr></table></figure><ul><li>排除特定的目录或文件</li></ul><p>搜索当前目录下除 lib 目录外的所有包含关键字 readme 的文件或目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ fd -E lib readme</span><br><span class="line">README.cn.md</span><br><span class="line">README.md</span><br></pre></td></tr></table></figure><p>搜索指定目录下除文件名后缀为 js 的所有文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ fd  -E &#39;*.js&#39; -tf  . source&#x2F;lib&#x2F;fastclick</span><br><span class="line">source&#x2F;lib&#x2F;fastclick&#x2F;LICENSE</span><br><span class="line">source&#x2F;lib&#x2F;fastclick&#x2F;README.md</span><br><span class="line">source&#x2F;lib&#x2F;fastclick&#x2F;bower.json</span><br></pre></td></tr></table></figure><ul><li>结合外部命令对结果进行批量处理</li></ul><p>实现的方式有两种：一是和 find 命令的类似的处理方法，通过 <code>xargs</code> 命令来关联相关命令处理。二是通过 fd 自己的 <code>-x</code> 参数来实现。</p><p>我们来看一个具体的例子，统计当前目录下所有文件名后缀为 js 的文件的行数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 通过 -x 参数实现</span><br><span class="line">$ fd -e js -x  wc  -l</span><br><span class="line"></span><br><span class="line">      30 scripts&#x2F;merge-configs.js</span><br><span class="line">    2225 scripts&#x2F;merge.js</span><br><span class="line">      31 scripts&#x2F;tags&#x2F;button.js</span><br><span class="line">      12 scripts&#x2F;tags&#x2F;center-quote.js</span><br><span class="line">      59 scripts&#x2F;tags&#x2F;exturl.js</span><br><span class="line">      26 scripts&#x2F;tags&#x2F;full-image.js</span><br><span class="line">     833 scripts&#x2F;tags&#x2F;group-pictures.js</span><br><span class="line"></span><br><span class="line"># 通过 xargs 参数实现</span><br><span class="line">$ fd -0 -e js | xargs -0 wc  -l</span><br><span class="line">      30 scripts&#x2F;merge-configs.js</span><br><span class="line">    2225 scripts&#x2F;merge.js</span><br><span class="line">      31 scripts&#x2F;tags&#x2F;button.js</span><br><span class="line">      12 scripts&#x2F;tags&#x2F;center-quote.js</span><br><span class="line">      59 scripts&#x2F;tags&#x2F;exturl.js</span><br><span class="line">      26 scripts&#x2F;tags&#x2F;full-image.js</span><br><span class="line">     833 scripts&#x2F;tags&#x2F;group-pictures.js</span><br></pre></td></tr></table></figure><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RD03Aom" target="_blank" rel="noopener">http://t.cn/RD03Aom</a><br><a href="http://t.cn/ROV0Xos" target="_blank" rel="noopener">http://t.cn/ROV0Xos</a></p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;fd 是基于 Rust 开发的一个速度超快的命令行搜索工具，fd 旨在成为 Linux / Unix 下 find 命令的替代品。&lt;/p&gt;
&lt;p&gt;fd 虽然不能提供现在 find 命令所有的强大功能，但它也提供了足够强大的功能来满足你日常需要。比如：简洁的语法、彩色的终端输出、超快的查询速度、智能大小写、支持正则表达式以及可并行执行命令等特性。&lt;/p&gt;
&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/sharkdp/fd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/sharkdp/fd&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装-fd&quot;&gt;安装 fd&lt;/h3&gt;
&lt;p&gt;fd 具有良好跨平台特性，支持在 Linux、macOS、Windows 等多种平台下安装。下面我们介绍下几个比较常用平台的安装方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ubuntu / Debain&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ wget https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;sharkdp&amp;#x2F;fd&amp;#x2F;releases&amp;#x2F;download&amp;#x2F;v7.0.0&amp;#x2F;fd_7.0.0_amd64.deb&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo dpkg -i fd_7.0.0_amd64.deb&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="工具" scheme="https://www.hi-linux.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="工具" scheme="https://www.hi-linux.com/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>Helm 入门指南</title>
    <link href="https://www.hi-linux.com/posts/21466.html"/>
    <id>https://www.hi-linux.com/posts/21466.html</id>
    <published>2018-08-10T01:00:00.000Z</published>
    <updated>2018-08-24T05:29:01.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Helm 是 Kubernetes 生态系统中的一个软件包管理工具。本文将介绍 Helm 中的相关概念和基本工作原理，并通过一个具体的示例学习如何使用 Helm 打包、分发、安装、升级及回退 Kubernetes 应用。</p><h3 id="kubernetes-应用部署的挑战">Kubernetes 应用部署的挑战</h3><p>Kubernetes 是一个提供了基于容器的应用集群管理解决方案，Kubernetes 为容器化应用提供了部署运行、资源调度、服务发现和动态伸缩等一系列完整功能。</p><p>Kubernetes 的核心设计理念是: 用户定义要部署的应用程序的规则，而 Kubernetes 则负责按照定义的规则部署并运行应用程序。如果应用程序出现问题导致偏离了定义的规格，Kubernetes 负责对其进行自动修正。例如：定义的应用规则要求部署两个实例（Pod），其中一个实例异常终止了，Kubernetes 会检查到并重新启动一个新的实例。</p><p>用户通过使用 Kubernetes API 对象来描述应用程序规则，包括 Pod、Service、Volume、Namespace、ReplicaSet、Deployment、Job等等。一般这些资源对象的定义需要写入一系列的 YAML 文件中，然后通过 Kubernetes 命令行工具 Kubectl 调 Kubernetes API 进行部署。</p><p>以一个典型的三层应用 Wordpress 为例，该应用程序就涉及到多个 Kubernetes API 对象，而要描述这些 Kubernetes API 对象就可能要同时维护多个 YAML 文件。</p><p><img src="https://www.hi-linux.com/img/linux/helm01.png" alt=""></p><p>从上图可以看到，在进行 Kubernetes 软件部署时，我们面临下述几个问题：</p><ul><li>如何管理、编辑和更新这些这些分散的 Kubernetes 应用配置文件。</li><li>如何把一套相关的配置文件作为一个应用进行管理。</li><li>如何分发和重用 Kubernetes 的应用配置。</li></ul><p>Helm 的出现就是为了很好地解决上面这些问题。</p><a id="more"></a><h3 id="helm-是什么">Helm 是什么？</h3><p>Helm 是 <a href="https://deis.com/" target="_blank" rel="noopener">Deis</a> 开发的一个用于 Kubernetes 应用的包管理工具，主要用来管理 Charts。有点类似于 Ubuntu 中的 APT 或 CentOS 中的 YUM。</p><p>Helm Chart 是用来封装 Kubernetes 原生应用程序的一系列 YAML 文件。可以在你部署应用的时候自定义应用程序的一些 Metadata，以便于应用程序的分发。</p><p>对于应用发布者而言，可以通过 Helm 打包应用、管理应用依赖关系、管理应用版本并发布应用到软件仓库。</p><p>对于使用者而言，使用 Helm 后不用需要编写复杂的应用部署文件，可以以简单的方式在 Kubernetes 上查找、安装、升级、回滚、卸载应用程序。</p><h3 id="helm-组件及相关术语">Helm 组件及相关术语</h3><ul><li>Helm</li></ul><p>Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。</p><ul><li>Tiller</li></ul><p>Tiller 是 Helm 的服务端，部署在 Kubernetes 集群中。Tiller 用于接收 Helm 的请求，并根据 Chart 生成 Kubernetes 的部署文件（ Helm 称为 Release ），然后提交给 Kubernetes 创建应用。Tiller 还提供了 Release 的升级、删除、回滚等一系列功能。</p><ul><li>Chart</li></ul><p>Helm 的软件包，采用 TAR 格式。类似于 APT 的 DEB 包或者 YUM 的 RPM 包，其包含了一组定义 Kubernetes 资源相关的 YAML 文件。</p><ul><li>Repoistory</li></ul><p>Helm 的软件仓库，Repository 本质上是一个 Web 服务器，该服务器保存了一系列的 Chart 软件包以供用户下载，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。</p><ul><li>Release</li></ul><p>使用 <code>helm install</code> 命令在 Kubernetes 集群中部署的 Chart 称为 Release。</p><blockquote><p>注：需要注意的是：Helm 中提到的 Release 和我们通常概念中的版本有所不同，这里的 Release 可以理解为 Helm 使用 Chart 包部署的一个应用实例。</p></blockquote><h3 id="helm-工作原理">Helm 工作原理</h3><p>这张图描述了 Helm 的几个关键组件 Helm（客户端）、Tiller（服务器）、Repository（Chart 软件仓库）、Chart（软件包）之间的关系。</p><p><img src="https://www.hi-linux.com/img/linux/helm02.png" alt=""></p><p><strong>Chart Install 过程</strong></p><ul><li>Helm 从指定的目录或者 TAR 文件中解析出 Chart 结构信息。</li><li>Helm 将指定的 Chart 结构和 Values 信息通过 gRPC 传递给 Tiller。</li><li>Tiller 根据 Chart 和 Values 生成一个 Release。</li><li>Tiller 将 Release 发送给 Kubernetes 用于生成 Release。</li></ul><p><strong>Chart Update 过程</strong></p><ul><li>Helm 从指定的目录或者 TAR 文件中解析出 Chart 结构信息。</li><li>Helm 将需要更新的 Release 的名称、Chart 结构和 Values 信息传递给 Tiller。</li><li>Tiller 生成 Release 并更新指定名称的 Release 的 History。</li><li>Tiller 将 Release 发送给 Kubernetes 用于更新 Release。</li></ul><p><strong>Chart Rollback 过程</strong></p><ul><li>Helm 将要回滚的 Release 的名称传递给 Tiller。</li><li>Tiller 根据 Release 的名称查找 History。</li><li>Tiller 从 History 中获取上一个 Release。</li><li>Tiller 将上一个 Release 发送给 Kubernetes 用于替换当前 Release。</li></ul><p><strong>Chart 处理依赖说明</strong></p><p>Tiller 在处理 Chart 时，直接将 Chart 以及其依赖的所有 Charts 合并为一个 Release，同时传递给 Kubernetes。因此 Tiller 并不负责管理依赖之间的启动顺序。Chart 中的应用需要能够自行处理依赖关系。</p><h3 id="部署-helm">部署 Helm</h3><h4 id="安装-helm-客户端">安装 Helm 客户端</h4><p>Helm 的安装方式很多，这里采用二进制的方式安装。更多安装方法可以参考 Helm 的<a href="https://docs.helm.sh/using_helm/#installing-helm" target="_blank" rel="noopener">官方帮助文档</a>。</p><ul><li>使用官方提供的脚本一键安装</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ curl https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;kubernetes&#x2F;helm&#x2F;master&#x2F;scripts&#x2F;get &gt; get_helm.sh</span><br><span class="line">$ chmod 700 get_helm.sh</span><br><span class="line">$ .&#x2F;get_helm.sh</span><br></pre></td></tr></table></figure><ul><li>手动下载安装</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 下载 Helm </span><br><span class="line">$ wget https:&#x2F;&#x2F;storage.googleapis.com&#x2F;kubernetes-helm&#x2F;helm-v2.9.1-linux-amd64.tar.gz</span><br><span class="line"># 解压 Helm</span><br><span class="line">$ tar -zxvf helm-v2.9.1-linux-amd64.tar.gz</span><br><span class="line"># 复制客户端执行文件到 bin 目录下</span><br><span class="line">$ cp linux-amd64&#x2F;helm &#x2F;usr&#x2F;local&#x2F;bin&#x2F;</span><br></pre></td></tr></table></figure><blockquote><p>注：<a href="http://storage.googleapis.com" target="_blank" rel="noopener">storage.googleapis.com</a> 默认是不能访问的，该问题请自行解决。</p></blockquote><h4 id="安装-helm-服务器端-tiller">安装 Helm 服务器端 Tiller</h4><p>Tiller 是以 Deployment 方式部署在 Kubernetes 集群中的，只需使用以下指令便可简单的完成安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm init</span><br></pre></td></tr></table></figure><p>由于 Helm 默认会去 <a href="http://storage.googleapis.com" target="_blank" rel="noopener">storage.googleapis.com</a> 拉取镜像，如果你当前执行的机器不能访问该域名的话可以使用以下命令来安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用阿里云镜像安装并把默认仓库设置为阿里云上的镜像仓库</span><br><span class="line">$ helm init --upgrade --tiller-image registry.cn-hangzhou.aliyuncs.com&#x2F;google_containers&#x2F;tiller:v2.9.1 --stable-repo-url https:&#x2F;&#x2F;kubernetes.oss-cn-hangzhou.aliyuncs.com&#x2F;charts</span><br></pre></td></tr></table></figure><h5 id="给-tiller-授权">给 Tiller 授权</h5><p>因为 Helm 的服务端 Tiller 是一个部署在 Kubernetes 中 Kube-System Namespace 下 的 Deployment，它会去连接 Kube-Api 在 Kubernetes 里创建和删除应用。</p><p>而从 Kubernetes 1.6 版本开始，API Server 启用了 RBAC 授权。目前的 Tiller 部署时默认没有定义授权的 ServiceAccount，这会导致访问 API Server 时被拒绝。所以我们需要明确为 Tiller 部署添加授权。</p><ul><li>创建 Kubernetes 的服务帐号和绑定角色</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deployment --all-namespaces</span><br><span class="line">NAMESPACE     NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">kube-system   tiller-deploy          1         1         1            1           1h</span><br><span class="line">$ kubectl create serviceaccount --namespace kube-system tiller</span><br><span class="line">$ kubectl create clusterrolebinding tiller-cluster-rule --clusterrole&#x3D;cluster-admin --serviceaccount&#x3D;kube-system:tiller</span><br></pre></td></tr></table></figure><ul><li>为 Tiller 设置帐号</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 使用 kubectl patch 更新 API 对象</span><br><span class="line">$ kubectl patch deploy --namespace kube-system tiller-deploy -p &#39;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&#39;</span><br><span class="line">deployment.extensions &quot;tiller-deploy&quot; patched</span><br></pre></td></tr></table></figure><ul><li>查看是否授权成功</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deploy --namespace kube-system   tiller-deploy  --output yaml|grep  serviceAccount</span><br><span class="line">serviceAccount: tiller</span><br><span class="line">serviceAccountName: tiller</span><br></pre></td></tr></table></figure><h5 id="验证-tiller-是否安装成功">验证 Tiller 是否安装成功</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get pods|grep tiller</span><br><span class="line">tiller-deploy-6d68f5c78f-nql2z          1&#x2F;1       Running   0          5m</span><br><span class="line"></span><br><span class="line">$ helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure><h4 id="卸载-helm-服务器端-tiller">卸载 Helm 服务器端 Tiller</h4><p>如果你需要在 Kubernetes 中卸载已部署的 Tiller，可使用以下命令完成卸载。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm reset</span><br></pre></td></tr></table></figure><h3 id="构建一个-helm-chart">构建一个 Helm Chart</h3><p>下面我们通过一个完整的示例来学习如何使用 Helm 创建、打包、分发、安装、升级及回退Kubernetes应用。</p><h4 id="创建一个名为-mychart-的-chart">创建一个名为 mychart 的 Chart</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm create mychart</span><br></pre></td></tr></table></figure><p>该命令创建了一个 mychart 目录，该目录结构如下所示。这里我们主要关注目录中的 Chart.yaml、values.yaml、NOTES.txt 和 Templates 目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ tree mychart&#x2F;</span><br><span class="line">mychart&#x2F;</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   └── service.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">2 directories, 7 files</span><br></pre></td></tr></table></figure><ul><li>Chart.yaml 用于描述这个 Chart的相关信息，包括名字、描述信息以及版本等。</li><li>values.yaml 用于存储 templates 目录中模板文件中用到变量的值。</li><li>NOTES.txt 用于介绍 Chart 部署后的一些信息，例如：如何使用这个 Chart、列出缺省的设置等。</li><li>Templates 目录下是 YAML 文件的模板，该模板文件遵循 Go template 语法。</li></ul><p>Templates 目录下 YAML 文件模板的值默认都是在 values.yaml 里定义的，比如在 deployment.yaml 中定义的容器镜像。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image: &quot;&#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag &#125;&#125;&quot;</span><br></pre></td></tr></table></figure><p>其中的 <code>.Values.image.repository</code> 的值就是在  values.yaml 里定义的 nginx，<code>.Values.image.tag</code> 的值就是 stable。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cat mychart&#x2F;values.yaml|grep repository</span><br><span class="line">repository: nginx</span><br><span class="line"></span><br><span class="line">$ cat mychart&#x2F;values.yaml|grep tag</span><br><span class="line">tag: stable</span><br></pre></td></tr></table></figure><p>以上两个变量值是在 <code>create chart</code> 的时候就自动生成的默认值，你可以根据实际情况进行修改。</p><blockquote><p>如果你需要了解更多关于 Go 模板的相关信息，可以查看 <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a> 的一个关于 <a href="https://gohugo.io/templates/go-templates/" target="_blank" rel="noopener">Go 模板</a> 的介绍。</p></blockquote><h4 id="编写应用的介绍信息">编写应用的介绍信息</h4><p>打开 Chart.yaml, 填写你部署的应用的详细信息，以 mychart 为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cat mychart&#x2F;Chart.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: &quot;1.0&quot;</span><br><span class="line">description: A Helm chart for Kubernetes</span><br><span class="line">name: mychart</span><br><span class="line">version: 0.1.0</span><br></pre></td></tr></table></figure><h4 id="编写应用具体部署信息">编写应用具体部署信息</h4><p>编辑 values.yaml，它默认会在 Kubernetes 部署一个 Nginx。下面是 mychart 应用的 values.yaml 文件的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">$ cat mychart&#x2F;values.yaml</span><br><span class="line"># Default values for mychart.</span><br><span class="line"># This is a YAML-formatted file.</span><br><span class="line"># Declare variables to be passed into your templates.</span><br><span class="line"></span><br><span class="line">replicaCount: 1</span><br><span class="line"></span><br><span class="line">image:</span><br><span class="line">  repository: nginx</span><br><span class="line">  tag: stable</span><br><span class="line">  pullPolicy: IfNotPresent</span><br><span class="line"></span><br><span class="line">service:</span><br><span class="line">  type: ClusterIP</span><br><span class="line">  port: 80</span><br><span class="line"></span><br><span class="line">ingress:</span><br><span class="line">  enabled: false</span><br><span class="line">  annotations: &#123;&#125;</span><br><span class="line">    # kubernetes.io&#x2F;ingress.class: nginx</span><br><span class="line">    # kubernetes.io&#x2F;tls-acme: &quot;true&quot;</span><br><span class="line">  path: &#x2F;</span><br><span class="line">  hosts:</span><br><span class="line">    - chart-example.local</span><br><span class="line">  tls: []</span><br><span class="line">  #  - secretName: chart-example-tls</span><br><span class="line">  #    hosts:</span><br><span class="line">  #      - chart-example.local</span><br><span class="line"></span><br><span class="line">resources: &#123;&#125;</span><br><span class="line">  # We usually recommend not to specify default resources and to leave this as a conscious</span><br><span class="line">  # choice for the user. This also increases chances charts run on environments with little</span><br><span class="line">  # resources, such as Minikube. If you do want to specify resources, uncomment the following</span><br><span class="line">  # lines, adjust them as necessary, and remove the curly braces after &#39;resources:&#39;.</span><br><span class="line">  # limits:</span><br><span class="line">  #  cpu: 100m</span><br><span class="line">  #  memory: 128Mi</span><br><span class="line">  # requests:</span><br><span class="line">  #  cpu: 100m</span><br><span class="line">  #  memory: 128Mi</span><br><span class="line"></span><br><span class="line">nodeSelector: &#123;&#125;</span><br><span class="line"></span><br><span class="line">tolerations: []</span><br><span class="line"></span><br><span class="line">affinity: &#123;&#125;</span><br></pre></td></tr></table></figure><h4 id="检查依赖和模板配置是否正确">检查依赖和模板配置是否正确</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ helm lint mychart&#x2F;</span><br><span class="line">&#x3D;&#x3D;&gt; Linting .</span><br><span class="line">[INFO] Chart.yaml: icon is recommended</span><br><span class="line"></span><br><span class="line">1 chart(s) linted, no failures</span><br></pre></td></tr></table></figure><p>如果文件格式错误，可以根据提示进行修改。</p><h4 id="将应用打包">将应用打包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm package mychart</span><br><span class="line">Successfully packaged chart and saved it to: &#x2F;home&#x2F;k8s&#x2F;mychart-0.1.0.tgz</span><br></pre></td></tr></table></figure><p>mychart 目录会被打包为一个 mychart-0.1.0.tgz 格式的压缩包，该压缩包会被放到当前目录下，并同时被保存到了 Helm 的本地缺省仓库目录中。</p><p>如果你想看到更详细的输出，可以加上 <code>--debug</code> 参数来查看打包的输出，输出内容应该类似如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm package mychart --debug</span><br><span class="line">Successfully packaged chart and saved it to: &#x2F;home&#x2F;k8s&#x2F;mychart-0.1.0.tgz</span><br><span class="line">[debug] Successfully saved &#x2F;home&#x2F;k8s&#x2F;mychart-0.1.0.tgz to &#x2F;home&#x2F;k8s&#x2F;.helm&#x2F;repository&#x2F;local</span><br></pre></td></tr></table></figure><h4 id="将应用发布到-repository">将应用发布到 Repository</h4><p>虽然我们已经打包了 Chart 并发布到了 Helm 的本地目录中，但通过 <code>helm search</code> 命令查找，并不能找不到刚才生成的 mychart包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm search mychart</span><br><span class="line">No results found</span><br></pre></td></tr></table></figure><p>这是因为 Repository 目录中的 Chart 包还没有被 Helm 管理。通过 <code>helm repo list</code> 命令可以看到目前 Helm 中已配置的 Repository 的信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm repo list</span><br><span class="line">NAME    URL</span><br><span class="line">stable  https:&#x2F;&#x2F;kubernetes.oss-cn-hangzhou.aliyuncs.com&#x2F;charts</span><br></pre></td></tr></table></figure><blockquote><p>注：新版本中执行 helm init 命令后默认会配置一个名为 local 的本地仓库。</p></blockquote><p>我们可以在本地启动一个 Repository Server，并将其加入到 Helm Repo 列表中。Helm Repository 必须以 Web 服务的方式提供，这里我们就使用 <code>helm serve</code> 命令启动一个 Repository Server，该 Server 缺省使用 <code>$HOME/.helm/repository/local</code> 目录作为 Chart 存储，并在 8879 端口上提供服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm serve &amp;</span><br><span class="line">Now serving you on 127.0.0.1:8879</span><br></pre></td></tr></table></figure><p>默认情况下该服务只监听 127.0.0.1，如果你要绑定到其它网络接口，可使用以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm serve --address 192.168.100.211:8879 &amp;</span><br></pre></td></tr></table></figure><p>如果你想使用指定目录来做为 Helm Repository 的存储目录，可以加上 <code>--repo-path</code> 参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm serve --address 192.168.100.211:8879 --repo-path &#x2F;data&#x2F;helm&#x2F;repository&#x2F; --url http:&#x2F;&#x2F;192.168.100.211:8879&#x2F;charts&#x2F;</span><br></pre></td></tr></table></figure><p>通过 <code>helm repo index</code> 命令将 Chart 的 Metadata 记录更新在 index.yaml 文件中:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 更新 Helm Repository 的索引文件</span><br><span class="line">$ cd &#x2F;home&#x2F;k8s&#x2F;.helm&#x2F;repository&#x2F;local</span><br><span class="line">$ helm repo index --url&#x3D;http:&#x2F;&#x2F;192.168.100.211:8879 .</span><br></pre></td></tr></table></figure><p>完成启动本地 Helm Repository Server 后，就可以将本地 Repository 加入 Helm 的 Repo 列表。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm repo add local http:&#x2F;&#x2F;127.0.0.1:8879</span><br><span class="line">&quot;local&quot; has been added to your repositories</span><br></pre></td></tr></table></figure><p>现在再次查找 mychart 包，就可以搜索到了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ helm repo update</span><br><span class="line">$ helm search mychart</span><br><span class="line">NAME         CHART VERSIONAPP VERSIONDESCRIPTION</span><br><span class="line">local&#x2F;mychart0.1.0        1.0        A Helm chart for Kubernetes</span><br></pre></td></tr></table></figure><h4 id="在-kubernetes-中部署应用">在 Kubernetes 中部署应用</h4><h5 id="部署一个应用">部署一个应用</h5><p>Chart 被发布到仓储后，就可以通过 <code>helm install</code> 命令部署该 Chart。</p><ul><li>检查配置和模板是否有效</li></ul><p>当使用 <code>helm install</code> 命令部署应用时，实际上就是将 templates 目录下的模板文件渲染成 Kubernetes 能够识别的 YAML 格式。</p><p>在部署前我们可以使用 <code>helm install --dry-run --debug &lt;chart_dir&gt;  --name &lt;release_name&gt; </code>命令来验证 Chart 的配置。该输出中包含了模板的变量配置与最终渲染的 YAML 文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --dry-run --debug local&#x2F;mychart --name mike-test</span><br><span class="line">[debug] Created tunnel using local port: &#39;46649&#39;</span><br><span class="line"></span><br><span class="line">[debug] SERVER: &quot;127.0.0.1:46649&quot;</span><br><span class="line"></span><br><span class="line">[debug] Original chart version: &quot;&quot;</span><br><span class="line">[debug] Fetched local&#x2F;mychart to &#x2F;home&#x2F;k8s&#x2F;.helm&#x2F;cache&#x2F;archive&#x2F;mychart-0.1.0.tgz</span><br><span class="line"></span><br><span class="line">[debug] CHART PATH: &#x2F;home&#x2F;k8s&#x2F;.helm&#x2F;cache&#x2F;archive&#x2F;mychart-0.1.0.tgz</span><br><span class="line"></span><br><span class="line">NAME:   mike-test</span><br><span class="line">REVISION: 1</span><br><span class="line">RELEASED: Mon Jul 23 10:39:49 2018</span><br><span class="line">CHART: mychart-0.1.0</span><br><span class="line">USER-SUPPLIED VALUES:</span><br><span class="line">&#123;&#125;</span><br><span class="line"></span><br><span class="line">COMPUTED VALUES:</span><br><span class="line">affinity: &#123;&#125;</span><br><span class="line">image:</span><br><span class="line">  pullPolicy: IfNotPresent</span><br><span class="line">  repository: nginx</span><br><span class="line">  tag: stable</span><br><span class="line">ingress:</span><br><span class="line">  annotations: &#123;&#125;</span><br><span class="line">  enabled: false</span><br><span class="line">  hosts:</span><br><span class="line">  - chart-example.local</span><br><span class="line">  path: &#x2F;</span><br><span class="line">  tls: []</span><br><span class="line">nodeSelector: &#123;&#125;</span><br><span class="line">replicaCount: 1</span><br><span class="line">resources: &#123;&#125;</span><br><span class="line">service:</span><br><span class="line">  port: 80</span><br><span class="line">  type: ClusterIP</span><br><span class="line">tolerations: []</span><br><span class="line"></span><br><span class="line">HOOKS:</span><br><span class="line">MANIFEST:</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"># Source: mychart&#x2F;templates&#x2F;service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: mike-test-mychart</span><br><span class="line">  labels:</span><br><span class="line">    app: mychart</span><br><span class="line">    chart: mychart-0.1.0</span><br><span class="line">    release: mike-test</span><br><span class="line">    heritage: Tiller</span><br><span class="line">spec:</span><br><span class="line">  type: ClusterIP</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">      targetPort: http</span><br><span class="line">      protocol: TCP</span><br><span class="line">      name: http</span><br><span class="line">  selector:</span><br><span class="line">    app: mychart</span><br><span class="line">    release: mike-test</span><br><span class="line">---</span><br><span class="line"># Source: mychart&#x2F;templates&#x2F;deployment.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1beta2</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: mike-test-mychart</span><br><span class="line">  labels:</span><br><span class="line">    app: mychart</span><br><span class="line">    chart: mychart-0.1.0</span><br><span class="line">    release: mike-test</span><br><span class="line">    heritage: Tiller</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: mychart</span><br><span class="line">      release: mike-test</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: mychart</span><br><span class="line">        release: mike-test</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: mychart</span><br><span class="line">          image: &quot;nginx:stable&quot;</span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">              protocol: TCP</span><br><span class="line">          livenessProbe:</span><br><span class="line">            httpGet:</span><br><span class="line">              path: &#x2F;</span><br><span class="line">              port: http</span><br><span class="line">          readinessProbe:</span><br><span class="line">            httpGet:</span><br><span class="line">              path: &#x2F;</span><br><span class="line">              port: http</span><br><span class="line">          resources:</span><br><span class="line">            &#123;&#125;</span><br></pre></td></tr></table></figure><p>验证完成没有问题后，我们就可以使用以下命令将其部署到 Kubernetes 上了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 部署时需指定 Chart 名及 Release（部署的实例）名。</span><br><span class="line">$ helm install local&#x2F;mychart --name mike-test</span><br><span class="line">NAME:   mike-test</span><br><span class="line">LAST DEPLOYED: Mon Jul 23 10:41:20 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">mike-test-mychart  ClusterIP  10.254.120.177  &lt;none&gt;       80&#x2F;TCP   1s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta2&#x2F;Deployment</span><br><span class="line">NAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">mike-test-mychart  1        0        0           0          0s</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                                READY  STATUS   RESTARTS  AGE</span><br><span class="line">mike-test-mychart-6d56f8c8c9-d685v  0&#x2F;1    Pending  0         0s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export POD_NAME&#x3D;$(kubectl get pods --namespace default -l &quot;app&#x3D;mychart,release&#x3D;mike-test&quot; -o jsonpath&#x3D;&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">  echo &quot;Visit http:&#x2F;&#x2F;127.0.0.1:8080 to use your application&quot;</span><br><span class="line">  kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><blockquote><p>注：helm install 默认会用到 socat，需要在所有节点上安装 socat 软件包。</p></blockquote><p>完成部署后，现在 Nginx 就已经部署到 Kubernetes 集群上。在本地主机上执行提示中的命令后，就可在本机访问到该 Nginx 实例。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ export POD_NAME&#x3D;$(kubectl get pods --namespace default -l &quot;app&#x3D;mychart,release&#x3D;mike-test&quot; -o jsonpath&#x3D;&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">$ echo &quot;Visit http:&#x2F;&#x2F;127.0.0.1:8080 to use your application&quot;</span><br><span class="line">$ kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><p>在本地访问 Nginx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ curl http:&#x2F;&#x2F;127.0.0.1:8080</span><br><span class="line">.....</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;&#x2F;title&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;&#x2F;p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>使用下面的命令列出的所有已部署的 Release 以及其对应的 Chart。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm list</span><br><span class="line">NAME     REVISIONUPDATED                 STATUS  CHART        NAMESPACE</span><br><span class="line">mike-test1       Mon Jul 23 10:41:20 2018DEPLOYEDmychart-0.1.0default</span><br></pre></td></tr></table></figure><p>你还可以使用 <code>helm status</code> 查询一个特定的 Release 的状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ helm status mike-test</span><br><span class="line">LAST DEPLOYED: Mon Jul 23 10:41:20 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                                READY  STATUS   RESTARTS  AGE</span><br><span class="line">mike-test-mychart-6d56f8c8c9-d685v  1&#x2F;1    Running  0         1m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">mike-test-mychart  ClusterIP  10.254.120.177  &lt;none&gt;       80&#x2F;TCP   1m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta2&#x2F;Deployment</span><br><span class="line">NAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">mike-test-mychart  1        1        1           1          1m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export POD_NAME&#x3D;$(kubectl get pods --namespace default -l &quot;app&#x3D;mychart,release&#x3D;mike-test&quot; -o jsonpath&#x3D;&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">  echo &quot;Visit http:&#x2F;&#x2F;127.0.0.1:8080 to use your application&quot;</span><br><span class="line">  kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><h5 id="升级和回退一个应用">升级和回退一个应用</h5><p>从上面 <code>helm list</code> 输出的结果中我们可以看到有一个 Revision（更改历史）字段，该字段用于表示某一个 Release 被更新的次数，我们可以用该特性对已部署的 Release 进行回滚。</p><ul><li>修改 Chart.yaml 文件</li></ul><p>将版本号从 0.1.0 修改为 0.2.0, 然后使用 <code>helm package</code> 命令打包并发布到本地仓库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ cat mychart&#x2F;Chart.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: &quot;1.0&quot;</span><br><span class="line">description: A Helm chart for Kubernetes</span><br><span class="line">name: mychart</span><br><span class="line">version: 0.2.0</span><br><span class="line"></span><br><span class="line">$ helm package mychart</span><br><span class="line">Successfully packaged chart and saved it to: &#x2F;home&#x2F;k8s&#x2F;mychart-0.2.0.tgz</span><br></pre></td></tr></table></figure><ul><li>查询本地仓库中的 Chart 信息</li></ul><p>我们可以看到在本地仓库中 mychart 有两个版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ helm search mychart -l</span><br><span class="line">NAME         CHART VERSIONAPP VERSIONDESCRIPTION</span><br><span class="line">local&#x2F;mychart0.2.0        1.0        A Helm chart for Kubernetes</span><br><span class="line">local&#x2F;mychart0.1.0        1.0        A Helm chart for Kubernetes</span><br></pre></td></tr></table></figure><ul><li>升级一个应用</li></ul><p>现在用 <code>helm upgrade</code> 命令将已部署的 mike-test 升级到新版本。你可以通过 <code>--version</code> 参数指定需要升级的版本号，如果没有指定版本号，则缺省使用最新版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ helm upgrade mike-test local&#x2F;mychart</span><br><span class="line">Release &quot;mike-test&quot; has been upgraded. Happy Helming!</span><br><span class="line">LAST DEPLOYED: Mon Jul 23 10:50:25 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                                READY  STATUS   RESTARTS  AGE</span><br><span class="line">mike-test-mychart-6d56f8c8c9-d685v  1&#x2F;1    Running  0         9m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">mike-test-mychart  ClusterIP  10.254.120.177  &lt;none&gt;       80&#x2F;TCP   9m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1beta2&#x2F;Deployment</span><br><span class="line">NAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">mike-test-mychart  1        1        1           1          9m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export POD_NAME&#x3D;$(kubectl get pods --namespace default -l &quot;app&#x3D;mychart,release&#x3D;mike-test&quot; -o jsonpath&#x3D;&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">  echo &quot;Visit http:&#x2F;&#x2F;127.0.0.1:8080 to use your application&quot;</span><br><span class="line">  kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><p>完成后，可以看到已部署的 mike-test 被升级到 0.2.0 版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm list</span><br><span class="line">NAME     REVISIONUPDATED                 STATUS  CHART        NAMESPACE</span><br><span class="line">mike-test2       Mon Jul 23 10:50:25 2018DEPLOYEDmychart-0.2.0default</span><br></pre></td></tr></table></figure><ul><li>回退一个应用</li></ul><p>如果更新后的程序由于某些原因运行有问题，需要回退到旧版本的应用。首先我们可以使用 <code>helm history</code> 命令查看一个 Release 的所有变更记录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ helm history mike-test</span><br><span class="line">REVISIONUPDATED                 STATUS    CHART        DESCRIPTION</span><br><span class="line">1       Mon Jul 23 10:41:20 2018SUPERSEDEDmychart-0.1.0Install complete</span><br><span class="line">2       Mon Jul 23 10:50:25 2018DEPLOYED  mychart-0.2.0Upgrade complete</span><br></pre></td></tr></table></figure><p>其次，我们可以使用下面的命令对指定的应用进行回退。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm rollback mike-test 1</span><br><span class="line">Rollback was a success! Happy Helming!</span><br></pre></td></tr></table></figure><blockquote><p>注：其中的参数 1 是 helm history 查看到 Release 的历史记录中 REVISION 对应的值。</p></blockquote><p>最后，我们使用 <code>helm list</code> 和 <code>helm history</code> 命令都可以看到 mychart 的版本已经回退到 0.1.0 版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ helm list</span><br><span class="line">NAME     REVISIONUPDATED                 STATUS  CHART        NAMESPACE</span><br><span class="line">mike-test3       Mon Jul 23 10:53:42 2018DEPLOYEDmychart-0.1.0default</span><br><span class="line"></span><br><span class="line">$ helm history mike-test</span><br><span class="line">REVISIONUPDATED                 STATUS    CHART        DESCRIPTION</span><br><span class="line">1       Mon Jul 23 10:41:20 2018SUPERSEDEDmychart-0.1.0Install complete</span><br><span class="line">2       Mon Jul 23 10:50:25 2018SUPERSEDEDmychart-0.2.0Upgrade complete</span><br><span class="line">3       Mon Jul 23 10:53:42 2018DEPLOYED  mychart-0.1.0Rollback to 1</span><br></pre></td></tr></table></figure><h5 id="删除一个应用">删除一个应用</h5><p>如果需要删除一个已部署的 Release，可以利用 <code>helm delete</code> 命令来完成删除。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm delete mike-test</span><br><span class="line">release &quot;mike-test&quot; deleted</span><br></pre></td></tr></table></figure><p>确认应用是否删除，该应用已被标记为 DELETED 状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm ls -a mike-test</span><br><span class="line">NAME     REVISIONUPDATED                 STATUS CHART        NAMESPACE</span><br><span class="line">mike-test3       Mon Jul 23 10:53:42 2018DELETEDmychart-0.1.0default</span><br></pre></td></tr></table></figure><p>也可以使用 <code>--deleted</code> 参数来列出已经删除的 Release</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm ls --deleted</span><br><span class="line">NAME     REVISIONUPDATED                 STATUS CHART        NAMESPACE</span><br><span class="line">mike-test3       Mon Jul 23 10:53:42 2018DELETEDmychart-0.1.0default</span><br></pre></td></tr></table></figure><p>从上面的结果也可以看出，默认情况下已经删除的 Release 只是将状态标识为 DELETED 了 ，但该 Release 的历史信息还是继续被保存的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ helm hist mike-test</span><br><span class="line">REVISIONUPDATED                 STATUS    CHART        DESCRIPTION</span><br><span class="line">1       Mon Jul 23 10:41:20 2018SUPERSEDEDmychart-0.1.0Install complete</span><br><span class="line">2       Mon Jul 23 10:50:25 2018SUPERSEDEDmychart-0.2.0Upgrade complete</span><br><span class="line">3       Mon Jul 23 10:53:42 2018DELETED   mychart-0.1.0Deletion complete</span><br></pre></td></tr></table></figure><p>如果要移除指定 Release 所有相关的 Kubernetes 资源和 Release 的历史记录，可以用如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm delete --purge mike-test</span><br><span class="line">release &quot;mike-test&quot; deleted</span><br></pre></td></tr></table></figure><p>再次查看已删除的 Release，已经无法找到相关信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ helm hist mike-test</span><br><span class="line">Error: release: &quot;mike-test&quot; not found</span><br><span class="line"></span><br><span class="line"># helm ls 命令也已均无查询记录。</span><br><span class="line">$ helm ls --deleted</span><br><span class="line">$ helm ls -a mike-test</span><br></pre></td></tr></table></figure><h3 id="helm-部署应用实例">Helm 部署应用实例</h3><h4 id="部署-wordpress">部署 Wordpress</h4><p>这里以一个典型的三层应用 Wordpress 为例，包括 MySQL、PHP 和 Apache。</p><p>由于测试环境没有可用的 PersistentVolume（持久卷，简称 PV），这里暂时将其关闭。关于 Persistent Volumes 的相关信息我们会在后续的相关文章进行讲解。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --name wordpress-test --set &quot;persistence.enabled&#x3D;false,mariadb.persistence.enabled&#x3D;false,serviceType&#x3D;NodePort&quot;  stable&#x2F;wordpress</span><br><span class="line"></span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;Deployment</span><br><span class="line">NAME                      DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">wordpress-test-mariadb    1        1        1           1          26m</span><br><span class="line">wordpress-test-wordpress  1        1        1           1          26m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                                       READY  STATUS   RESTARTS  AGE</span><br><span class="line">wordpress-test-mariadb-84b866bf95-n26ff    1&#x2F;1    Running  1         26m</span><br><span class="line">wordpress-test-wordpress-5ff8c64b6c-sgtvv  1&#x2F;1    Running  6         26m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Secret</span><br><span class="line">NAME                      TYPE    DATA  AGE</span><br><span class="line">wordpress-test-mariadb    Opaque  2     26m</span><br><span class="line">wordpress-test-wordpress  Opaque  2     26m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;ConfigMap</span><br><span class="line">NAME                          DATA  AGE</span><br><span class="line">wordpress-test-mariadb        1     26m</span><br><span class="line">wordpress-test-mariadb-tests  1     26m</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME                      TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)                   AGE</span><br><span class="line">wordpress-test-mariadb    ClusterIP  10.254.99.67   &lt;none&gt;       3306&#x2F;TCP                  26m</span><br><span class="line">wordpress-test-wordpress  NodePort   10.254.175.16  &lt;none&gt;       80:8563&#x2F;TCP,443:8839&#x2F;TCP  26m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the WordPress URL:</span><br><span class="line"></span><br><span class="line">  Or running:</span><br><span class="line"></span><br><span class="line">  export NODE_PORT&#x3D;$(kubectl get --namespace default -o jsonpath&#x3D;&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services wordpress-test-wordpress)</span><br><span class="line">  export NODE_IP&#x3D;$(kubectl get nodes --namespace default -o jsonpath&#x3D;&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">  echo http:&#x2F;&#x2F;$NODE_IP:$NODE_PORT&#x2F;admin</span><br><span class="line"></span><br><span class="line">2. Login with the following credentials to see your blog</span><br><span class="line"></span><br><span class="line">  echo Username: user</span><br><span class="line">  echo Password: $(kubectl get secret --namespace default wordpress-test-wordpress -o jsonpath&#x3D;&quot;&#123;.data.wordpress-password&#125;&quot; | base64 --decode)</span><br></pre></td></tr></table></figure><h4 id="访问-wordpress">访问 Wordpress</h4><p>部署完成后，我们可以通过上面的提示信息生成相应的访问地址和用户名、密码等相关信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 生成 Wordpress 管理后台地址</span><br><span class="line">$ export NODE_PORT&#x3D;$(kubectl get --namespace default -o jsonpath&#x3D;&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services wordpress-test-wordpress)</span><br><span class="line">$ export NODE_IP&#x3D;$(kubectl get nodes --namespace default -o jsonpath&#x3D;&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">$ echo http:&#x2F;&#x2F;$NODE_IP:$NODE_PORT&#x2F;admin</span><br><span class="line">http:&#x2F;&#x2F;192.168.100.211:8433&#x2F;admin</span><br><span class="line"></span><br><span class="line"># 生成 Wordpress 管理帐号和密码</span><br><span class="line">$ echo Username: user</span><br><span class="line">Username: user</span><br><span class="line">$ echo Password: $(kubectl get secret --namespace default wordpress-test-wordpress -o jsonpath&#x3D;&quot;&#123;.data.wordpress-password&#125;&quot; | base64 --decode)</span><br><span class="line">Password: 9jEXJgnVAY</span><br></pre></td></tr></table></figure><p>给一张访问效果图吧：</p><p><img src="https://www.hi-linux.com/img/linux/helm03.png" alt=""></p><h3 id="helm-其它使用技巧">Helm 其它使用技巧</h3><ul><li>如何设置 helm 命令自动补全？</li></ul><p>为了方便 <code>helm</code> 命令的使用，Helm 提供了自动补全功能，如果使用 ZSH 请执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ source &lt;(helm completion zsh)</span><br></pre></td></tr></table></figure><p>如果使用 BASH 请执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ source &lt;(helm completion bash)</span><br></pre></td></tr></table></figure><ul><li>如何使用第三方的 Chart 存储库？</li></ul><p>随着 Helm 越来越普及，除了使用预置官方存储库，三方仓库也越来越多了（前提是网络是可达的）。你可以使用如下命令格式添加三方 Chart 存储库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm repo add 存储库名 存储库URL</span><br><span class="line">$ helm repo update</span><br></pre></td></tr></table></figure><p>一些三方存储库资源:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Prometheus Operator</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;coreos&#x2F;prometheus-operator&#x2F;tree&#x2F;master&#x2F;helm</span><br><span class="line"></span><br><span class="line"># Bitnami Library for Kubernetes</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;bitnami&#x2F;charts</span><br><span class="line"></span><br><span class="line"># Openstack-Helm</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;att-comdev&#x2F;openstack-helm</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;sapcc&#x2F;openstack-helm</span><br><span class="line"></span><br><span class="line"># Tick-Charts</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;jackzampolin&#x2F;tick-charts</span><br></pre></td></tr></table></figure><ul><li>Helm 如何结合 CI/CD ？</li></ul><p>采用 Helm 可以把零散的 Kubernetes 应用配置文件作为一个 Chart 管理，Chart 源码可以和源代码一起放到 Git 库中管理。通过把 Chart 参数化，可以在测试环境和生产环境采用不同的 Chart 参数配置。</p><p>下图是采用了 Helm 的一个 CI/CD 流程</p><p><img src="https://www.hi-linux.com/img/linux/helm04.png" alt=""></p><ul><li>Helm 如何管理多环境下 (Test、Staging、Production) 的业务配置？</li></ul><p>Chart 是支持参数替换的，可以把业务配置相关的参数设置为模板变量。使用 <code>helm install</code> 命令部署的时候指定一个参数值文件，这样就可以把业务参数从 Chart 中剥离了。例如： <code>helm install --values=values-production.yaml wordpress</code>。</p><ul><li>Helm 如何解决服务依赖？</li></ul><p>在 Chart 里可以通过 requirements.yaml 声明对其它 Chart 的依赖关系。如下面声明表明 Chart 依赖 Apache 和 MySQL 这两个第三方 Chart。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dependencies:</span><br><span class="line">- name: mariadb</span><br><span class="line">  version: 2.1.1</span><br><span class="line">  repository: https:&#x2F;&#x2F;kubernetes-charts.storage.googleapis.com&#x2F;</span><br><span class="line">  condition: mariadb.enabled</span><br><span class="line">  tags:</span><br><span class="line">    - wordpress-database</span><br><span class="line">- name: apache</span><br><span class="line">    version: 1.4.0</span><br><span class="line">    repository: https:&#x2F;&#x2F;kubernetes-charts.storage.googleapis.com&#x2F;</span><br></pre></td></tr></table></figure><ul><li>如何让 Helm 连接到指定 Kubernetes 集群？</li></ul><p>Helm 默认使用和 kubectl 命令相同的配置访问 Kubernetes 集群，其配置默认在 <code>~/.kube/config</code> 中。</p><ul><li>如何在部署时指定命名空间？</li></ul><p><code>helm install</code> 默认情况下是部署在 default 这个命名空间的。如果想部署到指定的命令空间，可以加上 <code>--namespace</code> 参数，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm install local&#x2F;mychart --name mike-test --namespace mynamespace</span><br></pre></td></tr></table></figure><ul><li>如何查看已部署应用的详细信息？</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm get wordpress-test</span><br></pre></td></tr></table></figure><p>默认情况下会显示最新的版本的相关信息，如果想要查看指定发布版本的信息可加上 <code>--revision</code> 参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm get  --revision 1  wordpress-test</span><br></pre></td></tr></table></figure><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RgEE0dm" target="_blank" rel="noopener">http://t.cn/RgEE0dm</a><br><a href="http://t.cn/RgE3MyP" target="_blank" rel="noopener">http://t.cn/RgE3MyP</a><br><a href="http://t.cn/RgpiUAz" target="_blank" rel="noopener">http://t.cn/RgpiUAz</a></p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Helm 是 Kubernetes 生态系统中的一个软件包管理工具。本文将介绍 Helm 中的相关概念和基本工作原理，并通过一个具体的示例学习如何使用 Helm 打包、分发、安装、升级及回退 Kubernetes 应用。&lt;/p&gt;
&lt;h3 id=&quot;Kubernetes-应用部署的挑战&quot;&gt;Kubernetes 应用部署的挑战&lt;/h3&gt;
&lt;p&gt;Kubernetes 是一个提供了基于容器的应用集群管理解决方案，Kubernetes 为容器化应用提供了部署运行、资源调度、服务发现和动态伸缩等一系列完整功能。&lt;/p&gt;
&lt;p&gt;Kubernetes 的核心设计理念是: 用户定义要部署的应用程序的规则，而 Kubernetes 则负责按照定义的规则部署并运行应用程序。如果应用程序出现问题导致偏离了定义的规格，Kubernetes 负责对其进行自动修正。例如：定义的应用规则要求部署两个实例（Pod），其中一个实例异常终止了，Kubernetes 会检查到并重新启动一个新的实例。&lt;/p&gt;
&lt;p&gt;用户通过使用 Kubernetes API 对象来描述应用程序规则，包括 Pod、Service、Volume、Namespace、ReplicaSet、Deployment、Job等等。一般这些资源对象的定义需要写入一系列的 YAML 文件中，然后通过 Kubernetes 命令行工具 Kubectl 调 Kubernetes API 进行部署。&lt;/p&gt;
&lt;p&gt;以一个典型的三层应用 Wordpress 为例，该应用程序就涉及到多个 Kubernetes API 对象，而要描述这些 Kubernetes API 对象就可能要同时维护多个 YAML 文件。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/helm01.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;从上图可以看到，在进行 Kubernetes 软件部署时，我们面临下述几个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何管理、编辑和更新这些这些分散的 Kubernetes 应用配置文件。&lt;/li&gt;
&lt;li&gt;如何把一套相关的配置文件作为一个应用进行管理。&lt;/li&gt;
&lt;li&gt;如何分发和重用 Kubernetes 的应用配置。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helm 的出现就是为了很好地解决上面这些问题。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>从 Kubectl run 开始揭开 Kubernetes 的神迷面纱</title>
    <link href="https://www.hi-linux.com/posts/30305.html"/>
    <id>https://www.hi-linux.com/posts/30305.html</id>
    <published>2018-08-08T01:00:00.000Z</published>
    <updated>2018-08-24T06:01:16.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>想象一下，如果我想将 <code>Nginx</code> 部署到 <code>Kubernetes</code> 集群，我可能会在终端中输入类似这样的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run --image&#x3D;nginx --replicas&#x3D;3</span><br></pre></td></tr></table></figure><p>然后回车。几秒钟后，你就会看到三个 <code>Nginx Pod</code> 分布在所有的工作节点上。这一切就像变魔术一样，但你并不知道这一切的背后究竟发生了什么事情。</p><p><code>Kubernetes</code> 的神奇之处在于：它可以通过用户友好的 <code>API</code> 来处理跨基础架构的 <code>Deployments</code>，而背后的复杂性被隐藏在简单的抽象中。但为了充分理解它为我们提供的价值，我们需要理解它的内部原理。</p><p>本指南将引导您理解从 <code>Client</code> 到 <code>Kubelet</code> 请求的完整生命周期，必要时会通过源代码来说明背后发生了什么。</p><p>这是一份可以在线修改的文档，如果你发现有什么可以改进或重写的，欢迎提供帮助！</p><h3 id="1-kubectl">1. Kubectl</h3><p><strong>验证和生成器</strong></p><p>当敲下回车键以后，<code>Kubectl</code> 首先会执行一些客户端验证操作，以确保不合法的请求（例如：创建不支持的资源或使用格式错误的镜像名称）将会快速失败，也不会发送给 <code>Kube-Apiserver</code>。通过减少不必要的负载来提高系统性能。</p><p>验证通过之后，<code>Kubectl</code> 开始将发送给 <code>Kube-Apiserver</code> 的 <code>HTTP</code> 请求进行封装。<code>Kube-Apiserver</code> 与 <code>Etcd</code> 进行通信，所有尝试访问或更改 <code>Kubernetes</code> 系统状态的请求都会通过 <code>Kube-Apiserver</code> 进行，<code>Kubectl</code> 也不例外。<code>Kubectl</code> 使用生成器（<a href="https://kubernetes.io/docs/user-guide/kubectl-conventions/#generators" target="_blank" rel="noopener">Generators</a>）来构造 <code>HTTP</code> 请求。生成器是一个用来处理序列化的抽象概念。</p><p>通过 <code>kubectl run</code> 不仅可以运行 <code>Deployment</code>，还可以通过指定参数 <code>--generator</code> 来部署其他多种资源类型。如果没有指定 <code>--generator</code> 参数的值，<code>Kubectl</code> 将会自动判断资源的类型。</p><p>例如：带有参数 <code>--restart-policy=Always</code> 的资源将被部署为 <code>Deployment</code>，而带有参数 <code>--restart-policy=Never</code> 的资源将被部署为 <code>Pod</code>。同时 <code>Kubectl</code> 也会检查是否需要触发其他操作，例如：记录命令（用来进行回滚或审计）。</p><p>在 <code>Kubectl</code> 判断出要创建一个 <code>Deployment</code> 后，它将使用 <code>DeploymentV1Beta1</code> 生成器从我们提供的参数中生成一个运行时对象。</p><a id="more"></a><p><strong>API 版本协商与 API 组</strong></p><p>为了更容易地消除字段或者重新组织资源结构，<code>Kubernetes</code> 支持多个 <code>API</code> 版本，每个版本都在不同的 <code>API</code> 路径下，例如 <code>/api/v1</code> 或者 <code>/apis/extensions/v1beta1</code>。不同的 <code>API</code> 版本表明不同的稳定性和支持级别，更详细的描述可以参考 <a href="https://k8smeetup.github.io/docs/reference/api-overview/" target="_blank" rel="noopener">Kubernetes API 概述</a>。</p><p><code>API</code> 组主要作用是对类似资源进行分类，以便使得 <code>Kubernetes API</code> 更容易扩展。<code>API</code> 的组别在 <code>REST</code> 路径或者序列化对象的 <code>apiVersion</code> 字段中指定。例如：<code>Deployment</code> 的 <code>API</code> 组名是 <code>apps</code>，最新的 <code>API</code> 版本是 <code>v1beta2</code>，这就是为什么你要在 <code>Deployment manifests</code> 顶部输入 <code>apiVersion: apps/v1beta2</code>。</p><p><code>Kubectl</code> 在生成运行时对象后，开始为它找到适当的 <code>API</code> 组和 <code>API</code> 版本，然后组装成一个版本化客户端，该客户端知道资源的各种 <code>REST</code> 语义。该阶段被称为版本协商，<code>Kubectl</code> 会扫描 <code>Remote API</code> 上的 <code>/apis</code> 路径来检索所有可能的 <code>API</code> 组。由于 <code>Kube-Apiserver</code> 在 <code>/apis</code> 路径上公开了 <code>OpenAPI</code> 格式的模式文档， 因此客户端很容易找到合适的 <code>API</code>。</p><p>为了提高性能，<code>Kubectl</code> 将 <code>OpenAPI</code> 模式缓存到了 <code>~/.kube/cache</code> 目录。如果你想了解 <code>API</code> 发现的过程，请尝试删除该目录并在运行 <code>kubectl</code> 命令时将 <code>-v</code> 参数的值设为最大值，然后你将会看到所有试图找到这些 <code>API</code> 版本的 <code>HTTP</code> 请求。参考 <a href="https://k8smeetup.github.io/docs/reference/kubectl/cheatsheet/" target="_blank" rel="noopener">Kubectl 备忘单</a>。</p><p>最后一步才是真正地发送 <code>HTTP</code> 请求。一旦请求发送之后获得成功的响应，<code>Kubectl</code> 将会根据所需的输出格式打印 Success Message。</p><p><strong>客户端身份认证</strong></p><p>在发送 <code>HTTP</code> 请求之前还要进行客户端认证，这是之前没有提到的，现在可以来看一下。</p><p>为了能够成功发送请求，<code>Kubectl</code> 需要先进行身份认证。用户凭证保存在 <code>Kubeconfig</code> 文件中，<code>Kubectl</code> 通过以下顺序来找到 <code>Kubeconfig</code> 文件：</p><ul><li>如果提供了 <code>--kubeconfig</code> 参数， <code>Kubectl</code> 就使用 <code>--kubeconfig</code> 参数提供的 <code>Kubeconfig</code> 文件。</li><li>如果没有提供 <code>--kubeconfig</code> 参数，但设置了环境变量 <code>$KUBECONFIG</code>，则使用该环境变量提供的 <code>Kubeconfig</code> 文件。</li><li>如果 <code>--kubeconfig</code> 参数和环境变量 <code>$KUBECONFIG</code> 都没有提供，<code>Kubectl</code> 就使用默认的 <code>Kubeconfig</code> 文件 <code>$HOME/.kube/config</code>。</li></ul><p>解析完 <code>Kubeconfig</code> 文件后，<code>Kubectl</code> 会确定当前要使用的上下文、当前指向的群集以及与当前用户关联的任何认证信息。如果用户提供了额外的参数（例如: <code>--username</code>），则优先使用这些参数覆盖 <code>Kubeconfig</code> 中指定的值。一旦拿到这些信息之后， <code>Kubectl</code> 就会把这些信息填充到将要发送的 <code>HTTP</code> 请求头中：</p><ul><li><code>x509</code> 证书使用 <code>tls.TLSConfig</code> 发送（包括 <code>CA</code> 证书）。</li><li><code>bearer tokens</code> 在 HTTP 请求头 <code>Authorization</code> 中发送。</li><li>用户名和密码通过 <code>HTTP</code> 基本认证发送。</li><li><code>OpenID</code> 认证过程是由用户事先手动处理的，产生一个像 <code>Bearer Token</code> 一样被发送的 <code>Token</code>。</li></ul><h3 id="2-kube-apiserver">2. Kube-Apiserver</h3><p><strong>认证</strong></p><p>现在我们的请求已经成功发送了，接下来将会发生什么？这时候就该 <code>Kube-Apiserver</code> 闪亮登场了！<code>Kube-Apiserver</code> 是客户端和系统组件用来保存和检索集群状态的主要接口。为了执行相应的功能，<code>Kube-Apiserver</code> 需要能够验证请求者是合法的，这个过程被称为认证。</p><p>那么 <code>Apiserver</code> 如何对请求进行认证呢？当 <code>Kube-Apiserver</code> 第一次启动时，它会查看用户提供的所有 <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/" target="_blank" rel="noopener">CLI</a> 参数，并组合成一个合适的令牌列表。</p><p>举个例子：如果提供了 <code>--client-ca-file</code> 参数，则会将 <code>x509</code> 客户端证书认证添加到令牌列表中；如果提供了 <code>--token-auth-file</code> 参数，则会将 <code>breaer token</code> 添加到令牌列表中。</p><p>每次收到请求时，<code>Apiserver</code> 都会通过令牌链进行认证，直到某一个认证成功为止：</p><ul><li><code>x509</code> 处理程序将验证 <code>HTTP</code> 请求是否是由 <code>CA</code> 根证书签名的 <code>TLS</code> 密钥进行编码的。</li><li><code>bearer token</code> 处理程序将验证 <code>--token-auth-file</code> 参数提供的 <code>Token</code> 文件是否存在。</li><li>基本认证处理程序确保 <code>HTTP</code> 请求的基本认证凭证与本地的状态匹配。</li></ul><p>如果认证失败，则请求失败并返回相应的错误信息；如果验证成功，则将请求中的 <code>Authorization</code> 请求头删除，并将用户信息添加到其上下文中。这给后续的授权和准入控制器提供了访问之前建立的用户身份的能力。</p><p><strong>授权</strong></p><p>OK，现在请求已经发送，并且 <code>Kube-Apiserver</code> 已经成功验证我们是谁，终于解脱了！</p><p>然而事情并没有结束，虽然我们已经证明了我们是合法的，但我们有权执行此操作吗？毕竟身份和权限不是一回事。为了进行后续的操作，<code>Kube-Apiserver</code> 还要对用户进行授权。</p><p><code>Kube-Apiserver</code> 处理授权的方式与处理身份验证的方式相似：通过 <code>Kube-Apiserver</code> 的启动参数 <code>--authorization_mode</code> 参数设置。它将组合一系列授权者，这些授权者将针对每个传入的请求进行授权。如果所有授权者都拒绝该请求，则该请求会被禁止响应并且不会再继续响应。如果某个授权者批准了该请求，则请求继续。</p><p><code>Kube-Apiserver</code> 目前支持以下几种授权方法：</p><ul><li><a href="https://k8smeetup.github.io/docs/admin/authorization/webhook/" target="_blank" rel="noopener">webhook</a>: 它与集群外的 <code>HTTP(S)</code> 服务交互。</li><li><a href="https://k8smeetup.github.io/docs/admin/authorization/abac/" target="_blank" rel="noopener">ABAC</a>: 它执行静态文件中定义的策略。</li><li><a href="https://k8smeetup.github.io/docs/admin/authorization/rbac/" target="_blank" rel="noopener">RBAC</a>: 它使用 <code>rbac.authorization.k8s.io</code> API Group实现授权决策，允许管理员通过 <code>Kubernetes API</code> 动态配置策略。</li><li><a href="https://k8smeetup.github.io/docs/admin/authorization/node/" target="_blank" rel="noopener">Node</a>: 它确保 <code>Kubelet</code> 只能访问自己节点上的资源。</li></ul><p><strong>准入控制</strong></p><p>突破了之前所说的认证和授权两道关口之后，客户端的调用请求就能够得到 <code>API Server</code> 的真正响应了吗？答案是：不能！</p><p>从 <code>Kube-Apiserver</code> 的角度来看，它已经验证了我们的身份并且赋予了相应的权限允许我们继续，但对于 <code>Kubernetes</code> 而言，其他组件对于应不应该允许发生的事情还是很有意见的。所以这个请求还需要通过 <code>Admission Controller</code> 所控制的一个准入控制链的层层考验，官方标准的关卡有近十个之多，而且还能自定义扩展！</p><p>虽然授权的重点是回答用户是否有权限，但准入控制器会拦截请求以确保它符合集群的更广泛的期望和规则。它们是资源对象保存到 <code>Etcd</code> 之前的最后一个堡垒，封装了一系列额外的检查以确保操作不会产生意外或负面结果。不同于授权和认证只关心请求的用户和操作，准入控制还处理请求的内容，并且仅对创建、更新、删除或连接（如：代理）等有效，而对读操作无效。</p><blockquote><p>注：准入控制器的工作方式与授权者和验证者的工作方式类似，但有一点区别：与验证链和授权链不同，如果某个准入控制器检查不通过，则整个链会中断，整个请求将立即被拒绝并且返回一个错误给终端用户。</p></blockquote><p>准入控制器设计的重点在于提高可扩展性，每个控制器都作为一个插件存储在 <code>plugin/pkg/admission</code> 目录中，并且与每一个接口相匹配，最后被编译到 <code>Kube-Apiserver</code> 二进制文件中。</p><p>大部分准入控制器都比较容易理解，接下来着重介绍 <code>SecurityContextDeny</code>、<code>ResourceQuota</code> 及 <code>LimitRanger</code> 这三个准入控制器。</p><ul><li><p><code>SecurityContextDeny</code> 该插件将禁止创建设置了 <code>Security Context</code> 的 <code>Pod</code>。</p></li><li><p><code>ResourceQuota</code> 不仅能限制某个 <code>Namespace</code> 中创建资源的数量，而且能限制某个 <code>Namespace</code> 中被 <code>Pod</code> 所请求的资源总量。该准入控制器和资源对象 <code>ResourceQuota</code> 一起实现了资源配额管理。</p></li><li><p><code>LimitRanger</code> 作用类似于上面的 <code>ResourceQuota</code> 控制器，针对 <code>Namespace</code> 资源的每个个体（<code>Pod</code> 与 <code>Container</code> 等）的资源配额。该插件和资源对象 <code>LimitRange</code> 一起实现资源配额管理。</p></li></ul><h3 id="3-etcd">3. Etcd</h3><p>到现在为止，<code>Kubernetes</code> 已经对该客户端的调用请求进行了全面彻底地审查，并且已经验证通过，运行它进入下一个环节。下一步 <code>Kube-Apiserver</code> 将对 <code>HTTP</code> 请求进行反序列化，然后利用得到的结果构建运行时对象（有点像 <code>Kubectl</code> 生成器的逆过程），并保存到 <code>Etcd</code> 中。下面我们将这个过程分解一下。</p><p>当收到请求时，<code>Kube-Apiserver</code> 是如何知道它该怎么做的呢？事实上，在客户端发送调用请求之前就已经产生了一系列非常复杂的流程。我们就从 <code>Kube-Apiserver</code> 二进制文件首次运行开始分析吧：</p><ol><li>当运行 <code>Kube-Apiserver</code> 二进制文件时，它会创建一个允许 <code>Apiserver</code> 聚合的服务链。这是一种对 <code>Kubernetes API</code> 进行扩展的方式。</li><li>同时会创建一个 <code>Generic Apiserver</code> 作为默认的 <code>Apiserver</code>。</li><li>然后生成 <code>OpenAPI</code> 规范的配置</li><li>然后 <code>Kube-Apiserver</code> 遍历数据结构中指定的所有 <code>API</code> 组，并将每一个 <code>API</code> 组作为通用的存储抽象保存到 <code>Etcd</code> 中。当你访问或变更资源状态时，<code>Kube-Apiserver</code> 就会调用这些 <code>API</code> 组。</li><li>每个 <code>API</code> 组都会遍历它的所有组版本，并且将每个 <code>HTTP</code> 路由映射到 <code>REST</code> 路径中。</li><li>当请求的 <code>METHOD</code> 是 <code>POST</code> 时，<code>Kube-Apiserver</code> 就会将请求转交给资源创建处理器。</li></ol><p>现在 <code>Kube-Apiserver</code> 已经知道了所有的路由及其对应的 <code>REST</code> 路径，以便在请求匹配时知道调用哪些处理器和键值存储。多么机智的设计！现在假设客户端的 <code>HTTP</code> 请求已经被 <code>Kube-Apiserver</code> 收到了：</p><ol><li>如果处理链可以将请求与已经注册的路由进行匹配，就会将该请求交给注册到该路由的专用处理器来处理；如果没有任何一个路由可以匹配该请求，就会将请求转交给基于路径的处理器（比如：当调用 /apis 时）；如果没有任何一个基于路径的处理器注册到该路径，请求就会被转交给 <code>not found</code> 处理器，最后返回 404。</li><li>幸运的是，我们有一个名为 <code>CreateHandler</code> 的注册路由！它有什么作用呢？首先它会解码 <code>HTTP</code> 请求并进行基本的验证，例如：确保请求提供的 <code>JSON</code> 与 <code>API</code> 资源的版本相匹配。</li><li>接下来进入审计和准入控制阶段。</li><li>然后资源将会通过 <code>Storage Provider</code> 保存到 <code>Etcd</code> 中。默认情况下保存到 <code>Etcd</code> 中的键的格式为 <code>&lt;namespace&gt;/&lt;name&gt;</code>，你也可以自定义。</li><li>资源创建过程中出现的任何错误都会被捕获，最后 <code>Storage Provider</code> 会执行 <code>Get</code> 调用来确认该资源是否被成功创建。如果需要额外的清理工作，就会调用后期创建的处理器和装饰器。</li><li>最后构造 <code>HTTP</code> 响应并返回给客户端。</li></ol><p>原来 <code>Apiserver</code> 做了这么多的工作，以前竟然没有发现呢！到目前为止，我们创建的 <code>Deployment</code> 资源已经保存到了 <code>Etcd</code> 中，但 <code>Apiserver</code> 仍然看不到它。</p><h3 id="4-初始化">4. 初始化</h3><p>在一个资源对象被持久化到数据存储之后，<code>Apiserver</code> 还无法完全看到或调度它，在此之前还要执行一系列<a href="https://kubernetes.io/docs/admin/extensible-admission-controllers/#initializers" target="_blank" rel="noopener">初始化器</a>。初始化器是一种与资源类型相关联的控制器，它会在资源对外可用之前执行某些逻辑。如果某个资源类型没有初始化器，就会跳过此初始化步骤立即使资源对外可见。</p><p>正如<a href="https://ahmet.im/blog/initializers/" target="_blank" rel="noopener">大佬的博客</a>指出的那样，初始化器是一个强大的功能，因为它允许我们执行通用引导操作。例如：</p><ul><li>将代理 <code>Sidecar</code> 容器注入到暴露 80 端口的 <code>Pod</code> 中，或者加上特定的 <code>Annotation</code>。</li><li>将保存着测试证书的<code> Volume</code> 注入到特定命名空间的所有 <code>Pod</code> 中。</li><li>如果 <code>Secret</code> 中的密码小于 20 个字符，就组织其创建。</li></ul><p><code>initializerConfiguration</code> 资源对象允许你声明某些资源类型应该运行哪些初始化器。如果你想每创建一个 <code>Pod</code> 时就运行一个自定义初始化器，你可以这样做：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">admissionregistration.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">InitializerConfiguration</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">custom-pod-initializer</span></span><br><span class="line"><span class="attr">initializers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">podimage.example.com</span></span><br><span class="line">    <span class="attr">rules:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">apiGroups:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">""</span></span><br><span class="line">        <span class="attr">apiVersions:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">v1</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">pods</span></span><br></pre></td></tr></table></figure><p>通过该配置创建资源对象 <code>InitializerConfiguration</code> 之后，就会在每个 <code>Pod</code> 的 <code>metadata.initializers.pending</code> 字段中添加 <code>custom-pod-initializer</code> 字段。该初始化控制器会定期扫描新的 <code>Pod</code>，一旦在 <code>Pod</code> 的 <code>pending</code> 字段中检测到自己的名称，就会执行其逻辑，执行完逻辑之后就会将 <code>pending</code> 字段下的自己的名称删除。</p><p>只有在 <code>pending</code> 字段下的列表中的第一个初始化器可以对资源进行操作，当所有的初始化器执行完成，并且 <code>pending</code> 字段为空时，该对象就会被认为初始化成功。</p><p>你可能会注意到一个问题：如果 <code>Kube-Apiserver</code> 不能显示这些资源，那么用户级控制器是如何处理资源的呢？</p><p>为了解决这个问题，<code>Kube-Apiserver</code> 暴露了一个 <code>?includeUninitialized</code> 查询参数，它会返回所有的资源对象（包括未初始化的）。</p><h3 id="5-控制循环">5. 控制循环</h3><p><strong>Deployments controller</strong></p><p>到了这个阶段，我们的 <code>Deployment</code> 记录已经保存在 <code>Etcd</code> 中，并且所有的初始化逻辑都执行完成，接下来的阶段将会涉及到该资源所依赖的拓扑结构。在 <code>Kubernetes</code> 中，<code>Deployment</code> 实际上只是一系列 <code>Replicaset</code> 的集合，而 <code>Replicaset</code> 是一系列 <code>Pod</code> 的集合。那么 <code>Kubernetes</code> 是如何从一个 <code>HTTP</code> 请求按照层级结构依次创建这些资源的呢？其实这些工作都是由 <code>Kubernetes</code> 内置的 <code>Controller</code>(控制器) 来完成的。</p><p><code>Kubernetes</code> 在整个系统中使用了大量的 <code>Controller</code>，<code>Controller</code> 是一个用于将系统状态从当前状态修正到期望状态的异步脚本。所有 <code>Controller</code> 都通过 <code>Kube-Controller-Manager</code> 组件并行运行，每种 <code>Controller</code> 都负责一种具体的控制流程。首先介绍一下 <code>Deployment Controller</code>：</p><p>将 <code>Deployment</code> 记录存储到 <code>Etcd</code> 并初始化后，就可以通过 <code>Kube-Apiserver</code> 使其可见，然后 <code>Deployment Controller</code> 就会检测到它（它的工作就是负责监听 <code>Deployment</code> 记录的更改）。在我们的例子中，控制器通过一个 <code>Informer</code> 注册一个创建事件的特定回调函数（更多信息参加下文）。</p><p>当 <code>Deployment</code> 第一次对外可见时，该 <code>Controller</code> 就会将该资源对象添加到内部工作队列，然后开始处理这个资源对象：</p><blockquote><p>通过使用标签选择器查询 <code>Kube-Apiserver</code> 来检查该 <code>Deployment</code> 是否有与其关联的 <code>ReplicaSet</code> 或 <code>Pod</code> 记录。</p></blockquote><p>有趣的是，这个同步过程是状态不可知的，它核对新记录与核对已经存在的记录采用的是相同的方式。</p><p>在意识到没有与其关联的 <code>ReplicaSet</code> 或 <code>Pod</code> 记录后，<code>Deployment Controller</code> 就会开始执行弹性伸缩流程：</p><blockquote><p>创建 <code>ReplicaSet</code> 资源，为其分配一个标签选择器并将其版本号设置为 1。</p></blockquote><p><code>ReplicaSet</code> 的 <code>PodSpec</code> 字段从 <code>Deployment</code> 的 <code>manifest</code> 以及其他相关元数据中复制而来。有时 <code>Deployment</code> 记录在此之后也需要更新（例如：如果设置了 <code>Process Deadline</code>）。</p><p>当完成以上步骤之后，该 <code>Deployment</code> 的 <code>Status</code> 就会被更新，然后重新进入与之前相同的循环，等待 <code>Deployment</code> 与期望的状态相匹配。由于 <code>Deployment Controller</code> 只关心 <code>ReplicaSet</code>，因此需要通过 <code>ReplicaSet Controller</code> 来继续协调。</p><p><strong>ReplicaSets controller</strong></p><p>在前面的步骤中，<code>Deployment Controller</code> 创建了第一个 <code>ReplicaSet</code>，但仍然还是没有 <code>Pod</code>，这时候就该 <code>ReplicaSet Controller</code> 登场了！<code>ReplicaSet Controller</code> 的工作是监视 <code>ReplicaSets</code> 及其相关资源（<code>Pod</code>）的生命周期。和大多数其他 <code>Controller</code> 一样，它通过触发某些事件的处理器来实现此目的。</p><p>当创建 <code>ReplicaSet</code> 时（由 <code>Deployment Controller</code> 创建），<code>RS Controller</code> 检查新 <code>ReplicaSet</code> 的状态，并检查当前状态与期望状态之间存在的偏差，然后通过调整 <code>Pod</code> 的副本数来达到期望的状态。</p><p><code>Pod</code> 的创建也是批量进行的，从 <code>SlowStartInitialBatchSize</code> 开始，然后在每次成功的迭代中以一种 <code>Slow Start</code> 操作加倍。这样做的目的是在大量 <code>Pod</code> 启动失败时（例如：由于资源配额），可以减轻 <code>Kube-Apiserver</code> 被大量不必要的 <code>HTTP</code> 请求吞没的风险。如果创建失败，最好能够优雅地失败，并且对其他的系统组件造成的影响最小！</p><p><code>Kubernetes</code> 通过 <code>Owner References</code>（在子级资源的某个字段中引用其父级资源的 ID） 来构造严格的资源对象层级结构。这确保了一旦 <code>Controller</code> 管理的资源被删除（级联删除），子资源就会被垃圾收集器删除，同时还为父级资源提供了一种有效的方式来避免他们竞争同一个子级资源（想象两对父母都认为他们拥有同一个孩子的场景）。</p><p><code>Owner References</code> 的另一个好处是：它是有状态的。如果有任何 <code>Controller</code> 重启了，那么由于资源对象的拓扑关系与 <code>Controller</code> 无关，该操作不会影响到系统的稳定运行。这种对资源隔离的重视也体现在 <code>Controller</code> 本身的设计中：<code>Controller</code> 不能对自己没有明确拥有的资源进行操作，它们应该选择对资源的所有权，互不干涉，互不共享。</p><p>有时系统中也会出现孤儿（<code>Orphaned</code>）资源，通常由以下两种途径产生：</p><ul><li>父级资源被删除，但子级资源没有被删除</li><li>垃圾收集策略禁止删除子级资源</li></ul><p>当发生这种情况时，<code>Controller</code> 将会确保孤儿资源拥有新的 <code>Owner</code>。多个父级资源可以相互竞争同一个孤儿资源，但只有一个会成功（其他父级资源会收到验证错误）。</p><p><strong>Informers</strong></p><p>你可能已经注意到，某些 <code>Controller</code>（例如：<code>RBAC</code> 授权器或 <code>Deployment Controller</code>）需要先检索集群状态然后才能正常运行。拿 <code>RBAC</code> 授权器举例，当请求进入时，授权器会将用户的初始状态缓存下来，然后用它来检索与 <code>Etcd</code> 中的用户关联的所有角色（<code>Role</code>）和 角色绑定（<code>RoleBinding</code>）。那么问题来了，<code>Controller</code> 是如何访问和修改这些资源对象的呢？事实上 <code>Kubernetes</code> 是通过 <code>Informer</code> 机制来解决这个问题的。</p><p><code>Infomer</code> 是一种模式，它允许 <code>Controller</code> 查找缓存在本地内存中的数据(这份数据由 <code>Informer</code> 自己维护)并列出它们感兴趣的资源。</p><p>虽然 <code>Informer</code> 的设计很抽象，但它在内部实现了大量的对细节的处理逻辑（例如：缓存），缓存很重要，因为它不但可以减少对 <code>Kubenetes API</code> 的直接调用，同时也能减少 <code>Server</code> 和 <code>Controller</code> 的大量重复性工作。通过使用 <code>Informer</code>，不同的 <code>Controller</code> 之间以线程安全（<code>Thread safety</code>）的方式进行交互，而不必担心多个线程访问相同的资源时会产生冲突。</p><p>有关 <code>Informer</code> 的更多详细解析，请参考这篇文章：<a href="https://borismattijssen.github.io/articles/kubernetes-informers-controllers-reflectors-stores" target="_blank" rel="noopener">Kubernetes: Controllers, Informers, Reflectors and Stores</a></p><p><strong>Scheduler</strong></p><p>当所有的 <code>Controller</code> 正常运行后，<code>Etcd</code> 中就会保存一个 <code>Deployment</code>、一个 <code>ReplicaSet</code> 和三个 <code>Pod</code> 资源记录，并且可以通过 <code>Kube-Apiserver</code> 查看。然而，这些 <code>Pod</code> 资源现在还处于 <code>Pending</code> 状态，因为它们还没有被调度到集群中合适的 <code>Node</code> 上运行。这个问题最终要靠调度器 <code>Scheduler</code> 来解决。</p><p><code>Scheduler</code> 作为一个独立的组件运行在集群控制平面上，工作方式与其他 <code>Controller</code> 相同：监听实际并将系统状态调整到期望的状态。具体来说，<code>Scheduler</code> 的作用是将待调度的 <code>Pod</code> 按照特定的算法和调度策略绑定到集群中某个合适的  <code>Node</code> 上，并将绑定信息写入 <code>Etcd</code> 中（它会过滤其 <code>PodSpec</code> 中 <code>NodeName</code> 字段为空的 <code>Pod</code>），默认的调度算法的工作方式如下：</p><ol><li><p>当 <code>Scheduler</code> 启动时，会注册一个默认的预选策略链，这些预选策略会对备选节点进行评估，判断备选节点是否满足备选 <code>Pod</code> 的需求。例如：如果 <code>PodSpec</code> 字段限制了 <code>CPU</code> 和内存资源，那么当备选节点的资源容量不满足备选 <code>Pod</code> 的需求时，备选 <code>Pod</code> 就不会被调度到该节点上（资源容量=备选节点资源总量-节点中已存在 <code>Pod</code> 的所有容器的需求资源（<code>CPU</code> 和内存的总和）</p></li><li><p>一旦筛选出符合要求的候选节点，就会采用优选策略计算出每个候选节点的积分，然后对这些候选节点进行排序，积分最高者胜出。例如：为了在整个系统中分摊工作负载，这些优选策略会从备选节点列表中选出资源消耗最小的节点。每个节点通过优选策略时都会算出一个得分，计算各项得分，最终选出分值大的节点作为优选的结果。</p></li></ol><p>一旦找到了合适的节点，<code>Scheduler</code> 就会创建一个 <code>Binding</code> 对象，该对象的 <code>Name</code> 和 <code>UID</code> 与 <code>Pod</code> 相匹配，并且其 <code>ObjectReference</code> 字段包含所选节点的名称，然后通过 <code>POST</code> 请求发送给 <code>Apiserver</code>。</p><p>当 <code>Kube-Apiserver</code> 接收到此 <code>Binding</code> 对象时，注册表会将该对象反序列化并更新 <code>Pod</code> 资源中的以下字段：</p><ul><li>将 <code>NodeName</code> 的值设置为 <code>ObjectReference</code> 中的 <code>NodeName</code>。</li><li>添加相关的注释。</li><li>将 <code>PodScheduled</code> 的 <code>Status</code> 值设置为 <code>True</code>。可以通过 <code>Kubectl</code> 来查看：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get &lt;PODNAME&gt; -o go-template=<span class="string">'&#123;&#123;range .status.conditions&#125;&#125;&#123;&#123;if eq .type "PodScheduled"&#125;&#125;&#123;&#123;.status&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;end&#125;&#125;'</span></span><br></pre></td></tr></table></figure><p>一旦 <code>Scheduler</code> 将 <code>Pod</code> 调度到某个节点上，该节点的 <code>Kubelet</code> 就会接管该 <code>Pod</code> 并开始部署。</p><blockquote><p>注：预选策略和优选策略都可以通过 <code>--policy-config-file</code> 参数来扩展，如果默认的调度器不满足要求，还可以部署自定义的调度器。如果 <code>podSpec.schedulerName</code> 的值设置为其他的调度器，则 <code>Kubernetes</code> 会将该 <code>Pod</code> 的调度转交给那个调度器。</p></blockquote><h3 id="6-kubelet">6. Kubelet</h3><p><strong>Pod 同步</strong></p><p>现在，所有的 <code>Controller</code> 都完成了工作，我们来总结一下：</p><ul><li><code>HTTP</code> 请求通过了认证、授权和准入控制阶段。</li><li>一个 <code>Deployment</code>、<code>ReplicaSet</code> 和三个 <code>Pod</code> 资源被持久化到 <code>Etcd</code> 存储中。</li><li>然后运行了一系列初始化器。</li><li>最后每个 <code>Pod</code> 都被调度到合适的节点。</li></ul><p>然而到目前为止，所有的状态变化仅仅只是针对保存在 <code>Etcd</code> 中的资源记录，接下来的步骤涉及到运行在工作节点之间的 <code>Pod</code> 的分布状况，这是分布式系统（比如：<code>Kubernetes</code>）的关键因素。这些任务都是由 <code>Kubelet</code> 组件完成的，让我们开始吧！</p><p>在 <code>Kubernetes</code> 集群中，每个 <code>Node</code> 节点上都会启动一个 <code>Kubelet</code> 服务进程，该进程用于处理 <code>Scheduler</code> 下发到本节点的任务，管理 <code>Pod</code> 的生命周期，包括挂载卷、容器日志记录、垃圾回收以及其他与 <code>Pod</code> 相关的事件。</p><p>如果换一种思维模式，你可以把 <code>Kubelet</code> 当成一种特殊的 <code>Controller</code>，它每隔 20 秒（可以自定义）向 <code>Kube-Apiserver</code> 通过 <code>NodeName</code> 获取自身 <code>Node</code> 上所要运行的 <code>Pod</code> 清单。一旦获取到了这个清单，它就会通过与自己的内部缓存进行比较来检测新增加的 <code>Pod</code>，如果有差异，就开始同步 <code>Pod</code> 列表。我们来详细分析一下同步过程：</p><ol><li><p>如果 <code>Pod</code> 正在创建， <code>Kubelet</code> 就会记录一些在 <code>Prometheus</code> 中用于追踪 <code>Pod</code> 启动延时的指标。</p></li><li><p>然后生成一个 <code>PodStatus</code> 对象，它表示 <code>Pod</code> 当前阶段的状态。<code>Pod</code> 的状态(<code>Phase</code>) 是 <code>Pod</code> 在其生命周期中的最精简的概要，包括 <code>Pending</code>、<code>Running</code>、<code>Succeeded</code>、<code>Failed</code> 和 <code>Unkown</code> 这几个值。状态的产生过程非常复杂，所以很有必要深入了解一下背后的原理：</p><ul><li><p>首先串行执行一系列 <code>Pod</code> 同步处理器（<code>PodSyncHandlers</code>），每个处理器检查 <code>Pod</code> 是否应该运行在该节点上。当所有的处理器都认为该 <code>Pod</code> 不应该运行在该节点上，则 <code>Pod</code> 的 <code>Phase</code> 值就会变成 <code>PodFailed</code>，并且将该 <code>Pod</code> 从该节点上驱逐出去。例如：当你创建一个 <code>Job</code> 时，如果 <code>Pod</code> 失败重试的时间超过了 <code>spec.activeDeadlineSeconds</code> 设置的值，就会将 <code>Pod</code> 从该节点驱逐出去。</p></li><li><p>接下来，<code>Pod</code> 的 <code>Phase</code> 值由 <code>Init</code> 容器和应用容器的状态共同来决定。因为目前容器还没有启动，容器被视为处于等待阶段，如果 <code>Pod</code> 中至少有一个容器处于等待阶段，则其 <code>Phase</code> 值为 <code>Pending</code>。</p></li><li><p>最后，<code>Pod</code> 的 <code>Condition</code> 字段由 <code>Pod</code> 内所有容器的状态决定。现在我们的容器还没有被容器运行时创建，所以 <code>PodReady</code> 的状态被设置为 <code>False</code>。可以通过 <code>Kubectl</code> 查看：</p></li></ul></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get &lt;PODNAME&gt; -o go-template=<span class="string">'&#123;&#123;range .status.conditions&#125;&#125;&#123;&#123;if eq .type "Ready"&#125;&#125;&#123;&#123;.status&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;end&#125;&#125;'</span></span><br></pre></td></tr></table></figure><ol start="3"><li><p>生成 <code>PodStatus</code> 之后（<code>Pod</code> 中的 <code>Status</code> 字段），<code>Kubelet</code> 就会将它发送到 <code>Pod</code> 的状态管理器，该管理器的任务是通过 <code>Apiserver</code> 异步更新 <code>Etcd</code> 中的记录。</p></li><li><p>接下来运行一系列准入处理器来确保该 <code>Pod</code> 是否具有相应的权限（包括强制执行 <code>AppArmor</code> 配置文件和 <code>NO_NEW_PRIVS</code>），被准入控制器拒绝的 <code>Pod</code> 将一直保持 <code>Pending</code> 状态。</p></li><li><p>如果 <code>Kubelet</code> 启动时指定了 <code>cgroups-per-qos</code> 参数，<code>Kubelet</code> 就会为该 <code>Pod</code> 创建 <code>Cgroup</code> 并进行相应的资源限制。这是为了更方便地对 <code>Pod</code> 进行服务质量（<code>QoS</code>）管理。</p></li><li><p>然后为 <code>Pod</code> 创建相应的目录，包括 <code>Pod</code> 的目录（/var/run/kubelet/pods/<podid>），该 <code>Pod</code> 的卷目录（<code>&lt;podDir&gt;/volumes</code>）和该 Pod 的插件目录（<code>&lt;podDir&gt;/plugins</code>）。</podid></p></li><li><p>卷管理器会挂载 <code>Spec.Volumes</code> 中定义的相关数据卷，然后等待是否挂载成功。根据挂载卷类型的不同，某些 <code>Pod</code> 可能需要等待更长的时间（比如：<code>NFS</code> 卷）。</p></li><li><p>从 <code>Apiserver</code> 中检索 <code>Spec.ImagePullSecrets</code> 中定义的所有 <code>Secret</code>，然后将其注入到容器中。</p></li><li><p>最后通过容器运行时接口（<code>Container Runtime Interface</code>（CRI））开始启动容器（下面会详细描述）。</p></li></ol><p><strong>CRI 与 Pause 容器</strong></p><p>到了这个阶段，大量的初始化工作都已经完成，容器已经准备好开始启动了，而容器是由容器运行时（例如：<code>Docker</code> 和 <code>Rkt</code>）启动的。</p><p>为了更容易扩展，<code>Kubelet</code> 从 1.5.0 开始通过容器运行时接口与容器运行时（<code>Container Runtime</code>）交互。简而言之，<code>CRI</code> 提供了 <code>Kubelet</code> 和特定的运行时之间的抽象接口，它们之间通过<a href="https://github.com/google/protobuf" target="_blank" rel="noopener">协议缓冲区</a>（它像一个更快的 <code>JSON</code>）和 <a href="https://grpc.io/" target="_blank" rel="noopener">gRPC API</a>（一种非常适合执行 <code>Kubernetes</code> 操作的 <code>API</code>）交互。这是一个非常酷的想法，通过使用 <code>Kubelet</code> 和运行时之间定义的契约关系，容器如何编排的具体实现细节已经变得无关紧要。由于不需要修改 <code>Kubernetes</code> 的核心代码，开发者可以以最小的开销添加新的运行时。</p><p>不好意思有点跑题了，让我们继续回到容器启动的阶段。第一次启动 <code>Pod</code> 时，<code>Kubelet</code> 会通过 <code>Remote Procedure Command</code>(RPC) 协议调用 <code>RunPodSandbox</code>。<code>Sandbox</code> 用于描述一组容器，例如：在 <code>Kubernetes</code> 中它表示的是 <code>Pod</code>。<code>Sandbox</code> 是一个很宽泛的概念，所以对于其他没有使用容器的运行时仍然是有意义的（比如：在一个基于 <code>Hypervisor</code> 的运行时中，<code>Sandbox</code> 可能指的就是虚拟机）。</p><p>我们的例子中使用的容器运行时是 <code>Docker</code>，创建 <code>Sandbox</code> 时首先创建的是 <code>Pause</code> 容器。<code>Pause</code> 容器作为同一个 <code>Pod</code> 中所有其他容器的基础容器，它为 <code>Pod</code> 中的每个业务容器提供了大量的 <code>Pod</code> 级别资源，这些资源都是 <code>Linux</code> 命名空间（包括：网络命名空间，<code>IPC</code> 命名空间和 <code>PID</code> 命名空间）。</p><p><code>Pause</code> 容器提供了一种方法来管理所有这些命名空间并允许业务容器共享它们，在同一个网络命名空间中的好处是：同一个 <code>Pod</code> 中的容器可以使用 <code>Localhost</code> 来相互通信。<code>Pause</code> 容器的第二个功能与 <code>PID</code> 命名空间的工作方式相关，在 <code>PID</code> 命名空间中，进程之间形成一个树状结构，一旦某个子进程由于父进程的错误而变成了孤儿进程，其便会被 <code>Init</code> 进程进行收养并最终回收资源。关于 <code>Pause</code> 工作方式的详细信息可以参考：<a href="https://www.ianlewis.org/en/almighty-pause-container" target="_blank" rel="noopener">The Almighty Pause Container</a>。</p><p>一旦创建好了 <code>Pause</code> 容器，下面就会开始检查磁盘状态然后开始启动业务容器。</p><p><strong>CNI 和 Pod 网络</strong></p><p>现在我们的 <code>Pod</code> 已经有了基本的骨架：一个共享所有命名空间以允许业务容器在同一个 <code>Pod</code> 里进行通信的 <code>Pause</code> 容器。但现在还有一个问题，那就是容器的网络是如何建立的？</p><p>当 <code>Kubelet</code> 为 <code>Pod</code> 创建网络时，它会将创建网络的任务交给 <code>CNI</code> 插件。<code>CNI</code> 表示容器网络接口（<code>Container Network Interface</code>），和容器运行时的运行方式类似，它也是一种抽象，允许不同的网络提供商为容器提供不同的网络实现。通过将 <code>JSON</code> 配置文件（默认在 <code>/etc/cni/net.d</code> 路径下）中的数据传送到相关的 <code>CNI</code> 二进制文件（默认在 <code>/opt/cni/bin</code> 路径下）中，<code>CNI</code> 插件可以给 <code>Pause</code> 容器配置相关的网络，然后<code> Pod</code> 中其他的容器都使用 <code>Pause</code> 容器的网络。下面是一个简单的示例配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#123;</span></span><br><span class="line">    <span class="attr">"cniVersion":</span> <span class="string">"0.3.1"</span><span class="string">,</span></span><br><span class="line">    <span class="attr">"name":</span> <span class="string">"bridge"</span><span class="string">,</span></span><br><span class="line">    <span class="attr">"type":</span> <span class="string">"bridge"</span><span class="string">,</span></span><br><span class="line">    <span class="attr">"bridge":</span> <span class="string">"cnio0"</span><span class="string">,</span></span><br><span class="line">    <span class="attr">"isGateway":</span> <span class="literal">true</span><span class="string">,</span></span><br><span class="line">    <span class="attr">"ipMasq":</span> <span class="literal">true</span><span class="string">,</span></span><br><span class="line">    <span class="attr">"ipam":</span> <span class="string">&#123;</span></span><br><span class="line">        <span class="attr">"type":</span> <span class="string">"host-local"</span><span class="string">,</span></span><br><span class="line">        <span class="attr">"ranges":</span> <span class="string">[</span></span><br><span class="line">          <span class="string">[&#123;"subnet":</span> <span class="string">"$&#123;POD_CIDR&#125;"</span><span class="string">&#125;]</span></span><br><span class="line">        <span class="string">],</span></span><br><span class="line">        <span class="attr">"routes":</span> <span class="string">[&#123;"dst":</span> <span class="string">"0.0.0.0/0"</span><span class="string">&#125;]</span></span><br><span class="line">    <span class="string">&#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p><code>CNI</code> 插件还会通过 <code>CNI_ARGS</code> 环境变量为 <code>Pod</code> 指定其他的元数据，包括 <code>Pod</code> 名称和命名空间。</p><p>下面我们以 <code>Bridge</code> 插件举例(步骤会因不同 <code>CNI</code> 插件而异)：</p><ul><li><p>该插件首先会在根网络命名空间（也就是宿主机的网络命名空间）中设置本地 <code>Linux</code> 网桥，以便为该主机上的所有容器提供网络服务。</p></li><li><p>然后它会将一个网络接口（<code>Veth</code> 设备对的一端）插入到 <code>Pause</code> 容器的网络命名空间中，并将另一端连接到网桥上。你可以这样来理解 <code>Veth</code> 设备对：它就像一根很长的管道，一端连接到容器，一端连接到根网络命名空间中，数据包就在管道中进行传播。</p></li><li><p>接下来 <code>JSON</code> 文件中指定的 <code>IPAM Plugin</code> 会为 <code>Pause</code> 容器的网络接口分配一个 <code>IP</code> 并设置相应的路由，现在 <code>Pod</code> 就有了自己的 <code>IP</code>。</p><ul><li><code>IPAM Plugin</code> 的工作方式和 <code>CNI Plugin</code> 类似：通过二进制文件调用并具有标准化的接口，每一个 <code>IPAM Plugin</code> 都必须要确定容器网络接口的 <code>IP</code>、子网以及网关和路由，并将信息返回给 <code>CNI</code> 插件。最常见的 <code>IPAM Plugin</code> 是 <code>host-local</code>，它从预定义的一组地址池中为容器分配 <code>IP</code> 地址。它将地址池的信息以及分配信息保存在主机的文件系统中，从而确保了同一主机上每个容器的 <code>IP</code> 地址的唯一性。</li></ul></li><li><p>最后 <code>Kubelet</code> 会将集群内部的 <code>DNS</code> 服务器的 <code>Cluster IP</code> 地址传给 <code>CNI</code> 插件，然后 <code>CNI</code> 插件将它们写到容器的 <code>/etc/resolv.conf</code> 文件中。</p></li></ul><p>一旦完成了上面的步骤，<code>CNI</code> 插件就会将操作的结果以 <code>JSON</code> 的格式返回给 <code>Kubelet</code>。更多关于 <code>CNI</code> 相关信息可参考 「<a href="https://zhuanlan.zhihu.com/p/27460083" target="_blank" rel="noopener">CNI网络插件指南</a>」一文。</p><p><strong>跨主机容器网络</strong></p><p>到目前为止，我们已经描述了容器如何与宿主机进行通信，但跨主机之间的容器如何通信呢？</p><p>通常情况下使用 <code>Overlay</code> 网络来进行跨主机容器通信，这是一种动态同步多个主机间路由的方法。 其中最常用的 <code>Overlay</code> 网络插件是 <code>Flannel</code>，<code>Flannel</code> 具体的工作方式可以参考 <a href="https://github.com/coreos/flannel" target="_blank" rel="noopener">CoreOS</a> 的文档。</p><p><strong>容器启动</strong></p><p>所有网络都配置完成后，接下来就开始真正启动业务容器了！</p><p>一旦 <code>Sanbox</code> 完成初始化并处于 <code>Active</code> 状态，<code>Kubelet</code> 就可以开始为其创建容器了。首先启动 <code>PodSpec</code> 中定义的 <code>Init</code> 容器，然后再启动业务容器。具体过程如下：</p><ol><li>首先拉取容器的镜像。如果是私有仓库的镜像，就会利用 <code>PodSpec</code> 中指定的 <code>Secret</code> 来拉取该镜像。</li><li>然后通过 <code>CRI</code> 接口创建容器。<code>Kubelet</code> 向 <code>PodSpec</code> 中填充了一个 <code>ContainerConfig</code> 数据结构（在其中定义了命令、镜像、标签、挂载卷、设备、环境变量等），然后通过 <code>Protobufs</code> 发送给 <code>CRI</code> 接口。对于 <code>Docker</code> 来说，它会将这些信息反序列化并填充到自己的配置信息中，然后再发送给 <code>Dockerd</code> 守护进程。在这个过程中，它会将一些元数据标签（例如：容器类型、日志路径、<code>Sandbox ID</code> 等）添加到容器中。</li><li>接下来会使用 <code>CPU</code> 管理器来约束容器，这是 1.8 中新添加的 <code>Alpha</code> 特性，它使用 <code>UpdateContainerResources</code> CRI 方法将容器分配给本节点上的 <code>CPU</code> 资源池。</li><li>最后容器开始真正启动。</li><li>如果 <code>Pod</code> 中配置了容器生命周期钩子（<code>Hook</code>），容器启动之后就会运行这些 <code>Hook</code>。<code>Hook</code> 的类型包括两种：<code>Exec</code>（执行一段命令） 和 <code>HTTP</code>（发送 <code>HTTP</code> 请求）。如果 <code>PostStart Hook</code> 启动的时间过长、挂起或者失败，容器将永远不会变成 <code>Running</code> 状态。</li></ol><h3 id="7-总结">7. 总结</h3><p>如果上面一切顺利，现在你的集群上应该会运行三个容器，所有的网络，数据卷和秘钥都被通过 <code>CRI</code> 接口添加到容器中并配置成功。</p><h3 id="8-原文链接">8. 原文链接</h3><p><a href="https://github.com/jamiehannaford/what-happens-when-k8s" target="_blank" rel="noopener">What happens when … Kubernetes edition!</a></p><blockquote><p>来源：Ryan Yang 的 Blog<br>原文：<a href="http://t.cn/RDhqlnT" target="_blank" rel="noopener">http://t.cn/RDhqlnT</a><br>题图：来自谷歌图片搜索<br>版权：本文版权归原作者所有</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;想象一下，如果我想将 &lt;code&gt;Nginx&lt;/code&gt; 部署到 &lt;code&gt;Kubernetes&lt;/code&gt; 集群，我可能会在终端中输入类似这样的命令：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ kubectl run --image&amp;#x3D;nginx --replicas&amp;#x3D;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后回车。几秒钟后，你就会看到三个 &lt;code&gt;Nginx Pod&lt;/code&gt; 分布在所有的工作节点上。这一切就像变魔术一样，但你并不知道这一切的背后究竟发生了什么事情。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Kubernetes&lt;/code&gt; 的神奇之处在于：它可以通过用户友好的 &lt;code&gt;API&lt;/code&gt; 来处理跨基础架构的 &lt;code&gt;Deployments&lt;/code&gt;，而背后的复杂性被隐藏在简单的抽象中。但为了充分理解它为我们提供的价值，我们需要理解它的内部原理。&lt;/p&gt;
&lt;p&gt;本指南将引导您理解从 &lt;code&gt;Client&lt;/code&gt; 到 &lt;code&gt;Kubelet&lt;/code&gt; 请求的完整生命周期，必要时会通过源代码来说明背后发生了什么。&lt;/p&gt;
&lt;p&gt;这是一份可以在线修改的文档，如果你发现有什么可以改进或重写的，欢迎提供帮助！&lt;/p&gt;
&lt;h3 id=&quot;1-Kubectl&quot;&gt;1. Kubectl&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;验证和生成器&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当敲下回车键以后，&lt;code&gt;Kubectl&lt;/code&gt; 首先会执行一些客户端验证操作，以确保不合法的请求（例如：创建不支持的资源或使用格式错误的镜像名称）将会快速失败，也不会发送给 &lt;code&gt;Kube-Apiserver&lt;/code&gt;。通过减少不必要的负载来提高系统性能。&lt;/p&gt;
&lt;p&gt;验证通过之后，&lt;code&gt;Kubectl&lt;/code&gt; 开始将发送给 &lt;code&gt;Kube-Apiserver&lt;/code&gt; 的 &lt;code&gt;HTTP&lt;/code&gt; 请求进行封装。&lt;code&gt;Kube-Apiserver&lt;/code&gt; 与 &lt;code&gt;Etcd&lt;/code&gt; 进行通信，所有尝试访问或更改 &lt;code&gt;Kubernetes&lt;/code&gt; 系统状态的请求都会通过 &lt;code&gt;Kube-Apiserver&lt;/code&gt; 进行，&lt;code&gt;Kubectl&lt;/code&gt; 也不例外。&lt;code&gt;Kubectl&lt;/code&gt; 使用生成器（&lt;a href=&quot;https://kubernetes.io/docs/user-guide/kubectl-conventions/#generators&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Generators&lt;/a&gt;）来构造 &lt;code&gt;HTTP&lt;/code&gt; 请求。生成器是一个用来处理序列化的抽象概念。&lt;/p&gt;
&lt;p&gt;通过 &lt;code&gt;kubectl run&lt;/code&gt; 不仅可以运行 &lt;code&gt;Deployment&lt;/code&gt;，还可以通过指定参数 &lt;code&gt;--generator&lt;/code&gt; 来部署其他多种资源类型。如果没有指定 &lt;code&gt;--generator&lt;/code&gt; 参数的值，&lt;code&gt;Kubectl&lt;/code&gt; 将会自动判断资源的类型。&lt;/p&gt;
&lt;p&gt;例如：带有参数 &lt;code&gt;--restart-policy=Always&lt;/code&gt; 的资源将被部署为 &lt;code&gt;Deployment&lt;/code&gt;，而带有参数 &lt;code&gt;--restart-policy=Never&lt;/code&gt; 的资源将被部署为 &lt;code&gt;Pod&lt;/code&gt;。同时 &lt;code&gt;Kubectl&lt;/code&gt; 也会检查是否需要触发其他操作，例如：记录命令（用来进行回滚或审计）。&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;Kubectl&lt;/code&gt; 判断出要创建一个 &lt;code&gt;Deployment&lt;/code&gt; 后，它将使用 &lt;code&gt;DeploymentV1Beta1&lt;/code&gt; 生成器从我们提供的参数中生成一个运行时对象。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>如何 10 步 Docker 化一个应用</title>
    <link href="https://www.hi-linux.com/posts/57498.html"/>
    <id>https://www.hi-linux.com/posts/57498.html</id>
    <published>2018-08-01T01:00:00.000Z</published>
    <updated>2018-08-24T08:19:54.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>本文将讲解如何将应用 Docker 化的一些很实用的技巧和准则，推荐一读。</p><p><strong>一、选择基础镜像</strong></p><p>每种对应技术几乎都有自己的基础镜像，例如：</p><ul><li><a href="https://hub.docker.com/_/java/" target="_blank" rel="noopener">https://hub.docker.com/_/java/</a></li><li><a href="https://hub.docker.com/_/python/" target="_blank" rel="noopener">https://hub.docker.com/_/python/</a></li><li><a href="https://hub.docker.com/_/nginx/" target="_blank" rel="noopener">https://hub.docker.com/_/nginx/</a></li></ul><p>如果不能直接使用这些镜像，我们就需要从基础操作系统镜像开始安装所有的依赖。</p><p>网上大多数教程使用的都是以 Ubuntu（例如：Ubuntu:16.04 ）作为基础镜像，这并不是一个问题，但是我建议优先考虑 Alpine 镜像：</p><ul><li><a href="https://hub.docker.com/_/alpine/" target="_blank" rel="noopener">https://hub.docker.com/_/alpine/</a></li></ul><p>Alpine 是一个非常小的基础镜像（它的容量大约只有 5MB）。</p><blockquote><p>注：在基于 Alpine 的镜像中你无法使用 apt-get 命令。不过你不必担心，因为 Alpine 系统有自己的软件包仓库和包管理工具 apk。关于 apk 的具体使用你可以详细参考：「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247484888&amp;idx=1&amp;sn=bfdd55a668b068ab34251512613e5267&amp;chksm=eac524f1ddb2ade7739c10440af8b33a39b81b8a4a57b628cc6c0d0ec3a82f18b914f4e12641&amp;mpshare=1&amp;scene=23&amp;srcid=0731dgw9eDCTtafPzAd3VxDV%23rd" target="_blank" rel="noopener">Alpine Linux配置使用技巧</a>」一文。</p></blockquote><p><strong>二、安装必要软件包</strong></p><p>这个步骤通常比较琐碎，有一些容易忽略的细节：</p><ul><li><p><code>apt-get update</code> 和 <code>apt-get install</code> 命令应该写在一行（如果使用 Alpine 则对应的是 apk 命令）。这不是一个常见的做法，但是在 Dockerfile 中应该要这么做。否则 <code>apt-get update</code> 命令产出的临时层可能会被缓存，导致构建时没有更新包信息。（具体可参见<a href="https://forums.docker.com/t/dockerfile-run-apt-get-install-all-packages-at-once-or-one-by-one/17191" target="_blank" rel="noopener">此文</a>）。</p></li><li><p>确认是否只安装了实际需要的软件（特别是在生产环境中运行这个容器）。</p></li></ul><blockquote><p>注：我见过有人在他们的镜像中安装了 vim 和其他开发工具。如果这是必要的，应该针对构建、调试和开发环境创建不同的 Dockerfile。这不仅仅关系到镜像大小，还涉及到安全性、可维护性等等。</p></blockquote><a id="more"></a><p><strong>三、添加自定义文件</strong></p><p>一些优化 Dockerfile 的小提示：</p><ul><li><p>理解 COPY 和 ADD 指令的区别，具体可参考<a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#add-or-copy" target="_blank" rel="noopener">此文</a>。</p></li><li><p>尽可能遵照<a href="http://www.pathname.com/fhs/" target="_blank" rel="noopener">文件系统</a>惯例来存放文件。例如：针对解释型应用程序（如：Python），使用 /usr/src 目录。</p></li><li><p>检查添加文件的属性。如果需要可执行权限，没有必要在镜像上新建一个层（ 通过 RUN chmod +x … 指令来增加权限）。你只需要在代码仓库的源文件上修正这些属性即可，即使开发平台是 Windows，也可以参照<a href="https://stackoverflow.com/questions/21691202/how-to-create-file-execute-mode-permissions-in-git-on-windows" target="_blank" rel="noopener">此文</a>给文件增加可执行权限。</p></li></ul><p><strong>四、定义容器运行时的用户权限</strong></p><ul><li>容器中的进程默认情况下是以 root 权限运行的。</li><li>如果容器中的应用程序需要使用特定的用户或组（/etc/passwd 或 /etc/group）来运行时，可以在容器启动时使用 <code>docker run</code> 命令的 <code>--user</code> 参数来指定其固定的 UID 或 GID。</li><li>尽可能避免容器中的进程以 root 权限运行。</li></ul><blockquote><p>注：现在不少热门应用程序镜像都需要用特定的用户 ID 来运行（例如:Elastic Search 需要 uid:gid = 1000:1000），请尽量不要在写出这样的镜像。更多关于容器内运行应用程序的权限说明可参考<a href="https://medium.com/@mccode/understanding-how-uid-and-gid-work-in-docker-containers-c37a01d01cf" target="_blank" rel="noopener">此文</a>。</p></blockquote><p><strong>五、定义暴露的端口</strong></p><p>不要为了暴露特权端口（例如：80）而将容器以 root 权限运行。如果有这样的需求，可以让容器暴露一个非特权端口（例如：8080），然后在启动时进行端口映射。</p><blockquote><p>注：低于 1024 的 TCP / IP 端口号就是特权端口，因为不允许普通用户在这些端口上运行服务。</p></blockquote><p><strong>六、定义入口点（entrypoint）</strong></p><ul><li><p>普通方式：直接运行可执行文件。</p></li><li><p>更好的方式：创建一个 <a href="http://docker-entrypoint.sh" target="_blank" rel="noopener">docker-entrypoint.sh</a> 脚本，这样可以通过环境变量来配置容器的入口点。这也是一个非常普遍的做法，可参考下面这些例子：elasticsearch 的 <a href="https://github.com/elastic/elasticsearch-docker/tree/master/build/elasticsearch/bin" target="_blank" rel="noopener">docker-entrypoint.sh</a> 文件 和 postgres 的 <a href="https://github.com/docker-library/postgres/tree/de8ba87d50de466a1e05e111927d2bc30c2db36d/10" target="_blank" rel="noopener">docker-entrypoint.sh</a> 文件。</p></li></ul><p><strong>七、定义一种配置方式</strong></p><p>每个应用程序都需要参数化，你基本上可以遵循以下两个原则：</p><ul><li><p>使用应用程序特定的配置文件：该方式需要通过文档来说明配置文件的格式、字段、放置位置等等（当运行环境比较复杂，例如：应用程序跨越不同的技术，则不太合适）。</p></li><li><p>使用操作系统环境变量：简单而有效。这也是 <a href="https://12factor.net/zh_cn/config" target="_blank" rel="noopener">12-factors</a> 推荐的方式。</p></li></ul><blockquote><p>注：使用环境变量方式并不意味着您需要丢弃配置文件并重构应用程序的配置机制，你只需要通过 <a href="https://blog.csdn.net/zh515858237/article/details/79218176" target="_blank" rel="noopener">envsubst</a> 命令来替换配置文件模板中的值就可以了（这个流程一般需要在 <a href="http://docker-entrypoint.sh" target="_blank" rel="noopener">docker-entrypoint.sh</a> 文件中完成，因为这需要在容器进程运行前完成）。例如：在 Nginx 配置中使用环境变量，具体方法可参考<a href="https://docs.docker.com/samples/library/nginx/#using-environment-variables-in-nginx-configuration" target="_blank" rel="noopener">此文</a>。</p></blockquote><p>这种方式可以将应用程序的配置文件封装在容器内部。</p><p><strong>八、外部化数据</strong></p><p>关于数据存储有一条黄金法则：绝对不要将任何持久化数据保存到容器内。</p><p>容器的文件系统本身是被设计成临时和短暂的。因此任何由应用程序生成的内容、数据文件和处理结果都应该保存到挂载的卷或者操作系统绑定挂载点上（既：将宿主机操作系统的目录挂载到容器中）。</p><p>如果将数据保存到绑定挂载点，对于要绑定到容器的宿主机上的目录，你需要注意以下几点：</p><ul><li>在宿主机操作系统上创建非特权用户和组。</li><li>所有需要绑定目录的所有者都是该用户。</li><li>根据使用场景给授权（仅针对这个特定的用户和组，其他用户无权访问）。</li><li>容器也以该用户运行。</li><li>容器可以完全控制这些目录。</li></ul><p><strong>九、确保处理好日志</strong></p><p>如果这是一个新的应用程序，并且希望它能够坚持 Docker 约定，就不应该将日志写入任何文件。应用程序应该使用标准输出和标准错误输出日志，这和之前推荐使用环境变量传递参数一样，这也是 12-factors 之一，具体可以参见<a href="https://12factor.net/zh_cn/logs" target="_blank" rel="noopener">这里</a>。</p><p>Docker 会自动捕捉应用程序的标准输出，并可以通过 <code>docker logs</code> 命令查看。有关于 <code>docker logs</code> 的具体使用你可以参考<a href="https://docs.docker.com/engine/reference/commandline/logs/" target="_blank" rel="noopener">这里</a>。</p><p>但是在一些实际场景下你可能会遇到问题，例如：运行一个简单的 Nginx 容器，至少会有两种不同的日志文件：</p><ul><li>HTTP 访问日志（Access Logs）</li><li>错误日志（Error Logs）</li></ul><p>对于这种按照特定结构输出日志的应用，就不太适合将它们的日志输出到标准输出。这种情况下，你需要按持久化的方式处理这些日志，并确保这些日志文件的能正常的轮转。</p><p><strong>十、轮转日志</strong></p><p>如果应用程序将日志写到文件，或者会无限追加内容到文件，就需要关注这些文件的轮转（rotation），这对于防止服务器空间耗尽非常有用的。</p><p>如果使用绑定挂载，我们可以依靠宿主机的一些工具来实现文件轮转功能。例如：logrotate，关于 logrotate 的使用你可以参考<a href="https://www.aerospike.com/docs/operations/configure/log/logrotate.html" target="_blank" rel="noopener">示例一</a>、<a href="https://www.digitalocean.com/community/tutorials/how-to-manage-logfiles-with-logrotate-on-ubuntu-16-04" target="_blank" rel="noopener">示例二</a>。</p><blockquote><p>注：本文在 「<a href="http://t.cn/ReT0AyJ" target="_blank" rel="noopener">如何 Docker 化任意一个应用</a>」的基础上整理和修改，原文地址：<a href="http://t.cn/ReT0AyJ" target="_blank" rel="noopener">http://t.cn/ReT0AyJ</a> 。</p><p>由于微信不允许外部链接，如果你需要访问文中链接可点击文末左下角的阅读原文。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将讲解如何将应用 Docker 化的一些很实用的技巧和准则，推荐一读。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一、选择基础镜像&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;每种对应技术几乎都有自己的基础镜像，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://hub.docker.com/_/java/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://hub.docker.com/_/java/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hub.docker.com/_/python/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://hub.docker.com/_/python/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hub.docker.com/_/nginx/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://hub.docker.com/_/nginx/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果不能直接使用这些镜像，我们就需要从基础操作系统镜像开始安装所有的依赖。&lt;/p&gt;
&lt;p&gt;网上大多数教程使用的都是以 Ubuntu（例如：Ubuntu:16.04 ）作为基础镜像，这并不是一个问题，但是我建议优先考虑 Alpine 镜像：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://hub.docker.com/_/alpine/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://hub.docker.com/_/alpine/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Alpine 是一个非常小的基础镜像（它的容量大约只有 5MB）。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注：在基于 Alpine 的镜像中你无法使用 apt-get 命令。不过你不必担心，因为 Alpine 系统有自己的软件包仓库和包管理工具 apk。关于 apk 的具体使用你可以详细参考：「&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247484888&amp;amp;idx=1&amp;amp;sn=bfdd55a668b068ab34251512613e5267&amp;amp;chksm=eac524f1ddb2ade7739c10440af8b33a39b81b8a4a57b628cc6c0d0ec3a82f18b914f4e12641&amp;amp;mpshare=1&amp;amp;scene=23&amp;amp;srcid=0731dgw9eDCTtafPzAd3VxDV%23rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Alpine Linux配置使用技巧&lt;/a&gt;」一文。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;二、安装必要软件包&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这个步骤通常比较琐碎，有一些容易忽略的细节：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;apt-get update&lt;/code&gt; 和 &lt;code&gt;apt-get install&lt;/code&gt; 命令应该写在一行（如果使用 Alpine 则对应的是 apk 命令）。这不是一个常见的做法，但是在 Dockerfile 中应该要这么做。否则 &lt;code&gt;apt-get update&lt;/code&gt; 命令产出的临时层可能会被缓存，导致构建时没有更新包信息。（具体可参见&lt;a href=&quot;https://forums.docker.com/t/dockerfile-run-apt-get-install-all-packages-at-once-or-one-by-one/17191&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;此文&lt;/a&gt;）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;确认是否只安装了实际需要的软件（特别是在生产环境中运行这个容器）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;注：我见过有人在他们的镜像中安装了 vim 和其他开发工具。如果这是必要的，应该针对构建、调试和开发环境创建不同的 Dockerfile。这不仅仅关系到镜像大小，还涉及到安全性、可维护性等等。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Docker" scheme="https://www.hi-linux.com/categories/docker/"/>
    
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>浅析从外部访问 Kubernetes 集群中应用的几种方式</title>
    <link href="https://www.hi-linux.com/posts/56619.html"/>
    <id>https://www.hi-linux.com/posts/56619.html</id>
    <published>2018-07-20T01:00:00.000Z</published>
    <updated>2018-08-24T06:08:59.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>一般情况下，Kubernetes 的 Cluster Network 是属于私有网络，只能在 Cluster Network 内部才能访问部署的应用。那么如何才能将 Kubernetes 集群中的应用暴露到外部网络，为外部用户提供服务呢？本文就来讲一讲从外部网络访问 Kubernetes Cluster 中 Pod 和 Serivce 的几种常用的实现方式。</p><h3 id="pod-和-service-的关系">Pod 和 Service 的关系</h3><p>我们首先来了解一下 Kubernetes 中的 Pod 和 Service 的概念以及两者间的关系。</p><p>Pod (容器组)，英文中 Pod 是豆荚的意思。从名字的含义可以看出，Pod 是一组有依赖关系的容器。Pod 是 Kubernetes 集群中最基本的资源对象，每个 Pod 由一个或多个业务容器和一个根容器 (Pause 容器) 组成。</p><p>Kubernetes 为每个 Pod 分配了唯一的 IP（即：Pod IP），Pod 里的多个容器共享这个 IP。Pod 内的容器除了 IP，还共享相同的网络命名空间、端口、存储卷等，也就是说这些容器之间能通过 Localhost 来通信。Pod 包含的容器都会运行在同一个节点上，也可以同时启动多个相同的 Pod 用于 Failover 或者 Load balance。</p><p>Pod 的生命周期是短暂的，Kubernetes 会根据应用的配置对 Pod 进行创建、销毁并根据监控指标进行伸缩扩容。Kubernetes 在创建 Pod 时可以选择集群中的任何一台空闲的节点上进行，因此其网络地址是不固定的。由于 Pod 的这一特点，一般不建议直接通过 Pod 的地址去访问应用。</p><p>为了解决访问 Pod 不方便直接访问的问题，Kubernetes 采用了 Service 对 Pod 进行封装。Service 是对后端提供服务的一组 Pod 的抽象，Service 会绑定到一个固定的虚拟 IP上。该虚拟 IP 只在 Kubernetes Cluster 中可见，但其实该虚拟 IP 并不对应一个虚拟或者物理设备，而只是 IPtables 中的规则，然后再通过 IPtables 将服务请求路由到后端的 Pod 中。通过这种方式，可以确保服务消费者可以稳定地访问 Pod 提供的服务，而不用关心 Pod 的创建、删除、迁移等变化以及如何用一组 Pod 来进行负载均衡。</p><a id="more"></a><p>实现 Service 这一功能的关键是由 Kubernetes 中的 Kube-Proxy 来完成的。Kube-Proxy 运行在每个节点上，监听 API Server 中服务对象的变化，再通过管理 IPtables 来实现网络的转发。Kube-Proxy 目前支持三种模式：UserSpace、IPtables、IPVS。下面我们来说说这几种模式的异同：</p><ul><li>UserSpace</li></ul><p>UserSpace 是让 Kube-Proxy 在用户空间监听一个端口，所有的 Service 都转发到这个端口，然后 Kube-Proxy 在内部应用层对其进行转发。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc1.png" alt=""></p><p>Kube-Proxy 会为每个 Service 随机监听一个端口 (Proxy Port)，并增加一条 IPtables 规则。从客户端到 ClusterIP:Port 的报文都会被重定向到 Proxy Port，Kube-Proxy 收到报文后，通过 Round Robin (轮询) 或者 Session Affinity（会话亲和力，即同一 Client IP 都走同一链路给同一 Pod 服务）分发给对应的 Pod。</p><p>这种方式最大的缺点显然就是 UserSpace 会造成所有报文都走一遍用户态，造成整体性能下降，这种方在 Kubernetes 1.2 以后已经不再使用了。</p><ul><li>IPtables</li></ul><p>IPtables 方式完全由 IPtables 来实现，这种方式直接使用 IPtables 来做用户态入口，而真正提供服务的是内核的 Netilter。Kube-Proxy 只作为 Controller，这也是目前默认的方式。</p><p>Kube-Proxy 的 IPtables 方式也是支持 Round Robin 和 Session Affinity 特性。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc2.png" alt=""></p><p>Kube-Proxy 监听 Kubernetes Master 增加和删除 Service 以及 Endpoint 的消息。对于每一个 Service，Kube Proxy 创建相应的 IPtables 规则，并将发送到 Service Cluster IP 的流量转发到 Service 后端提供服务的 Pod 的相应端口上。</p><blockquote><p>注：虽然可以通过 Service 的 Cluster IP 和服务端口访问到后端 Pod 提供的服务，但该 Cluster IP 是 Ping 不通的。其原因是 Cluster IP 只是 IPtables 中的规则，并不对应到一个任何网络设备。IPVS 模式的 Cluster IP 是可以 Ping 通的。</p></blockquote><ul><li>IPVS</li></ul><p>Kubernetes 从 1.8 开始增加了 IPVS 支持，IPVS 相对 IPtables 效率会更高一些。使用 IPVS 模式需要在运行 Kube-Proxy 的节点上安装 <code>ipvsadm</code>、<code>ipset</code> 工具包和加载 <code>ip_vs</code> 内核模块。</p><p>当 Kube-Proxy 以 IPVS 代理模式启动时，Kube-Proxy 将验证节点上是否安装了 IPVS 模块，如果未安装，则 Kube-Proxy 将回退到 IPtables 代理模式。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc3.png" alt=""></p><p>这种模式，Kube-Proxy 会监视 Kubernetes Service 对象 和 Endpoints，调用 Netlink 接口以相应地创建 IPVS 规则并定期与 Kubernetes Service 对象 和 Endpoints 对象同步 IPVS 规则，以确保 IPVS 状态与期望一致。访问服务时，流量将被重定向到其中一个后端 Pod。</p><p>与 IPtables 类似，IPVS 基于 Netfilter 的 Hook 功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着 IPVS 可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，IPVS 为负载均衡算法提供了更多选项，例如：rr (轮询调度)、lc (最小连接数)、dh (目标哈希)、sh (源哈希)、sed (最短期望延迟)、nq(不排队调度)等。</p><blockquote><p>注：IPVS 是 LVS 项目的一部分，是一款运行在 Linux Kernel 当中的 4 层负载均衡器，性能异常优秀。使用调优后的内核，可以轻松处理每秒 10 万次以上的转发请求。目前在中大型互联网项目中，IPVS 被广泛的用于承接网站入口处的流量。</p></blockquote><p>了解完 Pod 和 Service 的基本概念后，我们就来具体讲一讲从外部网络访问 Kubernetes Cluster 中 Pod 和 Serivce 的几种常见的实现方式。目前主要包括如下几种：</p><ul><li>hostNetwork</li><li>hostPort</li><li>ClusterIP</li><li>NodePort</li><li>LoadBalancer</li><li>Ingress</li></ul><h3 id="通过-pod-暴露">通过 Pod 暴露</h3><h4 id="hostnetwork-true">hostNetwork: true</h4><p>这是一种直接定义 Pod 网络的方式。</p><p>如果在 Pod 中使用 <code>hostNetwork:true</code> 配置的话，在这种 Pod 中运行的应用程序可以直接看到启动 Pod 主机的网络接口。在主机的所有网络接口上都可以访问到该应用程序，以下是使用主机网络的 Pod 的示例定义：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-hostnetwork</span><br><span class="line">spec:</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx-hostnetwork</span><br><span class="line">      image: nginx:1.7.9</span><br></pre></td></tr></table></figure><p>部署该 Pod：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create  -f nginx-hostnetwork.yml</span><br><span class="line">pod &quot;nginx-hostnetwork&quot; created</span><br></pre></td></tr></table></figure><p>访问该 Pod 所在主机的 80 端口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ curl http:&#x2F;&#x2F;$HOST_IP:80</span><br><span class="line">......</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;&#x2F;title&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;&#x2F;p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>将看到 Nginx 默认的欢迎页面，说明可以正常访问。</p><p>注意：每次启动这个 Pod 的时候都可能被调度到不同的节点上，这样所有外部访问 Pod 所在节点主机的 IP 也就是不固定的，而且调度 Pod 的时候还需要考虑是否与宿主机上的端口冲突，因此一般情况下除非您知道需要某个特定应用占用特定宿主机上的特定端口时才使用 <code>hostNetwork: true</code> 的方式。</p><p>这种 Pod 的网络模式有一个用处就是可以将网络插件包装在 Pod 中，然后部署在每个宿主机上，这样该 Pod 就可以控制该宿主机上的所有网络。</p><h4 id="hostport">hostPort</h4><p>这也是一种直接定义 Pod 网络的方式。</p><p><code>hostPort</code> 是直接将容器的端口与所调度的节点上的端口路由，这样用户就可以通过宿主机的IP加上 <code>&lt;hostPort&gt;</code> 来访问 Pod 了，如: <code>&lt;hostIP&gt;:&lt;hostPort&gt;</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-hostport</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx-hostport</span><br><span class="line">      image: nginx:1.7.9</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          hostPort: 8088</span><br></pre></td></tr></table></figure><p>部署该 Pod：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create  -f nginx-hostport.yml</span><br><span class="line">pod &quot;nginx-hostport&quot; created</span><br></pre></td></tr></table></figure><p>访问该 Pod 所在主机的 8088 端口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ curl http:&#x2F;&#x2F;$HOST_IP:8088</span><br><span class="line">......</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;&#x2F;title&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;&#x2F;p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>将看到 Nginx 默认的欢迎页面，说明可以正常访问。</p><p>这种方式和 <code>hostNetwork: true</code> 有同一个缺点，因为 Pod 重新调度的时候该 Pod 被调度到的宿主机可能会变动，这样 <code>&lt;hostIP&gt;</code> 就变化了，用户必须自己维护一个 Pod 与所在宿主机的对应关系。</p><p>这种网络方式可以用来做 <code>Ingress Controller</code>，外部流量都需要通过 Kubenretes 节点宿主机的 80 和 443 端口。</p><h4 id="port-forward">Port Forward</h4><p>这是一种通过 <code>kubectl port-forward</code> 指令来实现数据转发的方法。<code>kubectl port-forward</code> 命令可以为 Pod 设置端口转发，通过在本机指定监听端口，访问这些端口的请求将会被转发到 Pod 的容器中对应的端口上。</p><p>首先，我们来看下 Kubernetes Port Forward 这种方式的工作机制：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc4.png" alt=""></p><p>使用 Kubectl 创建 Port Forward 后，Kubectl 会主动监听指定的本地端口。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl port-forward pod-name local-port:container-port</span><br></pre></td></tr></table></figure><p>当向 Local-Port 建立端口连接并向该端口发送数据时，数据流向会经过以下步骤：</p><ul><li>数据发往 Kubctl 监听的 Local-Port。</li><li>Kubectl 通过 SPDY 协议将数据发送给 ApiServer。</li><li>ApiServer 与目标节点的 Kubelet 建立连接，并通过 SPDY 协议将数据发送到目标 Pod 的端口上。</li><li>目标节点的 Kubelet 收到数据后，通过 PIPE（STDIN、STDOUT）与 Socat 通信。</li><li>Socat 将 STDIN 的数据发送给 Pod 内部指定的容器端口，并将返回的数据写入到 STDOUT。</li><li>STDOUT 的数据由 Kubelet 接收并按照相反的路径发送回去。</li></ul><blockquote><p>注：SPDY 协议将来可能会被替换为 HTTP/2。</p></blockquote><p>接下来，我们用一个实例来演示如何将本地端口转发到 Pod 中的端口，这里以一个运行了 Nginx 的 Pod 为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE</span><br><span class="line">nginx                       1&#x2F;1       Running   2          9d</span><br></pre></td></tr></table></figure><p>验证 Nginx 服务器监听的端口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods nginx --template&#x3D;&#39;&#123;&#123;(index (index .spec.containers 0).ports 0).containerPort&#125;&#125;&#123;&#123;&quot;\n&quot;&#125;&#125;&#39;</span><br><span class="line">80</span><br></pre></td></tr></table></figure><p>将节点上的 8900 端口转发到 Nginx Pod 的 80 端口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 执行 Kubectl port-forward 命令的时候需要指定 Pod 名称和端口转发规则。</span><br><span class="line">$ kubectl port-forward nginx 8900:80</span><br><span class="line">Forwarding from 127.0.0.1:8900 -&gt; 80</span><br><span class="line">Forwarding from [::1]:8900 -&gt; 80</span><br></pre></td></tr></table></figure><blockquote><p>注：需要在所有 Kubernetes 节点上都需要安装 Socat，关于 Socat 更详细介绍可参考：「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247485897&amp;idx=1&amp;sn=555846ff170ac7799dde353271c06f98&amp;chksm=eac528e0ddb2a1f674ca4f9e99325fbccf26589a76b43594558eeb83ef5f65b64f84a116974c&amp;mpshare=1&amp;scene=23&amp;srcid=0719C3Onveftz5JQH2g6NLTO%23rd" target="_blank" rel="noopener">Socat 入门教程</a>」 。</p></blockquote><p>验证转发是否成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ curl http:&#x2F;&#x2F;127.0.0.1:8900</span><br><span class="line">......</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;&#x2F;title&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;&#x2F;p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>由于这种类型的转发端口是绑定在本地的，这种方式也仅适用于调试服务。</p><h3 id="通过-service-暴露">通过 Service 暴露</h3><p>Service 的类型 ( ServiceType ) 决定了 Service 如何对外提供服务。根据类型不同服务可以只在 Kubernetes Cluster 中可见，也可以暴露到 Cluster 外部。Service 目前有三种类型：ClusterIP、NodePort 和 LoadBalancer。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc5.png" alt=""></p><p>Service 中常见的三种端口的含义：</p><ul><li>port</li></ul><p>Service 暴露在 Cluster IP 上的端口，也就是虚拟 IP 要绑定的端口。port 是提供给集群内部客户访问 Service 的入口。</p><ul><li>nodePort</li></ul><p>nodePort 是 Kubernetes 提供给集群外部客户访问 Service 的入口。</p><ul><li>targetPort</li></ul><p>targetPort 是 Pod 里容器的端口，从 port 和 nodePort 上进入的数据最终会经过 Kube-Proxy 流入到后端 Pod 里容器的端口。如果 targetPort 没有被显式声明，那么会默认转发到 Service 接受请求的端口（和 port 端口的值一样）。</p><p>总的来说，port 和 nodePort 都是 Service 的端口，前者暴露给集群内客户访问服务，后者暴露给集群外客户访问服务。从这两个端口到来的数据都需要经过反向代理 Kube-Proxy 流入后端 Pod 里容器的端口，从而到达 Pod 上的容器内。</p><h4 id="clusterip">ClusterIP</h4><p>ClusterIP 是 Service 的缺省类型，这种类型的服务会自动分配一个只能在集群内部可以访问的虚拟 IP，即：ClusterIP。ClusterIP 为你提供一个集群内部其它应用程序可以访问的服务，外部无法访问。ClusterIP 服务的 YAML 类似这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:  </span><br><span class="line">  name: my-nginx</span><br><span class="line">selector:    </span><br><span class="line">  app: my-nginx</span><br><span class="line">spec:</span><br><span class="line">  type: ClusterIP</span><br><span class="line">  ports:  </span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br></pre></td></tr></table></figure><p>虽然我们从集群外部不能直接访问一个 ClusterIP 服务，但是你可以使用 Kubernetes Proxy API 来访问它。</p><p>Kubernetes Proxy API 是一种特殊的 API，Kube-APIServer 只是代理这类 API 的 HTTP 请求，然后将请求转发到某个节点上的 Kubelet 进程监听的端口上。最后实际是由该端口上的 REST API 响应请求。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc6.png" alt=""></p><p>在 Master 节点上创建 Kubernetes API 的代理服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl proxy --port&#x3D;8080</span><br></pre></td></tr></table></figure><p><code>kubectl proxy</code> 默认是监听在 127.0.0.1 上的，如果你需要对外提供访问，可使用一些基本的安全机制。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl proxy --port&#x3D;8080 --address&#x3D;192.168.100.211 --accept-hosts&#x3D;&#39;^192\.168\.100\.*&#39;</span><br></pre></td></tr></table></figure><p>如果需要更多的命令使用帮助，可以使用 <code>kubectl help proxy</code>。</p><p>现在，你可以使用 Kubernetes Proxy API 进行访问。比如：需要访问一个服务，可以使用 <code>/api/v1/namespaces/&lt;NAMESPACE&gt;/services/&lt;SERVICE-NAME&gt;/proxy/</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 适用于 Kubernetes 1.10</span><br><span class="line"></span><br><span class="line">$ kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)       AGE</span><br><span class="line">kubernetes   ClusterIP   10.254.0.1       &lt;none&gt;        443&#x2F;TCP       10d</span><br><span class="line">my-nginx     ClusterIP   10.254.154.119   &lt;none&gt;        80&#x2F;TCP        8d</span><br><span class="line"></span><br><span class="line">$ curl http:&#x2F;&#x2F;192.168.100.211:8080&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&#x2F;services&#x2F;my-nginx&#x2F;proxy&#x2F;</span><br><span class="line">......</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;&#x2F;title&gt;</span><br><span class="line">&lt;&#x2F;head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;&#x2F;p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>如果你需要直接访问一个 Pod，可以使用 <code>/api/v1/namespaces/&lt;NAMESPACE&gt;/pods/&lt;POD-NAME&gt;/proxy/</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 适用于 Kubernetes 1.10</span><br><span class="line"></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE</span><br><span class="line">my-nginx-86555897f9-5p9c2   1&#x2F;1       Running   2          8d</span><br><span class="line">my-nginx-86555897f9-ws674   1&#x2F;1       Running   6          8d</span><br><span class="line"></span><br><span class="line">$ curl http:&#x2F;&#x2F;192.168.100.211:8080&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&#x2F;pods&#x2F;my-nginx-86555897f9-ws674&#x2F;proxy&#x2F;</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;&#x2F;title&gt;</span><br><span class="line"></span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;&#x2F;h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;&#x2F;p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>由于访问 Kubernetes Proxy API 要求使用已授权用户运行 kubectl ，因此不应该使用此方法将你的服务公开到公网上或将其用于生产，这种方法主要还是用于调试服务。</p><h4 id="nodeport">NodePort</h4><p>NodePort 在 Kubenretes 里是一个广泛应用的服务暴露方式。基于 ClusterIP 提供的功能，为 Service 在 Kubernetes 集群的每个节点上绑定一个端口，即 NodePort。集群外部可基于任何一个 <code>NodeIP:NodePort</code> 的形式来访问 Service。Service 在每个节点的 NodePort 端口上都是可用的。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc7.png" alt=""></p><p>NodePort 服务与默认的 ClusterIP 服务在 YAML 定义上有两点区别：首先，type 是 NodePort。其次还有一个称为 nodePort 的参数用来指定在节点上打开哪个端口。 如果你不指定这个端口，它会选择一个随机端口。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: my-nginx</span><br><span class="line">  labels:</span><br><span class="line">    name: my-nginx</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: my-nginx</span><br><span class="line">      image: nginx:1.7.9</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure><p><code>nodePort</code> 值的默认范围是 30000-32767，这个值是可以在 API Server 的配置文件中用 <code>--service-node-port-range</code> 来自定义。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: my-nginx</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">      targetPort: 80</span><br><span class="line">      nodePort: 31000</span><br><span class="line">  selector:</span><br><span class="line">    name: my-nginx</span><br></pre></td></tr></table></figure><p>NodePort 类型的服务会在所有的 Kubenretes 节点（运行有 Kube-Proxy 的节点）上统一暴露出一个端口对外提供服务，这样集群外就可以使用 Kubernetes 任意一个节点的 IP 加上指定端口（这里定义的是：31000）访问该服务了。Kube-Proxy 会自动将流量以 Round-Robin 的方式转发给该 Service 的每一个 Pod。</p><p>NodePort 类型的服务并不影响原来虚拟 IP 的访问方式，内部节点依然可以通过 <code>VIP:Port</code> 的方式进行访问。NodePort 这种服务暴露方式也存在一些不足：</p><ul><li>节点上的每个端口只能有一个服务。</li><li>如果节点 IP 地址发生更改，则需要相应机制处理该问题。</li></ul><p>基于以上原因，NodePort 比较适用的场景为演示程序或临时应用，不建议在生产环境中使用这种方法对外暴露服务。</p><h4 id="loadbalancer">LoadBalancer</h4><p>LoadBalancer 是基于 NodePort 和云服务供应商提供的外部负载均衡器，通过这个外部负载均衡器将外部请求转发到各个 <code>NodeIP:NodePort</code> 以实现对外暴露服务。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc8.png" alt=""></p><p><code>LoadBalancer</code> 只能在 Service 上定义。LoadBalancer 是一些特定公有云提供的负载均衡器，需要特定的云服务商支持。比如：AWS、Azure、OpenStack 和 GCE (Google Container Engine) 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: my-nginx</span><br><span class="line">spec:</span><br><span class="line">  type: LoadBalancer</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">  selector:</span><br><span class="line">    name: my-nginx</span><br></pre></td></tr></table></figure><p>查看服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc my-nginx</span><br><span class="line">NAME       CLUSTER-IP     EXTERNAL-IP     PORT(S)          AGE</span><br><span class="line">my-nginx   10.97.121.42   10.13.242.236   8086:30051&#x2F;TCP   39s</span><br></pre></td></tr></table></figure><p>集群内部可以使用 ClusterIP 加端口来访问服务，如：10.97.121.42:8086。</p><p>外部可以用以下两种方式访问该服务：</p><ul><li>使用任一节点的 IP 加 30051 端口访问该服务。</li><li>使用 <code>EXTERNAL-IP</code> 来访问，这是一个 VIP，是云供应商提供的负载均衡器 IP，如：10.13.242.236:8086。</li></ul><p>LoadBalancer 这种方式最大的不足就是每个暴露的服务需要使用一个公有云提供的负载均衡器 IP，这可能会付出比较大的成本代价。</p><p>从上面几种 Service 的类型的结论来看，目前 Service 提供的负载均衡功能在使用上有以下限制：</p><ul><li>只提供 4 层负载均衡，不支持 7 层负载均衡功能，比如：不能按需要的匹配规则自定义转发请求。</li><li>使用 NodePort 类型的 Service，需要在集群外部部署一个外部的负载均衡器。</li><li>使用 LoadBalancer 类型的 Service，Kubernetes 必须运行在特定的云服务上。</li></ul><h3 id="通过-ingress-暴露">通过 Ingress 暴露</h3><p>与 Service 不同，Ingress 实际上不是一种服务。相反，它位于多个服务之前，充当集群中的智能路由器或入口点。</p><p><code>Ingress</code> 是自 Kubernetes 1.1 版本后引入的资源类型。Ingress 支持将 Service 暴露到 Kubernetes 集群外，同时可以自定义 Service 的访问策略。Ingress 能够把 Service 配置成外网能够访问的 URL，也支持提供按域名访问的虚拟主机功能。例如，通过负载均衡器实现不同的二级域名到不同 Service 的访问。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc9.png" alt=""></p><p>实际上 <code>Ingress</code> 只是一个统称，其由 <code>Ingress</code> 和 <code>Ingress Controller</code> 两部分组成。<code>Ingress</code> 用作将原来需要手动配置的规则抽象成一个 Ingress 对象，使用 YAML 格式的文件来创建和管理。<code>Ingress Controller</code> 用作通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化。</p><p>使用 <code>Ingress</code> 前必须要先部署 <code>Ingress Controller</code>，<code>Ingress Controller</code> 是以一种插件的形式提供。<code>Ingress Controller</code> 通常是部署在 Kubernetes 之上的 Docker 容器，<code>Ingress Controller</code> 的 Docker 镜像里包含一个像 Nginx 或 HAProxy 的负载均衡器和一个 <code>Ingress Controller</code>。<code>Ingress Controller</code> 会从 Kubernetes 接收所需的 <code>Ingress</code> 配置，然后动态生成一个 Nginx 或 HAProxy 配置文件，并重新启动负载均衡器进程以使更改生效。换句话说，<code>Ingress Controller</code> 是由 Kubernetes 管理的负载均衡器。</p><blockquote><p>注：无论使用何种负载均衡软件（ 比如：Nginx、HAProxy、Traefik等）来实现 Ingress Controller，官方都将其统称为 Ingress Controller。</p></blockquote><p>Kubernetes Ingress 提供了负载均衡器的典型特性：HTTP 路由、粘性会话、SSL 终止、SSL直通、TCP 和 UDP 负载平衡等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress</span><br><span class="line">spec:</span><br><span class="line">  backend:</span><br><span class="line">    serviceName: my-nginx-other</span><br><span class="line">    servicePort: 8080</span><br><span class="line">  rules:</span><br><span class="line">  - host: foo.mydomain.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: my-nginx-foo</span><br><span class="line">          servicePort: 8080</span><br><span class="line">  - host: mydomain.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: &#x2F;bar&#x2F;*</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: my-nginx-bar</span><br><span class="line">          servicePort: 8080</span><br></pre></td></tr></table></figure><p>外部可通过 <code>foo.yourdomain.com</code> 或者 <code>mydomain.com/bar/</code> 两个不同 URL 来访问对应的后端服务，然后 <code>Ingress Controller</code> 直接将流量转发给后端 Pod，不需再经过 Kube-Proxy 的转发，这种方式比 LoadBalancer 更高效。</p><p>总的来说 Ingress 是一个非常灵活和越来越得到厂商支持的服务暴露方式，包括：Nginx、HAProxy、Traefik、还有各种 Service Mesh，而其它服务暴露方式更适用于服务调试、特殊应用的部署。</p><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RdskHHt" target="_blank" rel="noopener">http://t.cn/RdskHHt</a><br><a href="http://t.cn/RdskYv5" target="_blank" rel="noopener">http://t.cn/RdskYv5</a><br><a href="http://t.cn/Rg5BG4h" target="_blank" rel="noopener">http://t.cn/Rg5BG4h</a><br><a href="http://t.cn/R6CD4ak" target="_blank" rel="noopener">http://t.cn/R6CD4ak</a><br><a href="http://t.cn/Rgcurto" target="_blank" rel="noopener">http://t.cn/Rgcurto</a><br><a href="http://t.cn/RgcdjDw" target="_blank" rel="noopener">http://t.cn/RgcdjDw</a><br><a href="http://t.cn/RgMWBpo" target="_blank" rel="noopener">http://t.cn/RgMWBpo</a><br><a href="http://t.cn/RgMWFM1" target="_blank" rel="noopener">http://t.cn/RgMWFM1</a><br><a href="http://t.cn/RgC3uk8" target="_blank" rel="noopener">http://t.cn/RgC3uk8</a></p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般情况下，Kubernetes 的 Cluster Network 是属于私有网络，只能在 Cluster Network 内部才能访问部署的应用。那么如何才能将 Kubernetes 集群中的应用暴露到外部网络，为外部用户提供服务呢？本文就来讲一讲从外部网络访问 Kubernetes Cluster 中 Pod 和 Serivce 的几种常用的实现方式。&lt;/p&gt;
&lt;h3 id=&quot;Pod-和-Service-的关系&quot;&gt;Pod 和 Service 的关系&lt;/h3&gt;
&lt;p&gt;我们首先来了解一下 Kubernetes 中的 Pod 和 Service 的概念以及两者间的关系。&lt;/p&gt;
&lt;p&gt;Pod (容器组)，英文中 Pod 是豆荚的意思。从名字的含义可以看出，Pod 是一组有依赖关系的容器。Pod 是 Kubernetes 集群中最基本的资源对象，每个 Pod 由一个或多个业务容器和一个根容器 (Pause 容器) 组成。&lt;/p&gt;
&lt;p&gt;Kubernetes 为每个 Pod 分配了唯一的 IP（即：Pod IP），Pod 里的多个容器共享这个 IP。Pod 内的容器除了 IP，还共享相同的网络命名空间、端口、存储卷等，也就是说这些容器之间能通过 Localhost 来通信。Pod 包含的容器都会运行在同一个节点上，也可以同时启动多个相同的 Pod 用于 Failover 或者 Load balance。&lt;/p&gt;
&lt;p&gt;Pod 的生命周期是短暂的，Kubernetes 会根据应用的配置对 Pod 进行创建、销毁并根据监控指标进行伸缩扩容。Kubernetes 在创建 Pod 时可以选择集群中的任何一台空闲的节点上进行，因此其网络地址是不固定的。由于 Pod 的这一特点，一般不建议直接通过 Pod 的地址去访问应用。&lt;/p&gt;
&lt;p&gt;为了解决访问 Pod 不方便直接访问的问题，Kubernetes 采用了 Service 对 Pod 进行封装。Service 是对后端提供服务的一组 Pod 的抽象，Service 会绑定到一个固定的虚拟 IP上。该虚拟 IP 只在 Kubernetes Cluster 中可见，但其实该虚拟 IP 并不对应一个虚拟或者物理设备，而只是 IPtables 中的规则，然后再通过 IPtables 将服务请求路由到后端的 Pod 中。通过这种方式，可以确保服务消费者可以稳定地访问 Pod 提供的服务，而不用关心 Pod 的创建、删除、迁移等变化以及如何用一组 Pod 来进行负载均衡。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>图解 Kubernetes 架构</title>
    <link href="https://www.hi-linux.com/posts/48037.html"/>
    <id>https://www.hi-linux.com/posts/48037.html</id>
    <published>2018-07-10T01:00:00.000Z</published>
    <updated>2018-08-24T06:04:36.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h3 id="kubernetes-整体架构图">Kubernetes 整体架构图</h3><p><img src="https://www.hi-linux.com/img/linux/k8s-arch1.png" alt=""></p><a id="more"></a><h3 id="kubernetes-各组件介绍">Kubernetes 各组件介绍</h3><h4 id="kube-master控制节点">Kube-Master「控制节点」</h4><ul><li>Kube-Master 的工作流程图</li></ul><p><img src="https://www.hi-linux.com/img/linux/k8s-arch2.png" alt=""></p><ol><li>Kubecfg 将特定的请求发送给 Kubernetes Client（比如：创建 Pod 的请求）。</li><li>Kubernetes Client 将请求发送给 API Server。</li><li>API Server 会根据请求的类型选择用何种 REST API 对请求作出处理（比如：创建 Pod 时 Storage 类型是 Pods 时，其对应的就是 REST Storage API）。</li><li>REST Storage API 会对请求作相应的处理并将处理的结果存入高可用键值存储系统 Etcd 中。</li><li>在 API Server 响应 Kubecfg 的请求后，Scheduler 会根据 Kubernetes Client 获取的集群中运行 Pod 及 Minion / Node 信息将未分发的 Pod 分发到可用的 Minion / Node 节点上。</li></ol><h5 id="api-server-资源操作入口">API Server 「资源操作入口」</h5><ul><li><p>API Server 提供了资源对象的唯一操作入口，其它所有组件都必须通过它提供的 API 来操作资源数据。只有 API Server 会与存储通信，其它模块都必须通过 API Server 访问集群状态。</p></li><li><p>API Server 作为 Kubernetes 系统的入口，封装了核心对象的增删改查操作。API Server 以 RESTFul 接口方式提供给外部客户和内部组件调用，API Server 再对相关的资源数据（全量查询 + 变化监听）进行操作，以达到实时完成相关的业务功能。</p></li><li><p>以 API Server 为 Kubernetes 入口的设计主要有以下好处：1. 保证了集群状态访问的安全。2. API Server 隔离了集群状态访问和后端存储实现，这样 API Server 状态访问的方式不会因为后端存储技术 Etcd 的改变而改变，让后端存储方式选择更加灵活，方便了整个架构的扩展。</p></li></ul><h5 id="controller-manager-内部管理控制中心">Controller Manager 「内部管理控制中心」</h5><p>Controller Manager 用于实现 Kubernetes 集群故障检测和恢复的自动化工作。Controller Manager 主要负责执行以下各种控制器：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-arch3.png" alt=""></p><ul><li>Replication Controller</li></ul><p>Replication Controller 的作用主要是定期关联 Replication Controller (RC) 和 Pod，以保证集群中一个 RC (一种资源对象) 所关联的 Pod 副本数始终保持为与预设值一致。</p><ul><li>Node Controller</li></ul><p>Kubelet 在启动时会通过 API Server 注册自身的节点信息，并定时向 API Server 汇报状态信息。API Server 在接收到信息后将信息更新到 Etcd 中。</p><p>Node Controller 通过 API Server 实时获取 Node 的相关信息，实现管理和监控集群中的各个 Node 节点的相关控制功能。</p><ul><li>ResourceQuota Controller</li></ul><p>资源配额管理控制器用于确保指定的资源对象在任何时候都不会超量占用系统上物理资源。</p><ul><li>Namespace Controller</li></ul><p>用户通过 API Server 可以创建新的 Namespace 并保存在 Etcd 中，Namespace Controller 定时通过 API Server 读取这些 Namespace 信息来操作 Namespace。</p><p>比如：Namespace 被 API 标记为优雅删除，则将该 Namespace 状态设置为 Terminating 并保存到 Etcd 中。同时 Namespace Controller 删除该 Namespace 下的 ServiceAccount、RC、Pod 等资源对象。</p><ul><li>Service Account Controller</li></ul><p>Service Account Controller (服务账号控制器)，主要在命名空间内管理 ServiceAccount，以保证名为 default 的 ServiceAccount 在每个命名空间中存在。</p><ul><li>Token Controller</li></ul><p>Token Controller（令牌控制器）作为 Controller Manager 的一部分，主要用作：监听 serviceAccount 的创建和删除动作以及监听 secret 的添加、删除动作。</p><ul><li>Service Controller</li></ul><p>Service Controller 是属于 Kubernetes 集群与外部平台之间的一个接口控制器，Service Controller 主要用作监听 Service 的变化。</p><p>比如：创建的是一个 LoadBalancer 类型的 Service，Service Controller 则要确保外部的云平台上对该 Service 对应的 LoadBalancer 实例被创建、删除以及相应的路由转发表被更新。</p><ul><li>Endpoint Controller</li></ul><p>Endpoints 表示了一个 Service 对应的所有 Pod 副本的访问地址，而 Endpoints Controller 是负责生成和维护所有 Endpoints 对象的控制器。</p><p>Endpoint Controller 负责监听 Service 和对应的 Pod 副本的变化。定期关联 Service 和 Pod (关联信息由 Endpoint 对象维护)，以保证 Service 到 Pod 的映射总是最新的。</p><h5 id="scheduler集群分发调度器">Scheduler「集群分发调度器」</h5><ul><li>Scheduler 主要用于收集和分析当前 Kubernetes 集群中所有 Minion / Node 节点的资源 (包括内存、CPU 等) 负载情况，然后依据资源占用情况分发新建的 Pod 到 Kubernetes 集群中可用的节点。</li><li>Scheduler 会实时监测 Kubernetes 集群中未分发和已分发的所有运行的 Pod。</li><li>Scheduler 会实时监测 Minion / Node 节点信息，由于会频繁查找 Minion/Node 节点，Scheduler 同时会缓存一份最新的信息在本地。</li><li>Scheduler 在分发 Pod 到指定的 Minion / Node 节点后，会把 Pod 相关的信息 Binding 写回 API Server，以方便其它组件使用。</li></ul><h4 id="kube-node服务节点">Kube-Node「服务节点」</h4><ul><li>Kubelet 结构图</li></ul><p><img src="https://www.hi-linux.com/img/linux/k8s-arch4.png" alt=""></p><h5 id="kubelet-节点上的-pod-管家">Kubelet 「节点上的 Pod 管家」</h5><ul><li>负责 Node 节点上 Pod 的创建、修改、监控、删除等全生命周期的管理。</li><li>定时上报本地 Node 的状态信息给 API Server。</li><li>Kubelet 是 Master API Server 和 Minion / Node 之间的桥梁，接收 Master API Server 分配给它的 Commands 和 Work。</li><li>Kubelet 通过 Kube ApiServer 间接与 Etcd 集群交互来读取集群配置信息。</li><li>Kubelet 在 Node 上做的主要工作具体如下：1. 设置容器的环境变量、给容器绑定 Volume、给容器绑定 Port、根据指定的 Pod 运行一个单一容器、给指定的 Pod 创建 Network 容器。2. 同步 Pod 的状态，从 cAdvisor 获取 Container Info、 Pod Info、 Root Info、 Machine info。3. 在容器中运行命令、杀死容器、删除 Pod 的所有容器。</li></ul><h5 id="proxy负载均衡-路由转发">Proxy「负载均衡、路由转发」</h5><ul><li><p>Proxy 是为了解决外部网络能够访问集群中容器提供的应用服务而设计的，Proxy 运行在每个 Minion / Node 上。</p></li><li><p>Proxy 提供 TCP / UDP 两种 Sockets 连接方式 。每创建一个 Service，Proxy 就会从 Etcd 获取 Services 和 Endpoints 的配置信息（也可以从 File 获取），然后根据其配置信息在 Minion / Node 上启动一个 Proxy 的进程并监听相应的服务端口。当外部请求发生时，Proxy 会根据 Load Balancer 将请求分发到后端正确的容器处理。</p></li><li><p>Proxy 不但解决了同一宿主机相同服务端口冲突的问题，还提供了 Service 转发服务端口对外提供服务的能力。Proxy 后端使用随机、轮循等负载均衡算法进行调度。</p></li></ul><h5 id="kubectl-集群管理命令行工具集">Kubectl 「集群管理命令行工具集」</h5><ul><li>Kubectl 是 Kubernetes 的 客户端的工具。 通过 Kubectl 命令对 API Server 进行操作，API Server 响应并返回对应的命令结果，从而达到对 Kubernetes 集群的管理。</li></ul><blockquote><p>本文在 「<a href="http://www.huweihuang.com/article/kubernetes/kubernetes-architecture/" target="_blank" rel="noopener">Kubernetes 总架构图</a>」的基础上整理和修改。</p></blockquote><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RdHiHXX" target="_blank" rel="noopener">http://t.cn/RdHiHXX</a><br><a href="http://t.cn/RdHimG5" target="_blank" rel="noopener">http://t.cn/RdHimG5</a><br><a href="http://t.cn/RdHRDq8" target="_blank" rel="noopener">http://t.cn/RdHRDq8</a></p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Kubernetes-整体架构图&quot;&gt;Kubernetes 整体架构图&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/k8s-arch1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
</feed>
