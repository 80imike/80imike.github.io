<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>奇妙的 Linux 世界</title>
  
  <subtitle>种一棵树最好的时间是十年前，其次是现在。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.hi-linux.com/"/>
  <updated>2022-10-26T01:58:08.007Z</updated>
  <id>https://www.hi-linux.com/</id>
  
  <author>
    <name>Mike</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>如何使用 Headscale ( Tailscale 开源版 ) 快速搭建一个私有专属的 P2P 内网穿透网络</title>
    <link href="https://www.hi-linux.com/posts/33684.html"/>
    <id>https://www.hi-linux.com/posts/33684.html</id>
    <published>2022-10-26T01:00:00.000Z</published>
    <updated>2022-10-26T01:58:08.007Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="一-内网穿透简述">一、内网穿透简述</span></h2><p>由于国内网络环境问题, 普遍家庭用户宽带都没有分配到公网 IP(我有固定公网 IP, 嘿嘿); 这时候一般我们需要从外部访问家庭网络时就需要通过一些魔法手段, 比如 VPN、远程软件(向日葵…)等; 但是这些工具都有一个普遍存在的问题: 慢+卡!</p><h3><span id="11-传统星型拓扑">1.1、传统星型拓扑</span></h3><p>究其根本因素在于, 在传统架构中如果两个位于多层 NAT(简单理解为多个路由器)之后的设备, 只能通过一些中央(VPN/远程软件)中转服务器进行链接, 这时网络连接速度取决于中央服务器带宽和速度; 这种网络架构我这里简称为: 星型拓扑</p><p><img src="https://img.hi-linux.com/staticfile/MKn6bf-20221025104144038-2022-10-25-vuD5sk.png" alt></p><p>从这张图上可以看出, <strong>你的 “工作笔记本” 和 “家庭 NAS” 之间通讯的最大传输速度为 <code>Up/Down: 512K/s</code></strong>; 因为流量经过了中央服务器中转, 由于网络木桶效应存在, 即使你两侧的网络速度再高也没用, 整体的速度取决于这个链路中最低的一个设备网速而不是你两端的设备.</p><p><strong>在这种拓扑下, 想提高速度只有一个办法: 加钱!</strong> 在不使用 “钞能力” 的情况下, 普遍免费的软件提供商不可能给予过多的资源来让用户白嫖, 而自己弄大带宽的中央服务器成本又过高.</p><a id="more"></a><h3><span id="12-nat-穿透与网状拓扑">1.2、NAT 穿透与网状拓扑</span></h3><blockquote><p>本部分只做简述, 具体里面有大量细节和规则可能描述不准确, 细节部分推荐阅读 <a href="https://tailscale.com/blog/how-nat-traversal-works/" target="_blank" rel="noopener">How NAT traversal works</a>.</p></blockquote><p>既然传统的星型拓扑有这么多问题, 那么有没有其他骚操作可以解决呢? 答案是有的, 简单来说就是利用 NAT 穿透原理. NAT 穿透简单理解如下: <strong>在 A 设备主动向 B 设备发送流量后, 整个链路上的防火墙会短时间打开一个映射规则, 该规则允许 B 设备短暂的从这个路径上反向向 A 设备发送流量.</strong> 更通俗的讲大概就是所谓的: <strong>“顺着网线来打你”</strong></p><p><img src="https://img.hi-linux.com/staticfile/by4z9m-2022-10-25-VhDRc6.png" alt></p><p>搞清了这个规则以后, 我们就可以<strong>弄一台 “低配” 的中央服务器</strong>, 让中央服务器来<strong>帮助我们协商</strong>两边的设备谁先访问谁(或者说是访问规则); 两个设备一起无脑访问对方, 然后触发防火墙的 NAT 穿透规则(防火墙打开), 此后两个设备就可以不通过中央服务器源源不断的通讯了. 在这种架构下我们的设备其实就组成了一个非标准的网状拓扑:</p><p><img src="https://img.hi-linux.com/staticfile/MrF6yn-2022-10-25-sXBQjt.png" alt></p><p>在这种拓扑下, 两个设备之间的通讯速度已经不在取决于中央服务器, 而是直接取决于两端设备的带宽, 也就是说达到了设备网络带宽峰值. <strong>当然 NAT 穿透也不是百分百能够成功的, 在复杂网络情况下有些防火墙不会按照预期工作或者说有更严格的限制;</strong> 比如 IP、端口、协议限制等等, 所以为了保证可靠性可以让中央服务器中转做后备方案, 即尽量尝试 NAT 穿透, 如果不行走中央服务器中继.</p><h2><span id="二-tailscale-简介">二、Tailscale 简介</span></h2><blockquote><p>第一部分是为了方便读者理解一些新型内网穿透的大致基本原理, 现在回到本文重点: Tailscale</p></blockquote><p>Tailscale 就是一种利用 NAT 穿透(aka: P2P 穿透)技术的 VPN 工具. Tailscale 客户端等是开源的, 不过遗憾的是中央控制服务器目前并不开源; Tailscale 目前也提供免费的额度给用户使用, 在 NAT 穿透成功的情况下也能保证满速运行.</p><p>不过一旦无法 NAT 穿透需要做中转时, Tailscale 官方的服务器由于众所周知的原因在国内访问速度很拉胯; 不过万幸的是开源社区大佬们搓了一个开源版本的中央控制服务器(Headscale), 也就是说: <strong>我们可以自己搭建中央服务器啦, 完全 “自主可控” 啦.</strong></p><h2><span id="三-搭建-headscale-服务端">三、搭建 Headscale 服务端</span></h2><blockquote><p>以下命令假设安装系统为 Ubuntu 22.04, 其他系统请自行调整.</p></blockquote><h3><span id="31-宿主机安装">3.1、宿主机安装</span></h3><p>Headscale 是采用 Go 语言编写的, 所以只有一个二进制文件, 在 <a href="https://github.com/juanfont/headscale/releases" target="_blank" rel="noopener">Github Releases</a> 页面下载最新版本即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 下载</span><br><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;juanfont&#x2F;headscale&#x2F;releases&#x2F;download&#x2F;v0.16.4&#x2F;headscale_0.16.4_linux_amd64 -O &#x2F;usr&#x2F;local&#x2F;bin&#x2F;headscale</span><br><span class="line"></span><br><span class="line"># 增加可执行权限</span><br><span class="line">chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;headscale</span><br></pre></td></tr></table></figure><p>下载完成后为了安全性我们需要创建单独的用户和目录用于 Headscale 运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 配置目录</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;headscale</span><br><span class="line"></span><br><span class="line"># 创建用户</span><br><span class="line">useradd \</span><br><span class="line">--create-home \</span><br><span class="line">--home-dir &#x2F;var&#x2F;lib&#x2F;headscale&#x2F; \</span><br><span class="line">--system \</span><br><span class="line">--user-group \</span><br><span class="line">--shell &#x2F;usr&#x2F;sbin&#x2F;nologin \</span><br><span class="line">headscale</span><br></pre></td></tr></table></figure><p>为了保证 Headscale 能持久运行, 我们需要创建 SystemD 配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;headscale.service</span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;headscale controller</span><br><span class="line">After&#x3D;syslog.target</span><br><span class="line">After&#x3D;network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">User&#x3D;headscale</span><br><span class="line">Group&#x3D;headscale</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;headscale serve</span><br><span class="line">Restart&#x3D;always</span><br><span class="line">RestartSec&#x3D;5</span><br><span class="line"></span><br><span class="line"># Optional security enhancements</span><br><span class="line">NoNewPrivileges&#x3D;yes</span><br><span class="line">PrivateTmp&#x3D;yes</span><br><span class="line">ProtectSystem&#x3D;strict</span><br><span class="line">ProtectHome&#x3D;yes</span><br><span class="line">ReadWritePaths&#x3D;&#x2F;var&#x2F;lib&#x2F;headscale &#x2F;var&#x2F;run&#x2F;headscale</span><br><span class="line">AmbientCapabilities&#x3D;CAP_NET_BIND_SERVICE</span><br><span class="line">RuntimeDirectory&#x3D;headscale</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure><h3><span id="32-配置-headscale">3.2、配置 Headscale</span></h3><p>安装完成以后我们需要在 <code>/etc/headscale/config.yaml</code> 中配置 Headscale 的启动配置, 以下为配置样例以及解释(仅列出重要配置):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"># Headscale 服务器的访问地址</span><br><span class="line"># </span><br><span class="line"># 这个地址是告诉客户端需要访问的地址, 即使你需要在跑在</span><br><span class="line"># 负载均衡器之后这个地址也必须写成负载均衡器的访问地址</span><br><span class="line">server_url: https:&#x2F;&#x2F;your.domain.com</span><br><span class="line"></span><br><span class="line"># Headscale 实际监听的地址</span><br><span class="line">listen_addr: 0.0.0.0:8080</span><br><span class="line"></span><br><span class="line"># 监控地址</span><br><span class="line">metrics_listen_addr: 127.0.0.1:9090</span><br><span class="line"></span><br><span class="line"># grpc 监听地址</span><br><span class="line">grpc_listen_addr: 0.0.0.0:50443</span><br><span class="line"></span><br><span class="line"># 是否允许不安全的 grpc 连接(非 TLS)</span><br><span class="line">grpc_allow_insecure: false</span><br><span class="line"></span><br><span class="line"># 客户端分配的内网网段</span><br><span class="line">ip_prefixes:</span><br><span class="line">  - fd7a:115c:a1e0::&#x2F;48</span><br><span class="line">  - 100.64.0.0&#x2F;10</span><br><span class="line"></span><br><span class="line"># 中继服务器相关配置</span><br><span class="line">derp:</span><br><span class="line">  server:</span><br><span class="line">    # 关闭内嵌的 derper 中继服务(可能不安全, 还没去看代码)</span><br><span class="line">    enabled: false</span><br><span class="line"></span><br><span class="line">  # 下发给客户端的中继服务器列表(默认走官方的中继节点)</span><br><span class="line">  urls:</span><br><span class="line">    - https:&#x2F;&#x2F;controlplane.tailscale.com&#x2F;derpmap&#x2F;default</span><br><span class="line"></span><br><span class="line">  # 可以在本地通过 yaml 配置定义自己的中继接待你</span><br><span class="line">  paths: []</span><br><span class="line"></span><br><span class="line"># SQLite config</span><br><span class="line">db_type: sqlite3</span><br><span class="line">db_path: &#x2F;var&#x2F;lib&#x2F;headscale&#x2F;db.sqlite</span><br><span class="line"></span><br><span class="line"># 使用自动签发证书是的域名</span><br><span class="line">tls_letsencrypt_hostname: &quot;&quot;</span><br><span class="line"></span><br><span class="line"># 使用自定义证书时的证书路径</span><br><span class="line">tls_cert_path: &quot;&quot;</span><br><span class="line">tls_key_path: &quot;&quot;</span><br><span class="line"></span><br><span class="line"># 是否让客户端使用随机端口, 默认使用 41641&#x2F;UDP</span><br><span class="line">randomize_client_port: false</span><br></pre></td></tr></table></figure><h3><span id="33-证书及反向代理">3.3、证书及反向代理</span></h3><p>可能很多人和我一样, 希望使用 ACME 自动证书, 又不想占用 80/443 端口, 又想通过负载均衡器负载, 配置又看的一头雾水; 所以这里详细说明一下 Headscale 证书相关配置和工作逻辑:</p><ul><li>1、Headscale 的 ACME 只支持 HTTP/TLS 挑战, 所以使用后必定占用 80/443</li><li>2、当配置了 <code>tls_letsencrypt_hostname</code> 时一定会进行 ACME 申请</li><li>3、在不配置 <code>tls_letsencrypt_hostname</code> 时如果配置了 <code>tls_cert_path</code> 则使用自定义证书</li><li>4、两者都不配置则不使用任何证书, 服务端监听 HTTP 请求</li><li>5、三种情况下(ACME 证书、自定义证书、无证书)主服务都只监听 <code>listen_addr</code> 地址, 与 <code>server_url</code> 没半毛钱关系</li><li>6、只有在有证书(ACME 证书或自定义证书)的情况下或者手动开启了 <code>grpc_allow_insecure</code> 才会监听 grpc 远程调用服务</li></ul><p>综上所述, 如果你想通过 Nginx、Caddy 反向代理 Headscale, 则你需要满足以下配置:</p><ul><li>1、删除掉 <code>tls_letsencrypt_hostname</code> 或留空, 防止 ACME 启动</li><li>2、删除掉 <code>tls_cert_path</code> 或留空, 防止加载自定义证书</li><li>3、<code>server_url</code> 填写 Nginx 或 Caddy 被访问的 HTTPS 地址</li><li>4、在你的 Nginx 或 Caddy 中反向代理填写 <code>listen_addr</code> 的 HTTP 地址</li></ul><p>Nginx 配置参考 <a href="https://github.com/juanfont/headscale/wiki/nginx-configuration" target="_blank" rel="noopener">官方 Wiki</a>, Caddy 只需要一行 <code>reverse_proxy headscale:8080</code> 即可(地址自行替换).</p><p>至于 ACME 证书你可以通过使用 <code>acme.sh</code> 自动配置 Nginx 或者使用 Caddy 自动申请等方式, 这些已经与 Headscale 无关了, 不在本文探讨范围内.</p><h3><span id="34-内网地址分配">3.4、内网地址分配</span></h3><p>请尽量不要将 <code>ip_prefixes</code> 配置为默认的 <code>100.64.0.0/10</code> 网段, 如果你有兴趣查询了该地址段, 那么你应该明白它叫 CGNAT; 很不幸的是例如 Aliyun 底层的 apt 源等都在这个范围内, 可能会有一些奇怪问题.</p><h3><span id="35-启动-headscale">3.5、启动 Headscale</span></h3><p>在处理完证书等配置后, 只需要愉快的启动一下即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 开机自启动 并 立即启动</span><br><span class="line">systemctl enable headscale --now</span><br></pre></td></tr></table></figure><p>再啰嗦一嘴, 如果你期望使用 Headscale ACME 自动申请证书, 你的关键配置应该像这样:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server_url: https:&#x2F;&#x2F;your.domain.com</span><br><span class="line">listen_addr: 0.0.0.0:443</span><br><span class="line">tls_letsencrypt_hostname: &quot;your.domain.com&quot;</span><br><span class="line">tls_cert_path: &quot;&quot;</span><br><span class="line">tls_key_path: &quot;&quot;</span><br></pre></td></tr></table></figure><p>如果你期望使用自定义证书, 则你的关键配置应该像这样:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server_url: https:&#x2F;&#x2F;your.domain.com</span><br><span class="line">listen_addr: 0.0.0.0:443</span><br><span class="line">tls_letsencrypt_hostname: &quot;&quot;</span><br><span class="line">tls_cert_path: &quot;&#x2F;path&#x2F;to&#x2F;cert&quot;</span><br><span class="line">tls_key_path: &quot;&#x2F;path&#x2F;to&#x2F;key&quot;</span><br></pre></td></tr></table></figure><p>如果你期望使用负载均衡器, 那么你的关键配置应该像这样:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server_url: https:&#x2F;&#x2F;your.domain.com</span><br><span class="line">listen_addr: 0.0.0.0:8080</span><br><span class="line">tls_letsencrypt_hostname: &quot;&quot;</span><br><span class="line">tls_cert_path: &quot;&quot;</span><br><span class="line">tls_key_path: &quot;&quot;</span><br></pre></td></tr></table></figure><p>在使用负载均衡器配置时, 启动后会有一行警告日志, 忽略即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2022-09-18T07:57:36Z WRN Listening without TLS but ServerURL does not start with http:&#x2F;&#x2F;</span><br></pre></td></tr></table></figure><h3><span id="36-docker-compose-安装">3.6、Docker Compose 安装</span></h3><p>Compose 配置样例文件如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># docker-compose.yaml</span><br><span class="line">version: &quot;3.9&quot;</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  headscale:</span><br><span class="line">    container_name: headscale</span><br><span class="line">    image: headscale&#x2F;headscale:0.16.4</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8080:8080&quot;</span><br><span class="line">    cap_add:</span><br><span class="line">      - NET_ADMIN</span><br><span class="line">      - NET_RAW</span><br><span class="line">      - SYS_MODULE</span><br><span class="line">    sysctls:</span><br><span class="line">      - net.ipv4.ip_forward&#x3D;1</span><br><span class="line">      - net.ipv6.conf.all.forwarding&#x3D;1</span><br><span class="line">    restart: always</span><br><span class="line">    volumes:</span><br><span class="line">      - .&#x2F;conf:&#x2F;etc&#x2F;headscale</span><br><span class="line">      - data:&#x2F;var&#x2F;lib&#x2F;headscale</span><br><span class="line">    command: [&quot;headscale&quot;, &quot;serve&quot;]</span><br><span class="line">volumes:</span><br><span class="line">  config:</span><br><span class="line">  data:</span><br></pre></td></tr></table></figure><p>你需要在与 <code>docker-compose.yaml</code> 同级目录下创建 <code>conf</code> 目录用于存储配置文件; 具体配置请参考上面的配置详解等部分, 最后不要忘记你的 Compose 文件端口映射需要和配置文件保持一致.</p><h2><span id="四-客户端安装">四、客户端安装</span></h2><p>对于客户端来说, Tailscale 提供了多个平台和发行版的预编译安装包, 并且部分客户端直接支持设置自定义的中央控制服务器.</p><h3><span id="41-linux-客户端">4.1、Linux 客户端</span></h3><p>Linux 用户目前只需要使用以下命令安装即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https:&#x2F;&#x2F;tailscale.com&#x2F;install.sh | sh</span><br></pre></td></tr></table></figure><p>默认该脚本会检测相关的 Linux 系统发行版并使用对应的包管理器安装 Tailscale, 安装完成后使用以下命令启动:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tailscale up --login-server https:&#x2F;&#x2F;your.domain.com --advertise-routes&#x3D;192.168.11.0&#x2F;24 --accept-routes&#x3D;true --accept-dns&#x3D;false</span><br></pre></td></tr></table></figure><p>关于选项设置:</p><ul><li><code>--login-server</code>: 指定使用的中央服务器地址(必填)</li><li><code>--advertise-routes</code>: 向中央服务器报告当前客户端处于哪个内网网段下, 便于中央服务器让同内网设备直接内网直连(可选的)或者将其他设备指定流量路由到当前内网(可选)</li><li><code>--accept-routes</code>: 是否接受中央服务器下发的用于路由到其他客户端内网的路由规则(可选)</li><li><code>--accept-dns</code>: 是否使用中央服务器下发的 DNS 相关配置(可选, 推荐关闭)</li></ul><p>启动完成后, <strong><code>tailscale</code> 将会卡住, 并打印一个你的服务器访问地址; 浏览器访问该地址后将会得到一条命令:</strong></p><p><img src="https://img.hi-linux.com/staticfile/l2zjmV-2022-10-25-U5QXXT.png" alt></p><p><img src="https://img.hi-linux.com/staticfile/JU8BVZ-20221025104046371-2022-10-25-MKL0fj.png" alt></p><p><strong>注意: 浏览器上显示的命令需要在中央控制服务器执行(Headscale), <code>NAMESAPCE</code> 位置应该替换为一个具体的 Namespace, 可以使用以下命令创建 Namespace (名字随意)并让设备加入:</strong></p><p><img src="https://img.hi-linux.com/staticfile/x9tA4A-2022-10-25-QqSXMn.png" alt></p><p>在 Headscale 服务器上执行命令成功后客户端命令行在稍等片刻便会执行完成, 此时该客户端已经被加入 Headscale 网络并分配了特定的内网 IP; 多个客户端加入后在 NAT 穿透成功时就可以互相 ping 通, 如果出现问题请阅读后面的调试细节, 只要能注册成功就算是成功了一半, 暂时不要慌.</p><h3><span id="42-macos-客户端">4.2、MacOS 客户端</span></h3><p>MacOS 客户端安装目前有两种方式, 一种是使用标准的 AppStore 版本(好像还有一个可以直接下载的), 需要先设置服务器地址然后再启动 App:</p><p>首先访问你的 Headscale 地址 <code>https://your.domain.com/apple</code>:</p><p><img src="https://img.hi-linux.com/staticfile/Y1cXjS-20221025104153717-2022-10-25-krooqA.png" alt></p><p>复制倒数第二行命令到命令行执行(可能需要 sudo 执行), 然后去 AppStore 搜索 Hailscale 安装并启动; 启动后会自动打开浏览器页面, 与 Linux 安装类似, 复制命令到 Headscale 服务器执行即可(Namespace 创建一次就行).</p><p><strong>第二种方式也是比较推荐的方式, 直接编译客户端源码安装, 体验与 Linux 版本一致:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 安装 go</span><br><span class="line">brew install go</span><br><span class="line"></span><br><span class="line"># 编译命令行客户端</span><br><span class="line">go install tailscale.com&#x2F;cmd&#x2F;tailscale&#123;,d&#125;@main</span><br><span class="line"></span><br><span class="line"># 安装为系统服务</span><br><span class="line">sudo tailscaled install-system-daemon</span><br></pre></td></tr></table></figure><p>安装完成后同样通过 <code>tailscale up</code> 命令启动并注册即可, 具体请参考 Linux 客户端安装部分.</p><h3><span id="43-其他客户端">4.3、其他客户端</span></h3><p>关于 Windows 客户端大致流程就是创建一个注册表, 然后同样安装官方 App 启动, 接着浏览器复制命令注册即可. 至于移动端本人没有需求, 所以暂未研究. <strong>Windows 具体的安装流程请访问 <code>https://your.domain.com/windows</code> 地址查看(基本与 MacOS AppStore 版本安装类似).</strong></p><h2><span id="五-中继服务器搭建">五、中继服务器搭建</span></h2><p>在上面的 Headscale 搭建完成并添加客户端后, 某些客户端可能无法联通; 这是由于网络复杂情况下导致了 NAT 穿透失败; 为此我们可以搭建一个中继服务器来进行传统的星型拓扑通信.</p><h3><span id="51-搭建-derp-server">5.1、搭建 DERP Server</span></h3><p>首先需要注意的是, 在需要搭建 DERP Server 的服务器上, 请先安装一个 Tailscale 客户端并注册到 Headscale; <strong>这样做的目的是让搭建的 DERP Server 开启客户端认证, 否则你的 DERP Server 可以被任何人白嫖.</strong></p><p>目前 Tailscale 官方并未提供 DERP Server 的安装包, 所以需要我们自行编译安装; 在编译之前请确保安装了最新版本的 Go 语言及其编译环境.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 编译 DERP Server</span><br><span class="line">go install tailscale.com&#x2F;cmd&#x2F;derper@main</span><br><span class="line"></span><br><span class="line"># 复制到系统可执行目录</span><br><span class="line">mv $&#123;GOPATH&#125;&#x2F;bin&#x2F;derper &#x2F;usr&#x2F;local&#x2F;bin</span><br><span class="line"></span><br><span class="line"># 创建用户和运行目录</span><br><span class="line">useradd \</span><br><span class="line">        --create-home \</span><br><span class="line">        --home-dir &#x2F;var&#x2F;lib&#x2F;derper&#x2F; \</span><br><span class="line">        --system \</span><br><span class="line">        --user-group \</span><br><span class="line">        --shell &#x2F;usr&#x2F;sbin&#x2F;nologin \</span><br><span class="line">        derper</span><br></pre></td></tr></table></figure><p>接下来创建一个 SystemD 配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># &#x2F;lib&#x2F;systemd&#x2F;system&#x2F;derper.service</span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;tailscale derper server</span><br><span class="line">After&#x3D;syslog.target</span><br><span class="line">After&#x3D;network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">User&#x3D;derper</span><br><span class="line">Group&#x3D;derper</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;derper -c&#x3D;&#x2F;var&#x2F;lib&#x2F;derper&#x2F;private.key -a&#x3D;:8989 -stun-port&#x3D;3456 -verify-clients</span><br><span class="line">Restart&#x3D;always</span><br><span class="line">RestartSec&#x3D;5</span><br><span class="line"></span><br><span class="line"># Optional security enhancements</span><br><span class="line">NoNewPrivileges&#x3D;yes</span><br><span class="line">PrivateTmp&#x3D;yes</span><br><span class="line">ProtectSystem&#x3D;strict</span><br><span class="line">ProtectHome&#x3D;yes</span><br><span class="line">ReadWritePaths&#x3D;&#x2F;var&#x2F;lib&#x2F;derper &#x2F;var&#x2F;run&#x2F;derper</span><br><span class="line">AmbientCapabilities&#x3D;CAP_NET_BIND_SERVICE</span><br><span class="line">RuntimeDirectory&#x3D;derper</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure><p>最后使用以下命令启动 Derper Server 即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable derper --now</span><br></pre></td></tr></table></figure><p><strong>注意: 默认情况下 Derper Server 会监听在 <code>:443</code> 上, 同时会触发自动 ACME 申请证书. 关于证书逻辑如下:</strong></p><ul><li>1、如果不指定 <code>-a</code> 参数, 则默认监听 <code>:443</code></li><li>2、如果监听 <code>:443</code> 并且未指定 <code>--certmode=manual</code> 则会强制使用 <code>--hostname</code> 指定的域名进行 ACME 申请证书</li><li>3、如果指定了 <code>--certmode=manual</code> 则会使用 <code>--certmode</code> 指定目录下的证书开启 HTTPS</li><li>4、如果指定了 <code>-a</code> 为非 <code>:443</code> 端口, 且没有指定 <code>--certmode=manual</code> 则只监听 HTTP</li></ul><p><strong>如果期望使用 ACME 自动申请只需要不增加 <code>-a</code> 选项即可(占用 443 端口), 如果期望通过负载均衡器负载, 则需要将 <code>-a</code> 选项指定到非 443 端口, 然后配置 Nginx、Caddy 等 LB 软件即可. 最后一点 <code>stun</code> 监听的是 UDP 端口, 请确保防火墙打开此端口.</strong></p><h3><span id="52-配置-headscale">5.2、配置 Headscale</span></h3><p>在创建完 Derper 中继服务器后, 我们还需要配置 Headscale 来告诉所有客户端在必要时可以使用此中继节点进行通信; 为了达到这个目的, 我们需要在 Headscale 服务器上创建以下配置:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># &#x2F;etc&#x2F;headscale&#x2F;derper.yaml</span><br><span class="line"></span><br><span class="line">regions:</span><br><span class="line">  901:</span><br><span class="line">    regionid: 901</span><br><span class="line">    regioncode: private-derper</span><br><span class="line">    regionname: &quot;My Private Derper Server&quot;</span><br><span class="line">    nodes:</span><br><span class="line">      - name: private-derper</span><br><span class="line">        regionid: 901</span><br><span class="line">        # 自行更改为自己的域名</span><br><span class="line">        hostname: derper.xxxxx.com</span><br><span class="line">        # Derper 节点的 IP</span><br><span class="line">        ipv4: 123.123.123.123</span><br><span class="line">        # Derper 设置的 STUN 端口</span><br><span class="line">        stunport: 3456</span><br></pre></td></tr></table></figure><p>在创建好基本的 Derper Server 节点信息配置后, 我们需要调整主配置来让 Headscale 加载:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">derp:</span><br><span class="line">  server:</span><br><span class="line">    # 这里关闭 Headscale 默认的 Derper Server</span><br><span class="line">    enabled: false</span><br><span class="line">  # urls 留空, 保证不加载官方的默认 Derper</span><br><span class="line">  urls: []</span><br><span class="line">  # 这里填写 Derper 节点信息配置的绝对路径</span><br><span class="line">  paths:</span><br><span class="line">  - &#x2F;etc&#x2F;headscale&#x2F;derper.yaml</span><br><span class="line"></span><br><span class="line">  # If enabled, a worker will be set up to periodically</span><br><span class="line">  # refresh the given sources and update the derpmap</span><br><span class="line">  # will be set up.</span><br><span class="line">  auto_update_enabled: true</span><br><span class="line"></span><br><span class="line">  # How often should we check for DERP updates?</span><br><span class="line">  update_frequency: 24h</span><br></pre></td></tr></table></figure><p>接下来重启 Headscale 并重启 client 上的 tailscale 即可看到中继节点:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">~ ❯❯❯ tailscale netcheck</span><br><span class="line"></span><br><span class="line">Report:</span><br><span class="line">        * UDP: true</span><br><span class="line">        * IPv4: yes, 124.111.111.111:58630</span><br><span class="line">        * IPv6: no, but OS has support</span><br><span class="line">        * MappingVariesByDestIP: false</span><br><span class="line">        * HairPinning: false</span><br><span class="line">        * PortMapping: UPnP, NAT-PMP, PCP</span><br><span class="line">        * CaptivePortal: true</span><br><span class="line">        * Nearest DERP: XXXX Derper Server</span><br><span class="line">        * DERP latency:</span><br><span class="line">                - XXXX: 10.1ms  (XXXX Derper Server)</span><br></pre></td></tr></table></figure><p>到此中继节点搭建完成.</p><h3><span id="53-docker-compose-安装">5.3、Docker Compose 安装</span></h3><p>目前官方似乎也没有提供 Docker 镜像, 我自己通过 GitHub Action 编译了一个 Docker 镜像, 以下是使用此镜像的 Compose 文件样例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">version: &#39;3.9&#39;</span><br><span class="line">services:</span><br><span class="line">  derper:</span><br><span class="line">    image: mritd&#x2F;derper</span><br><span class="line">    container_name: derper</span><br><span class="line">    restart: always</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;8080:8080&#x2F;tcp&quot;</span><br><span class="line">      - &quot;3456:3456&#x2F;udp&quot;</span><br><span class="line">    environment:</span><br><span class="line">      TZ: Asia&#x2F;Shanghai</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;etc&#x2F;timezone:&#x2F;etc&#x2F;timezone</span><br><span class="line">      - &#x2F;var&#x2F;run&#x2F;tailscale:&#x2F;var&#x2F;run&#x2F;tailscale</span><br><span class="line">      - data:&#x2F;var&#x2F;lib&#x2F;derper</span><br><span class="line">volumes:</span><br><span class="line">  data:</span><br></pre></td></tr></table></figure><p><strong>该镜像默认开启了客户端验证, 所以请确保 <code>/var/run/tailscale</code> 内存在已加入 Headscale 成功的 tailscaled 实例的 sock 文件. 其他具体环境变量等参数配置请参考 <a href="https://github.com/mritd/autobuild/blob/main/derper/Earthfile" target="_blank" rel="noopener">Earthfile</a>.</strong></p><h2><span id="六-客户端网络调试">六、客户端网络调试</span></h2><blockquote><p>在调试中继节点或者不确定网络情况时, 可以使用一些 Tailscale 内置的命令来调试网络.</p></blockquote><h3><span id="61-ping-命令">6.1、Ping 命令</span></h3><p><code>tailscale ping</code> 命令可以用于测试 IP 连通性, 同时可以看到时如何连接目标节点的. <strong>默认情况下 Ping 命令首先会使用 Derper 中继节点通信, 然后尝试 P2P 连接; 一旦 P2P 连接成功则自动停止 Ping:</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">~ ❯❯❯ tailscale ping 10.24.0.5</span><br><span class="line">pong from k8s13 (10.24.0.5) via DERP(XXXXX) in 14ms</span><br><span class="line">pong from k8s13 (10.24.0.5) via DERP(XXXXX) in 13ms</span><br><span class="line">pong from k8s13 (10.24.0.5) via DERP(XXXXX) in 14ms</span><br><span class="line">pong from k8s13 (10.24.0.5) via DERP(XXXXX) in 12ms</span><br><span class="line">pong from k8s13 (10.24.0.5) via DERP(XXXXX) in 12ms</span><br><span class="line">pong from k8s13 (10.24.0.5) via 3.4.170.23:2495 in 9ms</span><br></pre></td></tr></table></figure><p>由于其先走 Derper 的特性也可以用来测试 Derper 连通性.</p><h3><span id="62-status-命令">6.2、Status 命令</span></h3><p>通过 <code>tailscale status</code> 命令可以查看当前节点与其他对等节点的连接方式, 通过此命令可以查看到当前节点可连接的节点以及是否走了 Derper 中继:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">~ ❯❯❯ tailscale status</span><br><span class="line">10.24.0.8       xmac                 kovacs       macOS   -</span><br><span class="line">                alivpn               kovacs       linux   active; direct 4.3.4.5:41644, tx 1264 rx 944</span><br><span class="line">                aliyun               kovacs       linux   -</span><br><span class="line">                bob                  kovacs       macOS   offline</span><br><span class="line">                bob-imac             kovacs       macOS   offline</span><br><span class="line">                company              kovacs       linux   active; direct 114.114.114.114:41642, tx 1296 rx 880</span><br></pre></td></tr></table></figure><h3><span id="63-netcheck-命令">6.3、NetCheck 命令</span></h3><p>有些情况下我们可以确认是当前主机的网络问题导致没法走 P2P 连接, 但是我们又想了解一下当前的网络环境; 此时可以使用 <code>tailscale netcheck</code> 命令来检测当前的网络环境, 此命令将会打印出详细的网络环境报告:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">~ ❯❯❯ tailscale netcheck</span><br><span class="line">2022&#x2F;10&#x2F;19 21:15:27 portmap: [v1] Got PMP response; IP: 123.123.123.123, epoch: 297671</span><br><span class="line">2022&#x2F;10&#x2F;19 21:15:27 portmap: [v1] Got PCP response: epoch: 297671</span><br><span class="line">2022&#x2F;10&#x2F;19 21:15:27 portmap: [v1] UPnP reply &#123;Location:http:&#x2F;&#x2F;192.168.11.1:39735&#x2F;rootDesc.xml Server:AsusWRT&#x2F;386 UPnP&#x2F;1.1 MiniUPnPd&#x2F;2.2.0 USN:uuid:23345-2380-45f5-34534-04421abwb7cf0::urn:schemas-upnp-org:device:InternetGatewayDevice:1&#125;, &quot;HTTP&#x2F;1.1 200 OK\r\nCACHE-CONTROL: max-age&#x3D;120\r\nST: urn:schemas-upnp-org:device:InternetGatewayDevice:1\r\nUSN: uuid:34564645-2380-45f5-b069-sdfdght3245.....&quot;</span><br><span class="line">2022&#x2F;10&#x2F;19 21:15:27 portmap: UPnP meta changed: &#123;Location:http:&#x2F;&#x2F;192.168.11.1:39735&#x2F;rootDesc.xml Server:AsusWRT&#x2F;386 UPnP&#x2F;1.1 MiniUPnPd&#x2F;2.2.0 USN:uuid:23345-2380-45f5-b069-04421abwb7cf0::urn:schemas-upnp-org:device:InternetGatewayDevice:1&#125;</span><br><span class="line"></span><br><span class="line">Report:</span><br><span class="line">        * UDP: true</span><br><span class="line">        * IPv4: yes, 123.123.123.123:5935</span><br><span class="line">        * IPv6: no, but OS has support</span><br><span class="line">        * MappingVariesByDestIP: false</span><br><span class="line">        * HairPinning: true</span><br><span class="line">        * PortMapping: UPnP, NAT-PMP, PCP</span><br><span class="line">        * CaptivePortal: true</span><br><span class="line">        * Nearest DERP: XXXXX Aliyun</span><br><span class="line">        * DERP latency:</span><br><span class="line">                - XXXXX: 9.5ms   (XXXXX Aliyun)</span><br><span class="line">                - XXXXX: 53.1ms  (XXXXX BandwagonHost)</span><br></pre></td></tr></table></figure><h2><span id="七-其他补充">七、其他补充</span></h2><h3><span id="71-某些代理工具兼容性">7.1、某些代理工具兼容性</span></h3><p>MacOS 下使用一些增强代理工具时, 如果安装 App Store 的官方图形化客户端, 则可能与这些软件冲突, 推荐使用纯命令行版本<strong>并添加进程规则匹配 <code>tailscale</code> 和 <code>tailscaled</code> 两个进程, 让它们始终走 <code>DIRECT</code> 规则即可.</strong></p><h3><span id="72-macos-下-cpu-占用突然起飞">7.2、MacOS 下 CPU 占用突然起飞</span></h3><p>在使用一些网络代理工具时, 网络工具会设置默认路由; 这可能导致 <code>tailscaled</code> 无法获取到默认路由接口, 然后进入死循环并把 CPU 吃满, 同时会与 Derper 服务器产生大量上传流量. <strong>截止本文发布此问题已修复, 请使用 <code>mian</code> 分支编译安装, 具体见 <a href="https://github.com/tailscale/tailscale/issues/5879" target="_blank" rel="noopener">ISSUE/5879</a>.</strong></p><h3><span id="73-阿里云安装客户端后无法更新软件">7.3、阿里云安装客户端后无法更新软件</span></h3><p>Tailscale 默认使用 CGNAT(<code>100.64.0.0/10</code>) 网段作为内部地址分配网段, <strong>目前 Tailscale 仅允许自己的接口使用此网段, 不巧的是阿里云的 DNS、Apt 源等也采用此网段.</strong> 这会导致阿里云服务器安装客户端后 DNS、Apt 等不可用, 解决方案目前只能修改源码删除掉这两个 DROP 规则并重新编译.</p><p><img src="https://img.hi-linux.com/staticfile/Nnv35j-20221025104030679-2022-10-25-rWqjrm.png" alt></p><h3><span id="74-开启路由转发">7.4、开启路由转发</span></h3><p>大多数时候我们可能并不会在每个服务器上都安装 Tailscale 客户端, 通常只安装 2、3 台, 然后想通过这两三台转发该内网的所有流量. <strong>此时你需要</strong></p><ul><li>启动 tailscale 时设置正确的路由提示 <code>--advertise-routes=192.168.1.0/24</code> 来告诉 Headscale 服务器 “我这个节点可以转发这些地址的路由”</li><li>其他节点启动时需要增加 <code>--accept-routes=true</code> 选项来声明 “我接受外部其他节点发布的路由”</li></ul><p><strong>以上两个选项配置后, 只需要 Headscale 服务器上使用 <code>headscale node route enable -a -i XX(ID)</code> 开启即可. 开启后目标节点(ID)的路由就会发布到接受外部路由的所有节点, 想要关闭的话去掉 <code>-a</code> 即可.</strong></p><h3><span id="75-其他问题">7.5、其他问题</span></h3><p>以上也只是我个人遇到的一些问题, 如果有其他问题推荐先搜索然后查看 ISSUE, 最后不行可以看看源码. 目前来说 Tailscale 很多选项很模糊, 可能需要阅读源码以后才能知道到底应该怎么做.</p><blockquote><p>本文转载自：「 bleem 」，原文：<a href="https://url.hi-linux.com/M3tzx" target="_blank" rel="noopener">https://url.hi-linux.com/M3tzx</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、内网穿透简述&quot;&gt;一、内网穿透简述&lt;/h2&gt;
&lt;p&gt;由于国内网络环境问题, 普遍家庭用户宽带都没有分配到公网 IP(我有固定公网 IP, 嘿嘿); 这时候一般我们需要从外部访问家庭网络时就需要通过一些魔法手段, 比如 VPN、远程软件(向日葵…)等; 但是这些工具都有一个普遍存在的问题: 慢+卡!&lt;/p&gt;
&lt;h3 id=&quot;1-1、传统星型拓扑&quot;&gt;1.1、传统星型拓扑&lt;/h3&gt;
&lt;p&gt;究其根本因素在于, 在传统架构中如果两个位于多层 NAT(简单理解为多个路由器)之后的设备, 只能通过一些中央(VPN/远程软件)中转服务器进行链接, 这时网络连接速度取决于中央服务器带宽和速度; 这种网络架构我这里简称为: 星型拓扑&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.hi-linux.com/staticfile/MKn6bf-20221025104144038-2022-10-25-vuD5sk.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;从这张图上可以看出, &lt;strong&gt;你的 “工作笔记本” 和 “家庭 NAS” 之间通讯的最大传输速度为 &lt;code&gt;Up/Down: 512K/s&lt;/code&gt;&lt;/strong&gt;; 因为流量经过了中央服务器中转, 由于网络木桶效应存在, 即使你两侧的网络速度再高也没用, 整体的速度取决于这个链路中最低的一个设备网速而不是你两端的设备.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;在这种拓扑下, 想提高速度只有一个办法: 加钱!&lt;/strong&gt; 在不使用 “钞能力” 的情况下, 普遍免费的软件提供商不可能给予过多的资源来让用户白嫖, 而自己弄大带宽的中央服务器成本又过高.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Tailscale" scheme="https://www.hi-linux.com/tags/Tailscale/"/>
    
      <category term="Headscale" scheme="https://www.hi-linux.com/tags/Headscale/"/>
    
      <category term="VPN" scheme="https://www.hi-linux.com/tags/VPN/"/>
    
  </entry>
  
  <entry>
    <title>保姆级零信任容器应用平台 Kasm 使用指南（全网最详细中文教程）</title>
    <link href="https://www.hi-linux.com/posts/19718.html"/>
    <id>https://www.hi-linux.com/posts/19718.html</id>
    <published>2022-10-14T01:00:00.000Z</published>
    <updated>2022-10-14T01:48:45.274Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="kasm-介绍">Kasm 介绍</span></h2><p><code>Kasm</code> 是一款基于 <code>Docker</code> 的容器应用平台，它提供企业级编排、数据丢失防护和 <code>Web</code> 流技术，以支持将容器化工作负载交付到你的浏览器。</p><p><code>Kasm</code> 可以在浏览器內运行各种应用，比如：<code>Linux</code> 桌面、浏览器、聊天工具、办公软件、多媒体工具等。</p><p><code>Kasm</code> 将这些应用隔离在独立的 <code>Docker</code> 容器内，在里面做的任何行为不会影响真实的主机，并且具备一次性特点、用完即删，保证了数据的安全性。</p><p><code>Kasm</code> 是个开源项目，你可以在个人及非营利条件下免费使用。</p><ul><li><code>Kasm</code> 官方网站: <a href="https://www.kasmweb.com/" target="_blank" rel="noopener">https://www.kasmweb.com/</a></li><li><code>Kasm Github</code>: <a href="https://github.com/kasmtech" target="_blank" rel="noopener">https://github.com/kasmtech</a></li></ul><p>简单来说 <code>Kasm</code> 可以让用户在浏览器（即开即用）使用各种（容器化）的软件和操作系统。</p><a id="more"></a><p><img src="https://img.hi-linux.com/staticfile/mobile01-d19f67c7496a3cb9062a1e9e43c0cdb3-2022-10-12-uKWaTN.gif" alt="在浏览器内运行 Chrome、Edge"></p><p><code>Kasm</code> 支持常用的主流浏览器：<code>Chrome</code>、<code>Edge</code>、<code>FireFox</code>、<code>Tor</code> 等。</p><p><img src="https://img.hi-linux.com/staticfile/mobile01-dd91659a413738e9545b76829c03a3b3-2022-10-12-iZyrRL.gif" alt="在浏览器内运行 Ubuntu 桌面"></p><p><code>Kasm</code> 支持常用的 <code>Linux</code> 桌面：<code>Ubuntu</code>、<code>CentOS</code>、<code>OPenSUSE</code>、<code>Kali Linux</code> 等。</p><h3><span id="kasm-优点"><code>Kasm</code> 优点:</span></h3><ul><li>支持受隔离保护的浏览器</li><li>支持受隔离保护的 <code>Linux</code> 桌面</li><li>支持受隔离保护的多种应用</li><li>在数秒间快速启动应用</li><li>使用完毕瞬间删除应用，不留痕迹</li><li>容器闲置超过指定时间自动删除应用，不留痕迹</li></ul><h3><span id="kasm-缺点"><code>Kasm</code> 缺点:</span></h3><ul><li>目前中文支持不友好，在部分应用内没有中文输入法。</li></ul><p>下表我们将比较下几种常用虚拟化服务 <code>VM</code>、<code>Docker</code>、<code>Kasm</code> 间的区别</p><table><thead><tr><th>VM</th><th>Docker</th><th>Kasm</th></tr></thead><tbody><tr><td>虚拟操作系统</td><td>虚拟容器</td><td>虚拟容器</td></tr><tr><td>硬件资源占用率高</td><td>👑硬件资源占用率最低</td><td>硬件资源占用率较低</td></tr><tr><td>需要安装专用应用程序</td><td>使用终端程序，图形操作不友好</td><td>👑使用网页浏览器，介面友好</td></tr><tr><td>安装操作系统耗时</td><td>部署容器快速</td><td>👑鼠标一键瞬间启动，支持多任务</td></tr><tr><td>虚拟机内的浏览器会储存 Cookie，无法达到完全的隐匿性</td><td>虽然可透过删除容器及重新部署来达到即开即用，但步骤较为繁琐</td><td>👑应用的启动或删除只需鼠标点击，真正实现即开即用。</td></tr><tr><td>👑中文支持度高</td><td>中文支持度低</td><td>中文支持度低</td></tr></tbody></table><h2><span id="部署-kasm">部署 Kasm</span></h2><ul><li>硬件要求</li></ul><table><thead><tr><th>硬件类型</th><th>配置规格</th></tr></thead><tbody><tr><td><strong>CPU</strong></td><td>2 cores</td></tr><tr><td><strong>Memory</strong></td><td>4GB</td></tr><tr><td><strong>Storage</strong></td><td>50GB (SSD)</td></tr></tbody></table><ul><li>操作系统要求</li></ul><table><thead><tr><th>支持的操作系统</th></tr></thead><tbody><tr><td>Ubuntu 18.04 / 20.04 / 22.04 (amd64/arm64)</td></tr><tr><td>Debian 9 / 10 / 11 (amd64/arm64)</td></tr><tr><td>CentOS 7 / 8 (amd64/arm64)</td></tr><tr><td>Oracle Linux 7 / 8 (amd64/arm64)</td></tr><tr><td>Raspberry Pi OS (Debian) 10 / 11 (arm64)</td></tr><tr><td><a href="https://www.kasmweb.com/docs/latest/how_to/other_operating_systems.html" target="_blank" rel="noopener">Other</a></td></tr></tbody></table><p><code>Kasm</code> 部署还是很容易的，基本只需执行以下几条命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ cd &#x2F;tmp</span><br><span class="line">$ curl -O https:&#x2F;&#x2F;kasm-static-content.s3.amazonaws.com&#x2F;kasm_release_1.11.0.18142e.tar.gz</span><br><span class="line">$ tar -xvf kasm_release*.tar.gz</span><br><span class="line">$ sudo bash kasm_release&#x2F;install.sh</span><br></pre></td></tr></table></figure><p>执行完成后，安装脚本会询问『是否接受协议』和『是否启用交换分区』。你可以根据实际情况回答，当然协议是必须接受的。</p><p><img src="https://img.hi-linux.com/staticfile/WX20221012-153755-2022-10-12-TrB2Oe.png" alt></p><p>默认情况下，<code>Kasm</code> 安装脚本会去 <code>Github</code> 下载 <code>Docker-Compose V2</code> 的执行文件。如果你的网络环境不能正常访问 <code>Github</code>，安装脚本就无法正常执行。</p><p>这时你可以提前使用以下命令，先手动完成 <code>Docker-Compose V2</code> 的安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl -L https:&#x2F;&#x2F;download.fastgit.org&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;v2.5.0&#x2F;docker-compose-linux-x86_64 -o &#x2F;usr&#x2F;local&#x2F;lib&#x2F;docker&#x2F;cli-plugins&#x2F;docker-compose</span><br><span class="line">$ chmod +x &#x2F;usr&#x2F;local&#x2F;lib&#x2F;docker&#x2F;cli-plugins&#x2F;docker-compose</span><br></pre></td></tr></table></figure><p>然后，等待脚本拉取完相应的 <code>Docker</code> 镜像。首次下载的镜像比较多，需要一定时间，请耐心等待。</p><p><img src="https://img.hi-linux.com/staticfile/WX20221012-160149-2022-10-12-ruVMVR.png" alt></p><p>最后，脚本安装完成后，会生成 <code>Kasm</code> 各组件默认的认证信息。（只显示一次，请注意保存。）</p><p><img src="https://img.hi-linux.com/staticfile/WX20221012-155525-2022-10-12-A4b779.png" alt></p><blockquote><p>注意：默认情况下，<code>Kasm Web</code> 应用程序是运行在 443 端口的，你需要在防火墙上对外开放此端口。如果你想在其他端口上运行 <code>Kasm Web</code> 应用程序，可在调用安装程序时通过 <code>-L</code> 参数指定。例如：<code>sudo bash kasm_release/install.sh -L 8443</code></p></blockquote><p>更详细官方安装教程可以查看：<a href="https://www.kasmweb.com/docs/latest/install/single_server_install.html" target="_blank" rel="noopener">Standard Installation</a>。</p><p>上面的方法是将所有 <code>Kasm</code> 服务组件部署到同一台机器上的。当然，你也可以将不同的 <code>Kasm</code> 服务角色分开安装到不同机器。具体可以参考: <a href="https://www.kasmweb.com/docs/latest/install/multi_server_install.html" target="_blank" rel="noopener">Multi Server Installation</a></p><h3><span id="访问-kasm">访问 Kasm</span></h3><p>默认情况下，你可以使用 <code>https://server_ip</code> 访问的 <code>Kasm</code> 的 Web 页面。</p><p>登陆信息就是上面安装过程中自动生成的默认凭据。</p><p><img src="https://img.hi-linux.com/staticfile/lFMyKV-2022-10-12-mb3ObR.png" alt></p><p><code>Kasm</code> 内置了很多常用的应用：</p><p><img src="https://img.hi-linux.com/staticfile/NlAxM0-2022-10-12-LNPZtO.png" alt></p><p>现在，我们来启动一个 <code>Chrome</code> 试试：</p><p><img src="https://img.hi-linux.com/staticfile/YWo2ol-2022-10-12-OvAv84.png" alt></p><p>点击一下图标后，就秒启动完成一个全新的 <code>Chrome</code> 环境。</p><p><img src="https://img.hi-linux.com/staticfile/Ycqfe7-2022-10-12-KZ0dut.png" alt></p><p>你还可以点击左面的键头图标，来使用一些辅助功能。比如：启用声音、剪贴板等。</p><p><img src="https://img.hi-linux.com/staticfile/myjMye-2022-10-12-8T0NdF.png" alt></p><p>简单体验了下，各种输入输出都很快，播放视频也是非常流畅的。接下来，再启动一个 <code>Ubuntu</code> 看看：</p><p><img src="https://img.hi-linux.com/staticfile/cZZ5O7-2022-10-12-R2qshk.png" alt></p><p><img src="https://img.hi-linux.com/staticfile/fA7mpX-2022-10-12-n4XsEm.png" alt></p><p><img src="https://img.hi-linux.com/staticfile/XhiYno-2022-10-12-TUEO7T.png" alt></p><p>太赞了，<code>Linux</code> 桌面应用，运行起来也是一样的丝般顺滑。</p><p><code>Kasm</code> 的强大功能远远不止这些，它还支持多用户、<code>LDAP</code>、<code>OpenID</code> 等三方认证、自定义容器应用等高级功能。有兴趣的同学可以到官网自行探索哟！</p><h2><span id="参考文档">参考文档</span></h2><ol><li><a href="https://www.google.com" target="_blank" rel="noopener">https://www.google.com</a></li><li><a href="https://www.mobile01.com/topicdetail.php?f=508&amp;t=6573952" target="_blank" rel="noopener">https://www.mobile01.com/topicdetail.php?f=508&amp;t=6573952</a></li><li><a href="https://www.kasmweb.com/docs/latest/index.html" target="_blank" rel="noopener">https://www.kasmweb.com/docs/latest/index.html</a></li><li><a href="https://www.kasmweb.com/docs/latest/install/single_server_install.html" target="_blank" rel="noopener">https://www.kasmweb.com/docs/latest/install/single_server_install.html</a></li><li><a href="https://www.kasmweb.com/docs/latest/install/multi_server_install.html" target="_blank" rel="noopener">https://www.kasmweb.com/docs/latest/install/multi_server_install.html</a></li></ol></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Kasm-介绍&quot;&gt;Kasm 介绍&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Kasm&lt;/code&gt; 是一款基于 &lt;code&gt;Docker&lt;/code&gt; 的容器应用平台，它提供企业级编排、数据丢失防护和 &lt;code&gt;Web&lt;/code&gt; 流技术，以支持将容器化工作负载交付到你的浏览器。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Kasm&lt;/code&gt; 可以在浏览器內运行各种应用，比如：&lt;code&gt;Linux&lt;/code&gt; 桌面、浏览器、聊天工具、办公软件、多媒体工具等。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Kasm&lt;/code&gt; 将这些应用隔离在独立的 &lt;code&gt;Docker&lt;/code&gt; 容器内，在里面做的任何行为不会影响真实的主机，并且具备一次性特点、用完即删，保证了数据的安全性。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Kasm&lt;/code&gt; 是个开源项目，你可以在个人及非营利条件下免费使用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Kasm&lt;/code&gt; 官方网站: &lt;a href=&quot;https://www.kasmweb.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.kasmweb.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Kasm Github&lt;/code&gt;: &lt;a href=&quot;https://github.com/kasmtech&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/kasmtech&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简单来说 &lt;code&gt;Kasm&lt;/code&gt; 可以让用户在浏览器（即开即用）使用各种（容器化）的软件和操作系统。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="DNS" scheme="https://www.hi-linux.com/tags/DNS/"/>
    
  </entry>
  
  <entry>
    <title>谷歌翻译停服？别慌，手把手教你一招修复 Chrome 浏览器无法翻译网页的问题</title>
    <link href="https://www.hi-linux.com/posts/43698.html"/>
    <id>https://www.hi-linux.com/posts/43698.html</id>
    <published>2022-10-09T01:00:00.000Z</published>
    <updated>2022-10-09T03:38:47.693Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>最近 Google 突然关停了「谷歌翻译中国版」以及「谷歌地图中国版」两大重磅产品，让无数人惊讶！官方称原因是用户使用率太低。这次关停不单是网页版，依靠其服务的相关功能也会受到影响。</p><p>比如谷歌浏览器「Google Chrome」目前是国内使用率最高的浏览器之一，此次停服则直接影响了浏览器内置的 「<strong>自动网页翻译</strong>」功能。由于很多用户访问英文或其他语言网页的时候，都常要用到谷歌浏览器网页翻译功能。</p><a id="more"></a><p>这里我们就介绍一下解决办法：通过修改 <code>hosts</code> 文件，将谷歌翻译 API 的域名解析到能正常访问的 IP 地址。</p><p><code>hosts</code> 是一个没有扩展名的系统文件，主要作用是定义 IP 地址和主机名的映射关系。</p><p>首先，找到 Hosts 文件所在路径：</p><ul><li>Windows 系统 Hosts 文件路径：<code>C:\Windows\System32\drivers\etc\hosts</code></li><li>Mac 或 Linux 系统 Hosts 文件路径：<code>/etc/hosts</code></li><li>Android 系统 Hosts 文件路径：<code>/system/etc/hosts</code></li></ul><p>然后，你可以用任何文本编辑器工具打开 hosts 文件进行修改。在 <code>macOS</code> 或 <code>Linux</code> 下可以使用 <code>sudo vi /etc/hosts</code> 命令进行编辑，在其末尾添加以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 谷歌翻译服务 IP</span><br><span class="line">113.108.239.162 translate.google.com</span><br><span class="line">113.108.239.162 translate.googleapis.com</span><br></pre></td></tr></table></figure><p>将以上内容加入 <code>hosts</code> 文件后，谷歌浏览器 <code>Chrome</code> 的翻译功能就可以正常使用了。</p><p>为防止不可用，下面再提供一些备选的 IP 地址：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 备选谷歌翻译服务 IP</span><br><span class="line">203.208.39.226</span><br><span class="line">120.253.253.34</span><br><span class="line">142.250.66.138</span><br><span class="line">142.250.0.90</span><br></pre></td></tr></table></figure><blockquote><p>需要注意的是，这种修改只能恢复 <code>Chrome</code> 浏览器的内置翻译功能，并不能恢复 Web 版谷歌翻译 <code>translate.google.com</code> 的访问。</p></blockquote><p><strong>参考文档</strong></p><ol><li><a href="https://www.google.com" target="_blank" rel="noopener">https://www.google.com</a></li><li><a href="https://www.williamlong.info/archives/6947.html" target="_blank" rel="noopener">https://www.williamlong.info/archives/6947.html</a></li><li><a href="https://www.iplaysoft.com/fix-chrome-translate.html" target="_blank" rel="noopener">https://www.iplaysoft.com/fix-chrome-translate.html</a></li></ol></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近 Google 突然关停了「谷歌翻译中国版」以及「谷歌地图中国版」两大重磅产品，让无数人惊讶！官方称原因是用户使用率太低。这次关停不单是网页版，依靠其服务的相关功能也会受到影响。&lt;/p&gt;
&lt;p&gt;比如谷歌浏览器「Google Chrome」目前是国内使用率最高的浏览器之一，此次停服则直接影响了浏览器内置的 「&lt;strong&gt;自动网页翻译&lt;/strong&gt;」功能。由于很多用户访问英文或其他语言网页的时候，都常要用到谷歌浏览器网页翻译功能。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Chrome" scheme="https://www.hi-linux.com/tags/Chrome/"/>
    
      <category term="Google" scheme="https://www.hi-linux.com/tags/Google/"/>
    
  </entry>
  
  <entry>
    <title>阿里开源，超强大的 Kubernetes 本地调试工具 Kt-Connect 使用指南</title>
    <link href="https://www.hi-linux.com/posts/39758.html"/>
    <id>https://www.hi-linux.com/posts/39758.html</id>
    <published>2022-09-27T01:00:00.000Z</published>
    <updated>2022-09-27T09:27:35.918Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="背景">背景</span></h2><blockquote><p>注：背景有点啰嗦，讲讲一路走来研发本地调试的变化，嫌烦的可以直接跳过，不影响阅读。</p></blockquote><h3><span id="2019年">2019年</span></h3><p>我在的公司当时是个什么情况，只有两个Java应用，还都跑在一个Tomcat Servlet容器。</p><p><img src="https://img.hi-linux.com/staticfile/1557258-20220819114243341-1314582490-2022-09-19-XCWjyz.png" alt></p><p>当时是如何本地调试？都是研发自己电脑装个 Mysql，装个 Tomcat，自己电脑运行调试，好处嘛就是后端研发互不干扰，想怎么改就怎么改，APP端研发就直连后端的笔记本调试。上线部署嘛就是一个研发手动编译个 Jar 包丢到云服务器上面，大体就是个草台班子，能干活，但是也就那样。</p><a id="more"></a><h3><span id="2020年">2020年</span></h3><p>到了 2020 年，公司买了一台服务器，Centos 的系统，给装上了 Mysql、Tomcat，用上了 Redis 缓存，RabbitMQ 消息队列，有了独立的测试环境，用上了 Jenkins 自动打包并部署应用，也算鸟枪换炮，起码不用自己打包了。</p><p><img src="https://img.hi-linux.com/staticfile/1557258-20220819134321534-273144804-2022-09-19-VC1hNI.png" alt></p><p>这个时候是如何本地调试呢？起码不用自己电脑装 Mysql 了，后面框架由 SpringMVC 和 Struts2 都改成 Spring Boot，外置的 Tomcat 也可以去掉了。后端研发本地运行 Spring Boot 时直连服务器的 Mysql 进行调试，APP 端再也不用连后端研发的笔记本了，有了相对稳定的调试环境。代价就是各个后端的数据库更新结构要保持兼容性，避免影响他人。</p><h3><span id="2021年">2021年</span></h3><p>随着业务增长，后端框架由 Spring Boot 进化为 Spring Cloud 全家桶，应用运行环境由 Linux 直接运行改为了 Docker 镜像部署，各类中间件同样也使用了 Docker 镜像。产品线增加，单一的开发分支已经不能满足需求，为此又开辟了另外一条后端代码分支，同样的开发测试环境也多了一份。</p><p><img src="https://img.hi-linux.com/staticfile/1557258-20220819135628647-203858289-20220919103031822-2022-09-19-26hYPm.png" alt></p><p>这个时候的本地调试，对于 APP 端来说变化不大，区别连接后端不同环境使用不同域名而已。对于后端的研发同学就不一样了，每次本地调试自己电脑要常驻一个 Eureka 和一个 Config Server，如果本地调试的微服务依赖比较多，没个大内存真是顶不住。</p><h3><span id="2022年">2022年</span></h3><p>业务量继续增加，产品同事数量增加了，那个需求量真是堆积如山，两个分支已经不能满足要求了，又开了第三个分支，还是不够。每次增加新的分支运行环境，后端研发同学也很痛苦，一堆环境和第三方平台回调需要配置。为了能动态扩容缩容，Spring Cloud 全家桶继续演进，抛弃了 Zuul 网关和 Eureka，改为使用 Spring Cloud Kubernetes，运行环境全面向 K8S 靠拢。在此期间公司又采购了一台服务器用于开发测试，内存 CPU 磁盘满上!</p><p><img src="https://img.hi-linux.com/staticfile/1557258-20220819153015204-416674218-20220919103037001-2022-09-19-BkYrUI.png" alt></p><p>进入 K8S 时代，后端研发本地的电脑没办法随意连接 Linux 服务器上面的各种中间件，每个新分支环境里面的每个 POD 都是一个新的 IP，也不可能像之前那样开放指定几个中间件的端口给后端连接，那么多环境每个都做设置的话，运维同学整天不用干别的事了。也由此引出了今天要说的 kt-connect 工具，通过这个工具，后端研发本地的电脑可以代理访问到各个分支环境，也就是 K8S 里面的命名空间的所有服务，并且只需要启动需要调试的服务，大大节省了电脑 CPU 内存占用。</p><h2><span id="选型">选型</span></h2><p>在选择代理访问 K8S 环境以便于本地调试的工具中，网上有几种。</p><h3><span id="1-端口转发">1. 端口转发</span></h3><p>使用 Ingress、NodePort、LoadBalancer 之类的将流量转发到指定端口，如上文所说，会让运维同学工作量比较大，也不便于分支环境的自动创建和回收，只适合需要暴露端口数量不多的场景。</p><h3><span id="2-vpn">2. VPN</span></h3><p>通过在 K8S 每个命名空间里面设置一个运行有 VPN 服务的 POD，后端研发笔记本通过 VPN 客户端连接代理进入到指定命名空间，可以正常访问和解析集群内各类服务，基本能满足日常的要求，缺点是每个命名空间都常驻了一个 VPN 服务的运行资源。</p><h3><span id="3-telepresence">3. Telepresence</span></h3><p>在搜索的过程中发现了这个代理工具，几乎可以说 9 成的中英文技术文章都推荐使用这个工具，功能非常强大，不但提供了 VPN 所具有的代理功能，可以访问到命名空间内所有服务，还能指定各种规则拦截指定服务的流量到本地机器，相当于本地机器也能作为一个普通的 POD 提供对外服务。大体设计原理如下：</p><p><img src="https://img.hi-linux.com/staticfile/1557258-20220825171140771-1117068712-2022-09-19-Gz6nqH.png" alt></p><p>在研发本地电脑执行如下命令</p><blockquote><p>telepresence helm install --kubeconfig .\kubeconfig<br>telepresence connect —kubeconfig .\kubeconfig</p></blockquote><p>就会自动在 K8S 集群创建一个命名空间 ambassador，并且部署一个 traffic-manager 的 Pod，用于流量管理，而在研发笔记本本地则会启动 2 个 Daemon 服务，其中一个叫 Root Daemon，用于建立一条双向代理通道，并管理本地电脑与 K8S 集群之间的流量，另外一个 User Daemon 则是负责与 Traffic Manager 通信，设置拦截规则，如果登录后还负责与 Ambassador Cloud 进行通信。</p><p>通过配置拦截规则，拦截的 Pod 里面会安装一个 traffic-agent，官方文档说明是类似 K8S 集群的 sidecar 模式，对注入 POD 进行流量劫持，所有流量出入通过 traffic-manager 进行重新路由。</p><blockquote><p>The Traffic Agent is a sidecar container that facilitates intercepts. When an intercept is first started, the Traffic Agent container is injected into the workload’s pod(s).</p></blockquote><p>虽然他的功能很强大，但是在目前 2.5 版本的使用过程中，为了使用他的拦截和 Preview Url 功能必须在他家的商业云平台 Ambassador Cloud 进行注册登陆(注：不知道为什么网上技术文章都没提到这点，测试的时候非得要登录他家云平台)，并且拦截规则的配置是通过云平台的网页进行操作的，联网的要求，包括可能存在的安全，泄露之类的隐患，我觉得是不可接受，也因此不得不放弃使用这个工具。</p><p>还有一个不得不说的缺点就是，老版本使用后可以清理掉自动创建的命名空间（namespace）和 Pod、拦截 Agent 的功能（telepresence uninstall）也没了，在 2.5 版本的命令参数里面完全消失了，这就导致每次使用后，如果想保持环境干净，还得麻烦运维同学去清理掉，非常麻烦，简直逼死洁癖患者。</p><h3><span id="4-kt-connect">4. kt-connect</span></h3><p>所幸开源社区又找到了另外一款类似 Telepresence 的工具，名为<a href="https://github.com/alibaba/kt-connect" target="_blank" rel="noopener">kt-connect</a>，使用版本为 v0.3.6（顺便说下我们使用的 K8S 版本是 1.24），并且它无需联网登陆什么账号，结束命令执行默认还会自动清理。阿里出品，不确定是不是又一个 KPI 开源项目，但是至少这一刻我对这个工具是非常满意的。</p><h2><span id="原理">原理</span></h2><p>同 Telepresence 类似，但不同的是，kt-connect 只会在指定连接的命名空间（namespace）里面新建一个自用的 Pod，然后部署一个 kt-connect-shadow 的镜像。相比 Telepresence，它在模式进行了细分扩展，分为四大模式：</p><h3><span id="1-connect模式">1. Connect模式</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ktctl.exe connect --kubeconfig .\kubeconfig --namespace feature-N --debug</span><br></pre></td></tr></table></figure><p>这个模式下，kt-connect 起到的是一个类似于VPN的作用，研发本地电脑可以访问到连接的命名空间(namespace)内的所有服务，但是并没有加到集群里面其他服务里面，其他服务的流量并不会转发到本地电脑。</p><blockquote><p>注1：与 telepresence 类似，kt-connect 所有命令都要带上 <code>--kubeconfig</code> ，确保有足够权限和能正确连接 K8S 集群的 API Server，很多文章都很少提到这点，假如K8S集群限制权限，或者与研发不在同一个网络，必须确保使用运维同学提供的有足够权限的授权文件 kubeconfig 来进行连接。</p></blockquote><blockquote><p>注2：</p><p>Failed to setup port forward local:28344 -&gt; pod kt-connect-shadow-gseak:53 error=“error upgrading connection: error sending request: Post “<a href="https://10.0.8.101:8443/api/v1/namespaces/feature-N/pods/kt-connect-shadow-gseak/portforward" target="_blank" rel="noopener">https://10.0.8.101:8443/api/v1/namespaces/feature-N/pods/kt-connect-shadow-gseak/portforward</a>”: dial tcp 10.0.8.101:8443: connectex: A socket operation was attempted to an unreachable host.”，</p></blockquote><p>如果出现以上报错的话，有可能是 kt-connect 路由 BUG，可能本地电脑的路由与新加的通往 API Server 的路由有冲突，增加参数 <code>--excludeIps 10.0.8.101/32</code> 即可，如果网段冲突比较多，可以扩大网段范围，例如<code>--excludeIps 10.0.8.0/24</code> 参考 <a href="https://github.com/alibaba/kt-connect/issues/302" target="_blank" rel="noopener">issue-302</a>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ktctl.exe connect --kubeconfig .\kubeconfig --namespace feature-N --excludeIps 10.0.8.101&#x2F;32 --debug</span><br></pre></td></tr></table></figure><h3><span id="2-exchange模式">2. Exchange模式</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ktctl.exe exchange serviceA --kubeconfig .\kubeconfig --namespace feature-N --expose 12001 --debug</span><br></pre></td></tr></table></figure><p>这个模式类似于 Telepresence 拦截模式，将指定服务的所有流量拦截下来转发到研发本地电脑的端口，使用这个模式能对环境里的访问请求直接进行调试。</p><p>具体原理就是将 service 里面的 Pod 替换成一个 <code>serviceA-kt-exchange</code> 的 Pod。</p><blockquote><p>注1：Exchange 模式的流量方向是单向的，并不会将本地电脑主动发起的请求代理过去，如果K8S集群跟研发本地电脑不在一个网段内，需要另外开一个命令行运行 Connect 模式，确保本地服务可以正常连接 K8S 集群的其他服务，参考<a href="https://github.com/alibaba/kt-connect/issues/216" target="_blank" rel="noopener">issue-216</a>。</p></blockquote><blockquote><p>注2：Exchange 模式是通过拦截 service 进行流量转发，假如集群的请求没有经过 service，例如直接解析到 Pod 类，可能就会出现拦截失败的情况（同理 Mesh 模式也是如此），所以出现问题记得跟运维同学确认K8S集群内的路由情况。</p></blockquote><h3><span id="3-mesh模式">3. Mesh模式</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kctl.exe mesh serviceA --kubeconfig .\kubeconfig --namespace feature-N --expose 12001 --debug</span><br></pre></td></tr></table></figure><p>执行命令后可以看到输出日志里面包含类似文字：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2:30PM INF Now you can access your service by header &#39;VERSION: xxxxx&#39;</span><br></pre></td></tr></table></figure><p>这个模式本地电脑的服务和 K8S 集群里面相同的服务同时对外响应请求，但是只有通过指定的 http 请求头 VERSION: xxxx 的请求才会转发到本地电脑，相比 Exchange 模式，保证了其他人服务正常使用，同时研发又能进行本地调试。每次生成的请求头 VERSION 的值都是动态生成的，如果要固定这个值，可以通过参数 <code>--versionMark</code> 写死，例如固定值为 test-version，命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kctl.exe mesh serviceA --kubeconfig .\kubeconfig --namespace feature-N --expose 12001 --debug --versionMark test-version</span><br></pre></td></tr></table></figure><p>具体原理就是将 serviceA 里面的 Pod 替换成一个 serviceA-kt-router 的路由镜像，负责根据请求头进行流量代理转发，另外生成一个 serviceA-kt-stuntman 服务，这个就是线上正常运行的 serviceA，还有一个serviceA-kt-mesh-xxxxx 服务，这个就负责将代理流量到本地电脑。</p><h3><span id="4-preview模式">4. Preview模式</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kctl.exe preview serviceB --kubeconfig .\kubeconfig --namespace feature-N --expose 12001</span><br></pre></td></tr></table></figure><p>不同于 Exchange 和 Mesh 模式要求 K8S 集群有一个在运行的服务，Preview 模式可以将本地电脑运行的程序部署到 K8S 集群中作为一个全新的 Service 对外提供服务，非常便于新建服务的开发调试、预览等作用。</p><blockquote><p>本文转载自：「 博客园 」，原文：<a href="https://url.hi-linux.com/9JmW5%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://url.hi-linux.com/9JmW5，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;注：背景有点啰嗦，讲讲一路走来研发本地调试的变化，嫌烦的可以直接跳过，不影响阅读。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;2019年&quot;&gt;2019年&lt;/h3&gt;
&lt;p&gt;我在的公司当时是个什么情况，只有两个Java应用，还都跑在一个Tomcat Servlet容器。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.hi-linux.com/staticfile/1557258-20220819114243341-1314582490-2022-09-19-XCWjyz.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;当时是如何本地调试？都是研发自己电脑装个 Mysql，装个 Tomcat，自己电脑运行调试，好处嘛就是后端研发互不干扰，想怎么改就怎么改，APP端研发就直连后端的笔记本调试。上线部署嘛就是一个研发手动编译个 Jar 包丢到云服务器上面，大体就是个草台班子，能干活，但是也就那样。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>如何使用 KubeSeal 高效加密和管理 Kubernetes 集群的 Secret</title>
    <link href="https://www.hi-linux.com/posts/35875.html"/>
    <id>https://www.hi-linux.com/posts/35875.html</id>
    <published>2022-09-23T01:00:00.000Z</published>
    <updated>2022-09-23T01:21:21.530Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>在 K8s 的管理过程中，像 Secret 这种资源并不好维护，KubeSeal 提供了一种相对简单的方式来对原始 Secret 资源进行加密，并通过控制器进行解密，以此来规避 Secret 泄露风险。</p><a id="more"></a><h2><span id="安装">安装</span></h2><h3><span id="安装-kubeseal">安装 <code>KubeSeal</code></span></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.18.0/kubeseal-0.18.0-linux-amd64.tar.gz</span><br><span class="line">$ tar -xvf kubeseal-0.18.0-linux-amd64.tar.gz</span><br><span class="line">$ cp kubeseal /usr/<span class="built_in">local</span>/bin/</span><br><span class="line">$ kubeseal --version</span><br></pre></td></tr></table></figure><h3><span id="安装controller">安装<code>controller</code></span></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.18.0/controller.yaml</span><br></pre></td></tr></table></figure><p>执行上述命令之后会在 <code>kube-system</code> 命名空间下启动一个控制器 Pod：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ k get pod -n kube-system |grep seal</span><br><span class="line">sealed-secrets-controller-b9fb75d85-k4csm    1/1     Running   0          7h28m</span><br></pre></td></tr></table></figure><p>Pod 启动之后，使用端口转发映射到本地：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system port-forward svc/sealed-secrets-controller 8080:8080</span><br></pre></td></tr></table></figure><h2><span id="使用方式">使用方式</span></h2><h3><span id="生成加密文件">生成加密文件</span></h3><p>首先在本地创建一个名为 <code>secret-example.yaml</code> 的文件，编码前的 <code>secret</code> 字段为：<code>mysupersecret</code></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">secret-example</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">secret:</span> <span class="string">bXlzdXBlcnNlY3JldAo=</span></span><br></pre></td></tr></table></figure><p>使用如下命令将 <code>secret-example.yaml</code>，转换为加密后的文件 <code>sealed-secret-example.yaml</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubeseal --secret-file secret-example.yaml --sealed-secret-file sealed-secret-example.yaml</span><br></pre></td></tr></table></figure><p><code>sealed-secret-example.yaml</code> 的内容如下，<code>spec.encryptedData.secret</code> 就是加密后的内容：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">bitnami.com/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">SealedSecret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">secret-example</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">encryptedData:</span></span><br><span class="line">    <span class="attr">secret:</span> <span class="string">AgB1ZZg8+J+0HLymOQZdTfWVQZiNkhm5X6WULJuBAAEaQQNhM8i2TV2I1SgKT4sUOCRv90XA1oeFld3XoGPjvYE3leOD1cvK1dDVqno6mNLRziokISk/9fB3cVE2GVgyCud//M53xNpVemDufgsJS2q/KGIOeNEijk9ZM2FaKoLDwtPaVNL0NfmC2xne2XtWJp+/eMOREhbubQhnj5M/Se75axazviuDNf6Ss9fAuR38Msd5DXnKBtyrckEHSa8TDn8ErssOh0ogX14e0/ThN3EWJecSBtx7Xfd0m90+vjmvWevMag442349aquR/qLo0mg40mhcCqSBw/MjaIGZ2F5XRufG1WEP43OgLMTixN2lLSU3eYTrv5t075taI9WJgoOl0DD8UA74EMpX7RMKTiXD6C0XngKmMKg5fUK7JNLFfwHMRPi4zNTwJa9ViDyD0iAJrGGbmMso/nHEtwOtrLE5Rrf0kLQ5N6Lj57gOBdqu903/vDM4Jm695GvEWL2aR3ShOxasHCuZeXj8Q5+KYWeF9sySiJH8bwEtaw6x7j9AxBOwjxWYD0Jvj9KhtlqBa4okSDc3bcgRKGhsSXQx6jOumI5rj+V542hkB6Z8JOtJ17VmzR6XDQDmqSl1FqqwKD5n5yUy5Kf6pJYBnsgKn3TzesQ6JfQbyRLTh1Pn3odOYCnp+Ixbd0Tgn0n5m0KO3RX0hiwGoe0hObIZcsF36g==</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">data:</span> <span class="literal">null</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">secret-example</span></span><br><span class="line">      <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><p>可以将加密后的文件保存到 Gitlab。</p><p>创建加密文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ k create -f sealed-secret-example.yaml</span><br><span class="line">sealedsecret.bitnami.com/secret-example created</span><br><span class="line"></span><br><span class="line">$ k get sealedsecrets.bitnami.com</span><br><span class="line">NAME             AGE</span><br><span class="line">secret-example   6s</span><br></pre></td></tr></table></figure><p>在创建完加密文件之后，Controller 会解密并生成对应的 <code>secret</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ k get secrets |grep secret-example</span><br><span class="line">secret-example                                   Opaque                                1      2m15s</span><br></pre></td></tr></table></figure><p>查看由 Controller 生成的 <code>secret</code> 资源内容，可以看到 <code>data.secret</code> 与上面创建的 <code>secret-example.yaml</code> 文件内容一致：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">k</span> <span class="string">get</span> <span class="string">secret</span> <span class="string">secret-example</span> <span class="string">-oyaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">secret:</span> <span class="string">bXlzdXBlcnNlY3JldAo=</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">creationTimestamp:</span> <span class="string">"2022-06-10T00:50:40Z"</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">secret-example</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">ownerReferences:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiVersion:</span> <span class="string">bitnami.com/v1alpha1</span></span><br><span class="line">    <span class="attr">controller:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">SealedSecret</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">secret-example</span></span><br><span class="line">    <span class="attr">uid:</span> <span class="string">57a5b691-9bb5-4dac-800a-1a1baa878299</span></span><br><span class="line">  <span class="attr">resourceVersion:</span> <span class="string">"675560"</span></span><br><span class="line">  <span class="attr">uid:</span> <span class="string">e0db31ad-082b-4596-9fd0-28cc810d86f4</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br></pre></td></tr></table></figure><blockquote><p>注：<code>SealedSecret</code> 和对应的 <code>secret</code> 资源必须位于相同的命名空间</p></blockquote><h3><span id="tips">TIPs</span></h3><ul><li><p><code>kubeseal</code> 支持如下<a href="https://github.com/bitnami-labs/sealed-secrets/blob/main/cmd/controller/server.go#L40" target="_blank" rel="noopener">API</a>：</p><table><thead><tr><th>Route</th><th>Description</th></tr></thead><tbody><tr><td>/healthz</td><td>Health check route useful for the readiness and liveness probes and for creating an external probe; for example with blackbox exporter.</td></tr><tr><td>/metrics</td><td>Endpoint for the Prometheus to retrieve the controller’s metrics.</td></tr><tr><td>/v1/verify</td><td>Validates a secret.</td></tr><tr><td>/v1/rotate</td><td>Rotates the secret.</td></tr><tr><td>/v1/cert.pem</td><td>Retrieves the public certificate.</td></tr></tbody></table></li><li><p>上例中 Controller 用的证书是自己生成的，还可以<a href="https://github.com/bitnami-labs/sealed-secrets/blob/main/docs/bring-your-own-certificates.md" target="_blank" rel="noopener">指定自己的证书</a>，更方便迁移和管理</p></li><li><p>使用 <code>KubeSeal</code> 可能会有一种困惑，如果用户直接挂载其他命名空间的 secret，那么这样可能会导致 secret 泄露。官方对此有作<a href="https://github.com/bitnami-labs/sealed-secrets#scopes" target="_blank" rel="noopener">解释</a>，如可以通过 RBAC 限制用户可以访问的命名空间以及资源类型。更多参见<a href="https://github.com/bitnami-labs/sealed-secrets#sealed-secrets-for-kubernetes" target="_blank" rel="noopener">README</a></p></li></ul><h3><span id="参考">参考</span></h3><ul><li><a href="https://carlosalca.medium.com/how-to-manage-all-my-k8s-secrets-in-git-securely-with-bitnami-sealed-secrets-43580b8fa0c7" target="_blank" rel="noopener">How to manage all my K8s secrets in git securely with Bitnami Sealed Secrets</a></li></ul><blockquote><p>本文转载自：「 博客园 」，原文：<a href="https://url.hi-linux.com/vakJR" target="_blank" rel="noopener">https://url.hi-linux.com/vakJR</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 K8s 的管理过程中，像 Secret 这种资源并不好维护，KubeSeal 提供了一种相对简单的方式来对原始 Secret 资源进行加密，并通过控制器进行解密，以此来规避 Secret 泄露风险。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>图解 Kubernetes 网络流量流转路径</title>
    <link href="https://www.hi-linux.com/posts/13487.html"/>
    <id>https://www.hi-linux.com/posts/13487.html</id>
    <published>2022-09-20T01:00:00.000Z</published>
    <updated>2022-09-20T06:07:22.929Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><blockquote><p>本文翻译自 <a href="https://learnk8s.io/kubernetes-network-packets%EF%BC%8C%E5%B9%B6%E6%B2%A1%E6%9C%89%E9%80%90%E5%AD%97%E7%BF%BB%E8%AF%91%EF%BC%8C%E5%B8%A6%E5%85%A5%E4%BA%86%E4%BA%9B%E8%87%AA%E5%B7%B1%E7%9A%84%E7%90%86%E8%A7%A3%E3%80%82" target="_blank" rel="noopener">https://learnk8s.io/kubernetes-network-packets，并没有逐字翻译，带入了些自己的理解。</a></p></blockquote><p><img src="https://img.hi-linux.com/staticfile/k8s-network-1-2022-09-19-c7m4AB.svg" alt></p><p>阅读本文，你可以了解在 Kubernetes 内外，数据包是如何转发的，从原始的 Web 请求开始，到托管应用程序的容器。</p><h2><span id="kubernetes-网络要求">Kubernetes 网络要求</span></h2><p>在深入了解在 Kubernetes 集群中数据包如何流转的细节之前，先明确一下 Kubernetes 对网络的要求。</p><p>Kubernetes 网络模型定义了一组基本规则：</p><ul><li>在不使用网络地址转换 (NAT) 的情况下，集群中的 Pod 能够与任意其他 Pod 进行通信。</li><li>在不使用网络地址转换 (NAT) 的情况下，在集群节点上运行的程序能与同一节点上的任何 Pod 进行通信。</li><li>每个 Pod 都有自己的 IP 地址（IP-per-Pod），并且任意其他 Pod 都可以通过相同的这个地址访问它。</li></ul><p>这些要求，不会将具体实现限制在某种解决方案上。</p><a id="more"></a><p>相反，它们笼统地描述了集群网络的特性。</p><p>为了满足这些限制，你必须解决以下挑战:</p><ol><li>如何确保同一个 Pod 中的容器行为就像它们在同一个主机上一样？</li><li>集群中的 Pod 能否访问其他 Pod？</li><li>Pod 可以访问服务吗？服务是负载均衡的吗？</li><li>Pod 可以接收集群外部的流量吗？</li></ol><p>在本文中，将重点关注前三点，从 Pod 内的网络，容器到容器的通信说起。</p><h2><span id="linux-网络命名空间如何在-pod-中工作">Linux 网络命名空间如何在 Pod 中工作</span></h2><p>让我们来看一个运行应用的主容器和伴随一起的另一个容器。</p><p>在示例中，有一个带有 nginx 和 busybox 容器的 Pod:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: multi-container-Pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: container-1</span><br><span class="line">      image: busybox</span><br><span class="line">      command: [&#39;&#x2F;bin&#x2F;sh&#39;, &#39;-c&#39;, &#39;sleep 1d&#39;]</span><br><span class="line">    - name: container-2</span><br><span class="line">      image: nginx</span><br></pre></td></tr></table></figure><p>部署时，会发生以下事情：</p><ol><li>Pod 在节点上拥有独立的网络命名空间。</li><li>分配一个 IP 地址给 Pod ，两个容器之间共享端口。</li><li>两个容器共享相同的网络命名空间，并在本地彼此可见。</li></ol><p>网络配置在后台迅速完成。</p><p>但是，让我们退后一步，尝试理解为什么运行容器需要上述动作。</p><p><a href="https://iximiuz.com/en/posts/container-networking-is-simple/" target="_blank" rel="noopener">在 Linux 中，网络命名空间是独立的、隔离的逻辑空间。</a></p><p>你可以将网络命名空间视为，将物理网络接口分割小块之后的独立部分。</p><p>每个部分都可以单独配置，并拥有自己的网络规则和资源。</p><p>这些包括防火墙规则、接口（虚拟的或物理的）、路由以及与网络相关的所有内容。</p><ol><li>物理网络接口持有根网络命名空间。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-2-2022-09-19-2lGj37.svg" alt></p><ol start="2"><li>你可以使用 Linux 网络命名空间来创建独立的网络。每个网络都是独立的，除非你进行配置，默认不会与其他网络互通。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-3-2022-09-19-0Iv3p7.svg" alt></p><p>但最终，还是需要物理接口处理所有真实的数据包，所有虚拟接口都是基于物理接口创建的。</p><p>网络命名空间可以通过 <a href="https://man7.org/linux/man-pages/man8/ip-netns.8.html" target="_blank" rel="noopener">ip-netns</a> 进行管理，使用 <code>ip netns list</code> 可以列出主机上的命名空间。</p><blockquote><p>需要注意的是，创建的网络命名空间会出现在 <code>/var/run/netns</code> 下面，但 Docker 并没有遵循这一规则。</p></blockquote><p>例如，这是 Kubernetes 节点的一些命名空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ip netns list</span><br><span class="line"></span><br><span class="line">cni-0f226515-e28b-df13-9f16-dd79456825ac (id: 3)</span><br><span class="line">cni-4e4dfaac-89a6-2034-6098-dd8b2ee51dcd (id: 4)</span><br><span class="line">cni-7e94f0cc-9ee8-6a46-178a-55c73ce58f2e (id: 2)</span><br><span class="line">cni-7619c818-5b66-5d45-91c1-1c516f559291 (id: 1)</span><br><span class="line">cni-3004ec2c-9ac2-2928-b556-82c7fb37a4d8 (id: 0)</span><br></pre></td></tr></table></figure><blockquote><p>注意 cni- 前缀；这意味着命名空间是由 CNI 插件创建的。</p></blockquote><p>当你创建一个 Pod，Pod 被分配给一个节点后，CNI 将：</p><ol><li>分配 IP 地址。</li><li>将容器连接到网络。</li></ol><p>如果 Pod 包含多个容器，那么这些容器都将被放在同一个命名空间中。</p><ol><li>当创建 Pod 时，容器运行时会给容器创建一个网络命名空间。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-4-20220919141522939-2022-09-19-nMCwoz.svg" alt></p><ol start="2"><li>然后 CNI 负责给 Pod 分配一个 IP 地址。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-5-2022-09-19-sBtEl3.svg" alt></p><ol start="3"><li>最后 CNI 将容器连接到网络的其余部分。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-6-2022-09-19-MiJzyw.svg" alt></p><p>那么，当你列出节点上的容器的命名空间会发生什么呢？</p><p>你可以通过 SSH 连接到 Kubernetes 节点并查看命名空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lsns -t net</span><br><span class="line"></span><br><span class="line">        NS TYPE NPROCS   PID USER     NETNSID NSFS                           COMMAND</span><br><span class="line">4026531992 net     171     1 root  unassigned &#x2F;run&#x2F;docker&#x2F;netns&#x2F;default      &#x2F;sbin&#x2F;init noembed norestore</span><br><span class="line">4026532286 net       2  4808 65535          0 &#x2F;run&#x2F;docker&#x2F;netns&#x2F;56c020051c3b &#x2F;pause</span><br><span class="line">4026532414 net       5  5489 65535          1 &#x2F;run&#x2F;docker&#x2F;netns&#x2F;7db647b9b187 &#x2F;pause</span><br></pre></td></tr></table></figure><p><code>lsns</code> 是一个用于列出主机上所有可用命名空间的命令。</p><blockquote><p>请记住，Linux 中有<a href="https://man7.org/linux/man-pages/man7/namespaces.7.html" target="_blank" rel="noopener">多种命名空间类型</a>。</p></blockquote><p>Nginx 容器在哪里？</p><p>那些 pause 容器是什么？</p><h2><span id="在-pod-中pause-容器创建了网络命名空间">在 Pod 中，pause 容器创建了网络命名空间</span></h2><p>先列出节点上的所有命名空间，看看能否找到 Nginx 容器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ lsns</span><br><span class="line">        NS TYPE   NPROCS   PID USER            COMMAND</span><br><span class="line"># truncated output</span><br><span class="line">4026532414 net         5  5489 65535           &#x2F;pause</span><br><span class="line">4026532513 mnt         1  5599 root            sleep 1d</span><br><span class="line">4026532514 uts         1  5599 root            sleep 1d</span><br><span class="line">4026532515 pid         1  5599 root            sleep 1d</span><br><span class="line">4026532516 mnt         3  5777 root            nginx: master process nginx -g daemon off;</span><br><span class="line">4026532517 uts         3  5777 root            nginx: master process nginx -g daemon off;</span><br><span class="line">4026532518 pid         3  5777 root            nginx: master process nginx -g daemon off;</span><br></pre></td></tr></table></figure><p>Nginx 容器在挂载 (<code>mnt</code>)、Unix time-sharing (<code>uts</code>) 和 PID (<code>pid</code>) 命名空间中，但不在网络命名空间 (<code>net</code>) 中。</p><p>不幸的是，<code>lsns</code> 只显示每个进程最小的 PID，但你可以根据这个进程 ID 进一步过滤。</p><p>使用以下命令，在所有命名空间中检索 Nginx 容器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo lsns -p 5777</span><br><span class="line"></span><br><span class="line">       NS TYPE   NPROCS   PID USER  COMMAND</span><br><span class="line">4026531835 cgroup    178     1 root  &#x2F;sbin&#x2F;init noembed norestore</span><br><span class="line">4026531837 user      178     1 root  &#x2F;sbin&#x2F;init noembed norestore</span><br><span class="line">4026532411 ipc         5  5489 65535 &#x2F;pause</span><br><span class="line">4026532414 net         5  5489 65535 &#x2F;pause</span><br><span class="line">4026532516 mnt         3  5777 root  nginx: master process nginx -g daemon off;</span><br><span class="line">4026532517 uts         3  5777 root  nginx: master process nginx -g daemon off;</span><br><span class="line">4026532518 pid         3  5777 root  nginx: master process nginx -g daemon off;</span><br></pre></td></tr></table></figure><p><code>pause</code> 进程再次出现，它劫持了网络命名空间。</p><p>这是怎么回事？</p><p><em><strong>集群中的每个 Pod 都有一个额外的隐藏容器在后台运行，称为 pause 容器。</strong></em></p><p>列出在节点上运行的容器并获取 pause 容器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps | grep pause</span><br><span class="line"></span><br><span class="line">fa9666c1d9c6   k8s.gcr.io&#x2F;pause:3.4.1  &quot;&#x2F;pause&quot;  k8s_POD_kube-dns-599484b884-sv2js…</span><br><span class="line">44218e010aeb   k8s.gcr.io&#x2F;pause:3.4.1  &quot;&#x2F;pause&quot;  k8s_POD_blackbox-exporter-55c457d…</span><br><span class="line">5fb4b5942c66   k8s.gcr.io&#x2F;pause:3.4.1  &quot;&#x2F;pause&quot;  k8s_POD_kube-dns-599484b884-cq99x…</span><br><span class="line">8007db79dcf2   k8s.gcr.io&#x2F;pause:3.4.1  &quot;&#x2F;pause&quot;  k8s_POD_konnectivity-agent-84f87c…</span><br></pre></td></tr></table></figure><p>可以看到，节点上的每一个 Pod 都会有一个对应的 pause 容器。</p><p>这个 <code>pause</code> 容器负责创建和维持网络命名空间。</p><p><a href="https://www.aquasec.com/cloud-native-academy/container-security/container-runtime/" target="_blank" rel="noopener">底层容器运行时</a>会完成网络命名空间的创建，通常是由 <code>containerd</code> 或 <code>CRI-O</code> 完成。</p><p>在部署 Pod 和创建容器之前，由运行时创建网络命名空间。</p><p>容器运行时会自动完成这些，不需要手工执行 <code>ip netns</code> 创建命名空间。</p><p>话题回到 pause 容器。</p><p>它包含非常少的代码，并且在部署后立即进入睡眠状态。</p><p>但是，<a href="https://www.ianlewis.org/en/almighty-pause-container" target="_blank" rel="noopener">它是必不可少的，并且在 Kubernetes 生态系统中起着至关重要的作用</a>。</p><ol><li>创建 Pod 时，容器运行时会创建一个带有睡眠容器的网络命名空间。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-7-20220919141528218-2022-09-19-7EsuRT.svg" alt></p><ol start="2"><li>Pod 中的其他容器都会加入由 pause 容器创建的网络名称空间。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-8-20220919141535160-2022-09-19-JOFs8C.svg" alt></p><ol start="3"><li>此时，CNI 分配 IP 地址并将容器连接到网络。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-9-2022-09-19-Wxey6q.svg" alt></p><p>一个进入睡眠状态的容器有什么用？</p><p>为了理解它的用途，让我们想象一个 Pod 有两个容器，就像前面的例子一样，但没有 pause 容器。</p><p>一旦容器启动，CNI 将会：</p><ol><li>使 busybox 容器加入之前的网络命名空间。</li><li>分配 IP 地址。</li><li>将容器连接到网络。</li></ol><p>如果 Nginx 崩溃了怎么办？</p><p>CNI 将不得不再次执行所有步骤，并且两个容器的网络都将中断。</p><p>由于睡眠容器不太可能有任何错误，因此创建网络命名空间通常是一种更安全、更健壮的选择。</p><blockquote><p>如果 Pod 中的一个容器崩溃了，剩下的仍然可以回复其他网络请求。</p></blockquote><h2><span id="分配一个-ip-地址给-pod">分配一个 IP 地址给 Pod</span></h2><p>前面我提到 Pod 和两个容器将具有同一个 IP 地址。</p><p>那是怎样配置的呢？</p><blockquote><p>在 Pod 网络命名空间内，创建了一个接口，并分配了一个 IP 地址。</p></blockquote><p>让我们验证一下。</p><p>首先，找到 Pod 的 IP 地址：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get Pod multi-container-Pod -o jsonpath&#x3D;&#123;.status.PodIP&#125;</span><br><span class="line"></span><br><span class="line">10.244.4.40</span><br></pre></td></tr></table></figure><p>接下来，找到相关的网络命名空间。</p><p>由于网络命名空间是从物理接口创建的，需要先访问集群节点。</p><blockquote><p>如果你运行的是 minikube，使用 <code>minikube ssh</code> 访问节点。如果在云厂中运行，那么应该有某种方法可以通过 SSH 访问节点。</p></blockquote><p>进入后，找到最新创建的命名网络命名空间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ls -lt &#x2F;var&#x2F;run&#x2F;netns</span><br><span class="line"></span><br><span class="line">total 0</span><br><span class="line">-r--r--r-- 1 root root 0 Sep 25 13:34 cni-0f226515-e28b-df13-9f16-dd79456825ac</span><br><span class="line">-r--r--r-- 1 root root 0 Sep 24 09:39 cni-4e4dfaac-89a6-2034-6098-dd8b2ee51dcd</span><br><span class="line">-r--r--r-- 1 root root 0 Sep 24 09:39 cni-7e94f0cc-9ee8-6a46-178a-55c73ce58f2e</span><br><span class="line">-r--r--r-- 1 root root 0 Sep 24 09:39 cni-7619c818-5b66-5d45-91c1-1c516f559291</span><br><span class="line">-r--r--r-- 1 root root 0 Sep 24 09:39 cni-3004ec2c-9ac2-2928-b556-82c7fb37a4d8</span><br></pre></td></tr></table></figure><p>在示例中，就是 <code>cni-0f226515-e28b-df13-9f16-dd79456825ac</code>。然后，可以在该命名空间内运行 <code>exec</code> 命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip a</span><br><span class="line"></span><br><span class="line"># output truncated</span><br><span class="line">3: eth0@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default</span><br><span class="line">    link&#x2F;ether 16:a4:f8:4f:56:77 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 10.244.4.40&#x2F;32 brd 10.244.4.40 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::14a4:f8ff:fe4f:5677&#x2F;64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>这个 IP 就是 Pod 的 IP 地址！通过查找 @if12 中的 12 找到网络接口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ip link | grep -A1 ^12</span><br><span class="line"></span><br><span class="line">12: vethweplb3f36a0@if16: mtu 1376 qdisc noqueue master weave state UP mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether 72:1c:73:d9:d9:f6 brd ff:ff:ff:ff:ff:ff link-netnsid 1</span><br></pre></td></tr></table></figure><p>你还可以验证 Nginx 容器是否监听了来自该命名空间内的 HTTP 流量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac netstat -lnp</span><br><span class="line"></span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID&#x2F;Program name</span><br><span class="line">tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      692698&#x2F;nginx: master</span><br><span class="line">tcp6       0      0 :::80                   :::*                    LISTEN      692698&#x2F;nginx: master</span><br></pre></td></tr></table></figure><blockquote><p>如果你无法通过 SSH 访问集群中的工作节点，你可以使用 <code>kubectl exec</code> 获取到 busybox 容器的 shell 并直接在内部使用 <code>ip</code> 和 <code>netstat</code> 命令。</p></blockquote><p>刚刚我们介绍了容器之间的通信，再来看看如何建立 Pod 到 Pod 的通信吧。</p><h2><span id="查看集群中-pod-到-pod-的流量">查看集群中 Pod 到 Pod 的流量</span></h2><p>Pod 到 Pod 的通信有两种可能的情况：</p><ol><li>Pod 流量的目的地是同一节点上的 Pod。</li><li>Pod 流量的目的地是在不同节点上的 Pod。</li></ol><p>整个工作流依赖于虚拟接口对和网桥，下面先来了解一下这部分的内容。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">为了让一个 Pod 与其他 Pod 通信，它必须先访问节点的根命名空间。</span><br></pre></td></tr></table></figure><p>通过虚拟以太网对来实现 Pod 和根命名空间的连接。</p><p>这些虚拟接口设备（veth 中的 v）连接并充当两个命名空间之间的隧道。</p><p>使用此 <code>veth</code> 设备，你将一端连接到 Pod 的命名空间，另一端连接到根命名空间。</p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-10-2022-09-19-zqzPW6.svg" alt></p><p>CNI 可以帮你执行这些操作，但你也可以手动执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ip link add veth1 netns Pod-namespace type veth peer veth2 netns root</span><br></pre></td></tr></table></figure><p>现在 Pod 的命名空间有一个可以访问根命名空间的 <code>隧道</code>。</p><p>节点上，新建的每一个 Pod 都会设置这样的 <code>veth</code> 对。</p><p>一个是，创建接口对；另一个是为以太网设备分配地址并配置默认路由。</p><p>下面看看如何在 Pod 的命名空间中设置 <code>veth1</code> 接口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip addr add 10.244.4.40&#x2F;24 dev veth1</span><br><span class="line">$ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip link set veth1 up</span><br><span class="line">$ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip route add default via 10.244.4.40</span><br></pre></td></tr></table></figure><p>在节点上，让我们创建另一个 <code>veth2</code> 对：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ip addr add 169.254.132.141&#x2F;16 dev veth2</span><br><span class="line">$ ip link set veth2 up</span><br></pre></td></tr></table></figure><p>可以像前面一样检查现有的 <code>veth</code> 对。</p><p>在 Pod 的命名空间中，检索 <code>eth0</code> 接口的后缀。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ip netns exec cni-0f226515-e28b-df13-9f16-dd79456825ac ip link show type veth</span><br><span class="line"></span><br><span class="line">3: eth0@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether 16:a4:f8:4f:56:77 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br></pre></td></tr></table></figure><p>在这种情况下，可以使用命令 <code>grep -A1 ^12</code> 查找（或滚动到目标所在处）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ip link show type veth</span><br><span class="line"></span><br><span class="line"># output truncated</span><br><span class="line">12: cali97e50e215bd@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP mode DEFAULT group default</span><br><span class="line">    link&#x2F;ether ee:ee:ee:ee:ee:ee brd ff:ff:ff:ff:ff:ff link-netns cni-0f226515-e28b-df13-9f16-dd79456825ac</span><br></pre></td></tr></table></figure><blockquote><p>也可以使用 <code>ip -n cni-0f226515-e28b-df13-9f16-dd79456825ac link show type veth</code>.命令</p></blockquote><p>注意 <code>3: eth0@if12和12: cali97e50e215bd@if3</code> 接口上的符号。</p><p>从 Pod 命名空间，该 <code>eth0</code> 接口连接到根命名空间的 12 号接口，因此是 <code>@if12</code>.</p><p>在 <code>veth</code> 对的另一端，根命名空间连接到 Pod 命名空间的 3 号接口。</p><p>接下来是连接 <code>veth</code> 对两端的桥接器。</p><h2><span id="pod-网络命名空间连接到以太网桥">Pod 网络命名空间连接到以太网桥</span></h2><p>网桥会汇聚位于根命名空间中的每一个虚拟接口。这个网桥允许虚拟 pair 之间的流量，也允许穿过公共根命名空间的流量。</p><p>补充一下相关原理。</p><p>以太网桥位于 <a href="https://en.wikipedia.org/wiki/OSI_model" target="_blank" rel="noopener">OSI 网络模型</a> 的第 2 层。</p><p>你可以将网桥视为接受来自不同命名空间和接口的连接的虚拟交换机。</p><p><a href="https://ops.tips/blog/using-network-namespaces-and-bridge-to-isolate-servers/" target="_blank" rel="noopener">以太网桥可以连接节点上的多个可用网络。</a></p><p>因此，可以使用网桥连接两个接口，即 Pod 命名空间的 <code>veth</code> 连接到同一节点上另一个 Pod 的 <code>veth</code>。</p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-11-20220919141543937-2022-09-19-lVtOgN.svg" alt></p><p>接下来，继续看网桥和 veth 对的用途。</p><h2><span id="跟踪在同一节点上-pod-到-pod-的流量">跟踪在同一节点上 Pod 到 Pod 的流量</span></h2><p>假设同一个节点上有两个 Pod，Pod-A 向 Pod-B 发送消息。</p><ol><li>由于访问目标不在同一个命名空间，Pod-A 将数据包发送到其默认接口 eth0。 这个接口与 veth 对的一端绑定，作为隧道。这样，数据包会被转发到节点上的根命名空间。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-12-20220919141632527-2022-09-19-NRPgWl.svg" alt></p><ol start="2"><li>以太网网桥作为一个虚拟交换机，需要目标 Pod-B 的 MAC 地址才能工作。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-13-2022-09-19-M7QRru.svg" alt></p><ol start="3"><li>ARP 协议会解决这个问题。当帧到达网桥时，会向所有连接的设备发送 ARP 广播。网桥广播询问持有 Pod-B 的 IP 地址</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-14-2022-09-19-Ibitey.svg" alt></p><ol start="4"><li>此时会收到一个带有 Pod-B IP 的 MAC 地址应答，这条消息会被存储在桥接 ARP 缓存(查找表)中。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-15-2022-09-19-y5ldCP.svg" alt></p><ol start="5"><li>IP 地址和 MAC 地址的映射关系存储之后，网桥就在表中查找，并将数据包转发到正确的端点。数据包到达根命名空间内 Pod-B 的 veth 之后，很快又到达 Pod-B 命名空间内的 eth0 接口。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-16-20220919141551029-2022-09-19-DusApt.svg" alt></p><p>至此，Pod-A 和 Pod-B 之间的通信就成功了。</p><h2><span id="跟踪不同节点上的-pod-到-pod-通信">跟踪不同节点上的 Pod 到 Pod 通信</span></h2><p>对于跨节点 Pod 之间的通信，会经过额外的通信跳跃。</p><ol><li>前几个步骤保持不变，直到数据包到达根命名空间并需要发送到 Pod-B。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-17-2022-09-19-xXNQzL.svg" alt></p><ol start="2"><li>当目的 IP 不在本地网络中时，报文被转发到节点的默认网关。节点的出口网关或默认网关，通常位于节点与网络相连的物理接口 eth0 上。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-18-2022-09-19-h2DI4L.svg" alt></p><p>此时 不会发生 ARP 解析，因为源 IP 和目标 IP 不在同一个网段中。</p><p>网段的检查是使用按位运算完成的。</p><p>当目的 IP 不在当前网络段时，数据包被转发到节点的默认网关。</p><h2><span id="按位运算的工作原理">按位运算的工作原理</span></h2><p>在确定数据包的转发位置时，源节点必须执行位运算</p><p><a href="https://en.wikipedia.org/wiki/Bitwise_operation#AND" target="_blank" rel="noopener">这也称为与操作。</a></p><p>复习一下，按位与运算的规则：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0 AND 0 &#x3D; 0</span><br><span class="line">0 AND 1 &#x3D; 0</span><br><span class="line">1 AND 0 &#x3D; 0</span><br><span class="line">1 AND 1 &#x3D; 1</span><br></pre></td></tr></table></figure><p>除了 1 与 1 以外的都是 false。</p><p>如果源节点的 IP 为 192.168.1.1，子网掩码为 /24，目标 IP 为 172.16.1.1/16，则按位与运算将得知它们位于不同的网段上。</p><p>这意味着目标 IP 与数据包的源不在同一个网络上，数据包将通过默认网关转发。</p><p>数学时间。</p><p>我们必须从二进制的 32 位地址开始进行 AND 操作。</p><p>先找出源 IP 网络和目标 IP 网段。</p><table><thead><tr><th style="text-align:left">Type</th><th style="text-align:left">Binary</th><th style="text-align:left">Converted</th></tr></thead><tbody><tr><td style="text-align:left">Src. IP Address</td><td style="text-align:left">11000000.10101000.00000001.00000001</td><td style="text-align:left">192.168.1.1</td></tr><tr><td style="text-align:left">Src. Subnet Mask</td><td style="text-align:left">11111111.11111111.11111111.00000000</td><td style="text-align:left">255.255.255.0(/24)</td></tr><tr><td style="text-align:left">Src. Network</td><td style="text-align:left">11000000.10101000.00000001.00000000</td><td style="text-align:left">192.168.1.0</td></tr><tr><td style="text-align:left"></td><td style="text-align:left"></td><td style="text-align:left"></td></tr><tr><td style="text-align:left">Dst. IP Address</td><td style="text-align:left">10101100.00010000.00000001.00000001</td><td style="text-align:left">172.16.1.1</td></tr><tr><td style="text-align:left">Dst. Subnet Mask</td><td style="text-align:left">11111111.11111111.00000000.00000000</td><td style="text-align:left">255.255.0.0(/16)</td></tr><tr><td style="text-align:left">Dst. Network</td><td style="text-align:left">10101100.00010000.00000000.00000000</td><td style="text-align:left">172.16.0.0</td></tr></tbody></table><p>按位运算之后，需要将目标 IP 与数据包源节点的子网进行比较。</p><table><thead><tr><th style="text-align:left">Type</th><th style="text-align:left">Binary</th><th style="text-align:left">Converted</th></tr></thead><tbody><tr><td style="text-align:left">Dst. IP Address</td><td style="text-align:left">10101100.00010000.00000001.00000001</td><td style="text-align:left">172.16.1.1</td></tr><tr><td style="text-align:left">Src. Subnet Mask</td><td style="text-align:left">11111111.11111111.11111111.00000000</td><td style="text-align:left">255.255.255.0(/24)</td></tr><tr><td style="text-align:left">Network Result</td><td style="text-align:left">10101100.00010000.00000001.00000000</td><td style="text-align:left">172.16.1.0</td></tr></tbody></table><p>运算的结果是 172.16.1.0，不等于 192.168.1.0（源节点的网络）。说明源 IP 地址和目标 IP 地址不在同一个网络上。</p><p>如果目标 IP 是 192.168.1.2，即与发送 IP 在同一子网中，则 AND 操作将得到节点的本地网络。</p><table><thead><tr><th style="text-align:left">Type</th><th style="text-align:left">Binary</th><th style="text-align:left">Converted</th></tr></thead><tbody><tr><td style="text-align:left">Dst. IP Address</td><td style="text-align:left">11000000.10101000.00000001.00000010</td><td style="text-align:left">192.168.1.2</td></tr><tr><td style="text-align:left">Src. Subnet Mask</td><td style="text-align:left">11111111.11111111.11111111.00000000</td><td style="text-align:left">255.255.255.0(/24)</td></tr><tr><td style="text-align:left">Network</td><td style="text-align:left">11000000.10101000.00000001.00000000</td><td style="text-align:left">192.168.1.0</td></tr></tbody></table><p>进行逐位比较后，ARP 通过查找表查找默认网关的 MAC 地址。</p><p>如果有条目，将立即转发数据包。</p><p>否则，先进行广播以找到网关的 MAC 地址。</p><ol><li>现在，数据包路由到另一个节点的默认接口，我们称为 Node-B。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-19-2022-09-19-O2NAMf.svg" alt></p><ol start="2"><li>以相反的顺序。现在，数据包位于 Node-B 的根命名空间，并到达网桥，这里会进行 ARP 解析。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-20-2022-09-19-sncbQT.svg" alt></p><ol start="3"><li>路由系统将返回与 Pod-B 相连的接口的 MAC 地址。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-21-2022-09-19-l9iVbm.svg" alt></p><ol start="4"><li>网桥通过 Pod-B 的 <code>veth</code> 设备转发帧，并到达 Pod-B 的命名空间。</li></ol><p><img src="https://img.hi-linux.com/staticfile/k8s-network-22-20220919141556537-2022-09-19-8ckOcx.svg" alt></p><p>至此，你应该已经熟悉了 Pod 之间的流量是如何流转的。下面，让我们花点时间来看看 CNI 如何管理上诉内容。</p><h2><span id="容器网络接口-cni">容器网络接口 - CNI</span></h2><p><a href="https://github.com/containernetworking/cni/blob/main/SPEC.md" target="_blank" rel="noopener">容器网络接口（CNI）主要关注的是当前节点中的网络。</a></p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-23-2022-09-19-PBGpbi.svg" alt></p><p>可以将 CNI 看作为解决 Kubernetes 网络需求，而遵循的一组规则。</p><p>有这些 CNI 实现可供使用：</p><ul><li><a href="https://www.tigera.io/project-calico/" target="_blank" rel="noopener">Calico</a></li><li><a href="https://cilium.io/" target="_blank" rel="noopener">Cillium</a></li><li><a href="https://github.com/flannel-io/flannel" target="_blank" rel="noopener">Flannel</a></li><li><a href="https://www.weave.works/docs/net/latest/overview/" target="_blank" rel="noopener">Weave Net</a></li><li>其他网络插件</li></ul><p>他们都遵循相同的 CNI 标准。</p><p>如果没有 CNI，你需要人工完成如下操作：</p><ul><li>创建接口。</li><li>创建 veth 对。</li><li>设置网络命名空间。</li><li>设置静态路由。</li><li>配置以太网桥。</li><li>分配 IP 地址。</li><li>创建 NAT 规则。</li><li>还有其他大量事情。</li></ul><p>这还不包括，在删除或重启 Pod 时，需要进行类似的全部操作。</p><p>CNI 必须支持<a href="https://github.com/containernetworking/cni/blob/main/SPEC.md#cni-operations" target="_blank" rel="noopener">四种不同的操作</a>：</p><ul><li>ADD - 向网络添加一个容器。</li><li>DEL - 从网络中删除一个容器。</li><li>CHECK - 如果容器的网络出现问题，则返回错误。</li><li>VERSION - 显示插件的版本。</li></ul><p>我们一起看下，CNI 是如何工作的。</p><p>当 Pod 被分配到特定节点时，Kubelet 自身不会初始化网络。</p><p>相反，Kubelet 将这个任务交给 CNI。</p><p><em>但是，Kubelet 以 JSON 格式指定配置并发送至 CNI 插件。</em></p><p>你可以进入节点上的 <code>/etc/cni/net.d</code> 文件夹，使用以下命令查看当前的 CNI 配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">$ cat 10-calico.conflist</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;: &quot;k8s-Pod-network&quot;,</span><br><span class="line">  &quot;cniVersion&quot;: &quot;0.3.1&quot;,</span><br><span class="line">  &quot;plugins&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;type&quot;: &quot;calico&quot;,</span><br><span class="line">      &quot;datastore_type&quot;: &quot;kubernetes&quot;,</span><br><span class="line">      &quot;mtu&quot;: 0,</span><br><span class="line">      &quot;nodename_file_optional&quot;: false,</span><br><span class="line">      &quot;log_level&quot;: &quot;Info&quot;,</span><br><span class="line">      &quot;log_file_path&quot;: &quot;&#x2F;var&#x2F;log&#x2F;calico&#x2F;cni&#x2F;cni.log&quot;,</span><br><span class="line">      &quot;ipam&quot;: &#123; &quot;type&quot;: &quot;calico-ipam&quot;, &quot;assign_ipv4&quot; : &quot;true&quot;, &quot;assign_ipv6&quot; : &quot;false&quot;&#125;,</span><br><span class="line">      &quot;container_settings&quot;: &#123;</span><br><span class="line">          &quot;allow_ip_forwarding&quot;: false</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;policy&quot;: &#123;</span><br><span class="line">          &quot;type&quot;: &quot;k8s&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;kubernetes&quot;: &#123;</span><br><span class="line">          &quot;k8s_api_root&quot;:&quot;https:&#x2F;&#x2F;10.96.0.1:443&quot;,</span><br><span class="line">          &quot;kubeconfig&quot;: &quot;&#x2F;etc&#x2F;cni&#x2F;net.d&#x2F;calico-kubeconfig&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;type&quot;: &quot;bandwidth&quot;,</span><br><span class="line">      &quot;capabilities&quot;: &#123;&quot;bandwidth&quot;: true&#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;&quot;type&quot;: &quot;portmap&quot;, &quot;snat&quot;: true, &quot;capabilities&quot;: &#123;&quot;portMappings&quot;: true&#125;&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每个 CNI 插件都会使用不同类型的网络配置。</p><p>例如，Calico 使用基于 BGP 的三层网络连接 Pod</p><p>Cilium 从三层到七层使用的是基于 eBPF 的 overlay 网络</p><p>与 Calico 一样，Cilium 也支持通过配置网络策略来限制流量。</p><p>那么你应该使用哪一个呢？主要有两类 CNI。</p><p>在第一类中，使用基本网络设置（也称为平面网络），从集群的 IP 池为 Pod 分配 IP 地址的 CNI。</p><p>这种方式可能很快耗尽 IP 地址，而成为负担。</p><p>相反，另一类是使用 overlay 网络。</p><p>简单来说，overlay 网络是主（底层）网络之上的重建网络。</p><p>overlay 网络通过封装来自底层网络的数据包工作，这些数据包被发送到另一个节点上的 Pod。</p><p>overlay 网络的一种流行技术是 VXLAN，它可以在 L3 网络上建立 L2 域的隧道。</p><p>那么哪个更好呢？</p><p>没有单一的答案，这取决于你的需求。</p><p>你是否正在构建具有数万个节点的大型集群？</p><p>也许 overlay 网络更好。</p><p>你是否在意更简单的配置和审查网络流量，而不会愿意在复杂网络中丢失这种能力？</p><p>扁平网络更适合你。</p><p>现在我们讨论完了 CNI，接着让我们来看看 Pod 到服务的通信是如何连接的。</p><h2><span id="检查-pod-到-service-的流量">检查 Pod 到 Service 的流量</span></h2><p>由于 Pod 在 Kubernetes 中是动态的，分配给 Pod 的 IP 地址不是静态的。</p><p>Pod 的 IP 是短暂的，每次创建或删除 Pod 时都会发生变化。</p><p>Kubernetes 中的 Service 解决了这个问题，为连接一组 Pod 提供了可靠的机制。</p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-24-2022-09-19-8P1MI3.svg" alt></p><p>默认情况下，在 Kubernetes 中创建 Service 时，<a href="https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies" target="_blank" rel="noopener">被分配一个虚拟 IP</a>。</p><p>在 Service 中，可以使用选择器将 Service 与目标 Pod 相关联。</p><p>当删除或添加一个 Pod 时会发生什么呢？</p><blockquote><p>Service 的虚拟 IP 保持静态不变。</p></blockquote><p>但流量可以再无需干预的情况下，到达新创建的 Pod。</p><p>换句话说，Kubernetes 中的 Service 类似于负载均衡器。</p><p>但它们是如何工作的？</p><h2><span id="使用-netfilter-和-iptables-拦截和重写流量">使用 Netfilter 和 Iptables 拦截和重写流量</span></h2><p>Kubernetes 中的 Service 是基于 Linux 内核中的两个组件构建的：</p><ol><li><a href="https://en.wikipedia.org/wiki/Netfilter" target="_blank" rel="noopener">网络过滤器</a></li><li><a href="https://en.wikipedia.org/wiki/Iptables" target="_blank" rel="noopener">iptables</a></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Netfilter 是一个可以配置数据包过滤、创建 NAT 、端口转发规则以及管理网络中流量的框架</span><br></pre></td></tr></table></figure><p>此外，它可以屏蔽和禁止未经同意的访问。</p><p>另一方面，iptables 是一个用户态程序，可以用来配置 Linux 内核防火墙的 IP 数据包过滤规则。</p><p>iptables 是作为不同的 Netfilter 模块实现的。</p><p>可以使用 iptables CLI 即时修改过滤规则，并将它们插入 netfilters 挂载点。</p><p>过滤器配置在不同的表中，其中包含用于处理网络流量数据包的链。</p><p>不同的协议使用不同的内核模块和程序。</p><blockquote><p>当提到 iptables 时，通常指的是 IPv4。对于 IPv6 ，终端工具是 ip6tables。</p></blockquote><p>iptables 有五种链，每一种链都直接映射到 Netfilter 的钩子上。</p><p>从 iptables 的角度来看，它们是：</p><ul><li><code>PRE_ROUTING</code></li><li><code>INPUT</code></li><li><code>FORWARD</code></li><li><code>OUTPUT</code></li><li><code>POST_ROUTING</code></li></ul><p>它们对应地映射到 Netfilter 钩子：</p><ul><li><code>NF_IP_PRE_ROUTING</code></li><li><code>NF_IP_LOCAL_IN</code></li><li><code>NF_IP_FORWARD</code></li><li><code>NF_IP_LOCAL_OUT</code></li><li><code>NF_IP_POST_ROUTING</code></li></ul><p>当一个数据包到达时，根据它所处的阶段，将 “触发” 一个 Netfilter 钩子。这个钩子会执行特定的 iptables 过滤规则。</p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-25-20220919141602921-2022-09-19-tsberQ.svg" alt></p><p>哎呀！看起来很复杂！</p><p>不过没什么好担心的。</p><p>这就是我们使用 Kubernetes 的原因，以上所有内容都是通过使用 Service 抽象出来的，并且一个简单的 YAML 定义可以自动设置这些规则。</p><p>如果你有兴趣查看 iptables 规则，可以连接到节点并运行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ iptables-save</span><br></pre></td></tr></table></figure><p>你还可以使用<a href="https://github.com/Nudin/iptable_vis" target="_blank" rel="noopener">这个工具来可视化</a>节点上的 iptables 链。</p><p>这是来自 GKE 节点上的可视化 iptables 链的示例图：</p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-26-2022-09-19-GirLZG.svg" alt></p><p>注意，这里可能配置了几百条规则，想想一下自己动手怎么配置！</p><p>至此，我们已经了解了，相同节点上的 Pod 和不同节点上 Pod 之间是如何通信的。</p><p>在 Pod 与 Service 的通信中，链路的前半部分是一样的。</p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-27-2022-09-19-zcNnij.svg" alt></p><p>当请求从 Pod-A 走向 Pod-B 时，由于 Pod-B 在 Service 的 “后面”，在传输的过程中，会有一些不一样。</p><p>原始的请求，在 Pod-A 命名空间的 eth0 接口发出。</p><p>接着，请求通过 <code>veth</code>到达根名称空间的网桥。</p><p>一旦到达网桥，数据包就会立即通过默认网关转发。</p><p>与 Pod-to-Pod 部分一样，主机进行按位比较。由于服务的虚拟 IP 不是节点 CIDR 的一部分，因此数据包将立即通过默认网关转发。</p><p>如果默认网关的 MAC 地址尚未出现在查找表中，则会进行 ARP 解析找出默认网关的 MAC 地址。</p><p>现在神奇的事情发生了。</p><p>在数据包通过节点的路由之前，Netfilter 的 <code>NF_IP_PRE_ROUTING</code> 挂钩被触发，并执行 iptables 规则。这个规则会修改 Pod-A 数据包的目标 IP 地址 DNAT。</p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-28-2022-09-19-xAplcC.svg" alt></p><p>前面服务的虚拟 IP 地址被重写为 Pod-B 的 IP 地址。</p><p>接下来，数据包路由过程与 Pod 到 Pod 的通信一样。</p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-29-2022-09-19-7aqxsB.svg" alt></p><p>数据包重写后，通信是 Pod 到 Pod。</p><p>然而，在所有这些通信中，使用了一个第三方的功能。</p><p><a href="https://www.linuxtopia.org/Linux_Firewall_iptables/x1298.html" target="_blank" rel="noopener">此功能称为 conntrack</a> 或链路跟踪。</p><p>当 Pod-B 发回响应时，conntrack 会将数据包与链路相关联，并跟踪其来源。</p><p>NAT 严重依赖于 conntrack。</p><p>如果没有链路跟踪，将不知道将包含响应的数据包发回何处。</p><p>使用 conntrack 时，数据包的返回路径很容易设置为相同的源或目标 NAT 更改。</p><p>通信的另一部分与现在的链路相反。</p><p>Pod-B 接收并处理了请求，现在将数据发送回 Pod-A。</p><p>现在会发生什么呢？</p><h2><span id="检查来自服务的响应">检查来自服务的响应</span></h2><p>Pod-B 发送响应，将其 IP 地址设置为源地址，并将 Pod-A 的 IP 地址设置为目标地址。</p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-30-20220919141614676-2022-09-19-4nUE8X.svg" alt></p><p>当数据包到达 Pod-A 所在节点的接口时，会发生另一个 NAT。</p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-31-2022-09-19-PS5DQz.svg" alt></p><p>这时，conntrack 开始工作，修改源 IP 地址，iptables 规则执行 SNAT，并将 Pod-B 的源 IP 地址修改为原始服务的虚拟 IP。</p><p><img src="https://img.hi-linux.com/staticfile/k8s-network-32-2022-09-19-bQ6FAb.svg" alt></p><p>对于 Pod-A 来说，响应是来自于 Service 而不是 Pod-B。</p><p>其余的都是一样的。一旦 SNAT 完成，数据包就会到达根命名空间中的网桥，并通过 <code>veth</code> 对转发到 <code>Pod-A</code>。</p><h2><span id="总结一下">总结一下</span></h2><p>让我们一起回顾下本文相关要点</p><ul><li>容器如何在本地或 Pod 内通信。</li><li>在相同节点和不同节点上的 Pod 如何通信。</li><li>Pod-to-Service - Pod 如何将流量发送到 Kubernetes 中服务后面的 Pod 时。</li><li>什么是命名空间、veth、iptables、chains、conntrack、Netfilter、CNI、overlay 网络，以及 Kubernetes 网络工具箱中所需的一切。</li></ul><blockquote><p>本文转载自：「 陈少文的博客 」，原文：<a href="https://url.hi-linux.com/GQueR" target="_blank" rel="noopener">https://url.hi-linux.com/GQueR</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文翻译自 &lt;a href=&quot;https://learnk8s.io/kubernetes-network-packets%EF%BC%8C%E5%B9%B6%E6%B2%A1%E6%9C%89%E9%80%90%E5%AD%97%E7%BF%BB%E8%AF%91%EF%BC%8C%E5%B8%A6%E5%85%A5%E4%BA%86%E4%BA%9B%E8%87%AA%E5%B7%B1%E7%9A%84%E7%90%86%E8%A7%A3%E3%80%82&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://learnk8s.io/kubernetes-network-packets，并没有逐字翻译，带入了些自己的理解。&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https://img.hi-linux.com/staticfile/k8s-network-1-2022-09-19-c7m4AB.svg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;阅读本文，你可以了解在 Kubernetes 内外，数据包是如何转发的，从原始的 Web 请求开始，到托管应用程序的容器。&lt;/p&gt;
&lt;h2 id=&quot;Kubernetes-网络要求&quot;&gt;Kubernetes 网络要求&lt;/h2&gt;
&lt;p&gt;在深入了解在 Kubernetes 集群中数据包如何流转的细节之前，先明确一下 Kubernetes 对网络的要求。&lt;/p&gt;
&lt;p&gt;Kubernetes 网络模型定义了一组基本规则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在不使用网络地址转换 (NAT) 的情况下，集群中的 Pod 能够与任意其他 Pod 进行通信。&lt;/li&gt;
&lt;li&gt;在不使用网络地址转换 (NAT) 的情况下，在集群节点上运行的程序能与同一节点上的任何 Pod 进行通信。&lt;/li&gt;
&lt;li&gt;每个 Pod 都有自己的 IP 地址（IP-per-Pod），并且任意其他 Pod 都可以通过相同的这个地址访问它。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些要求，不会将具体实现限制在某种解决方案上。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>如何使用 cri-docker 解决 Kubernetes 1.24 不支持 Docker 的问题</title>
    <link href="https://www.hi-linux.com/posts/20680.html"/>
    <id>https://www.hi-linux.com/posts/20680.html</id>
    <published>2022-09-13T01:00:00.000Z</published>
    <updated>2022-09-13T05:38:43.480Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>从 Kubernetes 1.24 开始，dockershim 已经从 kubelet 中移除，但因为历史问题 Docker 却不支持 Kubernetes 主推的 CRI（容器运行时接口）标准，所以 Docker 不能再作为 Kubernetes 的容器运行时了，即从Kubernetes v1.24 开始不再使用 Docker了。</p><p>但是如果想继续使用 Docker 的话，可以在 Kubelet 和 Docker 之间加上一个中间层 cri-docker。cri-docker 是一个支持 CRI 标准的 shim（垫片）。一头通过 CRI 跟 Kubelet 交互，另一头跟 Docker Api 交互，从而间接的实现了 Kubernetes 以 Docker 作为容器运行时。但是这种架构缺点也很明显，调用链更长，效率更低。</p><a id="more"></a><p>虽然本文演示了 cri-docker 的使用，但是更推荐使用 Containerd 作为 Kubernetes 的容器运行时。</p><h2><span id="实验环境">实验环境</span></h2><ul><li>两台机器，vms41 和 vms42</li><li>系统：centos7.4</li><li>vms41 为 master，vms42 是worker</li></ul><h3><span id="1-所有节点的基本设置">1. 所有节点的基本设置</span></h3><p>1.1 所有节点设置好 /etc/hosts ,使它们之间能互相解析</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@vms4X ~]# cat &#x2F;etc&#x2F;hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.26.41 vms41.rhce.cc vms41</span><br><span class="line">192.168.26.42 vms42.rhce.cc vms42</span><br><span class="line">[root@vms4X ~]#</span><br></pre></td></tr></table></figure><p>1.2 在所有节点上关闭swap分区</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@vms4X ~]# swapoff -a ; sed -i &#39;&#x2F;fstab&#x2F;d&#39; &#x2F;etc&#x2F;fstab </span><br><span class="line">[root@vms4X ~]#</span><br></pre></td></tr></table></figure><p>1.3.在所有节点上更新yum源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@vms4X ~]#  rm -rf &#x2F;etc&#x2F;yum.repos.d&#x2F;*  ; wget ftp:&#x2F;&#x2F;ftp.rhce.cc&#x2F;k8s&#x2F;* -P &#x2F;etc&#x2F;yum.repos.d&#x2F;</span><br><span class="line">[root@vms4X ~]# yum clean all</span><br><span class="line">[root@vms4X ~]#</span><br></pre></td></tr></table></figure><p>1.4 在所有节点安装 Docker</p><ul><li>所有节点安装 docker-ce。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@vms4X ~]#  yum install docker-ce -y</span><br></pre></td></tr></table></figure><ul><li>在所有节点启动 Docker 并设置开机自动启动</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@vms4X ~]# systemctl enable docker --now</span><br></pre></td></tr></table></figure><ul><li>所有节点设置 Docker加速器</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">   &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;frz7i079.mirror.aliyuncs.com&quot;],</span><br><span class="line">    &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>所有节点重启 Docker</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@vms4X ~]#  systemctl restart docker</span><br></pre></td></tr></table></figure><p>1.5 在所有节点安装 cri-docker</p><p>到下面的链接下载最新版 cri-docker</p><blockquote><p><a href="https://github.com/Mirantis/cri-dockerd/tags" target="_blank" rel="noopener">https://github.com/Mirantis/cri-dockerd/tags</a></p></blockquote><p>先在 vms41 上解压出 cri-docker，然后拷贝到 vms42 上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]# tar zxf cri-dockerd-0.2.1.amd64.tgz </span><br><span class="line">[root@vms41 ~]# cp cri-dockerd&#x2F;cri-dockerd &#x2F;usr&#x2F;bin&#x2F;</span><br><span class="line">[root@vms41 ~]# scp &#x2F;usr&#x2F;bin&#x2F;cri-dockerd vms42:&#x2F;usr&#x2F;bin&#x2F;</span><br><span class="line">root@vms42&#39;s password: </span><br><span class="line">cri-dockerd         100%   50MB 117.2MB&#x2F;s   00:00    </span><br><span class="line">[root@vms41 ~]#</span><br></pre></td></tr></table></figure><p>1.6 设置系统参数</p><p>在所有机器上执行下面的命令，目的是实现重启系统后，参数也能继续生效。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables &#x3D; 1</span><br><span class="line">net.ipv4.ip_forward &#x3D; 1</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>让上述参数立即生效。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@vms4X ~]# sysctl -p &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf</span><br><span class="line">[root@vms4X ~]#</span><br></pre></td></tr></table></figure><p>1.7 创建 cri-docker 启动文件</p><ul><li>启动文件从下面链接找到:</li></ul><blockquote><p><a href="https://github.com/Mirantis/cri-dockerd/tree/master/packaging/systemd" target="_blank" rel="noopener">https://github.com/Mirantis/cri-dockerd/tree/master/packaging/systemd</a></p></blockquote><p>创建 cri-docker 启动文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]# cat &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;cri-docker.service</span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;CRI Interface for Docker Application Container Engine</span><br><span class="line">Documentation&#x3D;https:&#x2F;&#x2F;docs.mirantis.com</span><br><span class="line">After&#x3D;network-online.target firewalld.service docker.service</span><br><span class="line">Wants&#x3D;network-online.target</span><br><span class="line">Requires&#x3D;cri-docker.socket</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;notify</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;cri-dockerd --network-plugin&#x3D;cni --pod-infra-container-image&#x3D;registry.aliyuncs.com&#x2F;google_containers&#x2F;pause:3.7</span><br><span class="line">ExecReload&#x3D;&#x2F;bin&#x2F;kill -s HUP $MAINPID</span><br><span class="line">TimeoutSec&#x3D;0</span><br><span class="line">RestartSec&#x3D;2</span><br><span class="line">Restart&#x3D;always</span><br><span class="line"></span><br><span class="line">StartLimitBurst&#x3D;3</span><br><span class="line"></span><br><span class="line">StartLimitInterval&#x3D;60s</span><br><span class="line"></span><br><span class="line">LimitNOFILE&#x3D;infinity</span><br><span class="line">LimitNPROC&#x3D;infinity</span><br><span class="line">LimitCORE&#x3D;infinity</span><br><span class="line"></span><br><span class="line">TasksMax&#x3D;infinity</span><br><span class="line">Delegate&#x3D;yes</span><br><span class="line">KillMode&#x3D;process</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br><span class="line">[root@vms41 ~]#</span><br></pre></td></tr></table></figure><p>这里 <code>/usr/bin/cri-dockerd</code> 一定要加上参数 <code>-–pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.7</code>，用来指定所用的 pause 镜像是哪个，否则默认拉取 <a href="http://k8s.gcr.io/pause:3.6%EF%BC%8C%E4%BC%9A%E5%AF%BC%E8%87%B4%E5%AE%89%E8%A3%85%E5%A4%B1%E8%B4%A5%E3%80%82" target="_blank" rel="noopener">k8s.gcr.io/pause:3.6，会导致安装失败。</a></p><ul><li>创建启动文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]# cat &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;cri-docker.socket</span><br><span class="line">[Unit]</span><br><span class="line">Description&#x3D;CRI Docker Socket for the API</span><br><span class="line">PartOf&#x3D;cri-docker.service</span><br><span class="line"></span><br><span class="line">[Socket]</span><br><span class="line">ListenStream&#x3D;%t&#x2F;cri-dockerd.sock</span><br><span class="line">SocketMode&#x3D;0660</span><br><span class="line">SocketUser&#x3D;root</span><br><span class="line">SocketGroup&#x3D;docker</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;sockets.target</span><br><span class="line">[root@vms41 ~]#</span><br></pre></td></tr></table></figure><p>1.8 把启动脚本拷贝到 vms42 上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]# scp &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;cri-docker.socket &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;cri-docker.service vms42:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;</span><br><span class="line">root@vms42&#39;s password: </span><br><span class="line">cri-docker.socket          100%  204   103.1KB&#x2F;s   00:00    </span><br><span class="line">cri-docker.service         100%  605   822.7KB&#x2F;s   00:00    </span><br><span class="line">[root@vms41 ~]#</span><br></pre></td></tr></table></figure><p>启动 cri-docker 并设置开机自动启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]# systemctl daemon-reload ; systemctl enable cri-docker --now</span><br><span class="line">Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi-user.target.wants&#x2F;cri-docker.service to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;cri-docker.service.</span><br><span class="line">[root@vms41 ~]#</span><br><span class="line"></span><br><span class="line">[root@vms4X ~]# systemctl is-active cri-docker</span><br><span class="line">active</span><br><span class="line">[root@vms4X ~]#</span><br></pre></td></tr></table></figure><h2><span id="2-安装-kubernetes">2. 安装 Kubernetes</span></h2><p>2.1 查看当前源里有哪些版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]#yum list --showduplicates kubeadm --disableexcludes&#x3D;kubernetes</span><br></pre></td></tr></table></figure><p>在本试验时最新的版本是 v1.24.1，所以本次就安装 v1.24.1版本的。</p><p>2.2 所有节点上安装软件包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@vms4X ~]#yum install -y kubelet-1.24.1-0 kubeadm-1.24.1-0 kubectl-1.24.1-0  --disableexcludes&#x3D;kubernetes</span><br><span class="line">[root@vms4X ~]#</span><br></pre></td></tr></table></figure><p>2.3 所有节点上启动 Kubelet 并设置开机自动启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@vms4X ~]# systemctl enable kubelet --now</span><br><span class="line">Created symlink from &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;multi-user.target.wants&#x2F;kubelet.service to &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;kubelet.service.</span><br><span class="line">[root@vms4X ~]#</span><br></pre></td></tr></table></figure><p>此时 Kubelet 状态是 activating 的，不是 active 的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]# systemctl is-active kubelet</span><br><span class="line">activating</span><br><span class="line">[root@vms41 ~]#</span><br></pre></td></tr></table></figure><h2><span id="3初始化-kubernetes">3.初始化 Kubernetes</span></h2><p>3.1 在 master（vms41）上初始化集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]# kubeadm init --image-repository registry.aliyuncs.com&#x2F;google_containers --kubernetes-version&#x3D;v1.24.1 --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 --cri-socket &#x2F;var&#x2F;run&#x2F;cri-dockerd.sock</span><br></pre></td></tr></table></figure><blockquote><p>注意，这里需要添加选项 -–cri-socket /var/run/cri-dockerd.sock</p></blockquote><p><img src="https://img.hi-linux.com/staticfile/4969830fe751d18275b23b2d92929617-2022-09-06-TlqU5p.png" alt></p><p>按提示创建 kubeconfig 文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]# mkdir -p $HOME&#x2F;.kube</span><br><span class="line">[root@vms41 ~]# sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">[root@vms41 ~]# sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">[root@vms41 ~]#</span><br></pre></td></tr></table></figure><p>3.2 把 worker 加入集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@vms42 ~]# kubeadm join 192.168.26.41:6443 --token l05cgf.kj5dvy5heki3jixt --discovery-token-ca-cert-hash sha256:07c1765ff4ac6eb2e54ed69fa57ca1afc728e825a6d4a11a83c96ff60ea545cd  --cri-socket &#x2F;var&#x2F;run&#x2F;cri-dockerd.sock</span><br><span class="line">[root@vms42 ~]#</span><br></pre></td></tr></table></figure><p>注意，这里也要加上选项 <code>-–cri-socket /var/run/cri-dockerd.sock</code></p><p>切换到 master，查看节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]# kubectl get nodes</span><br><span class="line">NAME            STATUS     ROLES           AGE     VERSION</span><br><span class="line">vms41.rhce.cc   NotReady   control-plane   4m12s   v1.24.1</span><br><span class="line">vms42.rhce.cc   NotReady   &lt;none&gt;          13s     v1.24.1</span><br><span class="line">[root@vms41 ~]#</span><br></pre></td></tr></table></figure><h2><span id="4安装-calico">4.安装 Calico</span></h2><p>4.1 下载最新版的 Calico 部署文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@vms71 ~]# wget https:&#x2F;&#x2F;docs.projectcalico.org&#x2F;manifests&#x2F;calico.yaml</span><br></pre></td></tr></table></figure><p>4.2 修改相应配置</p><p>修改 calico.yaml 找到 CALICO_IPV4POOL_CIDR 按下面修改。</p><p><img src="https://img.hi-linux.com/staticfile/839e789e218a3b6255911275b52ecc15-2022-09-06-j8alQ3.png" alt></p><p>改成</p><p><img src="https://img.hi-linux.com/staticfile/5b4040098c6ae58fa203d0fcc13ac56e-2022-09-06-il9eb3.png" alt></p><p>4.3 安装 Calico</p><p>在 vms41（master）上安装 calico，不需要在 vms42 上做什么。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]# kubectl apply -f calico.yaml</span><br><span class="line">[root@vms41 ~]#</span><br></pre></td></tr></table></figure><h2><span id="5验证">5.验证</span></h2><p>5.1 在 vms41 上再次查看节点状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@vms41 ~]# kubectl get nodes</span><br><span class="line">NAME            STATUS   ROLES           AGE     VERSION</span><br><span class="line">vms41.rhce.cc   Ready    control-plane   11m     v1.24.1</span><br><span class="line">vms42.rhce.cc   Ready    &lt;none&gt;          7m20s   v1.24.1</span><br><span class="line">[root@vms41 ~]# </span><br><span class="line">[root@vms41 ~]# kubectl get nodes -o wide</span><br><span class="line">NAME            STATUS   ROLES           AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION          CONTAINER-RUNTIME</span><br><span class="line">vms41.rhce.cc   Ready    control-plane   11m     v1.24.1   192.168.26.41   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-693.el7.x86_64   docker:&#x2F;&#x2F;20.10.17</span><br><span class="line">vms42.rhce.cc   Ready    &lt;none&gt;          7m23s   v1.24.1   192.168.26.42   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-693.el7.x86_64   docker:&#x2F;&#x2F;20.10.17</span><br><span class="line">[root@vms41 ~]#</span><br></pre></td></tr></table></figure><blockquote><p>本文转载自：「 老段工作室 」，原文：<a href="https://url.hi-linux.com/moiru%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://url.hi-linux.com/moiru，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从 Kubernetes 1.24 开始，dockershim 已经从 kubelet 中移除，但因为历史问题 Docker 却不支持 Kubernetes 主推的 CRI（容器运行时接口）标准，所以 Docker 不能再作为 Kubernetes 的容器运行时了，即从Kubernetes v1.24 开始不再使用 Docker了。&lt;/p&gt;
&lt;p&gt;但是如果想继续使用 Docker 的话，可以在 Kubelet 和 Docker 之间加上一个中间层 cri-docker。cri-docker 是一个支持 CRI 标准的 shim（垫片）。一头通过 CRI 跟 Kubelet 交互，另一头跟 Docker Api 交互，从而间接的实现了 Kubernetes 以 Docker 作为容器运行时。但是这种架构缺点也很明显，调用链更长，效率更低。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>如何在 Linux 下限制端口仅对指定 IP 开放访问</title>
    <link href="https://www.hi-linux.com/posts/30329.html"/>
    <id>https://www.hi-linux.com/posts/30329.html</id>
    <published>2022-08-27T01:00:00.000Z</published>
    <updated>2022-08-27T07:58:58.005Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="1-主机服务端口">1. 主机服务端口</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -I INPUT -p tcp --dport 80 -j DROP</span><br><span class="line">$ iptables -I INPUT -p tcp -s 1.2.3.4 --dport 80 -j ACCEPT</span><br></pre></td></tr></table></figure><p>这里仅允许 <code>1.2.3.4</code> 访问本地主机的 80 端口。</p><a id="more"></a><h2><span id="2-docker-服务端口">2. Docker 服务端口</span></h2><p>对于类似 <code>docker run -d -p 80:80 shaowenchen/demo-whoami</code> 运行的服务，上面的方法无效，需要在 DOCKER-USER 链中添加规则。</p><p>Docker 会将 iptables 规则添加到 DOCKER 链中，如果需要在 Docker 之前添加规则需要添加到 DOCKER-USER 链中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -I DOCKER-USER -i ens192 ! -s 1.2.3.4 -p tcp --dport 80 -j DROP</span><br></pre></td></tr></table></figure><p>ens192 是本地的网卡，这里仅允许 <code>1.2.3.4</code> 访问本地主机的 80 端口。</p><h2><span id="3-清理环境">3. 清理环境</span></h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y iptables-services</span><br><span class="line">$ systemctl restart iptables.service</span><br></pre></td></tr></table></figure><p>如果需要在主机重启之后 iptables 设置，依然有效，需要安装 <code>iptables-services</code> 并保存</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y iptables-services</span><br><span class="line">$ service iptables save</span><br></pre></td></tr></table></figure><h2><span id="4-参考">4. 参考</span></h2><ul><li><a href="https://docs.docker.com/network/iptables/" target="_blank" rel="noopener">https://docs.docker.com/network/iptables/</a></li></ul><blockquote><p>本文转载自：「 陈少文的博客 」，原文：<a href="https://url.hi-linux.com/SrAYO" target="_blank" rel="noopener">https://url.hi-linux.com/SrAYO</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-主机服务端口&quot;&gt;1. 主机服务端口&lt;/h2&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ iptables -I INPUT -p tcp --dport 80 -j DROP&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ iptables -I INPUT -p tcp -s 1.2.3.4 --dport 80 -j ACCEPT&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;这里仅允许 &lt;code&gt;1.2.3.4&lt;/code&gt; 访问本地主机的 80 端口。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>五种常见 Linux 系统安装包管理工具中文使用指南</title>
    <link href="https://www.hi-linux.com/posts/24537.html"/>
    <id>https://www.hi-linux.com/posts/24537.html</id>
    <published>2022-08-10T01:00:00.000Z</published>
    <updated>2022-08-10T01:16:12.360Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><blockquote><p><strong>介绍常见 Linux 操作系统的安装包管理工具，主要介绍其使用命令！</strong></p></blockquote><p>包管理系统除了安装软件外，它还提供了工具来更新已经安装的包。包存储库有助于确保你的系统中使用的代码是经过审查的，并且软件的安装版本已经得到了开发人员和包维护人员的认可。</p><a id="more"></a><h2><span id="1-dpkg">1. dpkg</span></h2><blockquote><p><strong>Ubuntu、Debian</strong></p></blockquote><p><code>dpkg</code> 命令是 <code>Debian Linux</code> 系统用来安装、创建和管理软件包的实用工具。</p><ul><li><strong>命令行使用</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># dpkg(选项)(参数)</span></span><br><span class="line">$ dpkg --<span class="built_in">help</span></span><br><span class="line">Usage: dpkg [&lt;option&gt; ...] &lt;<span class="built_in">command</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选项</span></span><br><span class="line">-i：安装软件包</span><br><span class="line">-r：删除软件包</span><br><span class="line">-P：删除软件包的同时删除其配置文件</span><br><span class="line">-L：显示于软件包关联的文件</span><br><span class="line">-l：显示已安装软件包列表</span><br><span class="line">--unpack：解开软件包</span><br><span class="line">-c：显示软件包内文件列表</span><br><span class="line">--confiugre：配置软件包</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">Deb软件包：指定要操作的.deb软件包</span><br></pre></td></tr></table></figure><ul><li><strong>示例演示说明</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装包</span></span><br><span class="line">$ dpkg -i package.deb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除包</span></span><br><span class="line">$ dpkg -r package</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除包（包括配置文件）</span></span><br><span class="line">$ dpkg -P package</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出与该包关联的文件</span></span><br><span class="line">$ dpkg -L package</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示该包的版本</span></span><br><span class="line">$ dpkg -l package</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解开deb包的内容</span></span><br><span class="line">$ dpkg --unpack package.deb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索所属的包内容</span></span><br><span class="line">$ dpkg -S keyword</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出当前已安装的包</span></span><br><span class="line">$ dpkg -l</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出deb包的内容</span></span><br><span class="line">$ dpkg -c package.deb</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置包</span></span><br><span class="line">$ dpkg --configure package</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出已安装软件包</span></span><br><span class="line">$ sudo dpkg-query -l</span><br><span class="line">$ sudo dpkg-query -l | less</span><br><span class="line">$ sudo dpkg-query -l | grep tmux</span><br></pre></td></tr></table></figure><h2><span id="2-apt">2. apt</span></h2><blockquote><p><strong>Ubuntu、Debian</strong></p></blockquote><p><code>apt-get</code> 命令是 <code>Debian Linux</code> 发行版中的 APT 软件包管理工具。所有基于 <code>Debian</code> 的发行都使用这个包管理系统。<code>deb</code> 包可以把一个应用的文件包在一起，大体就如同 <code>Windows</code> 上的安装文件。</p><ul><li><strong>命令行使用</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># apt-get(选项)(参数)</span></span><br><span class="line">$ apt --<span class="built_in">help</span></span><br><span class="line">Usage: apt [options] <span class="built_in">command</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选项</span></span><br><span class="line">-c：指定配置文件</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">管理指令：对APT软件包的管理操作</span><br><span class="line">软件包：指定要操纵的软件包</span><br></pre></td></tr></table></figure><ul><li><strong>示例演示说明</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更新所有已安装的软件包</span></span><br><span class="line">$ apt-get upgrade</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将系统升级到新版本</span></span><br><span class="line">$ apt-get dist-upgrade</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新</span></span><br><span class="line">$ apt-get update</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装一个新软件包</span></span><br><span class="line">$ apt-get install packagename</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载一个已安装的软件包（保留配置文件）</span></span><br><span class="line">$ apt-get remove packagename</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载一个已安装的软件包（删除配置文件）</span></span><br><span class="line">$ apt-get –purge remove packagename</span><br><span class="line"></span><br><span class="line"><span class="comment"># 来删除你已经删掉的软件</span></span><br><span class="line">$ apt-get autoclean apt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 会把安装的软件的备份也删除</span></span><br><span class="line">$ apt-get clean</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出已安装软件包</span></span><br><span class="line">$ sudo apt list --installed</span><br><span class="line">$ sudo apt list --installed | less</span><br><span class="line">$ sudo apt list --installed | grep tmux</span><br></pre></td></tr></table></figure><h2><span id="3-rpm">3. rpm</span></h2><blockquote><p><strong>RHEL、CentOS</strong></p></blockquote><p><code>rpm</code> 命令是 <code>RPM</code> 软件包的管理工具。<code>rpm</code> 原本是 <code>Red Hat Linux</code> 发行版专门用来管理 <code>Linux</code> 各项套件的程序，由于它遵循 <code>GPL</code> 规则且功能强大方便，因而广受欢迎。逐渐受到其他发行版的采用。<code>RPM</code> 套件管理方式的出现，让 <code>Linux</code> 易于安装，升级，间接提升了 <code>Linux</code> 的适用度。</p><ul><li><strong>命令行使用</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># rpm(选项)(参数)</span></span><br><span class="line">$ rpm --<span class="built_in">help</span></span><br><span class="line">Usage: rpm [OPTION...]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选项</span></span><br><span class="line">-a：查询所有套件</span><br><span class="line">-c：只列出组态配置文件，本参数需配合<span class="string">"-l"</span>参数使用</span><br><span class="line">-d：只列出文本文件，本参数需配合<span class="string">"-l"</span>参数使用</span><br><span class="line">-e&lt;套件档&gt;或--erase&lt;套件档&gt;：删除指定的套件</span><br><span class="line">-f&lt;文件&gt;+：查询拥有指定文件的套件</span><br><span class="line">-h或--<span class="built_in">hash</span>：套件安装时列出标记</span><br><span class="line">-i：显示套件的相关信息</span><br><span class="line">-i&lt;套件档&gt;或--install&lt;套件档&gt;：安装指定的套件档</span><br><span class="line">-l：显示套件的文件列表</span><br><span class="line">-p&lt;套件档&gt;+：查询指定的RPM套件档</span><br><span class="line">-q：使用询问模式，当遇到任何问题时，rpm指令会先询问用户</span><br><span class="line">-R：显示套件的关联性信息</span><br><span class="line">-s：显示文件状态，本参数需配合<span class="string">"-l"</span>参数使用</span><br><span class="line">-U&lt;套件档&gt;或--upgrade&lt;套件档&gt;：升级指定的套件档</span><br><span class="line">-v：显示指令执行过程</span><br><span class="line">-vv：详细显示指令执行过程，便于排错</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">软件包：指定要操纵的rpm软件包</span><br></pre></td></tr></table></figure><ul><li><strong>示例演示说明</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">$ rpm -ivh your-package.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 强制安装</span></span><br><span class="line">$ rpm --force -ivh your-package.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载</span></span><br><span class="line">$ rpm -e proftpd-1.2.8</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有安装过的包</span></span><br><span class="line">$ rpm -qa</span><br><span class="line">$ rpm -qa | grep sql</span><br><span class="line"></span><br><span class="line"><span class="comment"># rpm包中的文件安装到那里</span></span><br><span class="line">$ rpm -ql ***.rpm</span><br><span class="line"><span class="comment"># 一个没有安装过的软件包</span></span><br><span class="line">$ rpm -qlp ***.rpm</span><br><span class="line"><span class="comment"># 一个已经安装过的软件包</span></span><br><span class="line">$ rpm -ql ***.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 某个程序是哪个软件包安装</span></span><br><span class="line">$ rpm -qf `<span class="built_in">which</span> 程序名`   <span class="comment"># 返回软件包的全名</span></span><br><span class="line">$ rpm -qif `<span class="built_in">which</span> 程序名`  <span class="comment"># 返回软件包的有关信息</span></span><br><span class="line">$ rpm -qlf `<span class="built_in">which</span> 程序名`  <span class="comment"># 返回软件包的文件列表</span></span><br></pre></td></tr></table></figure><h2><span id="4-yum">4. yum</span></h2><blockquote><p><strong>CentOS6、CentOS7</strong></p></blockquote><p><code>yum</code> 命令是在 <code>Fedora</code> 和 <code>RedHat</code> 以及 <code>SUSE</code> 中基于 <code>rpm</code> 的软件包管理器，它可以使系统管理人员交互和自动化地更新与管理 <code>RPM</code> 软件包，能够从指定的服务器自动下载 <code>RPM</code> 包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。</p><ul><li><strong>命令行使用</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum(选项)(参数)</span></span><br><span class="line">$ yum --<span class="built_in">help</span></span><br><span class="line">Loaded plugins: fastestmirror, langpacks</span><br><span class="line">Usage: yum [options] COMMAND</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选项</span></span><br><span class="line">-h：显示帮助信息；</span><br><span class="line">-y：对所有的提问都回答“yes”；</span><br><span class="line">-c：指定配置文件；</span><br><span class="line">-q：安静模式；</span><br><span class="line">-v：详细模式；</span><br><span class="line">-d：设置调试等级（0-10）；</span><br><span class="line">-e：设置错误等级（0-10）；</span><br><span class="line">-R：设置yum处理一个命令的最大等待时间；</span><br><span class="line">-C：完全从缓存中运行，而不去下载或者更新任何头文件。</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">install：安装rpm软件包；</span><br><span class="line">update：更新rpm软件包；</span><br><span class="line">check-update：检查是否有可用的更新rpm软件包；</span><br><span class="line">remove：删除指定的rpm软件包；</span><br><span class="line">list：显示软件包的信息；</span><br><span class="line">search：检查软件包的信息；</span><br><span class="line">info：显示指定的rpm软件包的描述信息和概要信息；</span><br><span class="line">clean：清理yum过期的缓存；</span><br><span class="line">shell：进入yum的shell提示符；</span><br><span class="line">resolvedep：显示rpm软件包的依赖关系；</span><br><span class="line">localinstall：安装本地的rpm软件包；</span><br><span class="line">localupdate：显示本地rpm软件包进行更新；</span><br><span class="line">deplist：显示rpm软件包的所有依赖关系。</span><br></pre></td></tr></table></figure><ul><li><strong>示例演示说明</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装</span></span><br><span class="line">$ yum install             <span class="comment"># 全部安装</span></span><br><span class="line">$ yum install package1    <span class="comment"># 安装指定的安装包package1</span></span><br><span class="line">$ yum groupinsall group1  <span class="comment"># 安装程序组group1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新和升级</span></span><br><span class="line">$ yum update              <span class="comment"># 全部更新</span></span><br><span class="line">$ yum update package1     <span class="comment"># 更新指定程序包package1</span></span><br><span class="line">$ yum check-update        <span class="comment"># 检查可更新的程序</span></span><br><span class="line">$ yum upgrade package1    <span class="comment"># 升级指定程序包package1</span></span><br><span class="line">$ yum groupupdate group1  <span class="comment"># 升级程序组group1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找显示</span></span><br><span class="line">$ yum list installed | grep mysql</span><br><span class="line">$ yum list installed mysql*</span><br><span class="line">$ yum info package1     <span class="comment"># 显示安装包信息package1</span></span><br><span class="line">$ yum list              <span class="comment"># 显示所有已经安装和可以安装的程序包</span></span><br><span class="line">$ yum list package1     <span class="comment"># 显示指定程序包安装情况package1</span></span><br><span class="line">$ yum groupinfo group1  <span class="comment"># 显示程序组group1信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除程序</span></span><br><span class="line">$ yum remove/erase package1  <span class="comment"># 删除程序包package1</span></span><br><span class="line">$ yum groupremove group1     <span class="comment"># 删除程序组group1</span></span><br><span class="line">$ yum deplist package1       <span class="comment"># 查看程序package1依赖情况</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除缓存</span></span><br><span class="line">$ yum clean packages    <span class="comment"># 清除缓存目录下的软件包</span></span><br><span class="line">$ yum clean headers     <span class="comment"># 清除缓存目录下的 headers</span></span><br><span class="line">$ yum clean oldheaders  <span class="comment"># 清除缓存目录下旧的 headers</span></span><br></pre></td></tr></table></figure><h2><span id="5-dnf">5. dnf</span></h2><blockquote><p><strong>RHEL8、CentOS8</strong></p></blockquote><p><code>DNF</code> 使用 <code>libsolv</code> 进行依赖解析，由 <code>SUSE</code> 开发和维护，旨在提高性能。<code>Yum</code> 主要是用 <code>Python</code> 编写的，它有自己的应对依赖解析的方法。它的 <code>API</code> 没有完整的文档，它的扩展系统只允许 <code>Python</code> 插件。<code>Yum</code> 是 <code>RPM</code> 的前端工具，它管理依赖关系和资源库，然后使用 <code>RPM</code> 来安装、下载和删除包。</p><p>由于 <code>Yum</code> 中许多长期存在的问题仍未得到解决，因此 <code>Yum</code> 包管理器已被 <code>DNF</code> 包管理器取代。这些问题包括性能差、内存占用过多、依赖解析速度变慢等。两个管理包工具的更多区别可以查看，<a href="https://www.2daygeek.com/comparison-difference-between-dnf-vs-yum/" target="_blank" rel="noopener">What is the difference between DNF and YUM?</a> 进行阅读。</p><ul><li><strong>安装 DNF 包管理器</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 依赖</span></span><br><span class="line">$ yum install -y epel-release</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装</span></span><br><span class="line">$ yum install -y dnf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查</span></span><br><span class="line">$ dnf –version</span><br></pre></td></tr></table></figure><ul><li><strong>常用命令介绍</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装软件包</span></span><br><span class="line">$ dnf install nano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 升级软件包</span></span><br><span class="line">$ dnf update systemd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 升级所有系统软件包</span></span><br><span class="line">$ dnf update</span><br><span class="line">$ dnf upgrade</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查系统软件包的更新</span></span><br><span class="line">$ dnf check-update</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除软件包</span></span><br><span class="line">$ dnf remove nano</span><br><span class="line">$ dnf erase nano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除无用孤立的软件包</span></span><br><span class="line">$ dnf autoremove</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除缓存的无用软件包</span></span><br><span class="line">$ dnf clean all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看系统中可用的DNF软件库</span></span><br><span class="line">$ dnf repolist</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看系统中可用和不可用的所有的DNF软件库</span></span><br><span class="line">$ dnf repolist all</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有RPM包</span></span><br><span class="line">$ dnf list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有安装了的RPM包</span></span><br><span class="line">$ dnf list installed</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出所有可供安装的RPM包</span></span><br><span class="line">$ dnf list available</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索软件库中的RPM包</span></span><br><span class="line">$ dnf search nano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找某一文件的提供者</span></span><br><span class="line">$ dnf provides /bin/bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看软件包详情</span></span><br><span class="line">$ dnf info nano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看所有的软件包组</span></span><br><span class="line">$ dnf grouplist</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装一个软件包组</span></span><br><span class="line">$ dnf groupinstall <span class="string">'Educational Software'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 升级一个软件包组中的软件包</span></span><br><span class="line">$ dnf groupupdate <span class="string">'Educational Software'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除一个软件包组</span></span><br><span class="line">$ dnf groupremove <span class="string">'Educational Software'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新安装特定软件包</span></span><br><span class="line">$ dnf reinstall nano</span><br><span class="line"></span><br><span class="line"><span class="comment"># 回滚某个特定软件的版本</span></span><br><span class="line">$ dnf downgrade acpid</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看DNF命令的执行历史</span></span><br><span class="line">$ dnf <span class="built_in">history</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看所有的DNF命令及其用途</span></span><br><span class="line">$ dnf <span class="built_in">help</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取有关某条命令的使用帮助</span></span><br><span class="line">$ dnf <span class="built_in">help</span> clean</span><br></pre></td></tr></table></figure><blockquote><p>本文转载自：「 Escape 的博客 」，原文：<a href="https://url.hi-linux.com/No9Xd" target="_blank" rel="noopener">https://url.hi-linux.com/No9Xd</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;介绍常见 Linux 操作系统的安装包管理工具，主要介绍其使用命令！&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;包管理系统除了安装软件外，它还提供了工具来更新已经安装的包。包存储库有助于确保你的系统中使用的代码是经过审查的，并且软件的安装版本已经得到了开发人员和包维护人员的认可。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Shell" scheme="https://www.hi-linux.com/tags/Shell/"/>
    
  </entry>
  
  <entry>
    <title>理解 Docker 容器退出码</title>
    <link href="https://www.hi-linux.com/posts/52091.html"/>
    <id>https://www.hi-linux.com/posts/52091.html</id>
    <published>2022-07-29T01:00:00.000Z</published>
    <updated>2022-07-29T01:24:38.372Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>为什么我的容器没有运行？</p><p>回答这个问题需要知道 Docker 容器为什么退出，退出码会提示容器停止运行的情况。本文列出了最常见的退出码，来回答两个重要问题：</p><ul><li>这些退出码是什么意思？</li><li>导致该退出码的动作是什么？</li></ul><p>exit code： 代表一个进程的返回码，通过系统调用 exit_group 来触发。在 POSIX 中，0 代表正常的返回码，而 1-255 代表异常返回码，不过一般错误码都是 1。这里有一张附表 <a href="http://tldp.org/LDP/abs/html/exitcodes.html" target="_blank" rel="noopener">Appendix E. Exit Codes With Special Meanings</a></p><a id="more"></a><h2><span id="如何查看退出码">如何查看退出码</span></h2><p>方法一：查看 pod 中的容器退出码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe pod xxx</span><br></pre></td></tr></table></figure><p><img src="https://img.hi-linux.com/staticfile/3b8439b7-8a32-4e2f-9186-18773a333794-20220728150920792-2022-07-28-MAt1eU.jpg" alt></p><p>方法二：用 Docker 查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps --filter <span class="string">"status=exited"</span></span><br><span class="line">$ docker inspect &lt;container-id&gt; --format=<span class="string">'&#123;&#123;.State.ExitCode&#125;&#125;'</span></span><br></pre></td></tr></table></figure><p>方法三：手动输出</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker container run alpine sh -c <span class="string">"exit 1"</span></span><br><span class="line">$ docker container ls -a</span><br><span class="line"></span><br><span class="line">CONTAINER ID   IMAGE    COMMAND            CREATED              STATUS                       </span><br><span class="line">61c688005b3a   alpine   <span class="string">"sh -c 'exit 1'"</span>   About a minute ago   Exited (1) 3 seconds ago</span><br></pre></td></tr></table></figure><h2><span id="常见退出码">常见退出码</span></h2><h3><span id="exit-code-0">Exit Code 0</span></h3><ul><li>退出代码0表示特定容器没有附加前台进程。</li><li>该退出代码是所有其他后续退出代码的例外。</li><li>这不一定意味着发生了不好的事情。如果开发人员想要在容器完成其工作后自动停止其容器，则使用此退出代码。</li></ul><p>如果你执行 <code>docker run hello-world</code>, 你会得到“Hello from docker!”，但查看容器的时候<code>docker ps -a | grep hello-world</code>,会发现状态码为 0</p><p><img src="https://img.hi-linux.com/staticfile/99f9c7ed-fa66-4946-aaaf-d29bb0585bf8-2022-07-28-oFgCki.jpg" alt></p><h3><span id="exit-code-1">Exit Code 1</span></h3><ul><li>程序错误，或者 Dockerfile 中引用不存在的文件，如 entrypoint 中引用了错误的包</li><li>程序错误可以很简单，例如 “除以0”，也可以很复杂，比如空引用或者其他程序 crash</li></ul><h3><span id="exit-code-137">Exit Code 137</span></h3><ul><li>表明容器收到了 SIGKILL 信号，进程被杀掉，对应 kill -9</li><li>引发 SIGKILL 的是 Docker Kill。这可以由用户或由 Docker 守护程序来发起，手动执行：docker kill</li><li>137 比较常见，如果 pod 中的 limit 资源设置较小，会运行内存不足导致 OOMKilled，此时 state 中的 “OOMKilled” 值为 true，你可以在系统的 dmesg 中看到 oom 日志</li></ul><h3><span id="exit-code-139">Exit Code 139</span></h3><ul><li>表明容器收到了 SIGSEGV 信号，无效的内存引用，对应 kill -11</li><li>一般是代码有问题，或者 docker 的基础镜像有问题</li></ul><h3><span id="exit-code-143">Exit Code 143</span></h3><ul><li>表明容器收到了 SIGTERM 信号，终端关闭，对应 kill -15</li><li>一般对应 docker stop  命令</li><li>有时 docker stop 也会导致 Exit Code 137。发生在与代码无法处理 SIGTERM 的情况下，docker 进程等待十秒钟然后发出 SIGKILL 强制退出。</li></ul><h3><span id="不常用的一些-exit-code">不常用的一些 Exit Code</span></h3><ul><li>Exit Code 126: 权限问题或命令不可执行</li><li>Exit Code 127: Shell 脚本中可能出现错字且字符无法识别的情况</li><li>Exit Code 1 或 255：因为很多程序员写异常退出时习惯用 exit(1) 或 exit(-1)，-1 会根据转换规则转成 255。这个一般是自定义 code，要看具体逻辑。</li></ul><h3><span id="退出状态码的区间">退出状态码的区间</span></h3><ul><li>必须在 0-255 之间，0 表示正常退出</li><li>外界将程序中断退出，状态码在 129-255</li><li>程序自身异常退出，状态码一般在 1-128</li><li>假如写代码指定的退出状态码时不在 0-255 之间，例如: exit(-1)，这时会自动做一个转换，最终呈现的状态码还是会在 0-255 之间。我们把状态码记为 code，当指定的退出时状态码为负数，那么转换公式如下：<code>256 – (|code| % 256)</code></li></ul><h2><span id="参考">参考</span></h2><ul><li><a href="http://tldp.org/LDP/abs/html/exitcodes.html" target="_blank" rel="noopener">http://tldp.org/LDP/abs/html/exitcodes.html</a></li><li><a href="https://imroc.io/posts/kubernetes/analysis-exitcode/" target="_blank" rel="noopener">https://imroc.io/posts/kubernetes/analysis-exitcode/</a></li><li><a href="https://medium.com/better-programming/understanding-docker-container-exit-codes-5ee79a1d58f6" target="_blank" rel="noopener">https://medium.com/better-programming/understanding-docker-container-exit-codes-5ee79a1d58f6</a></li></ul><blockquote><p>本文转载自：「 Vermouth 的博客 」，原文：<a href="https://url.hi-linux.com/bfCGL" target="_blank" rel="noopener">https://url.hi-linux.com/bfCGL</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;为什么我的容器没有运行？&lt;/p&gt;
&lt;p&gt;回答这个问题需要知道 Docker 容器为什么退出，退出码会提示容器停止运行的情况。本文列出了最常见的退出码，来回答两个重要问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这些退出码是什么意思？&lt;/li&gt;
&lt;li&gt;导致该退出码的动作是什么？&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;exit code： 代表一个进程的返回码，通过系统调用 exit_group 来触发。在 POSIX 中，0 代表正常的返回码，而 1-255 代表异常返回码，不过一般错误码都是 1。这里有一张附表 &lt;a href=&quot;http://tldp.org/LDP/abs/html/exitcodes.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Appendix E. Exit Codes With Special Meanings&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>五分钟理解服务器 SMP、NUMA、MPP 三大体系结构</title>
    <link href="https://www.hi-linux.com/posts/12714.html"/>
    <id>https://www.hi-linux.com/posts/12714.html</id>
    <published>2022-07-02T01:00:00.000Z</published>
    <updated>2022-07-04T06:32:06.684Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>从系统架构来看，目前的商用服务器大体可以分为三类，即对称多处理器结构 (SMP ： Symmetric Multi-Processor) ，非一致存储访问结构 (NUMA ： Non-Uniform Memory Access) ，以及海量并行处理结构 (MPP ： Massive Parallel Processing) 。它们的特征分别描述如下：</p><h2><span id="1-smpsymmetric-multi-processor">1. SMP(Symmetric Multi-Processor)</span></h2><p>SMP (Symmetric Multi Processing),对称多处理系统内有许多紧耦合多处理器，在这样的系统中，所有的CPU共享全部资源，如总线，内存和I/O系统等，操作系统或管理数据库的复本只有一个，这种系统有一个最大的特点就是共享所有资源。多个CPU之间没有区别，平等地访问内存、外设、一个操作系统。操作系统管理着一个队列，每个处理器依次处理队列中的进程。如果两个处理器同时请求访问一个资源（例如同一段内存地址），由硬件、软件的锁机制去解决资源争用问题。Access to RAM is serialized; this and <a href="http://en.wikipedia.org/wiki/Cache_coherency" target="_blank" rel="noopener">cache coherency</a> issues causes performance to lag slightly behind the number of additional processors in the system.</p><p><img src="https://img.hi-linux.com/staticfile/clip_image001_d7728dc2-e525-4e5b-b84c-6f5a7f4de300-2022-06-29-09XWa8.gif" alt></p><p>所谓对称多处理器结构，是指服务器中多个 CPU 对称工作，无主次或从属关系。各 CPU 共享相同的物理内存，每个 CPU 访问内存中的任何地址所需时间是相同的，因此 SMP 也被称为一致存储器访问结构 (UMA ： Uniform Memory Access) 。对 SMP 服务器进行扩展的方式包括增加内存、使用更快的 CPU 、增加 CPU 、扩充 I/O( 槽口数与总线数 ) 以及添加更多的外部设备 ( 通常是磁盘存储 ) 。</p><a id="more"></a><p>SMP 服务器的主要特征是共享，系统中所有资源 (CPU 、内存、 I/O 等 ) 都是共享的。也正是由于这种特征，导致了 SMP 服务器的主要问题，那就是它的扩展能力非常有限。对于 SMP 服务器而言，每一个共享的环节都可能造成 SMP 服务器扩展时的瓶颈，而最受限制的则是内存。由于每个 CPU 必须通过相同的内存总线访问相同的内存资源，因此随着 CPU 数量的增加，内存访问冲突将迅速增加，最终会造成 CPU 资源的浪费，使 CPU 性能的有效性大大降低。实验证明， SMP 服务器 CPU 利用率最好的情况是 2 至 4 个 CPU 。</p><p><img src="https://img.hi-linux.com/staticfile/clip_image002_thumb-20220629133032132-2022-06-29-BqKbRu.gif" alt="图1. SMP 服务器 CPU 利用率状态"></p><p>8路服务器是服务器产业的分水岭。因为4路及以下服务器都采用SMP架构(Symmetric Multi-Processor，对称多处理结构)，实验证明，SMP服务器CPU利用率最好的情况是2至4个CPU。8是这种架构支持的处理器数量的极限，要支持8颗以上的处理器须采用另外的NUMA架构(Non-Uniform Memory Access，非一致性内存访问)。利用NUMA技术，可以较好地解决原来SMP系统的扩展问题，在一个物理服务器内可以支持上百个CPU。</p><h2><span id="2-numanon-uniform-memory-access">2. NUMA(Non-Uniform Memory Access)</span></h2><p>由于 SMP 在扩展能力上的限制，人们开始探究如何进行有效地扩展从而构建大型系统的技术， NUMA 就是这种努力下的结果之一。利用 NUMA 技术，可以把几十个 CPU( 甚至上百个 CPU) 组合在一个服务器内。其 CPU 模块结构如图 2 所示：</p><p><img src="https://img.hi-linux.com/staticfile/clip_image003_thumb-20220629133038040-2022-06-29-UEm1MW.gif" alt="图2. NUMA 服务器 CPU 模块结构"></p><p>NUMA 服务器的基本特征是具有多个 CPU 模块，每个 CPU 模块由多个 CPU( 如 4 个 ) 组成，并且具有独立的本地内存、 I/O 槽口等。由于其节点之间可以通过互联模块 ( 如称为 Crossbar Switch) 进行连接和信息交互，因此每个 CPU 可以访问整个系统的内存 ( 这是 NUMA 系统与 MPP 系统的重要差别 ) 。显然，访问本地内存的速度将远远高于访问远地内存 ( 系统内其它节点的内存 ) 的速度，这也是非一致存储访问 NUMA 的由来。由于这个特点，为了更好地发挥系统性能，开发应用程序时需要尽量减少不同 CPU 模块之间的信息交互。</p><p>利用 NUMA 技术，可以较好地解决原来 SMP 系统的扩展问题，在一个物理服务器内可以支持上百个 CPU 。比较典型的 NUMA 服务器的例子包括 HP 的 Superdome 、 SUN15K 、 IBMp690 等。</p><p>但 NUMA 技术同样有一定缺陷，由于访问远地内存的延时远远超过本地内存，因此当 CPU 数量增加时，系统性能无法线性增加。如 HP 公司发布 Superdome 服务器时，曾公布了它与 HP 其它 UNIX 服务器的相对性能值，结果发现， 64 路 CPU 的 Superdome (NUMA 结构 ) 的相对性能值是 20 ，而 8 路 N4000( 共享的 SMP 结构 ) 的相对性能值是 6.3 。从这个结果可以看到， 8 倍数量的 CPU 换来的只是 3 倍性能的提升。</p><p>2008年intel发布了Nehalem构架处理器，CPU内集成了内存控制器。当多CPU时任何一颗CPU都能访问全部内存。但CPU0访问本地内存(CPU0控制器直接控制的内存)消耗小，CPU0访问远地内存(CPU1内存控制器控制的内存)消耗大，NUMA功能的开启变成了必须了。</p><p>默认的NUMA功能是将计算和内存资源分配在一个NUMA内，有可能导致SWAP问题，即：NUMA0内存已经用完都开始用SWAP空间了，NUMA1还有很大的内存free。在数据库服务器上NUMA可能导致非常严重的性能问题，甚至有很多数据库死机的问题。就下图这个熊样。</p><p><img src="https://img.hi-linux.com/staticfile/1213536-20181228172000749-1913807104-2022-06-29-3ela76.png" alt></p><p>在虚拟化情况下，KVM虚机的CPU数量尽量不超过一个NUMA区域内的CPU数量，如果超过，则会出现一个KVM虚机使用了两个NUMA的情况，导致CPU等待内存时间过长，系统性能下降，此时需要手动调整KVM的配置才可以提高性能。</p><ul><li><p>Ubuntu 12.02自身带有Automatic NUMA balancing，可以支持NUMA自平衡，具体情况未测试。SUSE12也支持Automatic NUMA balancing</p></li><li><p>JUNO版的Openstack中，KVM的CPU的拓扑可以通过image或者flavor进行元数据传递来定义，如果没有特别的定义此类元数据，则模拟的CPU将是多Socket单Core单NUMA节点的CPU，这样的CPU与物理CPU完全不同。</p></li></ul><p>上面是KVM。Vmware ESX 5.0及之后的版本支持一种叫做vNUMA的特性，它将Host的NUMA特征暴露给了GuestOS，从而使得Guest OS可以根据NUMA特征进行更高性能的调度。</p><ul><li><p>CPU的热添加功能不支持vNUMA功能。</p></li><li><p>vmotion等功能一旦将vmware虚机迁移，则可能导致vNUMA失效，带来严重的性能降低。所以在ESXi中保持物理服务器的一致性是有必要的。</p></li></ul><p>中国第一台自主研发的，可支持32可处理器的高端服务器浪潮天梭K1，发布于2013年1月，系统可用性达到99.9994%，同时，我国也成为了时间上第三个掌握该技术的国家。</p><h2><span id="3-mppmassive-parallel-processing">3. MPP(Massive Parallel Processing)</span></h2><p>和 NUMA 不同， MPP 提供了另外一种进行系统扩展的方式，它由多个 SMP 服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务，从用户的角度来看是一个服务器系统。其基本特征是由多个 SMP 服务器 ( 每个 SMP 服务器称节点 ) 通过节点互联网络连接而成，每个节点只访问自己的本地资源 ( 内存、存储等 ) ，是一种完全无共享 (Share Nothing) 结构，因而扩展能力最好，理论上其扩展无限制，目前的技术可实现 512 个节点互联，数千个 CPU 。目前业界对节点互联网络暂无标准，如 NCR 的 Bynet ， IBM 的 SPSwitch ，它们都采用了不同的内部实现机制。但节点互联网仅供 MPP 服务器内部使用，对用户而言是透明的。</p><p>在 MPP 系统中，每个 SMP 节点也可以运行自己的操作系统、数据库等。但和 NUMA 不同的是，它不存在异地内存访问的问题。换言之，每个节点内的 CPU 不能访问另一个节点的内存。节点之间的信息交互是通过节点互联网络实现的，这个过程一般称为数据重分配 (Data Redistribution) 。</p><p>但是 MPP 服务器需要一种复杂的机制来调度和平衡各个节点的负载和并行处理过程。目前一些基于 MPP 技术的服务器往往通过系统级软件 ( 如数据库 ) 来屏蔽这种复杂性。举例来说， NCR 的 Teradata 就是基于 MPP 技术的一个关系数据库软件，基于此数据库来开发应用时，不管后台服务器由多少个节点组成，开发人员所面对的都是同一个数据库系统，而不需要考虑如何调度其中某几个节点的负载。</p><p>MPP (Massively Parallel Processing)，大规模并行处理系统，这样的系统是由许多松耦合的处理单元组成的，要注意的是这里指的是处理单元而不是处理器。每个单元内的CPU都有自己私有的资源，如总线，内存，硬盘等。在每个单元内都有操作系统和管理数据库的实例复本。这种结构最大的特点在于不共享资源。</p><p><img src="https://img.hi-linux.com/staticfile/clip_image004_a7d7609b-fc3c-4033-8181-5f4f4afaa572-20220629133045932-2022-06-29-XMfJa9.gif" alt></p><h2><span id="4-三种体系架构之间的差异">4. 三种体系架构之间的差异</span></h2><h3><span id="41-smp系统与mpp系统比较">4.1 </span></h3><p>既然有两种结构，那它们各有什么特点呢？采用什么结构比较合适呢？通常情况下，MPP系统因为要在不同处理单元之间传送信息（请注意上图），所以它的效率要比SMP要差一点，但是这也不是绝对的，因为MPP系统不共享资源，因此对它而言，资源比SMP要多，当需要处理的事务达到一定规模时，MPP的效率要比SMP好。这就是看通信时间占用计算时间的比例而定，如果通信时间比较多，那MPP系统就不占优势了，相反，如果通信时间比较少，那MPP系统可以充分发挥资源的优势，达到高效率。当前使用的OTLP程序中，用户访问一个中心数据库，如果采用SMP系统结构，它的效率要比采用MPP结构要快得多。而MPP系统在决策支持和数据挖掘方面显示了优势，可以这样说，如果操作相互之间没有什么关系，处理单元之间需要进行的通信比较少，那采用MPP系统就要好，相反就不合适了。</p><p>通过上面两个图我们可以看到，对于SMP来说，制约它速度的一个关键因素就是那个共享的总线，因此对于DSS程序来说，只能选择MPP，而不能选择SMP，当大型程序的处理要求大于共享总线时，总线就没有能力进行处理了，这时SMP系统就不行了。当然了，两个结构互有优缺点，如果能够将两种结合起来取长补短，当然最好了。</p><p><img src="https://img.hi-linux.com/staticfile/clip_image005_74c6d040-d7ed-476d-905a-91fb5d2a4255-20220629133052729-2022-06-29-Tfga9P.gif" alt></p><p><img src="https://img.hi-linux.com/staticfile/clip_image006_0219f756-4c51-408d-8646-83b91f6b158a-20220629133101304-2022-06-29-U6cJpB.gif" alt></p><h3><span id="42-numa-与-mpp-的区别">4.2 NUMA 与 MPP 的区别</span></h3><p>从架构来看， NUMA 与 MPP 具有许多相似之处：它们都由多个节点组成，每个节点都具有自己的 CPU 、内存、 I/O ，节点之间都可以通过节点互联机制进行信息交互。那么它们的区别在哪里？通过分析下面 NUMA 和 MPP 服务器的内部架构和工作原理不难发现其差异所在。</p><p>首先是节点互联机制不同， NUMA 的节点互联机制是在同一个物理服务器内部实现的，当某个 CPU 需要进行远地内存访问时，它必须等待，这也是 NUMA 服务器无法实现 CPU 增加时性能线性扩展的主要原因。而 MPP 的节点互联机制是在不同的 SMP 服务器外部通过 I/O 实现的，每个节点只访问本地内存和存储，节点之间的信息交互与节点本身的处理是并行进行的。因此 MPP 在增加节点时性能基本上可以实现线性扩展。</p><p>其次是内存访问机制不同。在 NUMA 服务器内部，任何一个 CPU 可以访问整个系统的内存，但远地访问的性能远远低于本地内存访问，因此在开发应用程序时应该尽量避免远地内存访问。在 MPP 服务器中，每个节点只访问本地内存，不存在远地内存访问的问题。</p><p><img src="https://img.hi-linux.com/staticfile/clip_image007_thumb-20220629133108009-2022-06-29-zu5Bnt.gif" alt="图3.MPP 服务器架构图"></p><p><strong>数据仓库的选择</strong></p><p>哪种服务器更加适应数据仓库环境？这需要从数据仓库环境本身的负载特征入手。众所周知，典型的数据仓库环境具有大量复杂的数据处理和综合分析，要求系统具有很高的 I/O 处理能力，并且存储系统需要提供足够的 I/O 带宽与之匹配。而一个典型的 OLTP 系统则以联机事务处理为主，每个交易所涉及的数据不多，要求系统具有很高的事务处理能力，能够在单位时间里处理尽量多的交易。显然这两种应用环境的负载特征完全不同。</p><p>从 NUMA 架构来看，它可以在一个物理服务器内集成许多 CPU ，使系统具有较高的事务处理能力，由于远地内存访问时延远长于本地内存访问，因此需要尽量减少不同 CPU 模块之间的数据交互。显然， NUMA 架构更适用于 OLTP 事务处理环境，当用于数据仓库环境时，由于大量复杂的数据处理必然导致大量的数据交互，将使 CPU 的利用率大大降低。</p><p>相对而言， MPP 服务器架构的并行处理能力更优越，更适合于复杂的数据综合分析与处理环境。当然，它需要借助于支持 MPP 技术的关系数据库系统来屏蔽节点之间负载平衡与调度的复杂性。另外，这种并行处理能力也与节点互联网络有很大的关系。显然，适应于数据仓库环境的 MPP 服务器，其节点互联网络的 I/O 性能应该非常突出，才能充分发挥整个系统的性能。</p><h3><span id="43-numa-mpp-smp-之间性能的区别">4.3 NUMA、MPP、SMP 之间性能的区别</span></h3><p>NUMA的节点互联机制是在同一个物理服务器内部实现的，当某个CPU需要进行远地内存访问时，它必须等待，这也是NUMA服务器无法实现CPU增加时性能线性扩展。</p><p>MPP的节点互联机制是在不同的SMP服务器外部通过I/O实现的，每个节点只访问本地内存和存储，节点之间的信息交互与节点本身的处理是并行进行的。因此MPP在增加节点时性能基本上可以实现线性扩展。</p><p>SMP所有的CPU资源是共享的，因此完全实现线性扩展。</p><h3><span id="44-numa-mpp-smp之间扩展的区别">4.4 NUMA、MPP、SMP之间扩展的区别</span></h3><p>NUMA理论上可以无限扩展，目前技术比较成熟的能够支持上百个CPU进行扩展。如HP的SUPERDOME。</p><p>MPP理论上也可以实现无限扩展，目前技术比较成熟的能够支持512个节点，数千个CPU进行扩展。</p><p>SMP扩展能力很差，目前2个到4个CPU的利用率最好，但是IBM的BOOK技术，能够将CPU扩展到8个。</p><p>MPP是由多个SMP构成，多个SMP服务器通过一定的节点互联网络进行连接，协同工作，完成相同的任务。</p><h3><span id="45-mpp-和-smp-numa-应用之间的区别">4.5 MPP 和 SMP、NUMA 应用之间的区别</span></h3><ul><li>MPP 的优势：</li></ul><p>MPP系统不共享资源，因此对它而言，资源比SMP要多，当需要处理的事务达到一定规模时，MPP的效率要比SMP好。由于MPP系统因为要在不同处理单元之间传送信息，在通讯时间少的时候，那MPP系统可以充分发挥资源的优势，达到高效率。也就是说：操作相互之间没有什么关系，处理单元之间需要进行的通信比较少，那采用MPP系统就要好。因此，MPP 系统在决策支持和数据挖掘方面显示了优势。</p><ul><li>SMP 的优势：</li></ul><p>MPP系统因为要在不同处理单元之间传送信息，所以它的效率要比SMP要差一点。在通讯时间多的时候，那MPP系统可以充分发挥资源的优势。因此当前使用的 OTLP程序中，用户访问一个中心数据库，如果采用 SMP 系统结构，它的效率要比采用MPP结构要快得多。</p><ul><li>NUMA 架构的优势：</li></ul><p>NUMA 架构来看，它可以在一个物理服务器内集成许多CPU，使系统具有较高的事务处理能力，由于远地内存访问时延远长于本地内存访问，因此需要尽量减少不同CPU模块之间的数据交互。显然，NUMA架构更适用于OLTP事务处理环境，当用于数据仓库环境时，由于大量复杂的数据处理必然导致大量的数据交互，将使CPU的利用率大大降低。</p><blockquote><p>本文转载自：「 博客园 」，原文：<a href="https://url.hi-linux.com/NWfZv%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://url.hi-linux.com/NWfZv，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;从系统架构来看，目前的商用服务器大体可以分为三类，即对称多处理器结构 (SMP ： Symmetric Multi-Processor) ，非一致存储访问结构 (NUMA ： Non-Uniform Memory Access) ，以及海量并行处理结构 (MPP ： Massive Parallel Processing) 。它们的特征分别描述如下：&lt;/p&gt;
&lt;h2 id=&quot;1-SMP-Symmetric-Multi-Processor&quot;&gt;1. SMP(Symmetric Multi-Processor)&lt;/h2&gt;
&lt;p&gt;SMP (Symmetric Multi Processing),对称多处理系统内有许多紧耦合多处理器，在这样的系统中，所有的CPU共享全部资源，如总线，内存和I/O系统等，操作系统或管理数据库的复本只有一个，这种系统有一个最大的特点就是共享所有资源。多个CPU之间没有区别，平等地访问内存、外设、一个操作系统。操作系统管理着一个队列，每个处理器依次处理队列中的进程。如果两个处理器同时请求访问一个资源（例如同一段内存地址），由硬件、软件的锁机制去解决资源争用问题。Access to RAM is serialized; this and &lt;a href=&quot;http://en.wikipedia.org/wiki/Cache_coherency&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;cache coherency&lt;/a&gt; issues causes performance to lag slightly behind the number of additional processors in the system.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.hi-linux.com/staticfile/clip_image001_d7728dc2-e525-4e5b-b84c-6f5a7f4de300-2022-06-29-09XWa8.gif&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;所谓对称多处理器结构，是指服务器中多个 CPU 对称工作，无主次或从属关系。各 CPU 共享相同的物理内存，每个 CPU 访问内存中的任何地址所需时间是相同的，因此 SMP 也被称为一致存储器访问结构 (UMA ： Uniform Memory Access) 。对 SMP 服务器进行扩展的方式包括增加内存、使用更快的 CPU 、增加 CPU 、扩充 I/O( 槽口数与总线数 ) 以及添加更多的外部设备 ( 通常是磁盘存储 ) 。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="CPU" scheme="https://www.hi-linux.com/tags/CPU/"/>
    
  </entry>
  
  <entry>
    <title>pwru：一款基于 eBPF 的细粒度网络数据包排查工具</title>
    <link href="https://www.hi-linux.com/posts/6471.html"/>
    <id>https://www.hi-linux.com/posts/6471.html</id>
    <published>2022-06-27T01:00:00.000Z</published>
    <updated>2022-06-27T03:21:04.627Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p><code>pwru</code> 是 Cilium 推出的基于 eBPF 开发的网络数据包排查工具，它提供了更细粒度的网络数据包排查方案。本文将介绍 <code>pwru</code> 的使用方法和经典场景，并介绍其实现原理。</p><a id="more"></a><h2><span id="安装部署">安装部署</span></h2><h3><span id="部署要求">部署要求</span></h3><p><code>pwru</code> 要求内核代码在 5.5 版本之上，<code>--output-skb</code> 要求内核版本在 5.9 之上，并且要求内核开启以下配置：</p><table><thead><tr><th style="text-align:left">Option</th><th style="text-align:left">Note</th></tr></thead><tbody><tr><td style="text-align:left">CONFIG_DEBUG_INFO_BTF=y</td><td style="text-align:left">Available since &gt;= 5.3</td></tr><tr><td style="text-align:left">CONFIG_KPROBES=y</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">CONFIG_PERF_EVENTS=y</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">CONFIG_BPF=y</td><td style="text-align:left"></td></tr><tr><td style="text-align:left">CONFIG_BPF_SYSCALL=y</td><td style="text-align:left"></td></tr></tbody></table><h3><span id="使用方法">使用方法</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Usage of .&#x2F;pwru:</span><br><span class="line">      --filter-dst-ip string        filter destination IP addr</span><br><span class="line">      --filter-dst-port uint16      filter destination port</span><br><span class="line">      --filter-func string          filter kernel functions to be probed by name (exact match, supports RE2 regular expression)</span><br><span class="line">      --filter-mark uint32          filter skb mark</span><br><span class="line">      --filter-netns uint32         filter netns inode</span><br><span class="line">      --filter-proto string         filter L4 protocol (tcp, udp, icmp)</span><br><span class="line">      --filter-src-ip string        filter source IP addr</span><br><span class="line">      --filter-src-port uint16      filter source port</span><br><span class="line">      --output-limit-lines uint     exit the program after the number of events has been received&#x2F;printed</span><br><span class="line">      --output-meta                 print skb metadata</span><br><span class="line">      --output-relative-timestamp   print relative timestamp per skb</span><br><span class="line">      --output-skb                  print skb</span><br><span class="line">      --output-stack                print stack</span><br><span class="line">      --output-tuple                print L4 tuple</span><br></pre></td></tr></table></figure><h2><span id="案例演示">案例演示</span></h2><p>下图案例演示了 <code>pwru</code> 展现出快速定位出数据包被 iptables 规则 drop 掉的原因：</p><p><img src="https://img.hi-linux.com/staticfile/demo-2022-06-16-LarlDo.gif" alt></p><p>在不设置 iptables 规则之前：</p><p><img src="https://img.hi-linux.com/staticfile/20211102110051-2022-06-16-ATTuCp.png" alt></p><p>添加了 iptables 规则之后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t filter -I OUTPUT 1 -m tcp --proto tcp --dst 1.1.1.1&#x2F;32 -j DROP</span><br></pre></td></tr></table></figure><p>可以看到在 <code>nf_hook_slow</code> 函数后发生了变化：</p><p><img src="https://img.hi-linux.com/staticfile/20211102110601-2022-06-16-pXKpVk.png" alt></p><p>我们可以看到数据包在 <code>nf_hook_slow</code> 判决为 <code>NF_DROP</code>，调用了 <code>kfree_skb</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">int nf_hook_slow(struct sk_buff *skb, struct nf_hook_state *state,</span><br><span class="line">     const struct nf_hook_entries *e, unsigned int s)</span><br><span class="line">&#123;</span><br><span class="line">  unsigned int verdict;</span><br><span class="line">  int ret;</span><br><span class="line"></span><br><span class="line">  for (; s &lt; e-&gt;num_hook_entries; s++) &#123;</span><br><span class="line">    verdict &#x3D; nf_hook_entry_hookfn(&amp;e-&gt;hooks[s], skb, state);</span><br><span class="line">    switch (verdict &amp; NF_VERDICT_MASK) &#123;</span><br><span class="line">    case NF_ACCEPT:</span><br><span class="line">      break;</span><br><span class="line">    case NF_DROP:</span><br><span class="line">      kfree_skb(skb);</span><br><span class="line">      ret &#x3D; NF_DROP_GETERR(verdict);</span><br><span class="line">      if (ret &#x3D;&#x3D; 0)</span><br><span class="line">        ret &#x3D; -EPERM;</span><br><span class="line">      return ret;</span><br><span class="line">    case NF_QUEUE:</span><br><span class="line">      ret &#x3D; nf_queue(skb, state, s, verdict);</span><br><span class="line">      if (ret &#x3D;&#x3D; 1)</span><br><span class="line">        continue;</span><br><span class="line">      return ret;</span><br><span class="line">    default:</span><br><span class="line">      &#x2F;* Implicit handling for NF_STOLEN, as well as any other</span><br><span class="line">       * non conventional verdicts.</span><br><span class="line">       *&#x2F;</span><br><span class="line">      return 0;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  return 1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="原理实现">原理实现</span></h2><p><code>pwru</code> 本质上是向 kprobe 注册了一些 eBPF code，根据 <code>pwru</code> 传入的参数可以更新 <code>eBPF Map</code>，改变限制条件，从而更新输出。</p><p>比如在 <code>FilterCfg</code> 里面制定了过滤的 IP 地址和协议等条件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">type FilterCfg struct &#123;</span><br><span class="line">FilterMark uint32</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;Filter l3</span><br><span class="line">FilterIPv6  uint8</span><br><span class="line">FilterSrcIP [16]byte</span><br><span class="line">FilterDstIP [16]byte</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;Filter l4</span><br><span class="line">FilterProto   uint8</span><br><span class="line">FilterSrcPort uint16</span><br><span class="line">FilterDstPort uint16</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;TODO: if there are more options later, then you can consider using a bit map</span><br><span class="line">OutputRelativeTS uint8</span><br><span class="line">OutputMeta       uint8</span><br><span class="line">OutputTuple      uint8</span><br><span class="line">OutputSkb        uint8</span><br><span class="line">OutputStack      uint8</span><br><span class="line"></span><br><span class="line">Pad byte</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>会根据 <code>pwru</code> 传入的参数更新这个 eBPF Map</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">func ConfigBPFMap(flags *Flags, cfgMap *ebpf.Map) &#123;</span><br><span class="line">cfg :&#x3D; FilterCfg&#123;</span><br><span class="line">FilterMark: flags.FilterMark,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if flags.FilterSrcPort &gt; 0 &#123;</span><br><span class="line">cfg.FilterSrcPort &#x3D; byteorder.HostToNetwork16(flags.FilterSrcPort)</span><br><span class="line">&#125;</span><br><span class="line">if flags.FilterDstPort &gt; 0 &#123;</span><br><span class="line">cfg.FilterDstPort &#x3D; byteorder.HostToNetwork16(flags.FilterDstPort)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">switch strings.ToLower(flags.FilterProto) &#123;</span><br><span class="line">case &quot;tcp&quot;:</span><br><span class="line">cfg.FilterProto &#x3D; syscall.IPPROTO_TCP</span><br><span class="line">case &quot;udp&quot;:</span><br><span class="line">cfg.FilterProto &#x3D; syscall.IPPROTO_UDP</span><br><span class="line">case &quot;icmp&quot;:</span><br><span class="line">cfg.FilterProto &#x3D; syscall.IPPROTO_ICMP</span><br><span class="line">case &quot;icmp6&quot;:</span><br><span class="line">cfg.FilterProto &#x3D; syscall.IPPROTO_ICMPV6</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; ... </span><br><span class="line">  </span><br><span class="line">if err :&#x3D; cfgMap.Update(uint32(0), cfg, 0); err !&#x3D; nil &#123;</span><br><span class="line">log.Fatalf(&quot;Failed to set filter map: %v&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 eBPF code 中，可以看到会读取配置 <code>bpf_map_lookup_elem</code>，然后进而执行真正的 filter：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">struct config &#123;</span><br><span class="line">u32 mark;</span><br><span class="line">u8 ipv6;</span><br><span class="line">union addr saddr;</span><br><span class="line">union addr daddr;</span><br><span class="line">u8 l4_proto;</span><br><span class="line">u16 sport;</span><br><span class="line">u16 dport;</span><br><span class="line">u8 output_timestamp;</span><br><span class="line">u8 output_meta;</span><br><span class="line">u8 output_tuple;</span><br><span class="line">u8 output_skb;</span><br><span class="line">u8 output_stack;</span><br><span class="line">u8 pad;</span><br><span class="line">&#125; __attribute__((packed));</span><br><span class="line"></span><br><span class="line">static __always_inline int</span><br><span class="line">handle_everything(struct sk_buff *skb, struct pt_regs *ctx) &#123;</span><br><span class="line">struct event_t event &#x3D; &#123;&#125;;</span><br><span class="line"></span><br><span class="line">u32 index &#x3D; 0;</span><br><span class="line">struct config *cfg &#x3D; bpf_map_lookup_elem(&amp;cfg_map, &amp;index);</span><br><span class="line"></span><br><span class="line">if (cfg) &#123;</span><br><span class="line">if (!filter(skb, cfg))</span><br><span class="line">return 0;</span><br><span class="line"></span><br><span class="line">set_output(ctx, skb, &amp;event, cfg);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">event.pid &#x3D; bpf_get_current_pid_tgid();</span><br><span class="line">event.addr &#x3D; PT_REGS_IP(ctx);</span><br><span class="line">event.skb_addr &#x3D; (u64) skb;</span><br><span class="line">event.ts &#x3D; bpf_ktime_get_ns();</span><br><span class="line">bpf_perf_event_output(ctx, &amp;events, BPF_F_CURRENT_CPU, &amp;event, sizeof(event));</span><br><span class="line"></span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，这里通过 <code>bpf_perf_event_output</code> 将过滤结果以 Perf event 传递上来。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">rd, err :&#x3D; perf.NewReader(events, os.Getpagesize())</span><br><span class="line">if err !&#x3D; nil &#123;</span><br><span class="line">log.Fatalf(&quot;Creating perf event reader: %s&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line">defer rd.Close()</span><br><span class="line"></span><br><span class="line"> &#x2F;&#x2F; ...</span><br><span class="line"> var event pwru.Event</span><br><span class="line">for &#123;</span><br><span class="line">record, err :&#x3D; rd.Read()</span><br><span class="line">if err !&#x3D; nil &#123;</span><br><span class="line">if perf.IsClosed(err) &#123;</span><br><span class="line">return</span><br><span class="line">&#125;</span><br><span class="line">log.Printf(&quot;Reading from perf event reader: %s&quot;, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if record.LostSamples !&#x3D; 0 &#123;</span><br><span class="line">log.Printf(&quot;Perf event ring buffer full, dropped %d samples&quot;, record.LostSamples)</span><br><span class="line">continue</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if err :&#x3D; binary.Read(bytes.NewBuffer(record.RawSample), binary.LittleEndian, &amp;event); err !&#x3D; nil &#123;</span><br><span class="line">log.Printf(&quot;Parsing perf event: %s&quot;, err)</span><br><span class="line">continue</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output.Print(&amp;event)</span><br><span class="line"></span><br><span class="line">select &#123;</span><br><span class="line">case &lt;-ctx.Done():</span><br><span class="line">break</span><br><span class="line">default:</span><br><span class="line">continue</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>本文转载自：「 Houmin 的博客 」，原文：<a href="https://url.hi-linux.com/et8wH" target="_blank" rel="noopener">https://url.hi-linux.com/et8wH</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;pwru&lt;/code&gt; 是 Cilium 推出的基于 eBPF 开发的网络数据包排查工具，它提供了更细粒度的网络数据包排查方案。本文将介绍 &lt;code&gt;pwru&lt;/code&gt; 的使用方法和经典场景，并介绍其实现原理。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Cilium" scheme="https://www.hi-linux.com/tags/Cilium/"/>
    
      <category term="eBPF" scheme="https://www.hi-linux.com/tags/eBPF/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Pod 多网卡解决方案 Multus 入门指南</title>
    <link href="https://www.hi-linux.com/posts/5706.html"/>
    <id>https://www.hi-linux.com/posts/5706.html</id>
    <published>2022-06-21T01:00:00.000Z</published>
    <updated>2022-06-21T03:01:32.502Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>在 Kubernetes中，网络是非常重要的一个领域。 Kubernetes 本身不提供网络解决方案，但是提供了 CN I规范。这些规范被许多 CNI 插件（例如 WeaveNet，Flannel，Calico 等）遵守。这些插件中任何一个都可以在集群上使用和部署以提供网络解决方案。该网络称为集群的默认网络。此默认网络使 Pods 不仅可以在同一节点上而且可以在群集中的各个节点之间相互通信。</p><p>随着发展，Kubernetes 缺乏支持 VNF 中多个网络接口的所需功能。传统上，网络功能使用多个网络接口分离控制，管理和控制用户/数据的网络平面。他们还用于支持不同的协议，满足不同的调整和配置要求。</p><p>为了解决这个需求，英特尔实现了 MULTUS 的 CNI 插件，其中提供了将多个接口添加到 Pod 的功能。这允许 POD 通过不同的接口连接到多个网络，并且每个接口都将使用其自己的 CNI 插件。</p><a id="more"></a><p>下面是 Multus CNI 提供的连接到 Pod 的网络接口的图示。该图显示了具有三个接口的容器：<code>eth0</code>，<code>net0</code>和<code>net1</code>。 <code>eth0</code>连接 Kubernetes 集群网络以连接kubernetes服务器/服务（例如 Kubernetes api-server，kubelet 等）。 <code>net0</code>和<code>net1</code>是其他网络附件，并通过使用其他 CNI 插件（例如vlan / vxlan / ptp）连接到其他网络。</p><p><img src="https://img.hi-linux.com/staticfile/1460000022848627-2022-06-16-tAgyzp.jpeg" alt></p><h2><span id="multus-工作原理">MULTUS 工作原理</span></h2><p>Kubernetes 当前没有提供为POD添加额外的接口选项的规定，或支持多个 CNI 插件同时工作的规定，但是它确实提供了一种由 API 服务器扩展受支持的API的机制。使用 “自定义资源定义” 可以做到这一点。 MULTUS依赖于 “自定义资源定义” 来存储其他接口和CNI插件所需的信息。</p><p><img src="https://img.hi-linux.com/staticfile/1460000022848626-20220616140455500-2022-06-16-l02xpE.jpeg" alt></p><p>我们首先需要确保将 MULTUS 二进制文件放置在 <code>/opt/cni/bin</code> 位置的所有节点上，并在<code>/etc/cni/net.d</code>位置创建一个新的配置文件。与 MULTUS 使用的 kubeconfig 文件一起使用。</p><p>在<code>/etc/cni/net.d</code>中创建的新配置文件基于集群中已经存在的默认网络配置。</p><p>在此之后，CRD 用于定义新的种类名称 “NetworkAttachmentDefinition”，以及服务帐户和 MULTUS 的集群角色以及相应的绑定。这个新的集群角色将提供对随 CRD 添加的新 API 组以及默认 API 组中 Pod 资源的访问权限。</p><p>然后创建类型为 “NetworkAttachmentDefinition” 的客户资源实例，该实例稍后将在创建具有多个接口的 Pod 时使用。</p><h2><span id="部署示例">部署示例</span></h2><p>在本文中，我们将多次提及两件事：</p><ul><li>“默认网络” - 这是您的Pod到Pod网络。这就是集群中 Pod 之间相互通信的方式，以及它们之间的连通性。一般而言，这被称为名为 eth0 的接口。此接口始终连接到您的 Pod，以便它们之间可以相互连接。除此之外，我们还将添加接口。</li><li>“ CRD”    - 自定义资源定义。自定义资源是扩展 Kubernetes API 的一种方式。我们在这里使用这些存储 Multus 可以读取的一些信息。首先，我们使用它们来存储附加到您的 Pod 的每个其他接口的配置。</li></ul><p>目前支持 Kubernetes 1.16+ 版本。</p><h2><span id="安装">安装</span></h2><p>我们建议的用于部署 Multus 的快速入门方法是使用 Daemonset（在群集中的每个节点上运行 Pod 的方法）进行部署，该 Pod 会安装 Multus 二进制文件并配置 Multus 以供使用。</p><p>首先，克隆此 GitHub 存储库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clone https:&#x2F;&#x2F;github.com&#x2F;intel&#x2F;multus-cni.git &amp;&amp; cd multus-cni</span><br></pre></td></tr></table></figure><p>我们将在此存储库中使用带有kubectl的YAML文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat .&#x2F;images&#x2F;multus-daemonset.yml | kubectl apply -f -</span><br></pre></td></tr></table></figure><h3><span id="multus-daemonset-完成了那些工作">Multus daemonset 完成了那些工作？</span></h3><ul><li>启动 Multus 守护程序集，这会在每个节点上运行一个pod，从而在<code>/opt/cni/bin</code>中的每个节点上放置一个 Multus 二进制文件</li><li>按照字母顺序读取<code>/etc/cni/net.d</code>中的第一个配置文件，并为 Multus 创建一个新的配置文件，即<code>/etc/cni/net.d/00-multus.conf</code>，此配置是自动生成并基于默认网络配置（假定是按字母顺序排列的第一个配置）</li><li>在每个节点上创建一个<code>/etc/cni/net.d/multus.d</code>目录，其中包含用于 Multus 访问 Kubernetes API 的身份验证信息。</li></ul><h2><span id="创建其他接口">创建其他接口</span></h2><p>我们要做的第一件事是为我们附加到Pod的每个其他接口创建配置。我们将通过创建自定义资源来做到这一点。快速入门安装的一部分会创建一个 “CRD” （自定义资源定义，它是我们保留这些自定义资源的位置），我们将在其中存储每个接口的配置。</p><h3><span id="cni-配置">CNI 配置</span></h3><p>我们将添加的每个配置都是CNI配置。如果您不熟悉它们，让我们快速分解它们。这是一个示例CNI配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;cniVersion&quot;: &quot;0.3.0&quot;,</span><br><span class="line">  &quot;type&quot;: &quot;loopback&quot;,</span><br><span class="line">  &quot;additional&quot;: &quot;information&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CNI配置是 JSON，我们这里有一个结构，其中包含一些我们感兴趣的东西：</p><ul><li><code>cniVersion</code>：告诉每个 CNI 插件正在使用哪个版本，如果使用的版本太晚（或太早），则可以提供插件信息。</li><li><code>type</code>：告诉 CNI 在磁盘上调用哪个二进制文件。每个 CNI 插件都是一个二进制文件。通常，这些二进制文件存储在每个节点上的<code>/opt/cni/bin</code>中，并且 CNI 执行此二进制文件。在这种情况下，我们指定了<code>loopback</code>二进制文件（它将创建一个<code>loopback</code>类型的网络接口）。如果这是您首次安装 Multus，则可能需要验证 “type” 字段中的插件是否确实在<code>/opt/cni/bin</code>目录中。</li><li><code>additional</code>：此字段以此处为例，每个 CNI 插件都可以在JSON中指定所需的任何配置参数。这些特定于您在 “type” 字段中调用的二进制文件。</li></ul><p>当 CNI 配置更改时，您不需要重新加载或刷新 Kubelets。每次创建和删除 Pod 时都会读取这些内容。因此，如果您更改配置，它将在下一次创建 Pod 时应用。如果现有 Pod 需要新配置，则可能需要重新启动。</p><h3><span id="将配置存储为自定义资源">将配置存储为自定义资源</span></h3><p>因此，我们要创建一个附加接口。让我们创建一个 macvlan 接口供 Pod 使用。我们将创建一个自定义资源，该资源定义接口的 CNI 配置。</p><p>请注意，在以下命令中有一种：<code>NetworkAttachmentDefinition</code>。这是我们配置的名字-它是 Kubernetes 的自定义扩展，定义了我们如何将网络连接到 Pod。</p><p>其次，注意配置字段。您将看到这是一个 CNI 配置，就像我们前面解释的那样。</p><p>最后但非常重要的一点是，在元数据下注意 name 字段-在这里我们为该配置指定名称，这是我们告诉 pod 使用此配置的方式。这里的名称是<code>macvlan-conf</code>-我们正在为 macvlan 创建配置。</p><p>这是创建此示例配置的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: &quot;k8s.cni.cncf.io&#x2F;v1&quot;</span><br><span class="line">kind: NetworkAttachmentDefinition</span><br><span class="line">metadata:</span><br><span class="line">  name: macvlan-conf</span><br><span class="line">spec:</span><br><span class="line">  config: &#39;&#123;</span><br><span class="line">      &quot;cniVersion&quot;: &quot;0.3.0&quot;,</span><br><span class="line">      &quot;type&quot;: &quot;macvlan&quot;,</span><br><span class="line">      &quot;master&quot;: &quot;eth0&quot;,</span><br><span class="line">      &quot;mode&quot;: &quot;bridge&quot;,</span><br><span class="line">      &quot;ipam&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;host-local&quot;,</span><br><span class="line">        &quot;subnet&quot;: &quot;192.168.1.0&#x2F;24&quot;,</span><br><span class="line">        &quot;rangeStart&quot;: &quot;192.168.1.200&quot;,</span><br><span class="line">        &quot;rangeEnd&quot;: &quot;192.168.1.216&quot;,</span><br><span class="line">        &quot;routes&quot;: [</span><br><span class="line">          &#123; &quot;dst&quot;: &quot;0.0.0.0&#x2F;0&quot; &#125;</span><br><span class="line">        ],</span><br><span class="line">        &quot;gateway&quot;: &quot;192.168.1.1&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;&#39;</span><br></pre></td></tr></table></figure><blockquote><p>本示例使用 eth0 作为主参数，此主参数应与集群中主机上的接口名称匹配。</p></blockquote><p>您可以查看使用 kubectl 创建的配置，方法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get network-attachment-definitions</span><br></pre></td></tr></table></figure><p>您可以通过描述它们来获得更多详细信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe network-attachment-definitions macvlan-conf</span><br></pre></td></tr></table></figure><h3><span id="创建一个附加附加接口的pod">创建一个附加附加接口的Pod</span></h3><p>我们将创建一个 pod。就像您之前可能创建的任何pod一样，它看起来都很熟悉，但是，我们将有一个特殊的注释字段-在这种情况下，我们将有一个名为<code>k8s.v1.cni.cncf.io/networks</code>的注释。如上创建的，该字段以逗号分隔的列表列出了 NetworkAttachmentDefinitions 的名称。请注意，在下面的命令中，我们具有 <code>k8s.v1.cni.cncf.io/networks</code> 的注释：<code>macvlan-conf</code>其中<code>macvlan-conf</code>是我们在创建配置时使用的名称。</p><p>让我们继续使用以下命令创建一个 pod：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: samplepod</span><br><span class="line">  annotations:</span><br><span class="line">    k8s.v1.cni.cncf.io&#x2F;networks: macvlan-conf</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: samplepod</span><br><span class="line">    command: [&quot;&#x2F;bin&#x2F;ash&quot;, &quot;-c&quot;, &quot;trap : TERM INT; sleep infinity &amp; wait&quot;]</span><br><span class="line">    image: alpine</span><br></pre></td></tr></table></figure><p>您现在可以检查Pod并查看连接了哪些接口，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl exec -it samplepod -- ip a</span><br></pre></td></tr></table></figure><p>您应该看到，有 3 个接口：</p><ul><li><code>lo</code>环回接口</li><li><code>eth0</code>我们的默认网络</li><li><code>net1</code>是我们使用macvlan配置创建的新接口</li></ul><h3><span id="网络状态-annotations">网络状态 Annotations</span></h3><p>为了确认，请使用<code>kubectl describe pod pod samplepod</code>，然后会有一个注释部分，类似于以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">Annotations:        k8s.v1.cni.cncf.io&#x2F;networks: macvlan-conf</span><br><span class="line">                    k8s.v1.cni.cncf.io&#x2F;networks-status:</span><br><span class="line">                      [&#123;</span><br><span class="line">                          &quot;name&quot;: &quot;cbr0&quot;,</span><br><span class="line">                          &quot;ips&quot;: [</span><br><span class="line">                              &quot;10.244.1.73&quot;</span><br><span class="line">                          ],</span><br><span class="line">                          &quot;default&quot;: true,</span><br><span class="line">                          &quot;dns&quot;: &#123;&#125;</span><br><span class="line">                      &#125;,&#123;</span><br><span class="line">                          &quot;name&quot;: &quot;macvlan-conf&quot;,</span><br><span class="line">                          &quot;interface&quot;: &quot;net1&quot;,</span><br><span class="line">                          &quot;ips&quot;: [</span><br><span class="line">                              &quot;192.168.1.205&quot;</span><br><span class="line">                          ],</span><br><span class="line">                          &quot;mac&quot;: &quot;86:1d:96:ff:55:0d&quot;,</span><br><span class="line">                          &quot;dns&quot;: &#123;&#125;</span><br><span class="line">                      &#125;]</span><br></pre></td></tr></table></figure><p>该元数据告诉我们，我们有两个成功运行的 CNI 插件。</p><h3><span id="如果我想要更多接口怎么办">如果我想要更多接口怎么办？</span></h3><p>您可以通过创建更多的自定义资源，然后在pod的注释中引用它们，来向pod添加更多接口。您还可以重复使用配置，例如，要将两个 macvlan 接口附加到 Pod，可以创建如下 Pod：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: samplepod</span><br><span class="line">  annotations:</span><br><span class="line">    k8s.v1.cni.cncf.io&#x2F;networks: macvlan-conf,macvlan-conf</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: samplepod</span><br><span class="line">    command: [&quot;&#x2F;bin&#x2F;ash&quot;, &quot;-c&quot;, &quot;trap : TERM INT; sleep infinity &amp; wait&quot;]</span><br><span class="line">    image: alpine</span><br></pre></td></tr></table></figure><p>请注意，注释现在读取为<code>k8s.v1.cni.cncf.io/networks：macvlan-conf，macvlan-conf</code>。如果我们有两次使用相同的配置，并用逗号分隔。</p><h2><span id="参考资料">参考资料</span></h2><ul><li><a href="https://zhuanlan.zhihu.com/p/73863683" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/73863683</a></li></ul><blockquote><p>本文转载自：「 Houmin 的博客 」，原文：<a href="https://url.hi-linux.com/uV1ne" target="_blank" rel="noopener">https://url.hi-linux.com/uV1ne</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Kubernetes中，网络是非常重要的一个领域。 Kubernetes 本身不提供网络解决方案，但是提供了 CN I规范。这些规范被许多 CNI 插件（例如 WeaveNet，Flannel，Calico 等）遵守。这些插件中任何一个都可以在集群上使用和部署以提供网络解决方案。该网络称为集群的默认网络。此默认网络使 Pods 不仅可以在同一节点上而且可以在群集中的各个节点之间相互通信。&lt;/p&gt;
&lt;p&gt;随着发展，Kubernetes 缺乏支持 VNF 中多个网络接口的所需功能。传统上，网络功能使用多个网络接口分离控制，管理和控制用户/数据的网络平面。他们还用于支持不同的协议，满足不同的调整和配置要求。&lt;/p&gt;
&lt;p&gt;为了解决这个需求，英特尔实现了 MULTUS 的 CNI 插件，其中提供了将多个接口添加到 Pod 的功能。这允许 POD 通过不同的接口连接到多个网络，并且每个接口都将使用其自己的 CNI 插件。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
      <category term="Multus" scheme="https://www.hi-linux.com/tags/Multus/"/>
    
  </entry>
  
  <entry>
    <title>如何在 Mac 上愉快的使用 Docker</title>
    <link href="https://www.hi-linux.com/posts/8579.html"/>
    <id>https://www.hi-linux.com/posts/8579.html</id>
    <published>2022-06-10T01:00:00.000Z</published>
    <updated>2022-06-10T02:17:56.955Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="一-目标任务">一、目标任务</span></h2><p>首先要明确的是, 作为了一个每天在 Linux Server 上 <code>rm -rf</code> 的人来说, 如果想在 Mac 上使用 Docker, 最舒服的也是兼容所有 docker cli 命令行操作即可; 至于图形化的界面完全不需要, 我们并不指望图形化界面能比敲命令快到哪里去, 也不指望图形化界面变为主力; 所以本篇文章的核心目标:</p><ul><li>在 Mac 上使用完整的 docker cli 命令, 包括对基本的 <code>-v</code> 挂载支持</li><li>可以支持 x86 的模拟, 可以为 x86 build 或者运行相关镜像</li><li>在尽可能的情况下可以进行 CPU 架构切换, arm64 与 x86 最好都可以支持</li></ul><a id="more"></a><h2><span id="二-工具选型">二、工具选型</span></h2><p>首先是我们最熟悉的 Docker Desktop, 安装包奇大无比, UI 卡成翔, 启动速度更不用提而且还时不时的卡死, 所以 Docker Desktop 是完全不考虑的; 那么剩下几种方案类型如下:</p><ul><li>VM 虚拟机方案</li><li>Colima 方案</li><li>Lima 方案</li></ul><p><strong>先说结论: Lima YES! VM 虚拟机方案要花钱且难受, Colima 暂且不稳定. Lima 方案直接看第五节.</strong></p><h2><span id="三-虚拟机方案">三、虚拟机方案</span></h2><p>目前在 M1 上, 唯一可用或者说堪用的虚拟机当属 Parallels Desktop, 至于其他的 VBox、VMware 目前还不成熟; 如果纯 qemu 有点过于硬核(愿意自己封装脚本的当我没说); 对于 Parallels Desktop 来说, 我们需要购买开发版本的 License, 因为我们需要借助 <code>prlctl</code> 来实现一些自动化 , 一年好几百… 经过测试这种方案也有一定可行性:</p><ul><li>1、首先通过 PD 创建 Ubuntu 之类的虚拟机</li><li>2、在虚拟机里安装好 Docker</li><li>3、通过 cli 程序启动虚拟机, 并且将 <code>~</code> rw 挂载到虚拟机里</li></ul><p>基于这个方案我个人尝试过, 曾经写过一个 <a href="https://github.com/mritd/pd/blob/010101042308ecbad805cffebb93973b55db4ff2/helper/prlctl_wrapper.go#L332" target="_blank" rel="noopener">PD</a> 的小工具来辅助完成挂载动作. 但是这种工具有一些明显的缺点:</p><ul><li>目前不支持 x86 的模拟, 可通过 binfmt 缓解, 但是不完善</li><li>虚拟机要花钱且需要虚拟机 cli 支持完善</li></ul><h2><span id="四-colima-方案">四、Colima 方案</span></h2><p>Colima 号称是专门为了解决 Mac 平台容器化工具链的, 但是实际测试发现目前 Colima 还不算稳定, 有时可能会有一些小问题; 当然 Colima 最大的问题是: <strong>可自定义化程度不高, 底层基于 Lima.</strong> Colima 具体的使用方式啥的这里暂不详细描述, 目前还不稳定不太推荐.</p><h2><span id="五-lima-方案">五、Lima 方案</span></h2><p>Lima 目前是基于 QEMU 的自动化 VM 方案, 当前由于其出色设计, 借助 Cloud Init 可以在很多阶段帮助我们完成 hook; 所以不论是装个 Docker 还是 k8s, 亦或是弄个其他的东西都很方便; 而且很多方案比如 docker 官方都有相关样例, 我们可以直接照抄外加做点自定义.</p><h3><span id="51-lima-安装">5.1、Lima 安装</span></h3><p>Lima 在 Mac 下安装相对简单, 以下命令将安装 master 分支版本.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install lima --HEAD</span><br></pre></td></tr></table></figure><p>在正常情况下, 安装 Lima 会附带安装 QEMU, 如果本机已经安装 QEMU, 可能需要执行以下命令<strong>将 QEMU 升级到 7.0</strong>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew upgrade qemu</span><br></pre></td></tr></table></figure><p>为了使用 docker, 还需要通过 brew 安装一下 docker cli:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install docker</span><br></pre></td></tr></table></figure><h3><span id="52-lima-使用">5.2、Lima 使用</span></h3><p>默认情况下 Lima 安装完成后会生成一个 <code>lima</code> 的快捷命令, 目前不太推荐使用, 原因是看起来方便一点但是没法控制太多参数, <strong>所以仍然建议使用标准的 <code>limactl</code> 命令进行操作.</strong> limactl 使用方式如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">Lima: Linux virtual machines</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  limactl [command]</span><br><span class="line"></span><br><span class="line">Examples:</span><br><span class="line">  Start the default instance:</span><br><span class="line">  $ limactl start</span><br><span class="line"></span><br><span class="line">  Open a shell:</span><br><span class="line">  $ lima</span><br><span class="line"></span><br><span class="line">  Run a container:</span><br><span class="line">  $ lima nerdctl run -d --name nginx -p 8080:80 nginx:alpine</span><br><span class="line"></span><br><span class="line">  Stop the default instance:</span><br><span class="line">  $ limactl stop</span><br><span class="line"></span><br><span class="line">  See also example YAMLs: &#x2F;opt&#x2F;homebrew&#x2F;share&#x2F;doc&#x2F;lima&#x2F;examples</span><br><span class="line"></span><br><span class="line">Available Commands:</span><br><span class="line">  completion    Generate the autocompletion script for the specified shell</span><br><span class="line">  copy          Copy files between host and guest</span><br><span class="line">  delete        Delete an instance of Lima.</span><br><span class="line">  edit          Edit an instance of Lima</span><br><span class="line">  factory-reset Factory reset an instance of Lima</span><br><span class="line">  help          Help about any command</span><br><span class="line">  info          Show diagnostic information</span><br><span class="line">  list          List instances of Lima.</span><br><span class="line">  prune         Prune garbage objects</span><br><span class="line">  shell         Execute shell in Lima</span><br><span class="line">  show-ssh      Show the ssh command line</span><br><span class="line">  start         Start an instance of Lima</span><br><span class="line">  stop          Stop an instance</span><br><span class="line">  sudoers       Generate &#x2F;etc&#x2F;sudoers.d&#x2F;lima file for enabling vmnet.framework support</span><br><span class="line">  validate      Validate YAML files</span><br><span class="line"></span><br><span class="line">Flags:</span><br><span class="line">      --debug     debug mode</span><br><span class="line">  -h, --help      help for limactl</span><br><span class="line">  -v, --version   version for limactl</span><br><span class="line"></span><br><span class="line">Use &quot;limactl [command] --help&quot; for more information about a command.</span><br></pre></td></tr></table></figure><h3><span id="53-lima-配置文件">5.3、Lima 配置文件</span></h3><p>Lima 通过读取一个 yaml 配置描述文件来决定如何创建一个虚拟机, 该文件基本结构如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"># 定义每个平台架构需要使用的启动镜像</span><br><span class="line">images:</span><br><span class="line">- location: &quot;https:&#x2F;&#x2F;cloud-images.ubuntu.com&#x2F;releases&#x2F;22.04&#x2F;release&#x2F;ubuntu-22.04-server-cloudimg-amd64.img&quot;</span><br><span class="line">  arch: &quot;x86_64&quot;</span><br><span class="line">- location: &quot;https:&#x2F;&#x2F;cloud-images.ubuntu.com&#x2F;releases&#x2F;22.04&#x2F;release&#x2F;ubuntu-22.04-server-cloudimg-arm64.img&quot;</span><br><span class="line">  arch: &quot;aarch64&quot;</span><br><span class="line"></span><br><span class="line"># 定义虚拟机需要使用哪个架构启动(对应上面的镜像)</span><br><span class="line">arch: &quot;x86_64&quot;</span><br><span class="line"></span><br><span class="line"># CPU 数量</span><br><span class="line">cpus: 4</span><br><span class="line"></span><br><span class="line"># 内存大小</span><br><span class="line">memory: &quot;16G&quot;</span><br><span class="line"></span><br><span class="line"># 磁盘大小</span><br><span class="line">disk: &quot;100G&quot;</span><br><span class="line"></span><br><span class="line"># 虚拟机与 macOS 宿主机挂载时使用的挂载技术</span><br><span class="line"># 目前推荐 9p, 可换成 sshfs, 但是 sshfs 会有权限问题</span><br><span class="line">mountType: 9p</span><br><span class="line"></span><br><span class="line"># 定义虚拟机和 macOS 宿主机有哪些目录可以共享</span><br><span class="line">mounts:</span><br><span class="line">- location: &quot;~&quot;</span><br><span class="line">  # 定义虚拟机对这个目录是否可写</span><br><span class="line">  writable: true</span><br><span class="line">  9p:</span><br><span class="line">    # 对于可写的共享目录, cache 推荐类型为 mmap, 不写好像默认 fscache</span><br><span class="line">    cache: &quot;mmap&quot;</span><br><span class="line">- location: &quot;&#x2F;tmp&#x2F;lima&quot;</span><br><span class="line">  writable: true</span><br><span class="line">  9p:</span><br><span class="line">    cache: &quot;mmap&quot;</span><br><span class="line"># containerd is managed by Docker, not by Lima, so the values are set to false here.</span><br><span class="line">containerd:</span><br><span class="line">  system: false</span><br><span class="line">  user: false</span><br><span class="line"></span><br><span class="line"># cloud-init hook 定义</span><br><span class="line">provision:</span><br><span class="line"># 定义以什么权限在虚拟机内执行脚本</span><br><span class="line">- mode: system</span><br><span class="line">  # This script defines the host.docker.internal hostname when hostResolver is disabled.</span><br><span class="line">  # It is also needed for lima 0.8.2 and earlier, which does not support hostResolver.hosts.</span><br><span class="line">  # Names defined in &#x2F;etc&#x2F;hosts inside the VM are not resolved inside containers when</span><br><span class="line">  # using the hostResolver; use hostResolver.hosts instead (requires lima 0.8.3 or later).</span><br><span class="line">  script: |</span><br><span class="line">    #!&#x2F;bin&#x2F;sh</span><br><span class="line">    sed -i &#39;s&#x2F;host.lima.internal.*&#x2F;host.lima.internal host.docker.internal&#x2F;&#39; &#x2F;etc&#x2F;hosts</span><br><span class="line">- mode: system</span><br><span class="line">  script: |</span><br><span class="line">    #!&#x2F;bin&#x2F;bash</span><br><span class="line">    set -eux -o pipefail</span><br><span class="line">    if command -v docker &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1; then</span><br><span class="line">      docker run --platform&#x3D;linux&#x2F;amd64 --privileged --rm tonistiigi&#x2F;binfmt --install all</span><br><span class="line">      exit 0</span><br><span class="line">    else</span><br><span class="line">      export DEBIAN_FRONTEND&#x3D;noninteractive</span><br><span class="line">      curl -fsSL https:&#x2F;&#x2F;get.docker.com | sh</span><br><span class="line">      docker run --platform&#x3D;linux&#x2F;amd64 --privileged --rm tonistiigi&#x2F;binfmt --install all</span><br><span class="line">      # NOTE: you may remove the lines below, if you prefer to use rootful docker, not rootless</span><br><span class="line">      systemctl disable --now docker</span><br><span class="line">      apt-get install -y uidmap dbus-user-session</span><br><span class="line">    fi</span><br><span class="line">- mode: user</span><br><span class="line">  script: |</span><br><span class="line">    #!&#x2F;bin&#x2F;bash</span><br><span class="line">    set -eux -o pipefail</span><br><span class="line">    systemctl --user start dbus</span><br><span class="line">    dockerd-rootless-setuptool.sh install</span><br><span class="line">    docker context use rootless</span><br><span class="line">probes:</span><br><span class="line">- script: |</span><br><span class="line">    #!&#x2F;bin&#x2F;bash</span><br><span class="line">    set -eux -o pipefail</span><br><span class="line">    if ! timeout 30s bash -c &quot;until command -v docker &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1; do sleep 3; done&quot;; then</span><br><span class="line">      echo &gt;&amp;2 &quot;docker is not installed yet&quot;</span><br><span class="line">      exit 1</span><br><span class="line">    fi</span><br><span class="line">    if ! timeout 30s bash -c &quot;until pgrep rootlesskit; do sleep 3; done&quot;; then</span><br><span class="line">      echo &gt;&amp;2 &quot;rootlesskit (used by rootless docker) is not running&quot;</span><br><span class="line">      exit 1</span><br><span class="line">    fi</span><br><span class="line">  hint: See &quot;&#x2F;var&#x2F;log&#x2F;cloud-init-output.log&quot;. in the guest</span><br><span class="line">hostResolver:</span><br><span class="line">  # hostResolver.hosts requires lima 0.8.3 or later. Names defined here will also</span><br><span class="line">  # resolve inside containers, and not just inside the VM itself.</span><br><span class="line">  hosts:</span><br><span class="line">    host.docker.internal: host.lima.internal</span><br><span class="line">portForwards:</span><br><span class="line">- guestSocket: &quot;&#x2F;run&#x2F;user&#x2F;&#123;&#123;.UID&#125;&#125;&#x2F;docker.sock&quot;</span><br><span class="line">  hostSocket: &quot;&#123;&#123;.Dir&#125;&#125;&#x2F;sock&#x2F;docker.sock&quot;</span><br><span class="line"># 自己定义的启动后消息输出</span><br><span class="line">message: |</span><br><span class="line">  To run &#96;docker&#96; on the host (assumes docker-cli is installed), run the following commands:</span><br><span class="line">  ------</span><br><span class="line">  docker context create amd64 --docker &quot;host&#x3D;unix:&#x2F;&#x2F;&#123;&#123;.Dir&#125;&#125;&#x2F;sock&#x2F;docker.sock&quot;</span><br><span class="line">  docker context use amd64</span><br><span class="line">  ------</span><br></pre></td></tr></table></figure><h3><span id="54-启动-vm">5.4、启动 VM</span></h3><p>limactl 命令提供了一个 <code>start</code> 子命令用于启动一个虚拟机, 子命令接受一个参数, 这个参数形式不同会产生不同的行为:</p><ul><li>如果参数为一个文件路径, 则假定文件为一个 lima 虚拟机的 yaml 配置, 读取并启动</li><li>如果参数是单纯字符串, 首先尝试从已存在的虚拟机中查找名字相同的, 找到则立即启动</li><li>如果参数是单纯字符串, 且未找到已存在同名的虚拟机, 则尝试通过内置模版来创建一个新的虚拟机</li></ul><p>以上面我自己定义的 docker 配置文件为例, 我们直接启动这个配置既可以创建一个 docker 虚拟机:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">limactl start .&#x2F;docker-amd64.yaml</span><br></pre></td></tr></table></figure><p>启动后会提示是否编辑然后再启动, 这是为了使用同一个配置来启动多个 vm 使用的, 所以不编辑直接启动即可:</p><p><img src="https://img.hi-linux.com/staticfile/dJwuuJ-2022-06-09-OmbQn6.png" alt></p><p>稍等片刻后虚拟机将启动成功:</p><p><img src="https://img.hi-linux.com/staticfile/MhWILZ-2022-06-09-fazpql.png" alt></p><p>启动完成后, <strong>执行最下面打印出的两条命令, 即可在宿主机上完整的使用 <code>docker</code>.</strong> 其本质上利用 docker context 功能, 然后通过将虚拟机中的 sock 文件挂载到宿主机, 并配置 docker context 来实现无缝使用 docker 命令.</p><h3><span id="55-虚拟机调整">5.5、虚拟机调整</span></h3><p>某些情况下, 我们需要定制一些 VM 里的配置, 在定制时主要需要调整配置文件的 <code>provision</code> 部分; 在该部分中, 如果 <code>mode</code> 被定义为 <code>system</code> 则会以 root 用户执行相关命令, 否则以普通用户来执行命令. <strong>需要注意的是, 我们定义的脚本需要具有幂等性, 因为脚本在每次都会执行一次, 所以一般对于可能造成数据擦除动作的命令都要写好判断逻辑, 避免重复执行.</strong></p><p>关于文件挂载, 这里推荐使用 <code>9p</code> 类型, 未来 lima 将完全切换到该挂载方式; 同时经过测试<strong>目前仅有 <code>9p</code> 挂载模式下, 本地目录 rw 映射到虚拟机时不会出现权限问题, sshfs 方式挂载如果遇到 <code>chown</code> 之类的命令会造成权限错误, 可能导致容器启动失败(例如 mysql).</strong></p><p>在测试虚拟机配置过程中, 可以直接使用 <code>limactl delete -f xxxx</code> 来强制删除目标虚拟机, 然后重新启动即可; <strong>虚拟机名称默认与 yaml 文件名相同, 可使用 <code>limactl ls</code> 命令查看.</strong></p><h3><span id="56-多平台兼容">5.6、多平台兼容</span></h3><p>在上面我的 docker 配置样例中, 每次虚拟机启动完成后会自动安装 binfmt:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --platform&#x3D;linux&#x2F;amd64 --privileged --rm tonistiigi&#x2F;binfmt --install all</span><br></pre></td></tr></table></figure><p>这样能保证无论 Lima 虚拟机原始架构是什么, 都能运行其他平台的 docker 镜像; 典型的例如某些 openjdk8 镜像只有 amd64 的版本, 但是在 lima 虚拟机为 aarch64 的情况下仍然可以使用.</p><p>除了这种 “速度较快” 的跨架构运行方式, lima 还支持直接在 VM 中定义架构, 这样在 qemu 启动时则会直接从 VM 系统层模拟目标架构; <strong>这种方式的好处是对目标架构兼容性很好, 但是运行速度会更慢.</strong> 调整 VM 架构只需要修改 <code>arch</code> 配置即可(注意, 目标架构的镜像一定要配置好):</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 定义每个平台架构需要使用的启动镜像</span><br><span class="line">images:</span><br><span class="line">- location: &quot;https:&#x2F;&#x2F;cloud-images.ubuntu.com&#x2F;releases&#x2F;22.04&#x2F;release&#x2F;ubuntu-22.04-server-cloudimg-amd64.img&quot;</span><br><span class="line">  arch: &quot;x86_64&quot;</span><br><span class="line">- location: &quot;https:&#x2F;&#x2F;cloud-images.ubuntu.com&#x2F;releases&#x2F;22.04&#x2F;release&#x2F;ubuntu-22.04-server-cloudimg-arm64.img&quot;</span><br><span class="line">  arch: &quot;aarch64&quot;</span><br><span class="line"></span><br><span class="line"># 定义本虚拟机需要使用哪个架构启动(对应会使用上面目标架构的镜像)</span><br><span class="line">arch: &quot;aarch64&quot;</span><br></pre></td></tr></table></figure><h2><span id="六-总结">六、总结</span></h2><p>目前整体来看, Docker Desktop 在 mac 上基本上是很难用的, Colima 现在还不太成熟, 适合轻度使用 docker 的用户; 而重度使用 docker 并且有定制化需求的用户还是推荐 Lima 虚拟机; 同时 Lima 也支持很多操作系统, 官方有大量的<a href="https://github.com/lima-vm/lima/tree/master/examples" target="_blank" rel="noopener">样例模版</a>(包括 k8s、k3s、podman 等), 非常适合重度容器使用者.</p><blockquote><p>本文转载自：「 bleem 的博客 」，原文：<a href="https://url.hi-linux.com/DZxIp/" target="_blank" rel="noopener">https://url.hi-linux.com/DZxIp/</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、目标任务&quot;&gt;一、目标任务&lt;/h2&gt;
&lt;p&gt;首先要明确的是, 作为了一个每天在 Linux Server 上 &lt;code&gt;rm -rf&lt;/code&gt; 的人来说, 如果想在 Mac 上使用 Docker, 最舒服的也是兼容所有 docker cli 命令行操作即可; 至于图形化的界面完全不需要, 我们并不指望图形化界面能比敲命令快到哪里去, 也不指望图形化界面变为主力; 所以本篇文章的核心目标:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 Mac 上使用完整的 docker cli 命令, 包括对基本的 &lt;code&gt;-v&lt;/code&gt; 挂载支持&lt;/li&gt;
&lt;li&gt;可以支持 x86 的模拟, 可以为 x86 build 或者运行相关镜像&lt;/li&gt;
&lt;li&gt;在尽可能的情况下可以进行 CPU 架构切换, arm64 与 x86 最好都可以支持&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>如何使用 Kubectl 优雅的滚动更新应用</title>
    <link href="https://www.hi-linux.com/posts/28253.html"/>
    <id>https://www.hi-linux.com/posts/28253.html</id>
    <published>2022-06-02T01:00:00.000Z</published>
    <updated>2022-06-02T06:02:39.234Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Kubernetes 中的 Pod 通常应该是 “Running” 状态，然而有时候我们需要针对正在运行的 Pod 调度到其它的节点或是基于其它特殊的原因，将正常运行的 Pod 进行重启。Pod 的重启方式也有不少，比如常见删除正在运行的 Pod 让其创建新的 Pod 实例（单个 Pod 无法直接使用该方式）。以下罗列出几种常见的方式仅供备忘：</p><h2><span id="方法1滚动重启方法推荐">方法1：滚动重启方法[推荐]</span></h2><p>自 Kubernetes 1.15 版本就开始支持滚动重启部署。这是 Kubernetes 中最快的重启机制，因为它是新增的。下面给出的命令会一个一个地关闭并重新启动部署中的每个 Pod。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl rollout restart deployment nginx-deploy</span><br></pre></td></tr></table></figure><blockquote><p>提示：由于大多数容器仍在运行，因此整个过程是纵享丝滑–无感知的。</p></blockquote><a id="more"></a><h2><span id="方法2环境变量方法">方法2：环境变量方法</span></h2><p>这种方式就是给运行的容器分配一个新的环境变量，来强制 Pod 重新启动。例如: 增加一个容器部署日期（实际可能未使用到该变量）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl set env deployment nginx-deploy DEPLOY_DATE&#x3D;&quot;$(date)&quot;</span><br></pre></td></tr></table></figure><blockquote><p>提示：在上面的场景中，使用了<code>set env</code> 修改了环境变量，deployment [deployment name] 选择了你的deployment，DEPLOY DATE=&quot;$(date)&quot; 修改了deployment date 并导致pod 更新。这种方式也是无感知的。</p></blockquote><h2><span id="方法3副本扩缩容">方法3：副本扩/缩容</span></h2><p>当副本数量设置为 0 时，Kubernetes 会消除它不再需要的副本。设置大于 0 后，Kubernetes 会生成新的副本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl scale --replicas&#x3D;0 deployment nginx-deploy</span><br><span class="line">$ kubectl scale --replicas&#x3D;N deployment nginx-deploy</span><br></pre></td></tr></table></figure><p>以上方式会中断业务，我们也可以使用不中断业务的方式来重启应用。比如记录当前的 Pod 副本数量，使用 <code>scale --replicas</code> 命令来分配一个大于当前的副本数值的值，使用 <code>kubectl delete pod</code> 方式删除旧 Pod，最后将使用<code>scale --replicas</code> 还原成正常的副本数量。</p><blockquote><p>提示：不建议使用这种方式来重启应用。</p></blockquote><h2><span id="结论">结论</span></h2><p>Kubernetes 是一个非常牛X的编排系统，然而只要是系统，就一定会出现问题。当出现问题时，可以利用上述 3 种方式快速安全地让您的应用程序恢复并运行，而不会影响用户体验。</p><h2><span id="参考引用">参考引用</span></h2><ul><li>[1] <a href="https://linuxhint.com/kubectl-rollout-restart/" target="_blank" rel="noopener">https://linuxhint.com/kubectl-rollout-restart/</a></li><li>[2] <a href="https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/</a></li></ul><blockquote><p>本文转载自：「 枯惠 」，原文：<a href="https://tinyurl.com/yse2usy9" target="_blank" rel="noopener">https://tinyurl.com/yse2usy9</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 中的 Pod 通常应该是 “Running” 状态，然而有时候我们需要针对正在运行的 Pod 调度到其它的节点或是基于其它特殊的原因，将正常运行的 Pod 进行重启。Pod 的重启方式也有不少，比如常见删除正在运行的 Pod 让其创建新的 Pod 实例（单个 Pod 无法直接使用该方式）。以下罗列出几种常见的方式仅供备忘：&lt;/p&gt;
&lt;h2 id=&quot;方法1：滚动重启方法-推荐&quot;&gt;方法1：滚动重启方法[推荐]&lt;/h2&gt;
&lt;p&gt;自 Kubernetes 1.15 版本就开始支持滚动重启部署。这是 Kubernetes 中最快的重启机制，因为它是新增的。下面给出的命令会一个一个地关闭并重新启动部署中的每个 Pod。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ kubectl rollout restart deployment nginx-deploy&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;提示：由于大多数容器仍在运行，因此整个过程是纵享丝滑–无感知的。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>如何使用 Skopeo 做一个优雅的镜像搬运工</title>
    <link href="https://www.hi-linux.com/posts/55385.html"/>
    <id>https://www.hi-linux.com/posts/55385.html</id>
    <published>2022-06-01T01:00:00.000Z</published>
    <updated>2022-06-01T08:29:56.618Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="1-基础介绍">1. 基础介绍</span></h2><p>描述: 作为公司内部 PaaS toB 产品的打包发布人员，容器镜像对我们打工人而言就像是工地上的砖头 🧱，而我的一部分工作就是将这些砖头在各个仓库之间搬来搬去，最终将这些砖头打包放在产品的安装包中，形成一个完整的 PaaS 产品安装包。</p><ul><li>Q: 在 PaaS (平台即服务)中的大家常说的 ToB 与 ToC 到底是什么?</li></ul><blockquote><p>ToC 面向普通用户服务, 主要是让用户体验感好，解决用户使用方面的问题记录，并返回给前后端开发。<br>ToB 是面向企业用户服务, 产品可用、其中最关键是让Boss使用Happly!</p></blockquote><ul><li>Q: 假如有如下场景，我们从 dockerhub 公共仓库中下载一个 GB 以上的镜像，到本地的私有仓库中，我想通常你会这样做先 <code>docker pull</code> 到本地，然后使用 <code>docker tag</code> 更改为私有仓库地址加上镜像名称版本，最后再使用<code>docker push</code> 上传镜像到私有仓库中，以供其它内网机器拉取并使用。虽然该方法是可行，但是如果有多个大于 GB 以上的镜像需要上传到私有仓库，每次都要先解压 layer 到本地，然后再压缩 layer 上传到私有仓库中，你能想象此过程花费的时间有多久吗? 对于我们运维工程师来说时间就是金钱，所以需想尽一切方法来节约时间成本，那有没有一种办法可以直接将 registry 上的 blob 复制到另一个 registry，中间过程不涉及对镜像 layer 的解压缩，这岂不美哉。</li></ul><blockquote><p>解决方案当然是存在的，如果你不想使用docker进行images镜像拉取上传，我们完成可以使用skope工具来完全替代 docker-cli 来搬运镜像，skopeo是一个命令行实用程序，可对容器映像和映像存储库执行各种操作。</p></blockquote><a id="more"></a><p><strong>什么是 Skopeo ?</strong></p><p>skopeo 使用 API V2 Registry，例如 Docker Registry、Atomic Registry、私有Registry、本地目录和本地 OCI 镜像目录。skopeo 不需要运行守护进程，它可以执行的操作包括：</p><ul><li>通过各种存储机制复制镜像，例如，可以在不需要特权的情况下将镜像从一个 Registry 复制到另一个 Registry</li><li>检测远程镜像并查看其属性，包括其图层，无需将镜像拉到本地</li><li>从镜像库中删除镜像</li><li>当存储库需要时，skopeo 可以传递适当的凭据和证书进行身份验证</li></ul><p><strong>镜像存储特点</strong></p><p>根据 Robin 大佬在 《镜像仓库中镜像存储的原理解析》文章里得出的结论：</p><ul><li><p>通过 Registry API 获得的两个镜像仓库中相同镜像的 manifest 信息完全相同。</p></li><li><p>两个镜像仓库中相同镜像的 manifest 信息的存储路径和内容完全相同。</p></li><li><p>两个镜像仓库中相同镜像的 blob 信息的存储路径和内容完全相同</p></li></ul><p><strong>项目信息</strong></p><ul><li>Github 官方地址: <a href="https://github.com/containers/skopeo" target="_blank" rel="noopener">https://github.com/containers/skopeo</a></li><li>Gitee mirror: <a href="https://gitee.com/mirrors/skopeo" target="_blank" rel="noopener">https://gitee.com/mirrors/skopeo</a></li></ul><h2><span id="2-安装编译">2. 安装编译</span></h2><p>描述: Skopeo 官方安装&amp;编译方式参考文档: <a href="https://github.com/containers/skopeo/blob/main/install.md" target="_blank" rel="noopener">https://github.com/containers/skopeo/blob/main/install.md</a></p><p>本节安装实践环境将在 Ubuntu 20.04 LTS 以及 docker 20.10.12 中进行实践源码编译以及 apt 仓库源下载安装实践。</p><h3><span id="21-源码编译静态">2.1 源码编译（静态）</span></h3><p>描述: 要构建 skopeo 二进制文件您至少需要 Go 1.12 版本以上, 其次构建 skopeo 有两种方法，即<code>在容器中</code>或者在本地环境中构建(安装环境较为复杂), 此处为了方便演示将采用容器方式进行编译构建。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 1.拉取skopeo源码到本地</span><br><span class="line">$ git clone --depth&#x3D;1 https:&#x2F;&#x2F;github.com&#x2F;containers&#x2F;skopeo.git  # https:&#x2F;&#x2F;github.com&#x2F;containers&#x2F;skopeo.git</span><br><span class="line">$ cd skopeo</span><br><span class="line">$ sed -i &#39;s#proxy.golang.org#https:&#x2F;&#x2F;goproxy.cn#g&#39; skopeo&#x2F;Makefile</span><br><span class="line"></span><br><span class="line"># 2.下载镜像构建依赖</span><br><span class="line">$ sudo apt-get install go-md2man  # 构建手册依赖于 go-md2man。</span><br><span class="line">$ whereis go-md2man  # 获得本机中go-md2man路径。</span><br><span class="line"></span><br><span class="line"># 3.构建静态二进制文件</span><br><span class="line">$ BUILD_IMAGE&#x3D;&quot;golang:latest&quot;</span><br><span class="line">$ docker run --name skopeo-build -v $PWD:&#x2F;src -v &#x2F;usr&#x2F;bin&#x2F;go-md2man:&#x2F;go&#x2F;bin&#x2F;go-md2man -w &#x2F;src -e CGO_ENABLED&#x3D;0 -e GOPROXY&#x3D;https:&#x2F;&#x2F;goproxy.cn,direct $&#123;BUILD_IMAGE&#125; \</span><br><span class="line">sh -c &#39;make BUILDTAGS&#x3D;containers_image_openpgp GO_DYN_FLAGS&#x3D;&#39;</span><br><span class="line">  # CGO_CFLAGS&#x3D;&quot;&quot; CGO_LDFLAGS&#x3D;&quot;&quot; GO111MODULE&#x3D;on go build -mod&#x3D;vendor  -ldflags &#39;-X main.gitCommit&#x3D;df4d82b960572c19e9333381a203c0ac475766d7 &#39; -gcflags &quot;&quot; -tags  &quot;containers_image_openpgp&quot; -o bin&#x2F;skopeo .&#x2F;cmd&#x2F;skopeo</span><br><span class="line"></span><br><span class="line"># 4.运行编译生成的skopeo可执行文件</span><br><span class="line">$ cd .&#x2F;bin # &#x2F;opt&#x2F;software&#x2F;skopeo&#x2F;bin</span><br><span class="line">$ .&#x2F;skopeo --help</span><br><span class="line">  # Various operations with container images and container image registries</span><br><span class="line">  # .......</span><br><span class="line">  # Use &quot;skopeo [command] --help&quot; for more information about a command.</span><br></pre></td></tr></table></figure><p><strong>构建关键参数解析:</strong></p><ul><li>CGO_ENABLED=0 : 设置该环境变量, 禁用 CGO 会导致 Go 在可能的情况下更喜欢静态连接库，而不是动态链接到系统库 (解决可以在Ubuntu或者其它linux发行版中执行编译后二进制文件)。</li><li>GOPROXY=https://goproxy.cn,direct : Golong 依赖下载镜像站,加快go get依赖拉拉取。</li><li>BUILDTAGS=containers_image_openpgp : 设置该make参数消除了对libgpgme 及其配套库的依赖, Skopeo 的一些特性依赖于非 Go 库，例如 libgpgme 和 libdevmapper。</li><li>GO_DYN_FLAGS= : 清空该make参数 (否则会强制创建动态可执行文件)</li></ul><h3><span id="22-分发包安装">2.2 分发包安装</span></h3><p>描述: skopeo 可能已经打包在您的发行版中，此处以 ubuntu 20.04 为例进行安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 1.只支持 Ubuntu 20.10 and newer 发行版 </span><br><span class="line">$ sudo apt-get -y update</span><br><span class="line">$ sudo apt-get -y install skopeo</span><br><span class="line"></span><br><span class="line"># 2.但 Kubic 项目为 Ubuntu 20.04 提供了软件包，我们可以通过如下方式在我们及其上进行安装。</span><br><span class="line">$ . &#x2F;etc&#x2F;os-release</span><br><span class="line">$ echo &quot;deb https:&#x2F;&#x2F;download.opensuse.org&#x2F;repositories&#x2F;devel:&#x2F;kubic:&#x2F;libcontainers:&#x2F;stable&#x2F;xUbuntu_$&#123;VERSION_ID&#125;&#x2F; &#x2F;&quot; | sudo tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;devel:kubic:libcontainers:stable.list</span><br><span class="line">$ curl -L https:&#x2F;&#x2F;download.opensuse.org&#x2F;repositories&#x2F;devel:&#x2F;kubic:&#x2F;libcontainers:&#x2F;stable&#x2F;xUbuntu_$&#123;VERSION_ID&#125;&#x2F;Release.key | sudo apt-key add -</span><br><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get -y upgrade</span><br><span class="line">$ sudo apt-get -y install skopeo</span><br></pre></td></tr></table></figure><h3><span id="23-容器安装运行">2.3 容器安装运行</span></h3><p>Skopeo 容器镜像可在 <a href="http://quay.io/skopeo/stable:latest" target="_blank" rel="noopener">quay.io/skopeo/stable:latest</a> 获得, 例如我们采用podman命令进行如下操作:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ podman run docker:&#x2F;&#x2F;quay.io&#x2F;skopeo&#x2F;stable:latest copy --help</span><br></pre></td></tr></table></figure><h2><span id="3-快速上手">3. 快速上手</span></h2><h3><span id="31-命令浅析">3.1 命令浅析</span></h3><p>描述: skopen 是操作各种容器映像和容器映像仓库的工具，其使用方法及其可用命令如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;skopeo --help    # 子命令可采用如下命令 skopeo [command] --help 命令</span><br><span class="line">Usage:</span><br><span class="line">  skopeo [flags]</span><br><span class="line">  skopeo [command]</span><br><span class="line">Available Commands: </span><br><span class="line">  copy          # 复制一个镜像从 A 到 B，这里的 A 和 B 可以为本地 docker 镜像或者 registry 上的镜像；</span><br><span class="line">  delete        # 删除一个镜像 tag，可以是本地 docker 镜像或者 registry 上的镜像；</span><br><span class="line">  help          # 帮助查看</span><br><span class="line">  inspect       # 查看一个镜像的 manifest 或者 image config 详细信息；</span><br><span class="line">  list-tags     # 列出存储库名称指定的镜像的tag</span><br><span class="line">  login           # 登陆某个镜像仓库,类似于 docker login 命令</span><br><span class="line">  logout          # 退出某个已认证的镜像仓库, 类似于 docker logout 命令</span><br><span class="line">  manifest-digest # 计算文件的清单摘要是一个sha256sum 值</span><br><span class="line">  standalone-sign   # 使用本地文件创建签名</span><br><span class="line">  standalone-verify # 验证本地文件的签名</span><br><span class="line">  sync              # 将一个或多个图像从一个位置同步到另一个位置 (该功能非常Nice)</span><br><span class="line">Flags:</span><br><span class="line">    --command-timeout duration   # 命令超时时间(单位秒)</span><br><span class="line">    --debug                      # 启用debug模式</span><br><span class="line">    --insecure-policy            # 在不进行任何策略检查的情况下运行该工具（如果没有配置 policy 的话需要加上该参数）</span><br><span class="line">    --override-arch ARCH         # 处理镜像时覆盖客户端 CPU 体系架构，如在 amd64 的机器上用 skopeo 处理 arm64 的镜像</span><br><span class="line">    --override-os OS             # 处理镜像时覆盖客户端 OS</span><br><span class="line">    --override-variant VARIANT   # 处理镜像时使用VARIANT而不是运行架构变量</span><br><span class="line">    --policy string              # 信任策略文件的路径 (为镜像配置安全策略情况下使用)</span><br><span class="line">    --registries.d DIR           # 在目录中使用Registry配置文件（例如，用于容器签名存储）</span><br><span class="line">    --tmpdir string              # 用于存储临时文件的目录</span><br><span class="line">-h, --help                       help for skopeo </span><br><span class="line">-v, --version                    Version for Skopeo</span><br></pre></td></tr></table></figure><h3><span id="32-skopeo-初体验">3.2 Skopeo 初体验</span></h3><p>描述: 在使用体验skopeo之前，我们需要了解一哈 Skopeo 可以在那些图像和存储库类型上执行镜像操作(官网文档走一波)：</p><table><thead><tr><th style="text-align:center">Repository types</th><th>Describe</th><th>Example</th></tr></thead><tbody><tr><td style="text-align:center"><code>containers-storage:docker-reference</code></td><td>适用于后端是 Podman, CRI-O, Buildah 的情况</td><td><code>containers-storage:</code></td></tr><tr><td style="text-align:center"><code>dir:path</code></td><td>适用于将manifest, layer tarballs 和 signatures 存储为单独文件的现有本地目录路径的情况</td><td><code>dir:/tmp/alpine:latest</code></td></tr><tr><td style="text-align:center"><code>docker://docker-reference</code></td><td>适用于Registry中实现&quot;Docker Registry HTTP API V2&quot;的镜像的情况</td><td><code>docker://harbor.weiyigeek.top/myblog:v2.8</code></td></tr><tr><td style="text-align:center"><code>docker-archive:path[:docker-reference]</code></td><td>适用于采用<code>docker save</code>命令导出镜像以tar格式存储的文件的情况</td><td><code>docker-archive:alpine.tar</code></td></tr><tr><td style="text-align:center"><code>docker-daemon:docker-reference</code></td><td>适用于存储在 docker 守护进程内部存储中的图像的情况</td><td><code>docker-daemon:alpine:latest</code></td></tr><tr><td style="text-align:center"><code>oci:path:tag</code></td><td>适用于符合&quot;Open Container Image Layout Specification&quot;的目录中的图像标记</td><td><code>oci:alpine:latest</code></td></tr></tbody></table><blockquote><p>温馨提示: 同一个镜像存在的方式有可能不同，不同类型方式存储对镜像的 layer 处理的方式也不一样,。</p></blockquote><p><strong>测试环境说明</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Docker 官方 hub 仓库 -&gt; docker.io             # 官网地址: https:&#x2F;&#x2F;hub.docker.com&#x2F;</span><br><span class="line">私有 Harbor 仓库     -&gt; harbor.weiyigeek.top</span><br><span class="line">临时创建的本地仓库    -&gt; 192.168.12.111:5000   # 一梭子解决: docker run -d -p 5000:5000 --name registry -v &#x2F;opt&#x2F;data&#x2F;registry:&#x2F;var&#x2F;lib&#x2F;registry registry:2</span><br></pre></td></tr></table></figure><blockquote><p>说明: 上述仓库都是在Registry中支持Docker Registry HTTP API V2版本的。</p></blockquote><h4><span id="321-skopeo-loginloout-远程仓库-auth">3.2.1 Skopeo login/loout - 远程仓库 Auth</span></h4><p>描述: 在使用 skopeo 前如果 src 或 dest 镜像是在 registry 仓库中的并且配置了非 public 的镜像需要相应的 auth 认证, 此时我们可以使用 <code>docker login</code> 或者 <code>skopeo login</code> 的方式登录到 registry 仓库，然后默认会在<code>~/.docker</code>目录下生成 registry 登录配置文件 config.json ,该文件里保存了登录需要的验证信息，skopeo 拿到该验证信息才有权限往 registry push 镜像。</p><ul><li><strong>登陆认证</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># (1) skopeo login 登陆示例 (两种方式)</span><br><span class="line">$ skopeo login -u WeiyiGeek -p testpassword harbor.weiyigeek.top</span><br><span class="line">  # Login Succeeded!</span><br><span class="line"></span><br><span class="line"># (2) docker login 登陆示例</span><br><span class="line">$ docker login -u WeiyiGeek docker.io</span><br><span class="line">$ docker login -u WeiyiGeek harbor.weiyigeek.top</span><br><span class="line">$ docker login -u anonymous -p anonymous 192.168.12.111:5000  # 实际上临时仓库没有配置认证, 账号密码随意即可。</span><br><span class="line">  # WARNING! Using --password via the CLI is insecure. Use --password-stdin.</span><br><span class="line">  # WARNING! Your password will be stored unencrypted in &#x2F;root&#x2F;.docker&#x2F;config.json.</span><br><span class="line">  # Configure a credential helper to remove this warning. See</span><br><span class="line">  # https:&#x2F;&#x2F;docs.docker.com&#x2F;engine&#x2F;reference&#x2F;commandline&#x2F;login&#x2F;#credentials-store</span><br><span class="line">  # Login Succeeded</span><br><span class="line"></span><br><span class="line"># (3) docker login 生成的 registry 登录配置文件（base64编码安全性不多说）</span><br><span class="line">$ cat ~&#x2F;.docker&#x2F;config.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;auths&quot;: &#123;</span><br><span class="line">    &quot;192.168.12.111:5000&quot;: &#123;</span><br><span class="line">        &quot;auth&quot;: &quot;YW5v*******Q&#x3D;&#x3D;&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">    &quot;harbor.weiyigeek.top&quot;: &#123;</span><br><span class="line">        &quot;auth&quot;: &quot;YWR*******LkA&#x3D;&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">    &quot;https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;&quot;: &#123;</span><br><span class="line">        &quot;auth&quot;: &quot;d2Vp**************kyZA&#x3D;&#x3D;&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>注销认证</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ skopeo logout myregistrydomain.com:5000</span><br></pre></td></tr></table></figure><blockquote><p>温馨提示: 如果企业自建harbor仓库(一般都会设置自签证书)或者其它私有仓库配置证书,为了防止出错建议进行以下操作(正式环境请根据需要进行配置)。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># (1) 在 &#x2F;etc&#x2F;docker&#x2F;daemon.json 中配置 insecure-registries 字段,表示允许不安全的仓库。</span><br><span class="line">&quot;insecure-registries&quot;: [&quot;harbor.weiyigeek.top&quot;,&quot;192.168.12.111:5000&quot;]</span><br><span class="line"> </span><br><span class="line"># (2) 从官方文档可知客户端要使用tls与Harbor通信使用的还是&#96;自签证书&#96;，那么必须建立一个目录：&#96;&#x2F;etc&#x2F;docker&#x2F;certs.d&#96;</span><br><span class="line"># 如果配置可能会出现 x509: certificate signed by unknown authority 错误提示。</span><br><span class="line">$ mkdir -vp &#x2F;etc&#x2F;docker&#x2F;certs.d&#x2F;harbor.weiyigeek.top</span><br><span class="line">$ cp -a &#x2F;deployapp&#x2F;harbor&#x2F;harbor.pem  &#x2F;etc&#x2F;docker&#x2F;certs.d&#x2F;harbor.weiyigeek.top&#x2F;harbor.crt</span><br></pre></td></tr></table></figure><blockquote><p>温馨提示: 为了防止后续执行 skopeo 命令操作镜像时出错, 建议忽略 policy 策略和证书校验参数如下:</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">--insecure-policy \</span><br><span class="line">--src-tls-verify&#x3D;false \ </span><br><span class="line">--dest-tls-verify&#x3D;false \</span><br></pre></td></tr></table></figure><h4><span id="322-skopeo-inspect-检查存储库中的镜像">3.2.2 Skopeo inspect - 检查存储库中的镜像</span></h4><p>描述: skopeo 能够检查容器 Registry 上的存储库并获取图像层。检查命令获取存储库的清单，它能够向您显示有关整个存储库或标签的类似 <code>docker inspect</code> 的 json 输出。与 docker inspect 相比,此工具可帮助您在拉取存储库或标签之前收集有用的信息(使用磁盘空间), 检查命令可以向您显示给定存储库可用的标签、映像具有的标签、映像的创建日期和操作系统等。</p><p>支持传输的类型 : <code>containers-storage, dir, docker, docker-archive, docker-daemon, oci, oci-archive, ostree, tarball</code></p><ul><li>步骤 01. 显示 busybox:latest 镜像的属性相关信息。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ skopeo inspect docker:&#x2F;&#x2F;docker.io&#x2F;busybox:latest</span><br><span class="line">&#123;</span><br><span class="line">  &quot;Name&quot;: &quot;docker.io&#x2F;library&#x2F;busybox&quot;,</span><br><span class="line">  &quot;Digest&quot;: &quot;sha256:5acba83a746c7608ed544dc1533b87c737a0b0fb730301639a0179f9344b1678&quot;,</span><br><span class="line">  &quot;RepoTags&quot;: [</span><br><span class="line">      &quot;1&quot;,</span><br><span class="line">      &quot;1-glibc&quot;,</span><br><span class="line">      &quot;1-musl&quot;,</span><br><span class="line">      &quot;1-ubuntu&quot;,</span><br><span class="line">      &quot;1-uclibc&quot;,</span><br><span class="line">      &quot;1.21-ubuntu&quot;,</span><br><span class="line">      &quot;1.21.0-ubuntu&quot;,</span><br><span class="line">        .......          # 镜像历史tags</span><br><span class="line">      &quot;unstable-uclibc&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;Created&quot;: &quot;2021-12-30T19:19:41.006954958Z&quot;,</span><br><span class="line">  &quot;DockerVersion&quot;: &quot;20.10.7&quot;,</span><br><span class="line">  &quot;Labels&quot;: null,</span><br><span class="line">  &quot;Architecture&quot;: &quot;amd64&quot;,</span><br><span class="line">  &quot;Os&quot;: &quot;linux&quot;,</span><br><span class="line">  &quot;Layers&quot;: [</span><br><span class="line">      &quot;sha256:5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;Env&quot;: [</span><br><span class="line">      &quot;PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>步骤 02. 显示 busybox:latest 镜像的容器配置相关信息。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ skopeo inspect --config docker:&#x2F;&#x2F;docker.io&#x2F;busybox:latest  | jq</span><br><span class="line">&#123;</span><br><span class="line">  &quot;created&quot;: &quot;2021-12-30T19:19:41.006954958Z&quot;,</span><br><span class="line">  &quot;architecture&quot;: &quot;amd64&quot;,</span><br><span class="line">  &quot;os&quot;: &quot;linux&quot;,</span><br><span class="line">  &quot;config&quot;: &#123;</span><br><span class="line">    &quot;Env&quot;: [</span><br><span class="line">      &quot;PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;Cmd&quot;: [</span><br><span class="line">      &quot;sh&quot;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;rootfs&quot;: &#123;</span><br><span class="line">    &quot;type&quot;: &quot;layers&quot;,</span><br><span class="line">    &quot;diff_ids&quot;: [</span><br><span class="line">      &quot;sha256:01fd6df81c8ec7dd24bbbd72342671f41813f992999a3471b9d9cbc44ad88374&quot;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;history&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;created&quot;: &quot;2021-12-30T19:19:40.833034683Z&quot;,</span><br><span class="line">      &quot;created_by&quot;: &quot;&#x2F;bin&#x2F;sh -c #(nop) ADD file:6db446a57cbd2b7f4cfde1f280177b458390ed5a6d1b54c6169522bc2c4d838e in &#x2F; &quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;created&quot;: &quot;2021-12-30T19:19:41.006954958Z&quot;,</span><br><span class="line">      &quot;created_by&quot;: &quot;&#x2F;bin&#x2F;sh -c #(nop)  CMD [\&quot;sh\&quot;]&quot;,</span><br><span class="line">      &quot;empty_layer&quot;: true</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>步骤 03. 显示未经验证的图像 Digest（摘要）</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ skopeo inspect --format &quot;Name: &#123;&#123;.Name&#125;&#125; Digest: &#123;&#123;.Digest&#125;&#125;&quot; docker:&#x2F;&#x2F;docker.io&#x2F;busybox:latest</span><br></pre></td></tr></table></figure><h4><span id="323-skopeo-copy-仓库镜像拷贝">3.2.3 Skopeo copy - 仓库镜像拷贝</span></h4><p>描述: skopeo 可以在各种存储机制之间复制容器镜像，支持包括容器仓库(<code>The Quay, Docker Hub, OpenShift, GCR, ，Artifactory ...</code>)以及容器存储后端 (<code>Podman, CRI-O, Docker</code>) 等、本地目录、本地 OCI-layout 目录。</p><p>例如，此处我从 hub 仓库复制 <code>busybox:latest</code> 镜像到私有 harbor 仓库中,在从私有 harbor 仓库中拷贝到本地指定目录中。</p><ul><li>步骤 01. 从 regsitry A 到 registry B 复制 busybox:latest 镜像。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ skopeo copy --insecure-policy --src-tls-verify&#x3D;false --dest-tls-verify&#x3D;false --dest-authfile &#x2F;root&#x2F;.docker&#x2F;config.json docker:&#x2F;&#x2F;docker.io&#x2F;busybox:latest docker:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;devops&#x2F;busybox:latest</span><br><span class="line">  # Getting image source signatures</span><br><span class="line">  # Copying blob 5cc84ad355aa done</span><br><span class="line">  # Copying config beae173cca done</span><br><span class="line">  # Writing manifest to image destination</span><br><span class="line">  # Storing signatures</span><br></pre></td></tr></table></figure><blockquote><p>Tips: 由上述日志可以看到 skopeo 是直接从 registry 中 copy 镜像 layer 的 blob 文件，传输是镜像在 registry 中存储的原始格式。</p></blockquote><ul><li>步骤 02. 从 registry B 复制 busybox:latest 镜像到本地 busybox:latest 目录中。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">$ skopeo copy --insecure-policy --src-tls-verify&#x3D;false docker:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;devops&#x2F;busybox:latest dir:busybox:latest</span><br><span class="line"># Getting image source signatures</span><br><span class="line"># Copying blob 5cc84ad355aa done</span><br><span class="line"># Copying config beae173cca done</span><br><span class="line"># Writing manifest to image destination</span><br><span class="line"># Storing signatures</span><br><span class="line"></span><br><span class="line">$ ls &amp;&amp; tree busybox\:latest&#x2F;</span><br><span class="line">busybox:latest&#x2F;</span><br><span class="line">├── 5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa   # blob 块文件 -&gt; vnd.docker.image.rootfs.diff.tar.gzip</span><br><span class="line">├── beae173ccac6ad749f76713cf4440fe3d21d1043fe616dfbe30775815d1d0f6a   # 镜像配置信息文件 -&gt; vnd.docker.container.image.v1+json </span><br><span class="line">├── manifest.json</span><br><span class="line">└── version</span><br><span class="line">0 directories, 4 files</span><br><span class="line"></span><br><span class="line"># 查看镜像的 manifest 文件</span><br><span class="line">$ cat busybox\:latest&#x2F;manifest.json</span><br><span class="line">&#123;</span><br><span class="line">   &quot;schemaVersion&quot;: 2,</span><br><span class="line">   &quot;mediaType&quot;: &quot;application&#x2F;vnd.docker.distribution.manifest.v2+json&quot;,</span><br><span class="line">   &quot;config&quot;: &#123;</span><br><span class="line">      &quot;mediaType&quot;: &quot;application&#x2F;vnd.docker.container.image.v1+json&quot;,</span><br><span class="line">      &quot;size&quot;: 1456,</span><br><span class="line">      &quot;digest&quot;: &quot;sha256:beae173ccac6ad749f76713cf4440fe3d21d1043fe616dfbe30775815d1d0f6a&quot;</span><br><span class="line">   &#125;,</span><br><span class="line">   &quot;layers&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">         &quot;mediaType&quot;: &quot;application&#x2F;vnd.docker.image.rootfs.diff.tar.gzip&quot;,</span><br><span class="line">         &quot;size&quot;: 772788,</span><br><span class="line">         &quot;digest&quot;: &quot;sha256:5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa&quot;</span><br><span class="line">      &#125;</span><br><span class="line">   ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 根据 manifest 文件查看镜像的 image config 文件 (存放镜像Build指令与镜像相关配置信息)</span><br><span class="line">$ jq &#39;.&#39; busybox\:latest&#x2F;beae173ccac6ad749f76713cf4440fe3d21d1043fe616dfbe30775815d1d0f6a</span><br><span class="line">&#123;</span><br><span class="line">  &quot;architecture&quot;: &quot;amd64&quot;,</span><br><span class="line">  &quot;config&quot;: &#123;</span><br><span class="line">    &quot;Hostname&quot;: &quot;&quot;,</span><br><span class="line">    &quot;Domainname&quot;: &quot;&quot;,</span><br><span class="line">    &quot;User&quot;: &quot;&quot;,</span><br><span class="line">    &quot;AttachStdin&quot;: false,</span><br><span class="line">    &quot;AttachStdout&quot;: false,</span><br><span class="line">    &quot;AttachStderr&quot;: false,</span><br><span class="line">    &quot;Tty&quot;: false,</span><br><span class="line">    &quot;OpenStdin&quot;: false,</span><br><span class="line">    &quot;StdinOnce&quot;: false,</span><br><span class="line">    &quot;Env&quot;: [</span><br><span class="line">      &quot;PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;sbin:&#x2F;bin&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;Cmd&quot;: [</span><br><span class="line">      &quot;sh&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;Image&quot;: &quot;sha256:da658412c37aa24e561eb7e16c61bc82a9711340d8fb5cf1a8f39d8e96d7f723&quot;,</span><br><span class="line">    &quot;Volumes&quot;: null,</span><br><span class="line">    &quot;WorkingDir&quot;: &quot;&quot;,</span><br><span class="line">    &quot;Entrypoint&quot;: null,</span><br><span class="line">    &quot;OnBuild&quot;: null,</span><br><span class="line">    &quot;Labels&quot;: null</span><br><span class="line">  &#125;,</span><br><span class="line">........</span><br><span class="line">  &quot;history&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;created&quot;: &quot;2021-12-30T19:19:40.833034683Z&quot;,</span><br><span class="line">      &quot;created_by&quot;: &quot;&#x2F;bin&#x2F;sh -c #(nop) ADD file:6db446a57cbd2b7f4cfde1f280177b458390ed5a6d1b54c6169522bc2c4d838e in &#x2F; &quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;created&quot;: &quot;2021-12-30T19:19:41.006954958Z&quot;,</span><br><span class="line">      &quot;created_by&quot;: &quot;&#x2F;bin&#x2F;sh -c #(nop)  CMD [\&quot;sh\&quot;]&quot;,</span><br><span class="line">      &quot;empty_layer&quot;: true</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  &quot;os&quot;: &quot;linux&quot;,</span><br><span class="line">  &quot;rootfs&quot;: &#123;</span><br><span class="line">    &quot;type&quot;: &quot;layers&quot;,</span><br><span class="line">    &quot;diff_ids&quot;: [</span><br><span class="line">      &quot;sha256:01fd6df81c8ec7dd24bbbd72342671f41813f992999a3471b9d9cbc44ad88374&quot;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>步骤 03. 将 busybox:latest 镜像从 registry B 复制到本地目录，并以 OCI 格式保存</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ skopeo copy --insecure-policy --src-tls-verify&#x3D;false docker:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;devops&#x2F;busybox:latest oci:busybox-latest</span><br><span class="line">  # Getting image source signatures</span><br><span class="line">  # Copying blob 5cc84ad355aa done</span><br><span class="line">  # Copying config 48edd9298a done</span><br><span class="line">  # Writing manifest to image destination</span><br><span class="line">  # Storing signatures</span><br><span class="line"></span><br><span class="line">$ tree -h busybox-latest&#x2F;</span><br><span class="line">  # busybox-latest&#x2F;</span><br><span class="line">  # ├── [4.0K]  blobs</span><br><span class="line">  # │   └── [4.0K]  sha256</span><br><span class="line">  # │       ├── [ 347]  1612e16ff3f6b0d09eefdc4e9d5c5c0624f63032743e016585b095b958778016</span><br><span class="line">  # │       ├── [ 575]  48edd9298a25de2c97cd574a5523026f87576c6b7202330a2b60ce7d304ec307</span><br><span class="line">  # │       └── [755K]  5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa  # Blob 块 -</span><br><span class="line">  # ├── [ 186]  index.json</span><br><span class="line">  # └── [  31]  oci-layout</span><br><span class="line">  # 2 directories, 5 files</span><br></pre></td></tr></table></figure><ul><li>步骤 04. 将 <code>alpine:3.13.1</code> 镜像从 docker 本地存储（ /var/lib/docker/image） push 到 registry B中(实际上替代 docker push 功能)</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 在  &#x2F;var&#x2F;lib&#x2F;docker&#x2F; 目录中此处主要关心 image (主要存放镜像中layer层的元数据) 和 overlay2 (各层的具体信息)</span><br><span class="line">$ docker images alpine:3.13.1</span><br><span class="line">  # REPOSITORY   TAG       IMAGE ID       CREATED         SIZE</span><br><span class="line">  # alpine       3.13.1    e50c909a8df2   11 months ago   5.61MB</span><br><span class="line"></span><br><span class="line">$ skopeo copy --insecure-policy --dest-tls-verify&#x3D;false --dest-authfile &#x2F;root&#x2F;.docker&#x2F;config.json docker-daemon:alpine:3.13.1 docker:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;devops&#x2F;alpine:3.13.1</span><br><span class="line">  # Getting image source signatures</span><br><span class="line">  # Copying blob 1119ff37d4a9 done</span><br><span class="line">  # Copying config e50c909a8d done</span><br><span class="line">  # Writing manifest to image destination</span><br><span class="line">  # Storing signatures</span><br></pre></td></tr></table></figure><p><img src="https://img.hi-linux.com/staticfile/640-20220520095506113-2022-05-20-jxlRy3.png" alt></p><h4><span id="324-skopeo-sync-镜像同步命令">3.2.4 Skopeo sync - 镜像同步命令</span></h4><p>描述: Skopeo sync可以在容器仓库和本地目录之间同步镜像，其功能类似于阿里云的 image-syncer (<a href="https://github.com/AliyunContainerService/image-syncer" target="_blank" rel="noopener">https://github.com/AliyunContainerService/image-syncer</a>) 工具, 实际上其比 image-syncer 更强大、灵活性更强一些，废话不多说实践为王。</p><p>skopeo sync 镜像同步文件示例:</p><ul><li>步骤 01. 将仓库中所有 busybox 镜像版本同步到本地目录。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ skopeo sync --insecure-policy --src-tls-verify&#x3D;false --src docker --dest dir harbor.weiyigeek.top&#x2F;devops&#x2F;busybox &#x2F;tmp</span><br><span class="line">  # INFO[0000] Tag presence check                            imagename&#x3D;harbor.weiyigeek.top&#x2F;devops&#x2F;busybox tagged&#x3D;false</span><br><span class="line">  # INFO[0000] Getting tags                                  image&#x3D;harbor.weiyigeek.top&#x2F;devops&#x2F;busybox</span><br><span class="line">  # INFO[0000] Copying image ref 1&#x2F;1                         from&#x3D;&quot;docker:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;devops&#x2F;busybox:latest&quot; to&#x3D;&quot;dir:&#x2F;tmp&#x2F;busybox:latest&quot;</span><br><span class="line">  # Getting image source signatures</span><br><span class="line">  # Copying blob 5cc84ad355aa done</span><br><span class="line">  # Copying config beae173cca done</span><br><span class="line">  # Writing manifest to image destination</span><br><span class="line">  # Storing signatures</span><br><span class="line">  # INFO[0000] Synced 1 images from 1 sources</span><br><span class="line"></span><br><span class="line">$ tree -h &#x2F;tmp&#x2F;busybox:latest</span><br><span class="line">&#x2F;tmp&#x2F;busybox:latest</span><br><span class="line">├── [755K]  5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa</span><br><span class="line">├── [1.4K]  beae173ccac6ad749f76713cf4440fe3d21d1043fe616dfbe30775815d1d0f6a</span><br><span class="line">├── [ 527]  manifest.json</span><br><span class="line">└── [  33]  version</span><br><span class="line">0 directories, 4 files</span><br></pre></td></tr></table></figure><ul><li>步骤 02. 从本地目录 <code>/tmp/</code> 同步到 docker 的 hub 容器仓库中，此外我们可以通过浏览器看到 <code>weiyigeek</code> 用户下的 <code>busybox</code> 镜像 (<a href="https://hub.docker.com/u/weiyigeek" target="_blank" rel="noopener">https://hub.docker.com/u/weiyigeek</a>)。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ skopeo sync --insecure-policy --dest-tls-verify&#x3D;false --src dir --dest docker &#x2F;tmp weiyigeek</span><br><span class="line">  # INFO[0000] Copying image ref 1&#x2F;1                         from&#x3D;&quot;dir:&#x2F;tmp&#x2F;busybox:latest&quot; to&#x3D;&quot;docker:&#x2F;&#x2F;weiyigeek&#x2F;busybox:latest&quot;</span><br><span class="line">  # Getting image source signatures</span><br><span class="line">  # Copying blob 5cc84ad355aa skipped: already exists</span><br><span class="line">  # Copying config beae173cca done</span><br><span class="line">  # Writing manifest to image destination</span><br><span class="line">  # Storing signatures</span><br><span class="line">  # INFO[0021] Synced 1 images from 1 sources</span><br></pre></td></tr></table></figure><p><img src="https://img.hi-linux.com/staticfile/640-20220520095506152-2022-05-20-wOfOkf.png" alt></p><ul><li>步骤 03. 从 hub 容器仓库中同步 alpine-jenkins-jnlp:v2.285 镜像到本地临时容器仓库中。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ skopeo sync --insecure-policy --src-tls-verify&#x3D;false --dest-tls-verify&#x3D;false --src docker --dest docker weiyigeek&#x2F;alpine-jenkins-jnlp:v2.285 192.168.12.111:5000&#x2F;</span><br><span class="line">  # INFO[0000] Tag presence check imagename&#x3D;&quot;weiyigeek&#x2F;alpine-jenkins-jnlp:v2.285&quot; tagged&#x3D;true</span><br><span class="line">  # INFO[0000] Copying image ref 1&#x2F;1 from&#x3D;&quot;docker:&#x2F;&#x2F;weiyigeek&#x2F;alpine-jenkins-jnlp:v2.285&quot; to&#x3D;&quot;docker:&#x2F;&#x2F;192.168.12.111:5000&#x2F;alpine-jenkins-jnlp:v2.285&quot;</span><br><span class="line">  # Getting image source signatures</span><br><span class="line">  # Copying blob 68517a8c32d3 [&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&gt;-------------------------------] 45.0MiB &#x2F; 255.7MiB</span><br><span class="line">  # Copying blob 4c0d98bf9879 done</span><br></pre></td></tr></table></figure><ul><li>步骤 04. 以配置文件方式进行同步, 首先我们需要准备一个需要同步的资源清单。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># YAML 文件内容（用于 **--src yaml** 的源）</span><br><span class="line">$ cat &lt;&lt;&#39;EOF&#39; &gt; skopeo-sync.yml</span><br><span class="line">registry.example.com:</span><br><span class="line">  images:</span><br><span class="line">    busybox: []</span><br><span class="line">    redis:</span><br><span class="line">      - &quot;1.0&quot;</span><br><span class="line">      - &quot;2.0&quot;</span><br><span class="line">      - &quot;sha256:111111&quot;</span><br><span class="line">  images-by-tag-regex:</span><br><span class="line">      nginx: ^1\.13\.[12]-alpine-perl$</span><br><span class="line">  credentials:</span><br><span class="line">      username: john</span><br><span class="line">      password: this is a secret</span><br><span class="line">  tls-verify: true</span><br><span class="line">  cert-dir: &#x2F;home&#x2F;john&#x2F;certs</span><br><span class="line">quay.io:</span><br><span class="line">  tls-verify: false</span><br><span class="line">  images:</span><br><span class="line">    coreos&#x2F;etcd:</span><br><span class="line">      - latest</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 以yaml文件方式进行同步镜像到 my-registry.local.lan&#x2F;repo&#x2F; 仓库中</span><br><span class="line">$ skopeo sync --src yaml --dest docker skopeo-sync.yml my-registry.local.lan&#x2F;repo&#x2F;</span><br></pre></td></tr></table></figure><p>skopeo-sync.yml 文件中镜像匹配复制镜像说明:</p><ul><li><a href="http://registry.example.com/busybox" target="_blank" rel="noopener">registry.example.com/busybox</a> : 所有版本的镜像.</li><li><a href="http://registry.example.com/redis" target="_blank" rel="noopener">registry.example.com/redis</a> : 标记为“1.0”和“2.0”的图像以及带有摘要的图像&quot;sha256:0000000000000000000000000000000011111111111111111111111111111111&quot;.</li><li><a href="http://registry.example.com/nginx" target="_blank" rel="noopener">registry.example.com/nginx</a> : 图片标记为“1.13.1-alpine-perl”和“1.13.2-alpine-perl”.</li><li><a href="http://quay.io/coreos/etcd" target="_blank" rel="noopener">quay.io/coreos/etcd</a> : 拉取最新版本的镜像。</li></ul><h4><span id="325-skopeo-list-tags-仓库中镜像-tag-查看">3.2.5 Skopeo list-tags - 仓库中镜像 Tag 查看</span></h4><p>描述: 利用该命令我们可以列出 registry 上的某个镜像的所有 tag ，它是使用标准的 registry API 来获取镜像 tag。</p><p>简单示例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ skopeo list-tags docker:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;devops&#x2F;busybox:latest</span><br></pre></td></tr></table></figure><h4><span id="326-skopeo-delete-删除仓库中镜像-tag">3.2.6 Skopeo delete - 删除仓库中镜像 Tag</span></h4><p>描述: 使用该命令我们可以删除镜像 Tag,注意此处仅仅只是通过 registry API 来删除镜像的 tag（即删除了 tag 对 manifests 文件的引用）并非真正将镜像删除掉，如果想要删除镜像的 layer 还是需要通过 registry GC 的方式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"># 方式1. 利用 skopeo delete</span><br><span class="line">$ skopeo delete docker:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;devops&#x2F;busybox:latest --debug</span><br><span class="line">  # DEBU[0000] Loading registries configuration &quot;&#x2F;etc&#x2F;containers&#x2F;registries.conf&quot;</span><br><span class="line">  # DEBU[0000] Found credentials for harbor.weiyigeek.top in credential helper containers-auth.json in file &#x2F;home&#x2F;weiyigeek&#x2F;.docker&#x2F;config.json</span><br><span class="line">  # DEBU[0000] Using registries.d directory &#x2F;etc&#x2F;containers&#x2F;registries.d for sigstore configuration</span><br><span class="line">  # DEBU[0000]  No signature storage configuration found for harbor.weiyigeek.top&#x2F;devops&#x2F;busybox:latest, using built-in default file:&#x2F;&#x2F;&#x2F;home&#x2F;weiyigeek&#x2F;.local&#x2F;share&#x2F;containers&#x2F;sigstore</span><br><span class="line">  # DEBU[0000] Looking for TLS certificates and private keys in &#x2F;etc&#x2F;docker&#x2F;certs.d&#x2F;harbor.weiyigeek.top</span><br><span class="line">  # DEBU[0000]  crt: &#x2F;etc&#x2F;docker&#x2F;certs.d&#x2F;harbor.weiyigeek.top&#x2F;harbor.crt</span><br><span class="line">  # DEBU[0000] GET https:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;v2&#x2F;</span><br><span class="line">  # DEBU[0000] Ping https:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;v2&#x2F; status 401</span><br><span class="line">  # DEBU[0000] GET https:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;service&#x2F;token?account&#x3D;WeiyiGeek&amp;scope&#x3D;repository%3Adevops%2Fbusybox%3A%2A&amp;service&#x3D;harbor-registry</span><br><span class="line">  # DEBU[0000] GET https:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;v2&#x2F;devops&#x2F;busybox&#x2F;manifests&#x2F;latest</span><br><span class="line">  # DEBU[0000] DELETE https:&#x2F;&#x2F;harbor.weiyigeek.top&#x2F;v2&#x2F;devops&#x2F;busybox&#x2F;manifests&#x2F;sha256:62ffc2ed7554e4c6d360bce40bbcf196573dd27c4ce080641a2c59867e732dee</span><br><span class="line">  # DEBU[0000] Deleting &#x2F;home&#x2F;weiyigeek&#x2F;.local&#x2F;share&#x2F;containers&#x2F;sigstore&#x2F;devops&#x2F;busybox@sha256&#x3D;62ffc2ed7554e4c6d360bce40bbcf196573dd27c4ce080641a2c59867e732dee&#x2F;signature-1</span><br><span class="line"></span><br><span class="line"># 方式2.利用 curl 命令进行 registery 进行删除 Tag。</span><br><span class="line">$ curl --header &quot;Accept: application&#x2F;vnd.docker.distribution.manifest.v2+json&quot; -I -X GET http:&#x2F;&#x2F;192.168.12.111:5000&#x2F;v2&#x2F;busybox&#x2F;manifests&#x2F;latest</span><br><span class="line">  # HTTP&#x2F;1.1 200 OK</span><br><span class="line">  # Content-Length: 527</span><br><span class="line">  # Content-Type: application&#x2F;vnd.docker.distribution.manifest.v2+json</span><br><span class="line">  # Docker-Content-Digest: sha256:62ffc2ed7554e4c6d360bce40bbcf196573dd27c4ce080641a2c59867e732dee</span><br><span class="line">  # Docker-Distribution-Api-Version: registry&#x2F;2.0</span><br><span class="line">  # Etag: &quot;sha256:62ffc2ed7554e4c6d360bce40bbcf196573dd27c4ce080641a2c59867e732dee&quot;</span><br><span class="line">  # X-Content-Type-Options: nosniff</span><br><span class="line">  # Date: Thu, 20 Jan 2022 13:18:28 GMT</span><br><span class="line"></span><br><span class="line"># 一把梭织搞定</span><br><span class="line">$ Docker-Content-Digest&#x3D;$(curl -s --header &quot;Accept: application&#x2F;vnd.docker.distribution.manifest.v2+json&quot; -I -X GET http:&#x2F;&#x2F;192.168.12.111:5000&#x2F;v2&#x2F;busybox&#x2F;manifests&#x2F;latest | grep &quot;Docker-Content-Digest&quot; | cut -d &#39; &#39; -f 2)</span><br><span class="line">$ curl -I -X DELETE http:&#x2F;&#x2F;192.168.12.111:5000&#x2F;v2&#x2F;busybox&#x2F;manifests&#x2F;$&#123;Docker-Content-Digest&#125;</span><br></pre></td></tr></table></figure><h2><span id="4-镜像同步最佳实践">4. 镜像同步最佳实践</span></h2><p>本节，主要参考我前同事木子.其博客地址(<a href="https://blog.k8s.li" target="_blank" rel="noopener">https://blog.k8s.li</a>)。</p><h3><span id="41-指定文本中镜像同步">4.1 指定文本中镜像同步</span></h3><p>假如,给你一个镜像列表 images-list.txt, 其格式如下, 我们可以直接采用 shell 脚本调用 skopeo 进行执行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># images-list.txt</span><br><span class="line">cat &lt;&lt;&#39;EOF&#39; &gt; images-list.txt</span><br><span class="line">kubesphere&#x2F;kube-apiserver:v1.20.6</span><br><span class="line">kubesphere&#x2F;kube-scheduler:v1.20.6</span><br><span class="line">kubesphere&#x2F;kube-proxy:v1.20.6</span><br><span class="line">kubesphere&#x2F;kube-controller-manager:v1.20.6</span><br><span class="line">kubesphere&#x2F;kube-apiserver:v1.19.8</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li><strong>同步的 shell 脚本 <a href="http://skopeo-copy.sh" target="_blank" rel="noopener">skopeo-copy.sh</a></strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">GREEN_COL&#x3D;&quot;\\033[32;1m&quot;</span><br><span class="line">RED_COL&#x3D;&quot;\\033[1;31m&quot;</span><br><span class="line">NORMAL_COL&#x3D;&quot;\\033[0;39m&quot;</span><br><span class="line"></span><br><span class="line">SOURCE_REGISTRY&#x3D;$1</span><br><span class="line">TARGET_REGISTRY&#x3D;$2</span><br><span class="line"></span><br><span class="line"># shell 变量赋值，当没有从命令行中传递值给SOURCE_REGISTRY和TARGET_REGISTRY变量时，便采用下述值进行覆盖。</span><br><span class="line">: $&#123;IMAGES_LIST_FILE:&#x3D;&quot;images-list.txt&quot;&#125;</span><br><span class="line">: $&#123;TARGET_REGISTRY:&#x3D;&quot;hub.k8s.li&quot;&#125;</span><br><span class="line">: $&#123;SOURCE_REGISTRY:&#x3D;&quot;docker.io&quot;&#125;</span><br><span class="line"></span><br><span class="line">BLOBS_PATH&#x3D;&quot;docker&#x2F;registry&#x2F;v2&#x2F;blobs&#x2F;sha256&quot;</span><br><span class="line">REPO_PATH&#x3D;&quot;docker&#x2F;registry&#x2F;v2&#x2F;repositories&quot;</span><br><span class="line"></span><br><span class="line">set -eo pipefail</span><br><span class="line"></span><br><span class="line">CURRENT_NUM&#x3D;0</span><br><span class="line">ALL_IMAGES&#x3D;&quot;$(sed -n &#39;&#x2F;#&#x2F;d;s&#x2F;:&#x2F;:&#x2F;p&#39; $&#123;IMAGES_LIST_FILE&#125; | sort -u)&quot;</span><br><span class="line">TOTAL_NUMS&#x3D;$(echo &quot;$&#123;ALL_IMAGES&#125;&quot; | wc -l)</span><br><span class="line"></span><br><span class="line"># shopeo 拷贝函数，注意其传递的参数，此处值得学习记录。</span><br><span class="line">skopeo_copy() &#123;</span><br><span class="line"> if skopeo copy --insecure-policy --src-tls-verify&#x3D;false --dest-tls-verify&#x3D;false \</span><br><span class="line"> --override-arch amd64 --override-os linux -q docker:&#x2F;&#x2F;$1 docker:&#x2F;&#x2F;$2; then</span><br><span class="line">  echo -e &quot;$GREEN_COL Progress: $&#123;CURRENT_NUM&#125;&#x2F;$&#123;TOTAL_NUMS&#125; sync $1 to $2 successful $NORMAL_COL&quot;</span><br><span class="line"> else</span><br><span class="line">  echo -e &quot;$RED_COL Progress: $&#123;CURRENT_NUM&#125;&#x2F;$&#123;TOTAL_NUMS&#125; sync $1 to $2 failed $NORMAL_COL&quot;</span><br><span class="line">  exit 2</span><br><span class="line"> fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 调用拷贝函数并记录当前执行序号。</span><br><span class="line">for image in $&#123;ALL_IMAGES&#125;; do</span><br><span class="line"> let CURRENT_NUM&#x3D;$&#123;CURRENT_NUM&#125;+1</span><br><span class="line"> skopeo_copy $&#123;SOURCE_REGISTRY&#125;&#x2F;$&#123;image&#125; $&#123;TARGET_REGISTRY&#125;&#x2F;$&#123;image&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><ul><li><strong>执行命令和结果:</strong></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ bash sync.sh docker.io localhost:5000</span><br><span class="line">Progress: 1&#x2F;143 sync docker.io&#x2F;alpine:3.14 to localhost:5000&#x2F;alpine:3.14 successful</span><br><span class="line">Progress: 2&#x2F;143 sync docker.io&#x2F;busybox:1.31.1 to localhost:5000&#x2F;busybox:1.31.1 successful</span><br><span class="line">....</span><br><span class="line">Progress: 142&#x2F;143 sync docker.io&#x2F;weaveworks&#x2F;scope:1.13.0 to localhost:5000&#x2F;weaveworks&#x2F;scope:1.13.0 successful</span><br><span class="line">Progress: 143&#x2F;143 sync docker.io&#x2F;wordpress:4.8-apache to localhost:5000&#x2F;wordpress:4.8-apache successful</span><br></pre></td></tr></table></figure><h3><span id="42-使用-registry-存储特性同步">4.2 使用 registry 存储特性同步</span></h3><p>描述: 将镜像从 registry 中同步到本地目录，使用 registry 存储的特性，将本地目录中的镜像转换成 registry 存储的格式, 这样的好处就是可以去除一些 skopeo dir 中重复的 layers，减少镜像的总大小。</p><ul><li><a href="http://convert-images.sh" target="_blank" rel="noopener">convert-images.sh</a></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">set -eo pipefail</span><br><span class="line"></span><br><span class="line">GREEN_COL&#x3D;&quot;\\033[32;1m&quot;</span><br><span class="line">RED_COL&#x3D;&quot;\\033[1;31m&quot;</span><br><span class="line">NORMAL_COL&#x3D;&quot;\\033[0;39m&quot;</span><br><span class="line"></span><br><span class="line"># 命令行参数</span><br><span class="line">SOURCE_REGISTRY&#x3D;$1</span><br><span class="line">TARGET_REGISTRY&#x3D;$2</span><br><span class="line">IMAGES_DIR&#x3D;$2</span><br><span class="line"></span><br><span class="line">: $&#123;IMAGES_DIR:&#x3D;&quot;images&quot;&#125;</span><br><span class="line">: $&#123;IMAGES_LIST_FILE:&#x3D;&quot;images-list.txt&quot;&#125;</span><br><span class="line">: $&#123;SOURCE_REGISTRY:&#x3D;&quot;docker.io&quot;&#125;</span><br><span class="line">: $&#123;TARGET_REGISTRY:&#x3D;&quot;hub.k8s.li&quot;&#125;</span><br><span class="line"></span><br><span class="line"># hub.k8s.li 仓库服务器中的目录</span><br><span class="line">BLOBS_PATH&#x3D;&quot;docker&#x2F;registry&#x2F;v2&#x2F;blobs&#x2F;sha256&quot;</span><br><span class="line">REPO_PATH&#x3D;&quot;docker&#x2F;registry&#x2F;v2&#x2F;repositories&quot;</span><br><span class="line"></span><br><span class="line"># 记录当前数和总镜像数</span><br><span class="line">CURRENT_NUM&#x3D;0</span><br><span class="line">ALL_IMAGES&#x3D;&quot;$(sed -n &#39;&#x2F;#&#x2F;d;s&#x2F;:&#x2F;:&#x2F;p&#39; $&#123;IMAGES_LIST_FILE&#125; | sort -u)&quot;</span><br><span class="line">TOTAL_NUMS&#x3D;$(echo &quot;$&#123;ALL_IMAGES&#125;&quot; | wc -l)</span><br><span class="line"></span><br><span class="line"># 从远程仓库同步指定镜像到本地目录中。</span><br><span class="line">skopeo_sync() &#123;</span><br><span class="line"> if skopeo sync --insecure-policy --src-tls-verify&#x3D;false --dest-tls-verify&#x3D;false \</span><br><span class="line"> --override-arch amd64 --override-os linux --src docker --dest dir $1 $2 &gt; &#x2F;dev&#x2F;null; then</span><br><span class="line">  echo -e &quot;$GREEN_COL Progress: $&#123;CURRENT_NUM&#125;&#x2F;$&#123;TOTAL_NUMS&#125; sync $1 to $2 successful $NORMAL_COL&quot;</span><br><span class="line"> else</span><br><span class="line">  echo -e &quot;$RED_COL Progress: $&#123;CURRENT_NUM&#125;&#x2F;$&#123;TOTAL_NUMS&#125; sync $1 to $2 failed $NORMAL_COL&quot;</span><br><span class="line">  exit 2</span><br><span class="line"> fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">convert_images() &#123;</span><br><span class="line"> rm -rf $&#123;IMAGES_DIR&#125;; mkdir -p $&#123;IMAGES_DIR&#125;</span><br><span class="line"> for image in $&#123;ALL_IMAGES&#125;; do</span><br><span class="line">  let CURRENT_NUM&#x3D;$&#123;CURRENT_NUM&#125;+1</span><br><span class="line">  </span><br><span class="line">  # 取 images-list.txt 文本中的每一行，并分隔存储。</span><br><span class="line">  image_name&#x3D;$&#123;image%%:*&#125;</span><br><span class="line">  image_tag&#x3D;$&#123;image##*:&#125;</span><br><span class="line">  image_repo&#x3D;$&#123;image%%&#x2F;*&#125;</span><br><span class="line"></span><br><span class="line">  # 函数调用 从仓库同步镜像到本地images目录</span><br><span class="line">  skopeo_sync $&#123;SOURCE_REGISTRY&#125;&#x2F;$&#123;image&#125; $&#123;IMAGES_DIR&#125;&#x2F;$&#123;image_repo&#125;</span><br><span class="line"></span><br><span class="line">  # 在本地images目录中，取得get image manifest sha256sum 信息</span><br><span class="line">  manifest&#x3D;&quot;$&#123;IMAGES_DIR&#125;&#x2F;$&#123;image&#125;&#x2F;manifest.json&quot;</span><br><span class="line">  manifest_sha256&#x3D;$(sha256sum $&#123;manifest&#125; | awk &#39;&#123;print $1&#125;&#39;)      # 62ffc2ed7554e4c6d360bce40bbcf196573dd27c4ce080641a2c59867e732dee</span><br><span class="line">  mkdir -p $&#123;BLOBS_PATH&#125;&#x2F;$&#123;manifest_sha256:0:2&#125;&#x2F;$&#123;manifest_sha256&#125; # docker&#x2F;registry&#x2F;v2&#x2F;blobs&#x2F;sha256&#x2F;62&#x2F;62ffc2ed7554e4c6d360bce40bbcf196573dd27c4ce080641a2c59867e732dee</span><br><span class="line">  ln -f $&#123;manifest&#125; $&#123;BLOBS_PATH&#125;&#x2F;$&#123;manifest_sha256:0:2&#125;&#x2F;$&#123;manifest_sha256&#125;&#x2F;data  #  该 data 文件实际上是镜像的 manifest.json 文件。</span><br><span class="line"></span><br><span class="line">  # make image repositories dir</span><br><span class="line">  mkdir -p $&#123;REPO_PATH&#125;&#x2F;$&#123;image_name&#125;&#x2F;&#123;_uploads,_layers,_manifests&#125;</span><br><span class="line">  mkdir -p $&#123;REPO_PATH&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;revisions&#x2F;sha256&#x2F;$&#123;manifest_sha256&#125;</span><br><span class="line">  mkdir -p $&#123;REPO_PATH&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;tags&#x2F;$&#123;image_tag&#125;&#x2F;&#123;current,index&#x2F;sha256&#125;</span><br><span class="line">  mkdir -p $&#123;REPO_PATH&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;tags&#x2F;$&#123;image_tag&#125;&#x2F;index&#x2F;sha256&#x2F;$&#123;manifest_sha256&#125;</span><br><span class="line"></span><br><span class="line">  # create image tag manifest link file</span><br><span class="line">  echo -n &quot;sha256:$&#123;manifest_sha256&#125;&quot; &gt; $&#123;REPO_PATH&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;revisions&#x2F;sha256&#x2F;$&#123;manifest_sha256&#125;&#x2F;link  # sha256:62ffc2ed7554e4c6d360bce40bbcf196573dd27c4ce080641a2c59867e732deer</span><br><span class="line">  echo -n &quot;sha256:$&#123;manifest_sha256&#125;&quot; &gt; $&#123;REPO_PATH&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;tags&#x2F;$&#123;image_tag&#125;&#x2F;current&#x2F;link</span><br><span class="line">  echo -n &quot;sha256:$&#123;manifest_sha256&#125;&quot; &gt; $&#123;REPO_PATH&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;tags&#x2F;$&#123;image_tag&#125;&#x2F;index&#x2F;sha256&#x2F;$&#123;manifest_sha256&#125;&#x2F;link</span><br><span class="line"></span><br><span class="line">  # link image layers file to registry blobs dir</span><br><span class="line">  for layer in $(sed &#39;&#x2F;v1Compatibility&#x2F;d&#39; $&#123;manifest&#125; | grep -Eo &quot;\b[a-f0-9]&#123;64&#125;\b&quot;); do  # 匹配 manifest.json 中&quot;digest&quot;两个不带sha256的值</span><br><span class="line">    mkdir -p $&#123;BLOBS_PATH&#125;&#x2F;$&#123;layer:0:2&#125;&#x2F;$&#123;layer&#125;                 # 5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa 、 beae173ccac6ad749f76713cf4440fe3d21d1043fe616dfbe30775815d1d0f6a</span><br><span class="line">    mkdir -p $&#123;REPO_PATH&#125;&#x2F;$&#123;image_name&#125;&#x2F;_layers&#x2F;sha256&#x2F;$&#123;layer&#125;  # 5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa 、 beae173ccac6ad749f76713cf4440fe3d21d1043fe616dfbe30775815d1d0f6a</span><br><span class="line">    echo -n &quot;sha256:$&#123;layer&#125;&quot; &gt; $&#123;REPO_PATH&#125;&#x2F;$&#123;image_name&#125;&#x2F;_layers&#x2F;sha256&#x2F;$&#123;layer&#125;&#x2F;link  # sha256:5cc84ad355aaa64f46ea9c7bbcc319a9d808ab15088a27209c9e70ef86e5a2aa</span><br><span class="line">    ln -f $&#123;IMAGES_DIR&#125;&#x2F;$&#123;image&#125;&#x2F;$&#123;layer&#125; $&#123;BLOBS_PATH&#125;&#x2F;$&#123;layer:0:2&#125;&#x2F;$&#123;layer&#125;&#x2F;data     # 复制images目录中 &quot;application&#x2F;vnd.docker.container.image.v1+json&quot; 容器配置 config 与 多个 &quot;application&#x2F;vnd.docker.image.rootfs.diff.tar.gzip&quot; layer </span><br><span class="line">  done</span><br><span class="line"> done</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">convert_images</span><br></pre></td></tr></table></figure><ul><li><a href="http://install.sh" target="_blank" rel="noopener">install.sh</a> : 使用这个脚本将 registry 存储中的镜像转换成 skopeo dir 的方式，然后再将镜像同步到 registry 中。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">REGISTRY_DOMAIN&#x3D;&quot;harbor.k8s.li&quot;</span><br><span class="line">REGISTRY_PATH&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;registry&quot;</span><br><span class="line"></span><br><span class="line"># 切换到 registry 存储主目录下</span><br><span class="line">cd $&#123;REGISTRY_PATH&#125;</span><br><span class="line">gen_skopeo_dir() &#123;</span><br><span class="line">   # 定义 registry 存储的 blob 目录 和 repositories 目录，方便后面使用</span><br><span class="line">    BLOB_DIR&#x3D;&quot;docker&#x2F;registry&#x2F;v2&#x2F;blobs&#x2F;sha256&quot;</span><br><span class="line">    REPO_DIR&#x3D;&quot;docker&#x2F;registry&#x2F;v2&#x2F;repositories&quot;</span><br><span class="line">    # 定义生成 skopeo 目录</span><br><span class="line">    SKOPEO_DIR&#x3D;&quot;docker&#x2F;skopeo&quot;</span><br><span class="line">    # 通过 find 出 current 文件夹可以得到所有带 tag 的镜像，因为一个 tag 对应一个 current 目录</span><br><span class="line">    for image in $(find $&#123;REPO_DIR&#125; -type d -name &quot;current&quot;); do</span><br><span class="line">        # 根据镜像的 tag 提取镜像的名字</span><br><span class="line">        name&#x3D;$(echo $&#123;image&#125; | awk -F &#39;&#x2F;&#39; &#39;&#123;print $5&quot;&#x2F;&quot;$6&quot;:&quot;$9&#125;&#39;)</span><br><span class="line">        link&#x3D;$(cat $&#123;image&#125;&#x2F;link | sed &#39;s&#x2F;sha256:&#x2F;&#x2F;&#39;)</span><br><span class="line">        mfs&#x3D;&quot;$&#123;BLOB_DIR&#125;&#x2F;$&#123;link:0:2&#125;&#x2F;$&#123;link&#125;&#x2F;data&quot;</span><br><span class="line">        # 创建镜像的硬链接需要的目录</span><br><span class="line">        mkdir -p &quot;$&#123;SKOPEO_DIR&#125;&#x2F;$&#123;name&#125;&quot;</span><br><span class="line">        # 硬链接镜像的 manifests 文件到目录的 manifest 文件</span><br><span class="line">        ln $&#123;mfs&#125; $&#123;SKOPEO_DIR&#125;&#x2F;$&#123;name&#125;&#x2F;manifest.json</span><br><span class="line">        # 使用正则匹配出所有的 sha256 值，然后排序去重</span><br><span class="line">        layers&#x3D;$(grep -Eo &quot;\b[a-f0-9]&#123;64&#125;\b&quot; $&#123;mfs&#125; | sort -n | uniq)</span><br><span class="line">        for layer in $&#123;layers&#125;; do</span><br><span class="line">          # 硬链接 registry 存储目录里的镜像 layer 和 images config 到镜像的 dir 目录</span><br><span class="line">            ln $&#123;BLOB_DIR&#125;&#x2F;$&#123;layer:0:2&#125;&#x2F;$&#123;layer&#125;&#x2F;data $&#123;SKOPEO_DIR&#125;&#x2F;$&#123;name&#125;&#x2F;$&#123;layer&#125;</span><br><span class="line">        done</span><br><span class="line">    done</span><br><span class="line">&#125;</span><br><span class="line">sync_image() &#123;</span><br><span class="line">    # 使用 skopeo sync 将 dir 格式的镜像同步到 harbor</span><br><span class="line">    for project in $(ls $&#123;SKOPEO_DIR&#125;); do</span><br><span class="line">        skopeo sync --insecure-policy --src-tls-verify&#x3D;false --dest-tls-verify&#x3D;false \</span><br><span class="line">        --src dir --dest docker $&#123;SKOPEO_DIR&#125;&#x2F;$&#123;project&#125; $&#123;REGISTRY_DOMAIN&#125;&#x2F;$&#123;project&#125;</span><br><span class="line">    done</span><br><span class="line">&#125;</span><br><span class="line">gen_skopeo_dir</span><br></pre></td></tr></table></figure><blockquote><p>温馨提示: 此种方式是有些复杂对于大镜像的复制是推荐的, 而对于一些小镜像且显得多余。</p></blockquote><h3><span id="43-从-registry-存储中-select-出镜像进行同步">4.3 从 registry 存储中 select 出镜像进行同步</span></h3><p>描述: 先将镜像同步到一个 registry 中，再将镜像从 registry 存储中捞出来，该 registry 可以当作一个镜像存储的池子，我们使用 Linux 中硬链接的特性将镜像&quot;复制&quot;一份出来，然后再打一个 tar 包, 这样做的好处就是每次打包镜像的时候都能复用历史的镜像数据，而且性能极快。</p><ul><li>步骤 01. 先将镜像同步到一个固定的 registry 中。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bash skopeo-copy.sh docker.io localhost:5000</span><br></pre></td></tr></table></figure><ul><li>步骤 02. 使用该脚本将镜像从 registry 存储中捞出来</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;bash</span><br><span class="line">set -eo pipefail</span><br><span class="line"># 命令行变量</span><br><span class="line">IMAGES_LIST&#x3D;&quot;$1&quot;</span><br><span class="line">REGISTRY_PATH&#x3D;&quot;$2&quot;</span><br><span class="line">OUTPUT_DIR&#x3D;&quot;$3&quot;</span><br><span class="line"></span><br><span class="line"># Registry 仓库数据目录</span><br><span class="line">BLOB_DIR&#x3D;&quot;docker&#x2F;registry&#x2F;v2&#x2F;blobs&#x2F;sha256&quot;</span><br><span class="line">REPO_DIR&#x3D;&quot;docker&#x2F;registry&#x2F;v2&#x2F;repositories&quot;</span><br><span class="line"></span><br><span class="line"># 判断输出目录是否存在如不存在则移除。</span><br><span class="line">if [ -d $&#123;OUTPUT_DIR&#125; ];then</span><br><span class="line">  rm -rf $&#123;OUTPUT_DIR&#125;;</span><br><span class="line">fi</span><br><span class="line">mkdir -p $&#123;OUTPUT_DIR&#125;</span><br><span class="line"></span><br><span class="line">for image in $(find $&#123;IMAGES_LIST&#125; -type f -name &quot;*.list&quot; | xargs grep -Ev &#39;^#|^&#x2F;&#39; | grep &#39;:&#39;); do</span><br><span class="line">  # 镜像名称和Tag</span><br><span class="line">  image_name&#x3D;$&#123;image%%:*&#125;</span><br><span class="line">  image_tag&#x3D;$&#123;image##*:&#125;</span><br><span class="line"></span><br><span class="line">  # link 路径获取</span><br><span class="line">  tag_link&#x3D;$&#123;REGISTRY_PATH&#125;&#x2F;$&#123;REPO_DIR&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;tags&#x2F;$&#123;image_tag&#125;&#x2F;current&#x2F;link</span><br><span class="line">  manifest_sha256&#x3D;$(sed &#39;s&#x2F;sha256:&#x2F;&#x2F;&#39; $&#123;tag_link&#125;)</span><br><span class="line">  manifest&#x3D;$&#123;REGISTRY_PATH&#125;&#x2F;$&#123;BLOB_DIR&#125;&#x2F;$&#123;manifest_sha256:0:2&#125;&#x2F;$&#123;manifest_sha256&#125;&#x2F;data</span><br><span class="line">  mkdir -p $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;BLOB_DIR&#125;&#x2F;$&#123;manifest_sha256:0:2&#125;&#x2F;$&#123;manifest_sha256&#125;</span><br><span class="line"></span><br><span class="line">  # 强制硬链接到指定目录</span><br><span class="line">  ln -f $&#123;manifest&#125; $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;BLOB_DIR&#125;&#x2F;$&#123;manifest_sha256:0:2&#125;&#x2F;$&#123;manifest_sha256&#125;&#x2F;data</span><br><span class="line"></span><br><span class="line">  # make image repositories dir</span><br><span class="line">  mkdir -p $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;REPO_DIR&#125;&#x2F;$&#123;image_name&#125;&#x2F;&#123;_uploads,_layers,_manifests&#125;</span><br><span class="line">  mkdir -p $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;REPO_DIR&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;revisions&#x2F;sha256&#x2F;$&#123;manifest_sha256&#125;</span><br><span class="line">  mkdir -p $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;REPO_DIR&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;tags&#x2F;$&#123;image_tag&#125;&#x2F;&#123;current,index&#x2F;sha256&#125;</span><br><span class="line">  mkdir -p $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;REPO_DIR&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;tags&#x2F;$&#123;image_tag&#125;&#x2F;index&#x2F;sha256&#x2F;$&#123;manifest_sha256&#125;</span><br><span class="line"></span><br><span class="line">  # create image tag manifest link file  </span><br><span class="line">  echo -n &quot;sha256:$&#123;manifest_sha256&#125;&quot; &gt; $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;REPO_DIR&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;tags&#x2F;$&#123;image_tag&#125;&#x2F;current&#x2F;link</span><br><span class="line">  echo -n &quot;sha256:$&#123;manifest_sha256&#125;&quot; &gt; $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;REPO_DIR&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;revisions&#x2F;sha256&#x2F;$&#123;manifest_sha256&#125;&#x2F;link</span><br><span class="line">  echo -n &quot;sha256:$&#123;manifest_sha256&#125;&quot; &gt; $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;REPO_DIR&#125;&#x2F;$&#123;image_name&#125;&#x2F;_manifests&#x2F;tags&#x2F;$&#123;image_tag&#125;&#x2F;index&#x2F;sha256&#x2F;$&#123;manifest_sha256&#125;&#x2F;link</span><br><span class="line"></span><br><span class="line">  # 强制创建 &#x2F;docker&#x2F;registry&#x2F;v2&#x2F;blobs&#x2F; 各 layer data 文件到指定目录之中</span><br><span class="line">  for layer in $(sed &#39;&#x2F;v1Compatibility&#x2F;d&#39; $&#123;manifest&#125; | grep -Eo &#39;\b[a-f0-9]&#123;64&#125;\b&#39; | sort -u); do</span><br><span class="line">      mkdir -p $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;BLOB_DIR&#125;&#x2F;$&#123;layer:0:2&#125;&#x2F;$&#123;layer&#125;</span><br><span class="line">      mkdir -p $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;REPO_DIR&#125;&#x2F;$&#123;image_name&#125;&#x2F;_layers&#x2F;sha256&#x2F;$&#123;layer&#125;</span><br><span class="line">      ln -f $&#123;BLOB_DIR&#125;&#x2F;$&#123;layer:0:2&#125;&#x2F;$&#123;layer&#125;&#x2F;data $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;BLOB_DIR&#125;&#x2F;$&#123;layer:0:2&#125;&#x2F;$&#123;layer&#125;&#x2F;data</span><br><span class="line">      echo -n &quot;sha256:$&#123;layer&#125;&quot; &gt; $&#123;OUTPUT_DIR&#125;&#x2F;$&#123;REPO_DIR&#125;&#x2F;$&#123;image_name&#125;&#x2F;_layers&#x2F;sha256&#x2F;$&#123;layer&#125;&#x2F;link</span><br><span class="line">  done</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>至此完毕！</p><blockquote><p>本文转载自：「 WeiyiGeek 」，原文：<a href="https://url.hi-linux.com/xCmXo/" target="_blank" rel="noopener">https://url.hi-linux.com/xCmXo/</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-基础介绍&quot;&gt;1. 基础介绍&lt;/h2&gt;
&lt;p&gt;描述: 作为公司内部 PaaS toB 产品的打包发布人员，容器镜像对我们打工人而言就像是工地上的砖头 🧱，而我的一部分工作就是将这些砖头在各个仓库之间搬来搬去，最终将这些砖头打包放在产品的安装包中，形成一个完整的 PaaS 产品安装包。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q: 在 PaaS (平台即服务)中的大家常说的 ToB 与 ToC 到底是什么?&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;ToC 面向普通用户服务, 主要是让用户体验感好，解决用户使用方面的问题记录，并返回给前后端开发。&lt;br&gt;
ToB 是面向企业用户服务, 产品可用、其中最关键是让Boss使用Happly!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Q: 假如有如下场景，我们从 dockerhub 公共仓库中下载一个 GB 以上的镜像，到本地的私有仓库中，我想通常你会这样做先 &lt;code&gt;docker pull&lt;/code&gt; 到本地，然后使用 &lt;code&gt;docker tag&lt;/code&gt; 更改为私有仓库地址加上镜像名称版本，最后再使用&lt;code&gt;docker push&lt;/code&gt; 上传镜像到私有仓库中，以供其它内网机器拉取并使用。虽然该方法是可行，但是如果有多个大于 GB 以上的镜像需要上传到私有仓库，每次都要先解压 layer 到本地，然后再压缩 layer 上传到私有仓库中，你能想象此过程花费的时间有多久吗? 对于我们运维工程师来说时间就是金钱，所以需想尽一切方法来节约时间成本，那有没有一种办法可以直接将 registry 上的 blob 复制到另一个 registry，中间过程不涉及对镜像 layer 的解压缩，这岂不美哉。&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;解决方案当然是存在的，如果你不想使用docker进行images镜像拉取上传，我们完成可以使用skope工具来完全替代 docker-cli 来搬运镜像，skopeo是一个命令行实用程序，可对容器映像和映像存储库执行各种操作。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Skopeo" scheme="https://www.hi-linux.com/tags/Skopeo/"/>
    
  </entry>
  
  <entry>
    <title>轻量级 Kubernetes 集群发行版 K3s 完全进阶指南</title>
    <link href="https://www.hi-linux.com/posts/907.html"/>
    <id>https://www.hi-linux.com/posts/907.html</id>
    <published>2022-05-20T01:00:00.000Z</published>
    <updated>2022-05-20T01:22:25.012Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><blockquote><p><strong>深入理解官方文档，轻松学会使用 K3S 工具！</strong></p></blockquote><p><code>K3s</code> 是一个轻量级的 <code>Kubernetes</code> 发行版，它针对边缘计算、物联网等场景进行了高度优化。</p><ul><li><code>CNCF</code> 认证的 <code>Kubernetes</code> 发行版</li><li>支持 <code>X86_64</code>, <code>ARM64</code>, <code>ARMv7</code> 平台</li><li>单一进程包含 <code>Kubernetes master</code>，<code>kubelet</code> 和 <code>containerd</code></li></ul><a id="more"></a><h2><span id="1-k3s-工具介绍">1. K3S 工具介绍</span></h2><blockquote><p><strong>为你提供 k3s 的产品介绍</strong></p></blockquote><p><code>K3s</code> 有以下增强功能：</p><ul><li>打包为单个二进制文件<ul><li>把 <code>K8S</code> 相关的组件，比如 <code>kube-api</code>/ <code>kube-manager</code> 都打包到同一个二进制文件里面，这样的话，只需要启动这个文件就可以快速的启动对应的组件。</li></ul></li><li>使用基于 sqlite3 的默认存储机制<ul><li>同时支持使用 <code>etcd3</code>、<code>MySQL</code> 和 <code>PostgreSQL</code> 作为存储机制。</li></ul></li><li>默认情况下是安全的<ul><li>在 <code>K3s</code> 中有一个默认的证书管理机制(默认一年有效期)，也有一个可以轮转证书的功能(就是在小于九十天之内重启 <code>K3s</code> 的话，就会自动续一年)。</li></ul></li><li>功能强大的 <code>batteries-included</code> 功能<ul><li>就是虽然有些服务本身这个二进制文件并没有提供，但是可以通过内置的服务，将配置文件放到指定的目录下面，就可以在启动的时候一并将该服务启动或替换默认组件。</li></ul></li><li>所有 <code>K8S control-plane</code> 组件都封装在单个二进制文件和进程中<ul><li>因为封装在二进制文件中，所以启动的时候只有一个进程。好处在于只需要管理这个单一进程就可以了，同时也具备操作复杂集群的能力。</li></ul></li><li>最大程度减轻了外部依赖性<ul><li>即稍新一点的 <code>Linux</code> 内核就可以了(需要 <code>kernel</code> 和 <code>cgroup</code> 挂载)。</li></ul></li></ul><p>之所以叫做 <code>K3S</code> 是因为希望安装的 <code>K8S</code> 在内存占用方面只是一半的大小，而一半大的东西就是一个 <code>5</code> 个字母的单词，简写为 <code>K3S</code>。</p><ul><li>生命周期<ul><li>同时支持 <code>3</code> 个 <code>K8s</code> 版本，支持的生命周期与 <code>K8s</code> 相同</li><li>可以参考: <a href="https://kubernetes.io/zh/docs/setup/release/version-skew-policy/" target="_blank" rel="noopener">Kubernetes 版本及版本偏差支持策略</a> 进行学习</li></ul></li><li>更新周期<ul><li>当 <code>K8s</code> 更新新版本后，一般 <code>K3s</code> 在一周内同步更新</li><li>可以通过 <a href="https://update.k3s.io/v1-release/channels" target="_blank" rel="noopener">这个链接</a> 获取 <strong><code>latest</code>/<code>stable</code>/<code>testing</code></strong> 版本</li><li>我们默认安装的是 <code>stable</code> 版本，可以运行通过命令进行查看</li></ul></li><li>命名规范<ul><li><strong>v1.20.4+k3s1</strong>: <code>v1.20.4</code> 为 <code>K8s</code> 版本，<code>k3s1</code> 为补丁版本</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># K3s软件包需要的依赖项</span></span><br><span class="line">containerd  <span class="comment"># 容器运行时(可以使用docker替代)</span></span><br><span class="line">Flannel     <span class="comment"># 网络</span></span><br><span class="line">CoreDNS     <span class="comment"># DNS</span></span><br><span class="line">CNI         <span class="comment"># CNI</span></span><br><span class="line">Traefik     <span class="comment"># 默认的controller服务(apisix/ingress-controller)</span></span><br><span class="line">iptables    <span class="comment"># 主机实用程序</span></span><br><span class="line">service load balancer     <span class="comment"># 嵌入式服务负载均衡器</span></span><br><span class="line">network policy controller <span class="comment"># 嵌入式网络策略控制器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># K3s适用于以下场景</span></span><br><span class="line">CI</span><br><span class="line">Development</span><br><span class="line">ARM</span><br><span class="line">嵌入 K8s</span><br><span class="line">物联网-IoT</span><br><span class="line">边缘计算-Edge</span><br></pre></td></tr></table></figure><p>与此同时，<code>Rancher</code> 中国团队推出了一款针对 <code>K3s</code> 的效率提升工具：<strong>AutoK3s</strong>。只需要输入一行命令，即可快速创建 <code>K3s</code> 集群并添加指定数量的 <code>master</code> 节点和 <code>worker</code> 节点。</p><h2><span id="2-k3s-快速入门">2. K3S 快速入门</span></h2><blockquote><p><strong>原理就是，将 K8S 的相关组件封装到 K3S 的二进制文件中去！</strong></p></blockquote><p>原理就是，将 <code>K8S</code> 的相关组件封装到 <code>K3S</code> 的二进制文件中去，然后启动这二进制文件就可以启动一个成熟的 <code>K8S</code> 集群。我们可以看到 <code>K3S</code> 和 <code>K8S</code> 的架构基本差不多，其中 <code>k3s-server</code> 对应这个 <code>control-plane</code>，而 <code>k3s-agent</code> 对应着 <code>node</code> 节点。</p><p>可以看到 <code>k3s</code> 中使用的默认存储是 <code>SQLite</code>(自带)，且默认的网络使用的是 <code>Flannel</code>(自带)。当服务端和客户端都启动之后，通过 <code>Tunnel-Proxy</code> 这个组件进行通信，通过这个通道去管理网络流量。在 <code>agent</code> 节点中，通过 <code>kubelet</code> 操作 <code>contaninerd</code> 来创建对应 <code>Pod</code>。</p><ul><li>K3s 架构</li></ul><p><img src="https://img.hi-linux.com/staticfile/advance-k3s-tool-01-2022-05-19-1NJdKE.jpg" alt="K3S工具进阶指南"></p><ul><li>K8s 架构</li></ul><p><img src="https://img.hi-linux.com/staticfile/advance-k3s-tool-02-20220519092405817-2022-05-19-tva27I.png" alt="K3S工具进阶指南"></p><p>国内的话，建议使用官方提供的 <a href="https://mirror.rancher.cn/" target="_blank" rel="noopener">镜像地址</a>，这样不但可以加速本地 <code>K3s</code> 的时候，而且方便部署和更新服务。这也是为什么建议国内使用 <code>k3s-install.sh</code> 部署服务的原因，因为其内部使用的地址都是从国内去获取的。</p><h2><span id="3-k3s-安装事项">3. K3S 安装事项</span></h2><h3><span id="31-安装指南">3.1 安装指南</span></h3><blockquote><p><strong>理解 Server 节点的安装，以及注册 Agent 节点的步骤！</strong></p></blockquote><p>虽然可以通过下载二进制文件进行服务端和工作节点的运行(<code>./k3s server</code>)，但是一旦我们退出进程，之前创建的节点也就立即销毁了，所以还是建议使用脚本进行安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主节点</span></span><br><span class="line">$ ./k3s server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 工作节点</span></span><br><span class="line">$ ./k3s agent K3S_URL=xxx K3S_TOKEN=xxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除垃圾文件</span></span><br><span class="line">$ rm -rf /etc/rancher /var/lib/rancher</span><br></pre></td></tr></table></figure><ul><li>镜像加速</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加配置</span></span><br><span class="line">$ cat &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt;EOF</span><br><span class="line">mirrors:</span><br><span class="line">  <span class="string">"docker.io"</span>:</span><br><span class="line">    endpoint:</span><br><span class="line">      - <span class="string">"https://fogjl973.mirror.aliyuncs.com"</span></span><br><span class="line">      - <span class="string">"https://registry-1.docker.io"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启服务</span></span><br><span class="line">$ sudo systemctl restart k3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 是否生效</span></span><br><span class="line">$ sudo crictl info | grep -A 2 <span class="string">"endpoint"</span></span><br></pre></td></tr></table></figure><p><code>K3s</code> 提供了一个安装脚本，可以方便的在 <code>systemd</code> 或 <code>openrc</code> 的系统上将其作为服务安装。运行此安装后，<code>K3s</code> 服务将被配置为在节点重启后或进程崩溃或被杀死时自动重启。</p><ul><li><p>安装内容</p><ul><li><code>kubectl</code>、<code>crictl</code>、<code>ctr</code></li><li><code>k3s-killall.sh</code>、<code>k3s-uninstall.sh</code></li></ul></li><li><p>执行操作</p><ul><li>将 <code>kubeconfig</code> 文件写入到 <code>/etc/rancher/k3s/k3s.yaml</code> 里面</li><li>由 <code>K3s</code> 安装的 <code>kubectl</code> 工具将自动使用该文件的配置来运行</li><li>其他机器可以通过复制这个配置文件并修改 <code>server</code> 地址来操作 <code>K3s</code> 集群</li></ul></li><li><p>主节点 - 192.168.100.100</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装脚本</span></span><br><span class="line"><span class="comment"># https://get.k3s.io</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建议使用这个安装脚本(国内化了)</span></span><br><span class="line">$ curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn K3S_NODE_NAME=k3s1 \</span><br><span class="line">    K3S_KUBECONFIG_OUTPUT=/home/escape/.kube/config \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">"--docker"</span> sh -</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找stable分支版本信息</span></span><br><span class="line">[INFO]  Finding release <span class="keyword">for</span> channel stable</span><br><span class="line">[INFO]  Using v1.23.6+k3s1 as release</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取国内镜像版本地址</span></span><br><span class="line">[INFO]  Downloading <span class="built_in">hash</span> https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/sha256sum-amd64.txt</span><br><span class="line">[INFO]  Downloading binary https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/k3s</span><br><span class="line">[INFO]  Verifying binary download</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装k3s二进制工具并链接相关工具(内置)</span></span><br><span class="line">[INFO]  Installing k3s to /usr/<span class="built_in">local</span>/bin/k3s</span><br><span class="line">[INFO]  Skipping installation of SELinux RPM</span><br><span class="line">[INFO]  Creating /usr/<span class="built_in">local</span>/bin/kubectl symlink to k3s</span><br><span class="line">[INFO]  Creating /usr/<span class="built_in">local</span>/bin/crictl symlink to k3s</span><br><span class="line">[INFO]  Skipping /usr/<span class="built_in">local</span>/bin/ctr symlink to k3s, <span class="built_in">command</span> exists <span class="keyword">in</span> PATH at /usr/bin/ctr</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装清除和卸载k3s生成的配置和工具</span></span><br><span class="line">[INFO]  Creating killall script /usr/<span class="built_in">local</span>/bin/k3s-killall.sh</span><br><span class="line">[INFO]  Creating uninstall script /usr/<span class="built_in">local</span>/bin/k3s-uninstall.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 常见了两个systemd的配置</span></span><br><span class="line">[INFO]  env: Creating environment file /etc/systemd/system/k3s.service.env</span><br><span class="line">[INFO]  systemd: Creating service file /etc/systemd/system/k3s.service</span><br><span class="line">[INFO]  systemd: Enabling k3s unit</span><br><span class="line">Created symlink /etc/systemd/system/multi-user.target.wants/k3s.service → /etc/systemd/system/k3s.service.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动k3s服务</span></span><br><span class="line">[INFO]  systemd: Starting k3s</span><br></pre></td></tr></table></figure><ul><li>工作节点 - 192.168.100.101</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 工作节点上安装并将它们添加到集群</span></span><br><span class="line"><span class="comment"># https://docs.rancher.cn/docs/k3s/architecture/_index#注册-agent-节点</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | \</span><br><span class="line">    K3S_URL=https://myserver:6443 \</span><br><span class="line">    K3S_TOKEN=mynodetoken sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 建议使用这个安装命令(国内化了)</span></span><br><span class="line"><span class="comment"># K3S_URL: 会使K3s以worker模式运行</span></span><br><span class="line"><span class="comment"># K3S_TOKEN: 使用的值存储在你的服务器节点上</span></span><br><span class="line"><span class="comment"># K3S_NODE_NAME: 为每个节点提供一个有效且唯一的主机名</span></span><br><span class="line">$ curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn K3S_NODE_NAME=k3s2 \</span><br><span class="line">    K3S_KUBECONFIG_OUTPUT=/home/escape/.kube/config \</span><br><span class="line">    K3S_URL=https://192.168.100.100:6443 \</span><br><span class="line">    K3S_TOKEN=mynodetoken sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># mynodetoken</span></span><br><span class="line">$ sudo cat /var/lib/rancher/k3s/server/token</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查找stable分支版本信息</span></span><br><span class="line">[INFO]  Finding release <span class="keyword">for</span> channel stable</span><br><span class="line">[INFO]  Using v1.23.6+k3s1 as release</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取国内镜像版本地址</span></span><br><span class="line">[INFO]  Downloading <span class="built_in">hash</span> https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/sha256sum-amd64.txt</span><br><span class="line">[INFO]  Downloading binary https://rancher-mirror.rancher.cn/k3s/v1.23.6-k3s1/k3s</span><br><span class="line">[INFO]  Verifying binary download</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装k3s二进制工具并链接相关工具(内置)</span></span><br><span class="line">[INFO]  Installing k3s to /usr/<span class="built_in">local</span>/bin/k3s</span><br><span class="line">[INFO]  Creating /usr/<span class="built_in">local</span>/bin/kubectl symlink to k3s</span><br><span class="line">[INFO]  Creating /usr/<span class="built_in">local</span>/bin/crictl symlink to k3s</span><br><span class="line">[INFO]  Skipping /usr/<span class="built_in">local</span>/bin/ctr symlink to k3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装清除和卸载k3s生成的配置和工具</span></span><br><span class="line">[INFO]  Creating killall script /usr/<span class="built_in">local</span>/bin/k3s-agent-killall.sh</span><br><span class="line">[INFO]  Creating uninstall script /usr/<span class="built_in">local</span>/bin/k3s-agent-uninstall.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 常见了两个systemd的配置</span></span><br><span class="line">[INFO]  env: Creating environment file /etc/systemd/system/k3s-agent.service.env</span><br><span class="line">[INFO]  systemd: Creating service file /etc/systemd/system/k3s-agent.service</span><br><span class="line">[INFO]  systemd: Enabling k3s-agent unit</span><br><span class="line">Created symlink /etc/systemd/system/multi-user.target.wants/k3s-agent.service → /etc/systemd/system/k3s-agent.service.</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动k3s服务</span></span><br><span class="line">[INFO]  systemd: Starting k3s-agent</span><br></pre></td></tr></table></figure><h3><span id="32-配置要求">3.2 配置要求</span></h3><blockquote><p><strong>理解 Server 节点的安装，以及注册 Agent 节点的步骤！</strong></p></blockquote><ul><li>[1] 先决条件<ul><li>选择上，两个节点不能有相同的主机名</li><li>不修改主机名可以通过添加随机后缀或指定主机名</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 为每个节点添加随机后缀</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.100.100:6443 \</span><br><span class="line">    K3S_TOKEN=xxx sh -s - --with-node-id</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个节点指定主机名</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    K3S_NODE_NAME=<span class="string">"k3s2"</span> INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    K3S_URL=https://192.168.64.3:6443 K3S_TOKEN=xxx sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为每个节点指定主机名</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.64.3:6443 \</span><br><span class="line">    K3S_TOKEN=xxx sh -s - --node-name k3s2</span><br></pre></td></tr></table></figure><ul><li>[2] 硬件信息<ul><li>操作系统：可以在大多数现代 <code>Linux</code> 系统上运行</li><li>磁盘设备：<code>K3s</code> 的性能取决于数据库的性能(建议使用 <code>SSD</code> 硬盘)</li><li>网络相关：<code>K3s Server</code> 节点的入站规则，所有出站流量都是允许的</li></ul></li></ul><table><thead><tr><th style="text-align:left">协议</th><th style="text-align:left">端口</th><th style="text-align:left">源</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left">TCP</td><td style="text-align:left">6443</td><td style="text-align:left">K3s agent 节点</td><td style="text-align:left">Kubernetes API Server</td></tr><tr><td style="text-align:left">UDP</td><td style="text-align:left">8472</td><td style="text-align:left">K3s server 和 agent 节点</td><td style="text-align:left">仅对 Flannel VXLAN 需要</td></tr><tr><td style="text-align:left">TCP</td><td style="text-align:left">10250</td><td style="text-align:left">K3s server 和 agent 节点</td><td style="text-align:left">Kubelet metrics</td></tr><tr><td style="text-align:left">TCP</td><td style="text-align:left">2379-2380</td><td style="text-align:left">K3s server 节点</td><td style="text-align:left">只有嵌入式 etcd 高可用才需要</td></tr></tbody></table><ul><li>[3] 安装选项<ul><li><a href="https://docs.rancher.cn/docs/k3s/autok3s/_index" target="_blank" rel="noopener">官方安装参数文档</a></li><li><a href="https://github.com/kingsd041/k3s-tutorial/blob/main/03-%E5%AE%89%E8%A3%85-%E8%A6%81%E6%B1%82%E5%8F%8A%E9%80%89%E9%A1%B9/README.md" target="_blank" rel="noopener">安装选项示例演示</a></li></ul></li></ul><table><thead><tr><th style="text-align:left">Environment Variable</th><th style="text-align:left">Description</th></tr></thead><tbody><tr><td style="text-align:left"><code>INSTALL_K3S_EXEC</code></td><td style="text-align:left">用于在服务中启动 <code>K3s</code> 的后续子命令</td></tr><tr><td style="text-align:left"><code>K3S_CONFIG_FILE</code></td><td style="text-align:left">指定配置文件的位置</td></tr><tr><td style="text-align:left"><code>K3S_TOKEN</code></td><td style="text-align:left">用于将 <code>server/agent</code> 加入集群的共享 <code>secret</code> 值</td></tr><tr><td style="text-align:left"><code>K3S_TOKEN_FILE</code></td><td style="text-align:left">用于将 <code>server/agent</code> 加入集群的共享 <code>secret</code> 文件</td></tr><tr><td style="text-align:left"><code>INSTALL_K3S_VERSION</code></td><td style="text-align:left">指定下载 <code>K3s</code> 的版本</td></tr><tr><td style="text-align:left"><code>K3S_TOKEN_FILE</code></td><td style="text-align:left">指定  <code>cluster-secret</code>/<code>token</code> 的文件目录</td></tr><tr><td style="text-align:left"><code>INSTALL_K3S_SKIP_START</code></td><td style="text-align:left">将不会启动 <code>K3s</code> 服务</td></tr><tr><td style="text-align:left"><code>INSTALL_K3S_SKIP_DOWNLOAD</code></td><td style="text-align:left">用于离线安装；设置之后不会下载远程工具</td></tr></tbody></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 其实就把对应参数加到systemd配置文件里面去了</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">"--docker"</span> sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动化部署(不用获取token值了)</span></span><br><span class="line"><span class="comment"># 主节点和工作节点使用我们指定的key来通信</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    K3S_TOKEN=rancher-k3s sh -</span><br><span class="line">$ sudo cat /var/lib/rancher/k3s/server/token</span><br></pre></td></tr></table></figure><ul><li>[4] 其他说明<ul><li>运行 <code>agent</code> 时还必须设置 <code>K3S_TOKEN</code></li><li>以 <code>K3S_</code> 开头的环境变量将被保留，供 <code>systemd/openrc</code> 使用</li><li>没有明确设置 <code>exec</code> 并设置 <code>K3S_URL</code> 的话会将命令默认为工作节点</li></ul></li></ul><h3><span id="33-命令参数">3.3 命令参数</span></h3><blockquote><p><strong>理解 Server 节点的安装，以及注册 Agent 节点的步骤！</strong></p></blockquote><p>在整个 K3s 文档中，你会看到一些选项可以作为命令标志和环境变量传递进来，那该如何使用标志和环境变量呢？</p><ul><li>[1] 使用标志和环境变量</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用标志</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | K3S_KUBECONFIG_MODE=<span class="string">"644"</span> sh -s -</span><br><span class="line">$ curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644</span><br><span class="line"></span><br><span class="line"><span class="comment"># 环境变量</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">"--flannel-backend none"</span> sh -s -</span><br><span class="line">$ curl -sfL https://get.k3s.io | \</span><br><span class="line">    sh -s - server --flannel-backend none</span><br></pre></td></tr></table></figure><ul><li>[2] K3s Server/Agent - 常用配置</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># write-kubeconfig</span></span><br><span class="line"><span class="comment"># 将管理客户端的kubeconfig写入这个文件</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    K3S_KUBECONFIG_OUTPUT=/root/.kube/config \</span><br><span class="line">    sh -</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用docker作为容器运行时</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">"--docker"</span> sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定运行时工具</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">"--container-runtime-endpoint containerd"</span> \</span><br><span class="line">    sh -</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置私有镜像仓库配置文件</span></span><br><span class="line"><span class="comment"># 默认配置文件: /etc/rancher/k3s/registries.yaml</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">"--private-registry xxx"</span> \</span><br><span class="line">    sh -</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 针对多网卡主机安装K3s集群</span></span><br><span class="line"><span class="comment"># 默认多网卡会使用默认网关的那个卡</span></span><br><span class="line">$ rout -n</span><br><span class="line"></span><br><span class="line"><span class="comment"># K3s server</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">"--node-ip=192.168.100.100"</span> \</span><br><span class="line">    sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># K3s agent</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    K3S_URL=https://192.168.99.211:6443 K3S_TOKEN=xxx \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">"--node-ip=192.168.100.100"</span> \</span><br><span class="line">    sh -</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --tls-san</span></span><br><span class="line"><span class="comment"># 在TLS证书中添加其他主机名或IP作为主机备用名称</span></span><br><span class="line"><span class="comment"># 即在公网环境下允许通过公网IP访问控制、操作远程集群</span></span><br><span class="line"><span class="comment"># 或者部署多个Server并使用LB进行负责，就需要保留公网地址</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">"--tls-san 1.1.1.1"</span>  \</span><br><span class="line">    sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取配置</span></span><br><span class="line">$ kubectl get secret k3s-serving -n kube-system -o yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后本机复制公网主节点对应的yaml文件即可本地操作了</span></span><br><span class="line">$ scp ci@1.1.1.1:/etc/rancher/k3s/k3s.yaml ~/.kube/config</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改启动的服务对应配置(调整节点的启动的最大Pod数量)</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--kubelet-arg=max-pods=200'</span> \</span><br><span class="line">    sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改启动的服务对应配置(使用ipvs作为服务调度工具)</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--kube-proxy-arg=proxy-mode=ipvs'</span> \</span><br><span class="line">    sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改启动的服务对应配置(调整服务启动的端口范围)</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--kube-apiserver-arg=service-node-port-range=40000-50000'</span> \</span><br><span class="line">    sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># kubelet-arg     --kubelet-arg</span></span><br><span class="line"><span class="comment"># kube-apiserver  --kube-apiserver-arg</span></span><br><span class="line"><span class="comment"># kube-proxy-arg  --kube-proxy-arg</span></span><br><span class="line"><span class="comment"># kube-proxy-arg  --kube-proxy-arg=proxy-mode=ipvs</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --data-dir</span></span><br><span class="line"><span class="comment"># 修改K3s数据存储目录</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--data-dir=/opt/k3s-data'</span> \</span><br><span class="line">    sh -</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 禁用组件</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--disable traefik'</span> \</span><br><span class="line">    sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自己加自己需要的服务</span></span><br><span class="line">$ ls /var/lib/rancher/k3s/server/manifests</span><br><span class="line">$ kubectl get pods -A | grep traefik</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加label和taint标识</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--node-label foo=bar,hello=world \</span></span><br><span class="line"><span class="string">        --node-taint key1=value1:NoExecute'</span></span><br><span class="line">    sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看一下</span></span><br><span class="line">$ kubectl describe nodes</span><br></pre></td></tr></table></figure><ul><li>[3] K3s Server/Agent - 数据库选项</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定数据源名称</span></span><br><span class="line"><span class="comment"># 标志位: --datastore-endpoint&amp;nbsp;value</span></span><br><span class="line"><span class="comment"># 环境变量: K3S_DATASTORE_ENDPOINT</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--datastore-endpoint&amp;nbsp;etcd'</span> \</span><br><span class="line">    sh -</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cron规范中的快照间隔时间</span></span><br><span class="line"><span class="comment"># --etcd-snapshot-schedule-cron&amp;nbsp;value</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--etcd-snapshot-schedule-cron *&amp;nbsp;*/5&amp;nbsp;*&amp;nbsp;*&amp;nbsp;*'</span> \</span><br><span class="line">    sh -</span><br></pre></td></tr></table></figure><h3><span id="34-网络选项">3.4 网络选项</span></h3><blockquote><p><strong>理解 Server 节点的安装，以及注册 Agent 节点的步骤！</strong></p></blockquote><p>默认情况下，<code>K3s</code> 将以 <code>flannel</code> 作为 <code>CNI</code> 运行，使用 <code>VXLAN</code> 作为默认后端，<code>CNI</code> 和默认后端都可以通过参数修改。要启用加密，请使用下面的 <code>IPSec</code> 或 <code>WireGuard</code> 选项。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认安装K3s之后的网络配置</span></span><br><span class="line">$ sudo cat /var/lib/rancher/k3s/agent/etc/flannel/net-conf.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"Network"</span>: <span class="string">"10.42.0.0/16"</span>,</span><br><span class="line">    <span class="string">"EnableIPv6"</span>: <span class="literal">false</span>,</span><br><span class="line">    <span class="string">"EnableIPv4"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"IPv6Network"</span>: <span class="string">"::/0"</span>,</span><br><span class="line">    <span class="string">"Backend"</span>: &#123;</span><br><span class="line">        <span class="string">"Type"</span>: <span class="string">"vxlan"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><table><thead><tr><th style="text-align:left">CLI Flag 和 Value</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left"><code>--flannel-backend=vxlan</code></td><td style="text-align:left">使用 <code>VXLAN</code> 后端(默认)</td></tr><tr><td style="text-align:left"><code>--flannel-backend=host-gw</code></td><td style="text-align:left">使用 <code>host-gw</code> 后端</td></tr><tr><td style="text-align:left"><code>--flannel-backend=ipsec</code></td><td style="text-align:left">使用 <code>IPSEC</code> 后端；对网络流量进行加密</td></tr><tr><td style="text-align:left"><code>--flannel-backend=wireguard</code></td><td style="text-align:left">使用 <code>WireGuard</code> 后端；对网络流量进行加密</td></tr></tbody></table><ul><li>配置 Flannel 选项</li></ul><p>这样，我就可以在安装 <code>K3s</code> 或者之后修改对应配置文件，来修改 <code>Flannel</code> 默认的后端网络配置选项(重启会覆盖不生效)了。下面，我们演示下，如何修改为 <code>host-gw</code> 模式。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主节点</span></span><br><span class="line"><span class="comment"># flannel-backend使用host-gw</span></span><br><span class="line"><span class="comment"># 该模式会把对端主机的IP当做默认网管(多Server情况)</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--flannel-backend=host-gw'</span> \</span><br><span class="line">    sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 工作节点</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn K3S_URL=https://192.168.100.100:6443 \</span><br><span class="line">    K3S_TOKEN=xxx sh -</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 默认的路由信息</span></span><br><span class="line">$ route -n</span><br><span class="line">0.0.0.0         172.16.64.1     0.0.0.0         UG    100    0        0 enp0s2</span><br><span class="line">10.42.1.0       172.16.64.9     255.255.255.0   UG    0      0        0 enp0s2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看配置之后的网络配置</span></span><br><span class="line">$ sudo cat /var/lib/rancher/k3s/agent/etc/flannel/net-conf.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"Network"</span>: <span class="string">"10.42.0.0/16"</span>,</span><br><span class="line">    <span class="string">"Backend"</span>: &#123;</span><br><span class="line">        <span class="string">"Type"</span>: <span class="string">"host-gw"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>启用 Directrouting 特性</li></ul><p><strong>Flannel 自身的特性</strong>：当主机在同一子网时，启用 <code>direct routes</code>(如 <code>host-gw</code>)。<code>vxlan</code> 只用于将数据包封装到不同子网的主机上，同子网的主机之间使用  <code>host-gw</code>，默认值为  <code>false</code>。</p><p>要添加我们就不能修改其对应的网络配置文件，因为重新安装或者重启都会把这个配置冲掉(变成默认配置)，所以需要折中下。我们自建一个网络配置文件，然后在启动的时候执行从哪个配置文件里面加载对应配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># k3s的master和agent</span></span><br><span class="line">$ sudo cat /etc/flannel/net-conf.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"Network"</span>: <span class="string">"10.42.0.0/16"</span>,</span><br><span class="line">    <span class="string">"Backend"</span>: &#123;</span><br><span class="line">        <span class="string">"Type"</span>: <span class="string">"vxlan"</span>,</span><br><span class="line">        <span class="string">"Directrouting"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># k3s master</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--flannel-backend=host-gw'</span> \</span><br><span class="line">    sh -</span><br></pre></td></tr></table></figure><ul><li>自定义 CNI</li></ul><p>使用  <code>--flannel-backend=none</code>(禁用) 运行 <code>K3s</code>，然后在安装你选择的 <code>CNI</code>。按照 <a href="https://docs.projectcalico.org/master/reference/cni-plugin/configuration" target="_blank" rel="noopener">Calico CNI 插件指南</a> 来修改 <code>Calico</code> 的 <code>YAML</code> 配置文件，在 <code>container_settings</code> 部分中允许 <code>IP</code> 转发。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加到Calico的YAML文件中</span></span><br><span class="line"><span class="comment"># 允许IP转发(这个是K3s的一个限制；需要开启)</span></span><br><span class="line"><span class="string">"container_settings"</span>: &#123;</span><br><span class="line">    <span class="string">"allow_ip_forwarding"</span>: <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">- name: CALICO_IPV4POOL_CIDR</span><br><span class="line">  value: <span class="string">"192.168.200.0/24"</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过在主机上运行以下命令，确保设置已被应用(true)</span></span><br><span class="line">$ sudo cat /etc/cni/net.d/10-canal.conflist</span><br><span class="line"></span><br><span class="line"><span class="comment"># calico</span></span><br><span class="line"><span class="comment"># 其中--cluster-cidr可不设置</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--flannel-backend=none \</span></span><br><span class="line"><span class="string">        --cluster-cidr=192.168.200.0/24"'</span> \</span><br><span class="line">    sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动网络服务</span></span><br><span class="line">$ kubectl apply -f ./calico.yaml</span><br></pre></td></tr></table></figure><h3><span id="35-外部数据库">3.5 外部数据库</span></h3><blockquote><p><strong>理解 Server 节点的安装，以及注册 Agent 节点的步骤！</strong></p></blockquote><ul><li>[1] 使用外部数据库实现高可用安装<ul><li>两个或多个<code>server</code> 节点</li><li>零个或多个<code>agent</code> 节点</li><li>外部数据存储(<code>Etcd/MySQL/PostgRES</code>)</li><li>固定的注册地址(<code>LB</code>)</li><li><a href="https://mp.weixin.qq.com/s/0Wk2MzfWqMqt8DfUK_2ICA" target="_blank" rel="noopener">这应该是最适合国内用户的 K3s HA 方案</a></li></ul></li></ul><p>虽然单节点 <code>k3s server</code> 集群可以满足各种用例，但是对于需要稳定运行的重要环境，可以在 <code>HA</code> 配置中运行 <code>K3s</code>，如何使用外部数据库安装一个高可用的 <code>K3s</code> 集群？</p><p><img src="https://img.hi-linux.com/staticfile/advance-k3s-tool-03-20220519092423240-2022-05-19-amkNto.png" alt="K3S安装事项 - 外部数据库"></p><table><thead><tr><th style="text-align:left">主机名</th><th style="text-align:left">角色</th><th style="text-align:left">IP</th></tr></thead><tbody><tr><td style="text-align:left">k3s-server-1</td><td style="text-align:left">k3s master</td><td style="text-align:left">172.31.2.134</td></tr><tr><td style="text-align:left">k3s-server-2</td><td style="text-align:left">k3s master</td><td style="text-align:left">172.31.2.42</td></tr><tr><td style="text-align:left">k3s-db</td><td style="text-align:left">DB</td><td style="text-align:left">172.31.10.251</td></tr><tr><td style="text-align:left">k3s-lb</td><td style="text-align:left">LB</td><td style="text-align:left">172.31.13.97</td></tr><tr><td style="text-align:left">k3s-agent</td><td style="text-align:left">k3s agent</td><td style="text-align:left">172.31.15.130</td></tr></tbody></table><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.创建一个外部数据存储</span></span><br><span class="line">$ docker run --name some-mysql \</span><br><span class="line">    --restart=unless-stopped -p 3306:3306 \</span><br><span class="line">    -e MYSQL_ROOT_PASSWORD=password -d mysql:5.7</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.启动k3s-server节点(有读写权限不用加库名)</span></span><br><span class="line"><span class="comment"># mysql://username:password@tcp(hostname:3306)/database-name</span></span><br><span class="line"><span class="comment"># 可加污点 --node-taint CriticalAddonsOnly=true:NoExecute</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn sh - server \</span><br><span class="line">    --datastore-endpoint=<span class="string">"mysql://root:password@ip:3306/k3s"</span> \</span><br><span class="line">    --tls-san 172.31.13.97</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 3.配置固定的注册地址(k3s-lb节点)</span></span><br><span class="line"><span class="comment"># Agent节点需要一个URL来注册(LB)</span></span><br><span class="line">$ cat &gt;&gt; /etc/nginx.conf &lt;&lt;EOF</span><br><span class="line">worker_processes 4;</span><br><span class="line">worker_rlimit_nofile 40000;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections 8192;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">stream &#123;</span><br><span class="line">    upstream k3s_api &#123;</span><br><span class="line">        least_conn;</span><br><span class="line">        server 172.31.2.134:6443 max_fails=3 fail_timeout=5s;</span><br><span class="line">        server 172.31.2.42:6443 max_fails=3 fail_timeout=5s;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen     6443;</span><br><span class="line">        proxy_pass k3s_api;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line">$ docker run -d --restart=unless-stopped \</span><br><span class="line">  -p 6443:6443 \</span><br><span class="line">  -v /etc/nginx.conf:/etc/nginx/nginx.conf \</span><br><span class="line">  nginx:1.14</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 4.加入Agent节点</span></span><br><span class="line"><span class="comment"># Agent会保存LB节点和每个Server节点的IP信息</span></span><br><span class="line"><span class="comment"># cat /var/lib/rancher/k3s/agent/etc/k3s-agent-load-balancer.json</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn</span><br><span class="line">    K3S_URL=https://172.31.13.97:6443 K3S_TOKEN=mynodetoken \</span><br><span class="line">    sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.通过kubeconfig访问K3s集群</span></span><br><span class="line">$ kubectl get nodes</span><br><span class="line">NAME           STATUS   ROLES                  AGE   VERSION</span><br><span class="line">k3s-server-1   Ready    control-plane,master   68s   v1.20.7+k3s1</span><br><span class="line">k3s-server-2   Ready    control-plane,master   66s   v1.20.7+k3s1</span><br></pre></td></tr></table></figure><ul><li>[2] 嵌入式 DB 的高可用</li></ul><p>要在这种模式下运行 <code>K3s</code>，你必须有奇数的服务器节点，建议从三个节点开始。在嵌入式中，默认使用 <code>Etcd</code> 作为高可用的数据库。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 服务器节点(启动etcd集群)</span></span><br><span class="line"><span class="comment"># SECRET我们预定一个key值</span></span><br><span class="line"><span class="comment"># 使用cluster-init标志来启用集群</span></span><br><span class="line"><span class="comment"># 并使用一个标记作为共享的密钥来加入其他服务器到集群中</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn K3S_TOKEN=SECRET \</span><br><span class="line">    sh -s - --cluster-init</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看类型</span></span><br><span class="line">$ sudo  kubectl get nodes</span><br><span class="line">NAME    STATUS  ROLES                      AGE  VERSION</span><br><span class="line">ip-xxx  Ready   control-plane,etcd,master  19h  v1.23.6+k3s1</span><br><span class="line"></span><br><span class="line"><span class="comment"># 其他服务器节点(2/3)</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn K3S_TOKEN=SECRET \</span><br><span class="line">    sh -s - --server https://&lt;ip-or-host-server&gt;:6443</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询ETCD集群状态</span></span><br><span class="line"><span class="comment"># etcd证书默认目录：/var/lib/rancher/k3s/server/tls/etcd</span></span><br><span class="line"><span class="comment"># etcd数据默认目录：/var/lib/rancher/k3s/server/db/etcd</span></span><br><span class="line">$ ETCDCTL_ENDPOINTS=<span class="string">'https://172.31.12.136:2379,\</span></span><br><span class="line"><span class="string">    https://172.31.4.43:2379,\</span></span><br><span class="line"><span class="string">    https://172.31.4.190:2379'</span> \</span><br><span class="line">ETCDCTL_CACERT=<span class="string">'/var/lib/rancher/k3s/server/tls/etcd/server-ca.crt'</span> \</span><br><span class="line">ETCDCTL_CERT=<span class="string">'/var/lib/rancher/k3s/server/tls/etcd/server-client.crt'</span>\</span><br><span class="line">ETCDCTL_KEY=<span class="string">'/var/lib/rancher/k3s/server/tls/etcd/server-client.key'</span> \</span><br><span class="line">ETCDCTL_API=3 etcdctl endpoint status --write-out=table</span><br></pre></td></tr></table></figure><ul><li>[3] 集群数据存储选项</li></ul><p>使用 <code>etcd</code> 以外的数据存储运行 <code>K8S</code> 的能力使 <code>K3s</code> 区别于其他 <code>K8S</code> 发行版。该功能为 <code>K8S</code> 操作者提供了灵活性，可用的数据存储选项允许你选择一个最适合用例的数据存储。</p><p>如果你的团队没有操作 <code>etcd</code> 的专业知识，可以选择 <code>MySQL</code> 或 <code>PostgreSQL</code> 等企业级 <code>SQL</code> 数据库。如果您需要在 <code>CI/CD</code> 环境中运行一个简单的、短暂的集群，可以使用嵌入式 <code>SQLite</code> 数据库</p><p>如果你想使用外部数据存储，如 <code>PostgreSQL</code>、<code>MySQL</code> 或 <code>etcd</code>，你必须设置 <code>datastore-endpoint</code> 参数，以便 <code>K3s</code> 知道如何连接到它，也可以指定参数来配置连接的认证和加密。下表总结了这些参数，它们可以作为 <code>CLI</code> 标志或环境变量传递。</p><table><thead><tr><th style="text-align:left">CLI Flag</th><th style="text-align:left">环境变量</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left"><code>--datastore-endpoint</code></td><td style="text-align:left"><code>K3S_DATASTORE_ENDPOINT</code></td><td style="text-align:left">指定一个 PostgresSQL、MySQL 或 etcd 连接字符串。用于描述与数据存储的连接。这个字符串的结构是特定于每个后端的，详情如下。</td></tr><tr><td style="text-align:left"><code>--datastore-cafile</code></td><td style="text-align:left"><code>K3S_DATASTORE_CAFILE</code></td><td style="text-align:left">TLS 证书颁发机构（CA）文件，用于帮助确保与数据存储的通信安全。如果你的数据存储通过 TLS 服务请求，使用由自定义证书颁发机构签署的证书，你可以使用这个参数指定该 CA，这样 K3s 客户端就可以正确验证证书。</td></tr><tr><td style="text-align:left"><code>--datastore-certfile</code></td><td style="text-align:left"><code>K3S_DATASTORE_CERTFILE</code></td><td style="text-align:left">TLS 证书文件，用于对数据存储进行基于客户端证书的验证。要使用这个功能，你的数据存储必须被配置为支持基于客户端证书的认证。如果你指定了这个参数，你还必须指定<code>datastore-keyfile</code>参数。</td></tr><tr><td style="text-align:left"><code>--datastore-keyfile</code></td><td style="text-align:left"><code>K3S_DATASTORE_KEYFILE</code></td><td style="text-align:left">TLS 密钥文件，用于对数据存储进行基于客户端证书的认证。更多细节请参见前面的<code>datastore-certfile</code>参数。</td></tr></tbody></table><p>作为最佳实践，我们建议将这些参数设置为环境变量，而不是命令行参数，这样你的数据库证书或其他敏感信息就不会作为进程信息的一部分暴露出来。</p><h3><span id="36-私有镜像仓库">3.6 私有镜像仓库</span></h3><blockquote><p><strong>理解 Server 节点的安装，以及注册 Agent 节点的步骤！</strong></p></blockquote><p><code>K3s</code> 默认使用 <code>containerd</code> 作为容器运行时，所以在 <code>docker</code> 上配置镜像仓库是不生效的。<code>K3s</code> 镜像仓库配置文件由两大部分组成：<code>mirrors</code> 和  <code>configs</code>。</p><ul><li><code>Mirrors</code> 是一个用于定义专用镜像仓库的名称和 <code>endpoint</code> 的指令</li><li><code>Configs</code> 部分定义了每个 <code>mirror</code> 的 <code>TLS</code> 和证书配置</li><li>对于每个 <code>mirror</code>，你可以定义 <code>auth</code> 和 <code>/</code> 或 <code>tls</code></li></ul><p><code>K3s registry</code> 配置目录为： <code>/etc/rancher/k3s/registries.yaml</code>。<code>K3s</code> 启动时会检查  <code>/etc/rancher/k3s/</code> 中是否存在  <code>registries.yaml</code> 文件，并指示 <code>containerd</code> 使用文件中定义的镜像仓库。如果你想使用一个私有的镜像仓库，那么你需要在每个使用镜像仓库的节点上以 <code>root</code> 身份创建这个文件。</p><p>请注意，<code>server</code> 节点默认是可以调度的。如果你没有在 <code>server</code> 节点上设置污点，那么将在它们上运行工作负载，请确保在每个 <code>server</code> 节点上创建  <code>registries.yaml</code> 文件。</p><p><code>containerd</code> 使用了类似 <code>K8S</code> 中 <code>svc</code> 与 <code>endpoint</code> 的概念，<code>svc</code> 可以理解为访问名称，这个名称会解析到对应的 <code>endpoint</code> 上。也可以理解 <code>mirror</code> 配置就是一个反向代理，它把客户端的请求代理到 <code>endpoint</code> 配置的后端镜像仓库。<code>mirror</code> 名称可以随意填写，但是必须符合 <code>IP</code> 或域名的定义规则。并且可以配置多个 <code>endpoint</code>，默认解析到第一个 <code>endpoint</code>，如果第一个 <code>endpoint</code> 没有返回数据，则自动切换到第二个 <code>endpoint</code>，以此类推。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /etc/rancher/k3s/registries.yaml</span></span><br><span class="line"><span class="comment"># 同时可以设置多个mirrors地址</span></span><br><span class="line"><span class="comment"># 可以对mirrors设置权限和证书</span></span><br><span class="line"><span class="attr">mirrors:</span></span><br><span class="line">  <span class="attr">"172.31.6.200:5000":</span></span><br><span class="line">    <span class="attr">endpoint:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"http://172.31.6.200:5000"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"http://x.x.x.x:5000"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"http://y.y.y.y:5000"</span></span><br><span class="line">  <span class="attr">"rancher.ksd.top:5000":</span></span><br><span class="line">    <span class="attr">endpoint:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"http://172.31.6.200:5000"</span></span><br><span class="line">  <span class="attr">"docker.io":</span></span><br><span class="line">    <span class="attr">endpoint:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"https://fogjl973.mirror.aliyuncs.com"</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"https://registry-1.docker.io"</span></span><br><span class="line"></span><br><span class="line"><span class="attr">configs:</span></span><br><span class="line">  <span class="attr">"172.31.6.200:5000":</span></span><br><span class="line">    <span class="attr">auth:</span></span><br><span class="line">      <span class="attr">username:</span> <span class="string">admin</span></span><br><span class="line">      <span class="attr">password:</span> <span class="string">Harbor@12345</span></span><br><span class="line">    <span class="attr">tls:</span></span><br><span class="line">      <span class="attr">cert_file:</span> <span class="string">/home/ubuntu/harbor2.escapelife.site.cert</span></span><br><span class="line">      <span class="attr">key_file:</span> <span class="string">/home/ubuntu/harbor2.escapelife.site.key</span></span><br><span class="line">      <span class="attr">ca_file:</span> <span class="string">/home/ubuntu/ca.crt</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 镜像都是从同一个仓库获取到的</span></span><br><span class="line">$ sudo systemctl restart k3s.service</span><br><span class="line">$ sudo crictl pull 172.31.6.200:5000/library/alpine</span><br><span class="line">$ sudo crictl pull rancher.ksd.top:5000/library/alpine</span><br></pre></td></tr></table></figure><p>这里我们介绍下，如何使用 <code>TLS</code> 配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 证书颁发机构颁发的证书</span></span><br><span class="line">$ cat &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt;EOF</span><br><span class="line">mirrors:</span><br><span class="line">  <span class="string">"harbor.escapelife.site"</span>:</span><br><span class="line">    endpoint:</span><br><span class="line">      - <span class="string">"https://harbor.escapelife.site"</span></span><br><span class="line">configs:</span><br><span class="line">  <span class="string">"harbor.escapelife.site"</span>:</span><br><span class="line">    auth:</span><br><span class="line">      username: admin</span><br><span class="line">      password: Harbor@12345</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ sudo systemctl restart k3s</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自签名证书</span></span><br><span class="line">$ cat &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt;EOF</span><br><span class="line">mirrors:</span><br><span class="line">  <span class="string">"harbor2.escapelife.site"</span>:</span><br><span class="line">    endpoint:</span><br><span class="line">      - <span class="string">"https://harbor2.escapelife.site"</span></span><br><span class="line">configs:</span><br><span class="line">  <span class="string">"harbor2.escapelife.site"</span>:</span><br><span class="line">    auth:</span><br><span class="line">      username: admin</span><br><span class="line">      password: Harbor@12345</span><br><span class="line">    tls:</span><br><span class="line">      cert_file: /home/ubuntu/harbor2.escapelife.site.cert</span><br><span class="line">      key_file:  /home/ubuntu/harbor2.escapelife.site.key</span><br><span class="line">      ca_file:   /home/ubuntu/ca.crt</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ sudo systemctl restart k3s</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不使用TLS证书</span></span><br><span class="line">$ cat &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt;EOF</span><br><span class="line">mirrors:</span><br><span class="line">  <span class="string">"docker.io"</span>:</span><br><span class="line">    endpoint:</span><br><span class="line">      - <span class="string">"https://fogjl973.mirror.aliyuncs.com"</span></span><br><span class="line">      - <span class="string">"https://registry-1.docker.io"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">$ sudo systemctl restart k3s</span><br></pre></td></tr></table></figure><p><code>K3s</code> 将会在 <code>/var/lib/rancher/k3s/agent/etc/containerd/config.toml</code> 中为 <code>containerd</code> 生成  <code>config.toml</code>。如果要对这个文件进行高级设置，你可以在同一目录中创建另一个名为  <code>config.toml.tmpl</code> 的文件，此文件将会代替默认设置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 完整示例</span></span><br><span class="line">$ cat &gt;&gt; /etc/rancher/k3s/registries.yaml</span><br><span class="line">mirrors:</span><br><span class="line">  <span class="string">"harbor.escapelife.site"</span>:</span><br><span class="line">     endpoint:</span><br><span class="line">     - <span class="string">"https://harbor.escapelife.site"</span></span><br><span class="line">  <span class="string">"harbor2.escapelife.site"</span>:</span><br><span class="line">     endpoint:</span><br><span class="line">     - <span class="string">"https://harbor2.escapelife.site"</span></span><br><span class="line">  <span class="string">"172.31.19.227:5000"</span>:</span><br><span class="line">     endpoint:</span><br><span class="line">     - <span class="string">"http://172.31.19.227:5000"</span></span><br><span class="line">  <span class="string">"docker.io"</span>:</span><br><span class="line">     endpoint:</span><br><span class="line">     - <span class="string">"https://fogjl973.mirror.aliyuncs.com"</span></span><br><span class="line">     - <span class="string">"https://registry-1.docker.io"</span></span><br><span class="line"></span><br><span class="line">configs:</span><br><span class="line">  <span class="string">"harbor.escapelife.site"</span>:</span><br><span class="line">     auth:</span><br><span class="line">       username: admin</span><br><span class="line">       password: Harbor@12345</span><br><span class="line"></span><br><span class="line">  <span class="string">"harbor2.escapelife.site"</span>:</span><br><span class="line">     auth:</span><br><span class="line">       username: admin</span><br><span class="line">       password: Harbor@12345</span><br><span class="line">     tls:</span><br><span class="line">       cert_file: /home/ubuntu/harbor2.escapelife.site.cert</span><br><span class="line">       key_file:  /home/ubuntu/harbor2.escapelife.site.key</span><br><span class="line">       ca_file:   /home/ubuntu/ca.crt</span><br></pre></td></tr></table></figure><h3><span id="37-离线安装">3.7 离线安装</span></h3><blockquote><p><strong>理解 Server 节点的安装，以及注册 Agent 节点的步骤！</strong></p></blockquote><p>离线安装的过程主要分为以下两个步骤：</p><ul><li><p>步骤 1：部署镜像</p><ul><li><strong>部署私有镜像仓库</strong></li><li><strong>手动部署镜像</strong></li></ul></li><li><p>步骤 2：安装 <code>K3s</code> 工具</p><ul><li><strong>单节点安装</strong></li><li><strong>高可用安装</strong></li></ul></li><li><p>通过私有镜像仓库安装 K3s</p><ul><li><code>k3s-images.txt</code> 包含对于版本依赖的镜像文件</li><li><code>k3s-airgap-images-amd64.tar</code> 包含对于版本的镜像文件</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将所需镜像上传到私有镜像仓库</span></span><br><span class="line"><span class="comment"># https://github.com/k3s-io/k3s/releases</span></span><br><span class="line">可以从K3s镜像列表获取到版本，下载上传到私有镜像仓库</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建镜像仓库(YAML)</span></span><br><span class="line"><span class="comment"># 按照私有镜像仓库配置指南创建并配置registry.yaml文件</span></span><br><span class="line">$ mkdir -p /etc/rancher/k3s/</span><br><span class="line">cat &gt;&gt; /etc/rancher/k3s/registries.yaml &lt;&lt;EOF</span><br><span class="line">mirrors:</span><br><span class="line">  <span class="string">"docker.io"</span>:</span><br><span class="line">    endpoint:</span><br><span class="line">      - <span class="string">"https://harbor.escapelife.site"</span></span><br><span class="line">configs:</span><br><span class="line">  <span class="string">"docker.io"</span>:</span><br><span class="line">    auth:</span><br><span class="line">      username: admin</span><br><span class="line">      password: Harbor@12345</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装单节点K3s集群</span></span><br><span class="line"><span class="comment"># https://github.com/k3s-io/k3s/releases</span></span><br><span class="line">可以从K3s仓库获取到版本(二进制文件)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取K3s安装脚本</span></span><br><span class="line">$ wget https://get.k3s.io -o ./install.sh</span><br><span class="line">$ wget http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装K3s-server</span></span><br><span class="line">$ INSTALL_K3S_SKIP_DOWNLOAD=<span class="literal">true</span> ./install.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将agent加入到K3s集群</span></span><br><span class="line">$ INSTALL_K3S_SKIP_DOWNLOAD=<span class="literal">true</span> \</span><br><span class="line">    K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken \</span><br><span class="line">    ./install.sh</span><br></pre></td></tr></table></figure><ul><li>通过手动部署镜像安装 K3s</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从Github页面获取你所运行的K3s版本及文件</span></span><br><span class="line"><span class="comment"># https://github.com/rancher/k3s/releases</span></span><br><span class="line">k3s二进制文件+镜像tar文件</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将tar文件放在images目录下</span></span><br><span class="line">$ sudo mkdir -p /var/lib/rancher/k3s/agent/images/</span><br><span class="line">$ sudo cp ./k3s-airgap-images-<span class="variable">$ARCH</span>.tar /var/lib/rancher/k3s/agent/images/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将k3s二进制文件放在/usr/local/bin/k3s路径上</span></span><br><span class="line">$ mv ./k3s /usr/<span class="built_in">local</span>/bin/</span><br><span class="line">$ chmod 755 /usr/<span class="built_in">local</span>/bin/k3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装K3s-server</span></span><br><span class="line">$ INSTALL_K3S_SKIP_DOWNLOAD=<span class="literal">true</span> ./install.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将agent加入到K3s集群</span></span><br><span class="line">$ INSTALL_K3S_SKIP_DOWNLOAD=<span class="literal">true</span> \</span><br><span class="line">    K3S_URL=https://myserver:6443 K3S_TOKEN=mynodetoken \</span><br><span class="line">    ./install.sh</span><br></pre></td></tr></table></figure><p>离线升级 <code>K3s</code> 版本，完成离线安装 <code>K3s</code> 后，还可以通过脚本升级 <code>K3s</code> 版本，或启用自动升级功能，以保持离线环境中的 <code>K3s</code> 版本与最新的 <code>K3s</code> 版本同步。</p><ul><li>升级 K3s 版本</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过脚本升级</span></span><br><span class="line"><span class="comment"># https://github.com/rancher/k3s/releases</span></span><br><span class="line">从Github页面下载要升级到的K3s版本</span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换</span></span><br><span class="line"><span class="comment"># 复制并替换每个节点上/usr/local/bin中的旧K3s二进制文件</span></span><br><span class="line">$ mv ./k3s /usr/<span class="built_in">local</span>/bin/</span><br><span class="line">$ chmod 755 /usr/<span class="built_in">local</span>/bin/k3s</span><br><span class="line">$ wget http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启K3s服务</span></span><br><span class="line">$ sudo systemctl restart k3s.service</span><br></pre></td></tr></table></figure><h3><span id="38-仪表盘及卸载">3.8 仪表盘及卸载</span></h3><blockquote><p><strong>理解 Server 节点的安装，以及注册 Agent 节点的步骤！</strong></p></blockquote><p>推荐使用三种仪表盘工具，分别是对应是 <code>Kubernetes Dashboard</code>、<code>kube-explorer</code> 和 <code>Rancher UI</code>，其各自各有优劣。</p><ul><li>[1] Kubernetes Dashboard</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署Kubernetes仪表盘</span></span><br><span class="line">$ GITHUB_URL=https://github.com/kubernetes/dashboard/releases</span><br><span class="line">$ VERSION_KUBE_DASHBOARD=$(curl -w <span class="string">'%&#123;url_effective&#125;'</span> -I -L -s -S \</span><br><span class="line">    <span class="variable">$&#123;GITHUB_URL&#125;</span>/latest -o /dev/null | sed -e <span class="string">'s|.*/||'</span>)</span><br><span class="line">$ sudo k3s kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/<span class="variable">$&#123;VERSION_KUBE_DASHBOARD&#125;</span>/aio/deploy/recommended.yaml</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 仪表盘RBAC配置</span></span><br><span class="line"><span class="comment"># 本指南中创建的admin-user将在仪表盘中拥有管理权限</span></span><br><span class="line">$ sudo k3s kubectl create \</span><br><span class="line">    -f dashboard.admin-user.yml \</span><br><span class="line">    -f dashboard.admin-user-role.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># dashboard.admin-user.yml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">  namespace: kubernetes-dashboard</span><br><span class="line"></span><br><span class="line"><span class="comment"># dashboard.admin-user-role.yml</span></span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: admin-user</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: admin-user</span><br><span class="line">    namespace: kubernetes-dashboard</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获得Bearer-Token</span></span><br><span class="line">$ sudo k3s kubectl -n kubernetes-dashboard \</span><br><span class="line">    describe secret admin-user-token | grep <span class="string">'^token'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本地访问仪表盘</span></span><br><span class="line"><span class="comment"># https://192.168.100.100:8443</span></span><br><span class="line"><span class="comment"># https://www.escapelife.site/posts/180e93f1.html</span></span><br><span class="line"><span class="comment"># https://www.escapelife.site/posts/538ec6b1.html</span></span><br><span class="line">$ sudo k3s kubectl proxy</span><br><span class="line">$ sudo kubectl -n kubernetes-dashboard port-forward \</span><br><span class="line">    --address 0.0.0.0 svc/kubernets-dashboard 8443:443</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 升级仪表盘</span></span><br><span class="line">$ sudo k3s kubectl delete ns kubernetes-dashboard</span><br><span class="line">$ GITHUB_URL=https://github.com/kubernetes/dashboard/releases</span><br><span class="line">$ VERSION_KUBE_DASHBOARD=$(curl -w <span class="string">'%&#123;url_effective&#125;'</span> -I -L -s -S <span class="variable">$&#123;GITHUB_URL&#125;</span>/latest -o /dev/null | sed -e <span class="string">'s|.*/||'</span>)</span><br><span class="line">$ sudo k3s kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/<span class="variable">$&#123;VERSION_KUBE_DASHBOARD&#125;</span>/aio/deploy/recommended.yaml -f dashboard.admin-user.yml -f dashboard.admin-user-role.yml</span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 删除仪表盘和admin-user配置</span></span><br><span class="line">$ sudo k3s kubectl delete ns kubernetes-dashboard</span><br><span class="line">$ sudo k3s kubectl delete clusterrolebinding kubernetes-dashboard</span><br><span class="line">$ sudo k3s kubectl delete clusterrole kubernetes-dashboard</span><br></pre></td></tr></table></figure><ul><li>[2] kube-explorer<ul><li><code>kube-explorer</code> 是 <code>K8S</code> 的便携式资源管理器，没有任何依赖</li><li>并提供了一个几乎完全无状态的 <code>K8S</code> 资源管理器</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从发布页面下载二进制文件</span></span><br><span class="line"><span class="comment"># https://github.com/cnrancher/kube-explorer</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行</span></span><br><span class="line"><span class="comment"># --kubeconfig 可以不配置(自己可以找到)</span></span><br><span class="line">$ ./kube-explorer --kubeconfig=/etc/rancher/k3s/kube.yaml \</span><br><span class="line">    --http-listen-port=9898 \</span><br><span class="line">    --https-listen-port=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开浏览器访问</span></span><br><span class="line">http://192.168.100.100:9898</span><br></pre></td></tr></table></figure><ul><li>[3] Rancher UI<ul><li>可以将 <code>K3s</code> 导入到 <code>Rancher UI</code> 中去管理</li><li>官网 <a href="http://docs.rancher.cn/docs/rancher2/cluster-provisioning/imported-clusters/_index/#%E5%AF%BC%E5%85%A5-k3s-%E9%9B%86%E7%BE%A4" target="_blank" rel="noopener">导入 K3s 集群</a> 指导文档</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入K3s集群时，Rancher会将其识别为K3s类型，并且附件额外功能</span></span><br><span class="line"><span class="comment"># 1.能够升级K3s版本</span></span><br><span class="line"><span class="comment"># 2.可配置升级集群时升级的最大节点数</span></span><br><span class="line"><span class="comment"># 3.在主机详情页能够查看启动K3s集群时每个节点的配置参数和环境变量</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置K3s集群以允许导入到Rancher</span></span><br><span class="line">$ curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn sh -s - \</span><br><span class="line">    --write-kubeconfig-mode 644</span><br></pre></td></tr></table></figure><ul><li>[4] 卸载 K3s 服务</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 主节点</span></span><br><span class="line">$ /usr/<span class="built_in">local</span>/bin/k3s-uninstall.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># 工作节点</span></span><br><span class="line">$ /usr/<span class="built_in">local</span>/bin/k3s-agent-uninstall.sh</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 包括docker等信息一并清理</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">KUBE_SVC=<span class="string">'</span></span><br><span class="line"><span class="string">kubelet</span></span><br><span class="line"><span class="string">kube-scheduler</span></span><br><span class="line"><span class="string">kube-proxy</span></span><br><span class="line"><span class="string">kube-controller-manager</span></span><br><span class="line"><span class="string">kube-apiserver</span></span><br><span class="line"><span class="string">'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> kube_svc <span class="keyword">in</span> <span class="variable">$&#123;KUBE_SVC&#125;</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="comment"># 停止服务</span></span><br><span class="line">  <span class="keyword">if</span> [[ `systemctl is-active <span class="variable">$&#123;kube_svc&#125;</span>` == <span class="string">'active'</span> ]]; <span class="keyword">then</span></span><br><span class="line">    systemctl stop <span class="variable">$&#123;kube_svc&#125;</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line">  <span class="comment"># 禁止服务开机启动</span></span><br><span class="line">  <span class="keyword">if</span> [[ `systemctl is-enabled <span class="variable">$&#123;kube_svc&#125;</span>` == <span class="string">'enabled'</span> ]]; <span class="keyword">then</span></span><br><span class="line">    systemctl <span class="built_in">disable</span> <span class="variable">$&#123;kube_svc&#125;</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止所有容器</span></span><br><span class="line">docker stop $(docker ps -aq)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除所有容器</span></span><br><span class="line">docker rm -f $(docker ps -qa)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除所有容器卷</span></span><br><span class="line">docker volume rm $(docker volume ls -q)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 卸载mount目录</span></span><br><span class="line"><span class="keyword">for</span> mount <span class="keyword">in</span> $(mount | grep tmpfs | grep <span class="string">'/var/lib/kubelet'</span> | awk <span class="string">'&#123; print $3 &#125;'</span>) /var/lib/kubelet /var/lib/rancher;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  umount <span class="variable">$mount</span>;</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 备份目录</span></span><br><span class="line">mv /etc/kubernetes /etc/kubernetes-bak-$(date +<span class="string">"%Y%m%d%H%M"</span>)</span><br><span class="line">mv /var/lib/etcd /var/lib/etcd-bak-$(date +<span class="string">"%Y%m%d%H%M"</span>)</span><br><span class="line">mv /var/lib/rancher /var/lib/rancher-bak-$(date +<span class="string">"%Y%m%d%H%M"</span>)</span><br><span class="line">mv /opt/rke /opt/rke-bak-$(date +<span class="string">"%Y%m%d%H%M"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除残留路径</span></span><br><span class="line">rm -rf /etc/ceph \</span><br><span class="line">    /etc/cni \</span><br><span class="line">    /opt/cni \</span><br><span class="line">    /run/secrets/kubernetes.io \</span><br><span class="line">    /run/calico \</span><br><span class="line">    /run/flannel \</span><br><span class="line">    /var/lib/calico \</span><br><span class="line">    /var/lib/cni \</span><br><span class="line">    /var/lib/kubelet \</span><br><span class="line">    /var/<span class="built_in">log</span>/containers \</span><br><span class="line">    /var/<span class="built_in">log</span>/kube-audit \</span><br><span class="line">    /var/<span class="built_in">log</span>/pods \</span><br><span class="line">    /var/run/calico \</span><br><span class="line">    /usr/libexec/kubernetes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理网络接口</span></span><br><span class="line">no_del_net_inter=<span class="string">'</span></span><br><span class="line"><span class="string">lo</span></span><br><span class="line"><span class="string">docker0</span></span><br><span class="line"><span class="string">eth</span></span><br><span class="line"><span class="string">ens</span></span><br><span class="line"><span class="string">bond</span></span><br><span class="line"><span class="string">'</span></span><br><span class="line"></span><br><span class="line">network_interface=`ls /sys/class/net`</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> net_inter <span class="keyword">in</span> <span class="variable">$network_interface</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="keyword">if</span> ! <span class="built_in">echo</span> <span class="string">"<span class="variable">$&#123;no_del_net_inter&#125;</span>"</span> | grep -qE <span class="variable">$&#123;net_inter:0:3&#125;</span>; <span class="keyword">then</span></span><br><span class="line">    ip link delete <span class="variable">$net_inter</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理残留进程</span></span><br><span class="line">port_list=<span class="string">'</span></span><br><span class="line"><span class="string">80</span></span><br><span class="line"><span class="string">443</span></span><br><span class="line"><span class="string">6443</span></span><br><span class="line"><span class="string">2376</span></span><br><span class="line"><span class="string">2379</span></span><br><span class="line"><span class="string">2380</span></span><br><span class="line"><span class="string">8472</span></span><br><span class="line"><span class="string">9099</span></span><br><span class="line"><span class="string">10250</span></span><br><span class="line"><span class="string">10254</span></span><br><span class="line"><span class="string">'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> port <span class="keyword">in</span> <span class="variable">$port_list</span>;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  pid=`netstat -atlnup | grep <span class="variable">$port</span> | awk <span class="string">'&#123;print $7&#125;'</span> | awk -F <span class="string">'/'</span> <span class="string">'&#123;print $1&#125;'</span> | grep -v - | sort -rnk2 | uniq`</span><br><span class="line">  <span class="keyword">if</span> [[ -n <span class="variable">$pid</span> ]]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">kill</span> -9 <span class="variable">$pid</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">kube_pid=`ps -ef | grep -v grep | grep kube | awk <span class="string">'&#123;print $2&#125;'</span>`</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [[ -n <span class="variable">$kube_pid</span> ]]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">kill</span> -9 <span class="variable">$kube_pid</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 清理Iptables表</span></span><br><span class="line"><span class="comment">## 注意：如果节点Iptables有特殊配置，以下命令请谨慎操作</span></span><br><span class="line">sudo iptables --flush</span><br><span class="line">sudo iptables --flush --table nat</span><br><span class="line">sudo iptables --flush --table filter</span><br><span class="line">sudo iptables --table nat --delete-chain</span><br><span class="line">sudo iptables --table filter --delete-chain</span><br><span class="line">systemctl restart docker</span><br></pre></td></tr></table></figure><h3><span id="39-注意事项">3.9 注意事项</span></h3><blockquote><p><strong>理解 Server 节点的安装，以及注册 Agent 节点的步骤！</strong></p></blockquote><ul><li><strong>Helm</strong><ul><li>如果需要使用 <code>helm</code> 操作 <code>K3s</code> 集群，需要创建 <code>~/.kube/conf</code> 目录</li><li>需要执行 <code>cp /etc/rancher/k3s/k3s.yaml ~/.kube/config</code> 命令</li></ul></li><li><strong>自动部署的清单</strong><ul><li>将由 <code>rancher/helm-controller</code> 在运行时安装</li><li>目录路径：<code>/var/lib/rancher/k3s/server/manifests</code></li><li>目录下面的每个 <code>yaml</code> 就代表这个一个需要启动的服务</li></ul></li></ul><p>对于我们希望使用的组件，可以在启动的时候禁用默认组件，在手动部署你需要的一些组件(通常是放到一个指定目录下面，随着服务启动自动拉起)，从而达到灵活使用的目的。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看所有Pod服务</span></span><br><span class="line"><span class="comment"># 比如helm/coredns也不是自带的就是通过这个方式创建的</span></span><br><span class="line">$ sudo kubectl get pods -A</span><br></pre></td></tr></table></figure><ul><li>注册 Agent 节点<ul><li>工作节点密码存储：<code>/etc/rancher/node/password</code></li><li>主节点的密码存储：<code>/var/lib/rancher/k3s/server/cred/node-passwd</code></li></ul></li></ul><p>在 <code>agent</code> 节点运行注册命令，会和 <code>server</code> 节点发起 <code>websocket</code> 连接，然后会在工作节点上面创建一个随机的密码。然后会拿着这个密码和工作节点的主机名，发送给主节点。然后主节点会将这个信息在保存(<code>k8s secrets</code>)起来，随后的任何尝试都必须使用相同的密码。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 工作节点的密码信息(password+hostname)</span></span><br><span class="line">$ sudo cat /etc/rancher/node/password</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看主节点的密码信息</span></span><br><span class="line"><span class="comment"># https://docs.rancher.cn/docs/k3s/architecture/_index#注册-agent-节点</span></span><br><span class="line">$ sudo kubectl get secret k3s2.node-password.k3s -o yaml -n kube-system</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以查看日志信息验证这个信息的存在</span></span><br><span class="line">$ sudo tail -f 200 /var/<span class="built_in">log</span>/syslog | grep k3s</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 发现节点信息提示NotReady状态</span></span><br><span class="line"><span class="comment"># 可以尝试删除节点的密码存储信息，之后会自动获取新的</span></span><br><span class="line">$ sudo kubectl delete secret k3s2.node-password.k3s -n kube-system</span><br></pre></td></tr></table></figure><ul><li>自定义存储类型</li></ul><p>集群启动之后，默认会启动一个 <code>local-path</code> 的组件，用于提供服务挂载存储使用，其默认以 <code>PVC</code> 的形式。之后，将其存储在 <code>/var/lib/rancher/k3s/server/storageclass</code> 目录下面。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看组件</span></span><br><span class="line">$ sudo kubectl get pods -A</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看对应存储</span></span><br><span class="line">$ sudo kubectl get storageclass</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以使用参数修改默认存储地址</span></span><br><span class="line"><span class="comment"># --default-local-storage-path&amp;nbsp;value</span></span><br><span class="line">$ curl -sfL http://rancher-mirror.cnrancher.com/k3s/k3s-install.sh | \</span><br><span class="line">    INSTALL_K3S_MIRROR=cn \</span><br><span class="line">    INSTALL_K3S_EXEC=<span class="string">'--etcd-snapshot-schedule-cron *&amp;nbsp;*/5&amp;nbsp;*&amp;nbsp;*&amp;nbsp;*'</span> \</span><br><span class="line">    sh -</span><br></pre></td></tr></table></figure><h2><span id="4-k3s-集群升级">4. K3S 集群升级</span></h2><blockquote><p><strong>手动升级 + 自动升级</strong></p></blockquote><p>当升级 <code>K3s</code> 时，<code>K3s</code> 服务会重启或停止，但 <code>K3s</code> 容器会继续运行。 要停止所有的 <code>K3s</code> 容器并重置容器的状态，可以使用  <code>k3s-killall.sh</code> 脚本。 <code>killall</code> 脚本清理容器、<code>K3s</code> 目录和网络组件，同时也删除了 <code>iptables</code> 链和所有相关规则。集群数据不会被删除。</p><ul><li>[1] 手动升级 - 使用安装脚本升级 K3s</li></ul><p>你可以通过使用安装脚本升级 <code>K3s</code>，或者手动安装所需版本的二进制文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 升级到最新stable版本</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 升级到latest版本</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=latest sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 升级到v1.20的最新版本</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=<span class="string">"v1.20"</span> sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 升级到指定版本</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -</span><br></pre></td></tr></table></figure><ul><li>[2] 手动升级 - 使用二进制文件手动升级 K3s</li></ul><p>你可以通过使用安装脚本升级 <code>K3s</code>，或者手动安装所需版本的二进制文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从发布下载所需版本的K3s二进制文件</span></span><br><span class="line">https://github.com/rancher/k3s/releases</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将下载的二进制文件复制到/usr/local/bin/k3s</span></span><br><span class="line">$ mv ./k3s /usr/<span class="built_in">local</span>/bin/k3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止旧的K3s二进制文件</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=<span class="string">"v1.20"</span> sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动新的K3s二进制文件</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=vX.Y.Z-rc1 sh -</span><br></pre></td></tr></table></figure><p>你可以使用 <code>Rancher</code> 的 <code>system-upgrad-controller</code> 来管理 <code>K3s</code> 集群升级。这是一种 <code>Kubernetes</code> 原生的集群升级方法。它利用自定义资源定义(<code>CRD</code>)、计划和控制器，根据配置的计划安排升级。</p><p>控制器通过监控计划和选择要在其上运行升级 <code>job</code> 的节点来调度升级，计划通过标签选择器定义哪些节点应该升级。当一个 <code>job</code> 成功运行完成后，控制器会给它运行的节点打上相应的标签。</p><ul><li>[3] 自动升级 - 使用二进制文件手动升级 K3s<ul><li><a href="https://github.com/rancher/k3s-upgrade" target="_blank" rel="noopener">k3s-upgrade</a></li><li><a href="https://github.com/rancher/system-upgrade-controller" target="_blank" rel="noopener">system-upgrade-controller</a></li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将system-upgrade-controller安装到您的集群中</span></span><br><span class="line">$ kubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/download/v0.6.2/system-upgrade-controller.yaml</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置计划</span></span><br><span class="line"><span class="comment"># 建议您最少创建两个计划</span></span><br><span class="line"><span class="comment"># 升级server节点的计划和升级agent节点的计划</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Server plan</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">upgrade.cattle.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Plan</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">server-plan</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">system-upgrade</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">concurrency:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">cordon:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">matchExpressions:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span> <span class="comment"># 选择主节点</span></span><br><span class="line">      <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">      <span class="attr">values:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">"true"</span></span><br><span class="line">  <span class="attr">serviceAccountName:</span> <span class="string">system-upgrade</span></span><br><span class="line">  <span class="attr">upgrade:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">rancher/k3s-upgrade</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">v1.20.4+k3s1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Agent plan</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">upgrade.cattle.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Plan</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">agent-plan</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">system-upgrade</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">concurrency:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">cordon:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">matchExpressions:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span> <span class="comment"># 选择工作节点</span></span><br><span class="line">      <span class="attr">operator:</span> <span class="string">DoesNotExist</span></span><br><span class="line">  <span class="attr">prepare:</span></span><br><span class="line">    <span class="attr">args:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">prepare</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">server-plan</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">rancher/k3s-upgrade</span></span><br><span class="line">  <span class="attr">serviceAccountName:</span> <span class="string">system-upgrade</span></span><br><span class="line">  <span class="attr">upgrade:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">rancher/k3s-upgrade</span></span><br><span class="line">  <span class="attr">version:</span> <span class="string">v1.20.4+k3s1</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自动升级到最新版本(不指定版本)</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">upgrade.cattle.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Plan</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="attr">upgrade:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">rancher/k3s-upgrade</span></span><br><span class="line">  <span class="attr">channel:</span> <span class="string">https://update.k3s.io/v1-release/channels/stable</span></span><br></pre></td></tr></table></figure><p><img src="https://img.hi-linux.com/staticfile/advance-k3s-tool-04-20220519092442639-2022-05-19-XowxJZ.png" alt="K3S集群升级"></p><h2><span id="5-k3s-备份恢复">5. K3S 备份恢复</span></h2><blockquote><p><strong>SQLite + etcd + 外部数据存储</strong></p></blockquote><ul><li>[1] 使用嵌入式 SQLite 数据存储进行备份和恢复</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式1：备份/恢复数据目录</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 备份</span></span><br><span class="line">$ cp -rf /var/lib/rancher/k3s/server/db /opt/db</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复</span></span><br><span class="line">$ systemctl stop k3s</span><br><span class="line">$ rm -rf /var/lib/rancher/k3s/server/db</span><br><span class="line">$ cp -rf /opt/db /var/lib/rancher/k3s/server/db</span><br><span class="line">$ systemctl start k3s</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方式2：通过 SQLite cli</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 备份</span></span><br><span class="line">sqlite3 /var/lib/rancher/k3s/server/db/state.db</span><br><span class="line">SQLite version 3.22.0 2018-01-22 18:45:57</span><br><span class="line">Enter <span class="string">".help"</span> <span class="keyword">for</span> usage hints.</span><br><span class="line">sqlite&gt; .backup <span class="string">"/opt/kine.db"</span></span><br><span class="line">sqlite&gt; .<span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复</span></span><br><span class="line">$ sudo systemctl stop k3s</span><br><span class="line"></span><br><span class="line">sqlite3 /var/lib/rancher/k3s/server/db/state.db</span><br><span class="line">SQLite version 3.22.0 2018-01-22 18:45:57</span><br><span class="line">Enter <span class="string">".help"</span> <span class="keyword">for</span> usage hints.</span><br><span class="line">sqlite&gt; .restore <span class="string">'/opt/kine.db'</span></span><br><span class="line">sqlite&gt; .<span class="built_in">exit</span></span><br><span class="line"></span><br><span class="line">$ sudo systemctl start k3s</span><br></pre></td></tr></table></figure><p>当使用外部数据存储时，备份和恢复操作是在 <code>K3s</code> 之外处理的。数据库管理员需要对外部数据库进行备份，或者从快照或转储中进行恢复。我们建议将数据库配置为执行定期快照。</p><ul><li>[2] 使用外部数据存储进行备份和恢复</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 备份</span></span><br><span class="line">$ mysqldump -uroot -p --all-databases --master-data &gt; k3s-dbdump.db</span><br><span class="line"></span><br><span class="line"><span class="comment"># 恢复</span></span><br><span class="line">$ systemctl stop k3s</span><br><span class="line">$ mysql -uroot -p  &lt; k3s-dbdump.db</span><br><span class="line">$ systemctl start k3s</span><br></pre></td></tr></table></figure><ul><li>[3] 使用嵌入式 etcd 数据存储进行备份和恢复</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建快照(K3s默认启用快照)</span></span><br><span class="line"><span class="comment"># 快照目录默认: /var/lib/rancher/k3s/server/db/snapshots</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 要配置快照间隔或保留的快照数量</span></span><br><span class="line">--etcd-disable-snapshots       禁用自动etcd快照</span><br><span class="line">--etcd-snapshot-schedule-cron  定时快照的时间点；认值为每12小时触发一次</span><br><span class="line">--etcd-snapshot-retention      保留的快照数量；默认值为5</span><br><span class="line">--etcd-snapshot-dir            保存数据库快照的目录路径</span><br><span class="line">--cluster-reset                忘记所有的对等体；成为新集群的唯一成员</span><br><span class="line">--cluster-reset-restore-path   要恢复的快照文件的路径</span><br></pre></td></tr></table></figure><p>当 <code>K3s</code> 从备份中恢复时，旧的数据目录将被移动到<code>/var/lib/rancher/k3s/server/db/etcd-old/</code>。然后 <code>K3s</code> 会尝试通过创建一个新的数据目录来恢复快照，然后从一个带有一个 <code>etcd</code> 成员的新 <code>K3s</code> 集群启动 <code>etcd</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从快照恢复集群</span></span><br><span class="line"><span class="comment"># 使用--cluster-reset选项运行K3s</span></span><br><span class="line"><span class="comment"># 同时给出--cluster-reset-restore-path</span></span><br><span class="line">$ ./k3s server \</span><br><span class="line">    --cluster-reset \</span><br><span class="line">    --cluster-reset-restore-path=&lt;PATH-TO-SNAPSHOT&gt;</span><br></pre></td></tr></table></figure><h2><span id="6-k3s-卷和存储">6. K3S 卷和存储</span></h2><blockquote><p><strong>介绍了如何通过 local storage provider 或 Longhorn 来设置持久存储。</strong></p></blockquote><p>当部署一个需要保留数据的应用程序时，你需要创建持久存储。持久存储允许您从运行应用程序的 <code>pod</code> 外部存储应用程序数据。即使应用程序的 <code>pod</code> 发生故障，这种存储方式也可以使您维护应用程序数据。</p><ul><li>[1] 设置 Local Storage Provider 支持</li></ul><p><code>K3s</code> 自带 <code>Rancher</code> 的 <code>Local Path Provisioner</code>(<code>LPP</code>)，这使得能够使用各自节点上的本地存储来开箱即用地创建 <code>pvc</code>。根据用户配置，<code>LPP</code> 将自动在节点上创建基于 <code>hostPath</code> 的持久卷。它利用了 <code>K8s</code> 的 <code>Local Persistent Volume</code> 特性引入的特性，但它比 <code>K8s</code> 中内置的  <code>local pv</code> 特性更简单的解决方案。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pvc.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">local-path-pvc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">local-path</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># pod.yaml</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volume-test</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">volume-test</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx:stable-alpine</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">volv</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">volv</span></span><br><span class="line">    <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">      <span class="attr">claimName:</span> <span class="string">local-path-pvc</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 应用yaml服务</span></span><br><span class="line">$ kubectl create -f pvc.yaml pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确认PV和PVC已创建</span></span><br><span class="line">$ kubectl get pv</span><br><span class="line">$ kubectl get pvc</span><br></pre></td></tr></table></figure><ul><li>[2] 设置 Longhorn 支持</li></ul><p><code>K3s</code> 支持 <code>Longhorn</code>(是 <code>K8s</code> 的一个开源分布式块存储系统)。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装Longhorn</span></span><br><span class="line"><span class="comment"># 将被安装在命名空间longhorn-system中</span></span><br><span class="line">$ kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># pvc.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: longhorn-volv-pvc</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  storageClassName: longhorn</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 2Gi</span><br><span class="line"></span><br><span class="line"><span class="comment"># pod.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: volume-test</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: volume-test</span><br><span class="line">    image: nginx:stable-alpine</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: volv</span><br><span class="line">      mountPath: /data</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">  volumes:</span><br><span class="line">  - name: volv</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: longhorn-volv-pvc</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 应用yaml服务</span></span><br><span class="line">$ kubectl create -f pvc.yaml pod.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确认PV和PVC已创建</span></span><br><span class="line">$ kubectl get pv</span><br><span class="line">$ kubectl get pvc</span><br></pre></td></tr></table></figure><h2><span id="7-k3s-网络相关">7. K3S 网络相关</span></h2><blockquote><p><strong>简单介绍下 K3s 相关的网络配置组件！</strong></p></blockquote><ul><li><strong>CoreDNS</strong></li></ul><p><code>CoreDNS</code> 是在 <code>agent</code> 节点启动时部署的。要禁用，请在每台服务器上运行 <code>--disable coredns</code> 选项。如果你不安装 <code>CoreDNS</code>，你将需要自己安装一个集群 <code>DNS</code> 提供商。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如何修改coredns参数</span></span><br><span class="line"><span class="comment"># /var/lib/rancher/k3s/server/manifests/coredns.yaml</span></span><br><span class="line"><span class="comment"># 该文件重启K3s服务的话会导致coredns配置重新初始化</span></span><br><span class="line">1.将coredns.yaml保存到其他目录</span><br><span class="line">2.通过 --<span class="built_in">disable</span> coredns 禁用coredns</span><br><span class="line">3.复制coredns.yaml到/var/lib/rancher/k3s/server/manifests/目录并修改参数</span><br></pre></td></tr></table></figure><ul><li><strong>Traefik Ingress Controller</strong></li></ul><p>启动 <code>server</code> 时，默认情况下会部署 <code>Traefik</code>，对该文件的任何修改都会以类似 <code>kubectl apply</code> 的方式自动部署到 <code>Kubernetes</code> 中，将使用主机上的 <code>80</code> 和 <code>443</code> 端口。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 操作和上面基本是一致的</span></span><br><span class="line"><span class="comment"># 请使用 --disable traefik 选项启动每个server</span></span><br><span class="line"><span class="comment"># /var/lib/rancher/k3s/server/manifests/traefik.yaml</span></span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如何启用 treafik2 dashboard</span></span><br><span class="line"><span class="comment"># http://traefik.example.com/dashboard</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">Note:</span> in a kubernetes secret the string (e.g. generated by htpasswd) must be base64-encoded first.</span></span><br><span class="line"><span class="comment"># To create an encoded user:password pair, the following command can be used:</span></span><br><span class="line"><span class="comment"># htpasswd -nb admin admin | openssl base64</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">authsecret</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">users:</span> <span class="string">|2</span></span><br><span class="line">    <span class="string">YWRtaW46JGFwcjEkLkUweHd1Z0EkUjBmLi85WndJNXZWRFMyR2F2LmtELwoK</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">traefik.containo.us/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">IngressRoute</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">traefik-dashboard</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">routes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">match:</span> <span class="string">Host(`traefik.example.com`)</span> <span class="string">&amp;&amp;</span> <span class="string">(PathPrefix(`/api`)</span> <span class="string">||</span> <span class="string">PathPrefix(`/dashboard`))</span></span><br><span class="line">      <span class="attr">kind:</span> <span class="string">Rule</span></span><br><span class="line">      <span class="attr">services:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">api@internal</span></span><br><span class="line">          <span class="attr">kind:</span> <span class="string">TraefikService</span></span><br><span class="line">      <span class="attr">middlewares:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">auth</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">traefik.containo.us/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Middleware</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">auth</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">basicAuth:</span></span><br><span class="line">    <span class="attr">secret:</span> <span class="string">authsecret</span> <span class="comment"># Kubernetes secret named "secretName"</span></span><br></pre></td></tr></table></figure><ul><li>Service Load Balancer</li></ul><p><code>K3s</code> 提供了一个名为 <code>Klipper Load Balancer</code> 的负载均衡器，它可以使用可用的主机端口。 允许创建 <code>LoadBalancer</code> 类型的 <code>Service</code>，但不包括 <code>LB</code> 的实现。某些 <code>LB</code> 服务需要云提供商，例如 <code>Amazon EC2</code>。相比之下，<code>K3s service LB</code> 使得可以在没有云提供商的情况下使用 <code>LB</code> 服务。</p><h2><span id="8-k3s-与-helm">8. K3S 与 Helm</span></h2><p><code>Helm</code> 是 <code>Kubernetes</code> 的包管理工具。<code>Helm Chart</code> 为 <code>Kubernetes YAML</code> 清单文件提供了模板化语法，可以通过 <code>Helm</code> 安装对应的 <code>chart</code>。<code>K3s</code> 不需要任何特殊的配置就可以使用 <code>Helm</code> 命令行工具。</p><ul><li>自动部署 Helm charts</li></ul><p>在 <code>/var/lib/rancher/k3s/server/manifests</code> 中找到的任何 <code>Kubernetes</code> 清单将以类似 <code>kubectl apply</code> 的方式自动部署到 <code>K3s</code>。以这种方式部署的 <code>manifests</code> 是作为 <code>AddOn</code> 自定义资源来管理的。你会发现打包组件的 <code>AddOns</code>，如 <code>CoreDNS</code>、<code>Local-Storage</code> 等。<code>AddOns</code> 是由部署控制器自动创建的，并根据它们在 <code>manifests</code> 目录下的文件名命名。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看运行AddOn资源</span></span><br><span class="line">$ kubectl get addon -A</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以将Helm-Chart作为AddOns部署</span></span><br><span class="line">https://github.com/rancher/helm-controller/</span><br></pre></td></tr></table></figure><ul><li>使用 Helm CRD</li></ul><p><code>HelmChart CRD</code> 捕获了大多数你通常会传递给 <code>helm</code> 命令行工具的选项。下面是一个例子，说明如何从默认的 <code>Chart</code> 资源库中部署 <code>Grafana</code>，覆盖一些默认的 <code>Chart</code> 值。请注意，<code>HelmChart</code> 资源本身在 <code>kube-system</code> 命名空间，但 <code>Chart</code> 资源将被部署到 <code>monitoring</code> 命名空间。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">helm.cattle.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">HelmChart</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">grafana</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">chart:</span> <span class="string">stable/grafana</span></span><br><span class="line">  <span class="attr">targetNamespace:</span> <span class="string">monitoring</span></span><br><span class="line">  <span class="attr">set:</span></span><br><span class="line">    <span class="attr">adminPassword:</span> <span class="string">"NotVerySafePassword"</span></span><br><span class="line">  <span class="attr">valuesContent:</span> <span class="string">|-</span></span><br><span class="line">    <span class="attr">image:</span></span><br><span class="line">      <span class="attr">tag:</span> <span class="string">master</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">      <span class="attr">GF_EXPLORE_ENABLED:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">adminUser:</span> <span class="string">admin</span></span><br><span class="line">    <span class="attr">sidecar:</span></span><br><span class="line">      <span class="attr">datasources:</span></span><br><span class="line">        <span class="attr">enabled:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><h2><span id="9-k3s-高级选项">9. K3S 高级选项</span></h2><blockquote><p><strong><a href="https://github.com/kingsd041/k3s-tutorial/blob/main/17-%E9%AB%98%E7%BA%A7%E9%80%89%E9%A1%B9%E5%92%8C%E9%85%8D%E7%BD%AE/README.md" target="_blank" rel="noopener">包含高级选项和配置</a></strong></p></blockquote><ul><li>证书轮换</li></ul><p>默认情况下，<code>K3s</code> 的证书在 <code>12</code> 个月内过期。如果证书已经过期或剩余的时间不足 <code>90</code> 天，则在 <code>K3s</code> 重启时轮换证书。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询K3s证书过期时间</span></span><br><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> `ls /var/lib/rancher/k3s/server/tls/*.crt`; \</span><br><span class="line">  <span class="keyword">do</span> \</span><br><span class="line">    <span class="built_in">echo</span> <span class="variable">$i</span>;\</span><br><span class="line">    openssl x509 -enddate -noout -<span class="keyword">in</span> <span class="variable">$i</span>; \</span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改系统时间为证书过期前90天或证书过期后</span></span><br><span class="line">$ timedatectl <span class="built_in">set</span>-ntp no</span><br><span class="line">$ date -s 20220807</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启K3s服务</span></span><br><span class="line">$ service k3s restart</span><br></pre></td></tr></table></figure><ul><li>Red Hat 和 CentOS 的额外准备</li></ul><p>建议运行以下命令，关闭 <code>firewalld</code> 防火墙。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl <span class="built_in">disable</span> firewalld --now</span><br></pre></td></tr></table></figure><h2><span id="10-参考链接">10. 参考链接</span></h2><blockquote><p><strong>送人玫瑰，手有余香！</strong></p></blockquote><ul><li><strong>[1] 文档教程</strong><ul><li><a href="http://mirror.cnrancher.com/" target="_blank" rel="noopener">K3s 中文文档 - 国外</a></li><li><a href="https://docs.rancher.cn/k3s" target="_blank" rel="noopener">K3s 中文文档 - 国内</a></li><li><a href="https://mirror.rancher.cn/" target="_blank" rel="noopener">K3s 国内镜像站 - 加速</a></li><li><a href="https://github.com/kingsd041/k3s-tutorial" target="_blank" rel="noopener">K3s 系列教程 - 官方制作</a></li></ul></li><li><strong>[2] 代码地址</strong><ul><li><a href="https://github.com/k3s-io" target="_blank" rel="noopener">K3s 仓库地址 - Github</a></li></ul></li><li><strong>[3] 周边项目</strong><ul><li>K3s 周边项目 - k3os<ul><li>完全基于 <code>K8S</code> 管理的轻量级操作系统</li></ul></li><li>K3s 周边项目 - autok3s<ul><li>用于简化 <code>K3s</code> 集群部署和管理的轻量级工具</li><li>即在阿里云和 <code>aws</code> 等云服务器上面部署 <code>k3s</code></li></ul></li><li>K3s 周边项目 - k3d<ul><li>可以在 <code>k3d</code> 创建容器化的 <code>k3s</code> 集群</li><li>可以使用容器在单台计算机上启动多节点 <code>k3s</code> 集群</li></ul></li><li>K3s 周边项目 - harvester<ul><li>基于 <code>K8S</code> 构建的开源超融合基础架构(<code>HCI</code>)软件</li><li>旨在替换 <code>vSphere</code> 和 <code>Nutanix</code> 的开源替代方案</li></ul></li><li>K3s 周边项目 - octopus<ul><li>主要用于边缘计算相关</li><li>用于 <code>K8S</code> 和 <code>k3s</code> 的轻量级云原生设备管理系统</li><li>集群可以将边缘设备作为自定义 <code>k8s</code> 资源进行管理</li></ul></li></ul></li></ul><blockquote><p>本文转载自：「 Escape 的博客 」，原文：<a href="https://url.hi-linux.com/ytP71" target="_blank" rel="noopener">https://url.hi-linux.com/ytP71</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;深入理解官方文档，轻松学会使用 K3S 工具！&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;K3s&lt;/code&gt; 是一个轻量级的 &lt;code&gt;Kubernetes&lt;/code&gt; 发行版，它针对边缘计算、物联网等场景进行了高度优化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;CNCF&lt;/code&gt; 认证的 &lt;code&gt;Kubernetes&lt;/code&gt; 发行版&lt;/li&gt;
&lt;li&gt;支持 &lt;code&gt;X86_64&lt;/code&gt;, &lt;code&gt;ARM64&lt;/code&gt;, &lt;code&gt;ARMv7&lt;/code&gt; 平台&lt;/li&gt;
&lt;li&gt;单一进程包含 &lt;code&gt;Kubernetes master&lt;/code&gt;，&lt;code&gt;kubelet&lt;/code&gt; 和 &lt;code&gt;containerd&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
      <category term="K3s" scheme="https://www.hi-linux.com/tags/K3s/"/>
    
  </entry>
  
  <entry>
    <title>再见 Alfred，是时候拥抱下一代快捷启动神器 Raycast 了</title>
    <link href="https://www.hi-linux.com/posts/21309.html"/>
    <id>https://www.hi-linux.com/posts/21309.html</id>
    <published>2022-04-28T01:00:00.000Z</published>
    <updated>2022-04-28T04:21:29.798Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><blockquote><p><strong>一款速度极快、完全可扩展的启动器。</strong></p></blockquote><p>今天给大家介绍一款可以帮助我们提升生产力或者改善效率的工具，那就是 <code>Raycast</code>。它的功能和我们知道的 <code>Spotlight</code> 和 <code>Alfred</code> 很类似，但是其 <code>UI</code> 涉及完全可以能够驾驭 <code>macOS</code> 的全新的视觉风格(如下图所示)。</p><p><img src="https://img.hi-linux.com/staticfile/raycast-terminal-tool-2022-04-28-WkhBCv.jpg" alt="Raycast 下一代快捷启动器"></p><h2><span id="1-工具介绍">1. 工具介绍</span></h2><blockquote><p><strong>你的新一代快捷启动器</strong></p></blockquote><p><code>Raycast</code> 是直到 <code>10</code> 月 <code>29</code> 日才放出这第一个公测版本，其开发者介绍 <code>Raycast</code> 正是受命令行的启发，作为软件工程师，他们注意到自己真正写代码的时间越来越少，反而需要更多的时间来管理软件开发，例如跟踪 <code>bug</code> 反馈、管理 <code>sprint</code>、发布新版本等等，这些都需要借助网页端或者其它的不同工具来完成。</p><p>于是，<code>Raycast</code> 正是为了解决他们的困扰而创建，尽可能将常用的管理开发、内部会议、任务规划等内容集成在一起，腾出更多地时间放在编写代码上。简单过一下 <code>Raycast</code> 目前所能实现的核心功能：</p><ul><li>在 <code>macOS</code> 上启动程序或者搜索文件，相当于聚焦；</li><li>在 <code>GitHub</code>、<code>Jira</code> 中创建、搜索和关闭 <code>issues</code>；</li><li>批准、合并和关闭 <code>GitHub</code> 的拉取请求；</li><li>调用 <code>Zoom</code> 管理日常会议；</li><li>支持快捷设置日程、待办事项以及其它诸多系统设置；</li><li>支持脚本扩展；</li><li>……</li></ul><p><img src="https://img.hi-linux.com/staticfile/raycast-terminal-tool-01-20220428092254655-2022-04-28-iV3UcV.png" alt="Raycast 下一代快捷启动器"></p><a id="more"></a><h2><span id="2-配置说明">2. 配置说明</span></h2><blockquote><p><strong>可完全取代聚焦</strong></p></blockquote><p>既然 <code>Raycast</code> 包含了 <code>Spotlight</code> 的功能，意味着你完全可以用 <code>Raycast</code> 替换掉它。默认快捷键为 <strong>「option + 空格」</strong> ，可以启动该工具，我们可以通过在输入框中输入信息来得到我们想要的东西或启动对应的应用服务。</p><ul><li>显示一个搜索框<ul><li>推荐项目(根据使用频率)</li><li>手动收藏项目以及快捷操作</li><li>下滑则能看到所有的项目</li></ul></li></ul><p><img src="https://img.hi-linux.com/staticfile/raycast-terminal-tool-02-2022-04-28-9jsJYr.png" alt="Raycast 下一代快捷启动器"></p><p>软件右下角有快捷菜单，除了打开应用/文件之外，还可以快速定位文件位置或者执行其它操作，而且这些所有的一系列操作都可以使用快捷键完成。文件搜索功能支持部分图片、文档预览，且能显示文件详细信息，这个比聚焦实用性更高。</p><h2><span id="3-高级特性">3. 高级特性</span></h2><blockquote><p><strong>支持管理日程、待办事项、剪贴板历史</strong></p></blockquote><p><code>Raycast</code> 对接了不少第三方应用的功能服务，也包括 <code>macOS</code> 系统自带的日程查看以及待办事项管理，这些所能实现的操作在设置中都可以直接看到，可手动选择关闭。不过同样是系统应用，通过 <code>Raycast</code> 可以查看、创建待办事项，而日历日程却只能查看，无法通过 <code>Raycast</code> 新建。</p><p><img src="https://img.hi-linux.com/staticfile/raycast-terminal-tool-03-20220428092302624-2022-04-28-TOPIrj.jpg" alt="Raycast 下一代快捷启动器"></p><p>剪贴板历史记录算是 <code>Raycast</code> 给我的一个小小惊喜，这是 <code>Raycast</code> 本身自带的小功能。而通过 <code>Raycast</code> 搜索「<code>Clipboard History</code>」，能够直接查看近期的剪贴板记录。</p><p><img src="https://img.hi-linux.com/staticfile/raycast-terminal-tool-04-20220428092310098-2022-04-28-kXgJZZ.jpg" alt="Raycast 下一代快捷启动器"></p><p>接入第三方服务高效协作，已经接入了 <code>GitHub</code>、<code>Jira</code>、<code>Zoom</code> 等服务，能够快速完成特定操作。用户需要通过 <code>OAuth</code> 协议登录指定服务，已完成自有账号内容的双向同步。</p><p><img src="https://img.hi-linux.com/staticfile/raycast-terminal-tool-05-2022-04-28-qMZDRk.jpg" alt="Raycast 下一代快捷启动器"></p><p>日常使用电脑，可能经常会需要调整一些系统设置项，例如音量调节、休眠、锁屏或者深色模式切换等等。不少用户会通过第三方工具来完成，例如  <code>One Switch</code>。<code>Raycast</code> 本身内置了一些系统功能调节，同时支持加载命令脚本，意味着能够将很多较为隐秘的系统设置融入 <code>Raycast</code> 实现快捷操作。</p><p><img src="https://img.hi-linux.com/staticfile/raycast-terminal-tool-06-2022-04-28-k0gQv8.jpg" alt="Raycast 下一代快捷启动器"></p><h2><span id="4-插件">4. 插件</span></h2><p>Raycast 官方有着相当丰富的插件生态，包括 <code>Vscode</code>、<code>Github</code>、<code>Google</code>、<code>Docker</code>、<code>Kubernetes</code> 等常用插件。</p><p><img src="https://img.hi-linux.com/staticfile/image-20220428093840589-2022-04-28-uLVFIV.png" alt="Raycast 下一代快捷启动器"></p><p><img src="https://img.hi-linux.com/staticfile/image-20220428093923976-2022-04-28-k9bfl4.png" alt="Raycast 下一代快捷启动器"></p><p>下面以 <a href="http://Ray.so" target="_blank" rel="noopener">Ray.so</a> 为例，来看看其强大的插件功能：</p><blockquote><p><strong>主要是介绍相关的一些配置集合我们应该怎么使用</strong></p></blockquote><ul><li>[1] 生成漂亮的分享代码 - <a href="http://Raycast+Ray.so" target="_blank" rel="noopener">Raycast+Ray.so</a><ul><li>安装：<code>Raycast</code> -&gt; <code>store</code> - <code>ray.so</code></li><li>使用：选中代码 -&gt; 打开搜索 <code>CI</code>(<code>Copy Image</code>) -&gt; 生成</li><li>定制：打开搜索 <code>Create Image from Code</code> 进行设置</li></ul></li></ul><p><code>Ray.so</code> 是一款提供代码图片分享的 <code>Web</code> 服务，可以将代码文本转化为美观的图片进行分享，类似的 Web 服务还有  <a href="https://carbon.now.sh/" target="_blank" rel="noopener">Carbon</a> 等。</p><p><img src="https://img.hi-linux.com/staticfile/raycast-terminal-tool-07-20220428092319067-2022-04-28-xISHXC.png" alt="Raycast 下一代快捷启动器"></p><p>更多好用的插件可到 Raycast Store 探索。</p><blockquote><p>Raycast Store 官方地址: <a href="https://www.raycast.com/store" target="_blank" rel="noopener">https://www.raycast.com/store</a></p></blockquote><p>如果官方 Raycast Store 提供的插件也不能满足你更多狂野的需求，你还可到开源社区获取更多的脚本插件。</p><p><img src="https://img.hi-linux.com/staticfile/dPV66e-2022-04-28-Ri7jvF.jpg" alt="Raycast 下一代快捷启动器"></p><p><img src="https://img.hi-linux.com/staticfile/add-directory-2022-04-28-2Wqr2O.png" alt="Raycast 下一代快捷启动器"></p><p>除此之外，你还可以按自身需求定制专用的脚本，支持的脚本语言有：<code>Bash</code>，<code>Python</code>，<code>Apple Script</code>，<code>Swift</code>，<code>Ruby</code>，<code>Node.js</code>。</p><p><img src="https://img.hi-linux.com/staticfile/Create-Script-Command-20220428114937278-2022-04-28-e1Y4Jy.png" alt="Raycast 下一代快捷启动器"></p><blockquote><p>Raycast Script 官方地址: <a href="https://github.com/raycast/script-commands" target="_blank" rel="noopener">https://github.com/raycast/script-commands</a></p></blockquote><h2><span id="5-参考链接">5. 参考链接</span></h2><ul><li><a href="https://sspai.com/post/63521" target="_blank" rel="noopener">Raycast 快捷启动器中的「潜力股」</a></li><li><a href="https://sspai.com/post/72627" target="_blank" rel="noopener">Raycast 分享让人眼前一亮的代码</a></li><li><a href="https://github.com/raycast/script-commands" target="_blank" rel="noopener">Raycast 脚本大集合 - 可参考和使用</a></li><li><a href="https://www.notion.so/Raycast-Manual-d5c85a7694dc4e4088b8b93557ea6d2d" target="_blank" rel="noopener">Raycast 官方使用手册 - 新手指南</a></li></ul><blockquote><p>本文转载自：「 Escape 的博客 」，原文：<a href="https://url.hi-linux.com/8wXQL" target="_blank" rel="noopener">https://url.hi-linux.com/8wXQL</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;一款速度极快、完全可扩展的启动器。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;今天给大家介绍一款可以帮助我们提升生产力或者改善效率的工具，那就是 &lt;code&gt;Raycast&lt;/code&gt;。它的功能和我们知道的 &lt;code&gt;Spotlight&lt;/code&gt; 和 &lt;code&gt;Alfred&lt;/code&gt; 很类似，但是其 &lt;code&gt;UI&lt;/code&gt; 涉及完全可以能够驾驭 &lt;code&gt;macOS&lt;/code&gt; 的全新的视觉风格(如下图所示)。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.hi-linux.com/staticfile/raycast-terminal-tool-2022-04-28-WkhBCv.jpg&quot; alt=&quot;Raycast 下一代快捷启动器&quot;&gt;&lt;/p&gt;
&lt;h2 id=&quot;1-工具介绍&quot;&gt;1. 工具介绍&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;你的新一代快捷启动器&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;code&gt;Raycast&lt;/code&gt; 是直到 &lt;code&gt;10&lt;/code&gt; 月 &lt;code&gt;29&lt;/code&gt; 日才放出这第一个公测版本，其开发者介绍 &lt;code&gt;Raycast&lt;/code&gt; 正是受命令行的启发，作为软件工程师，他们注意到自己真正写代码的时间越来越少，反而需要更多的时间来管理软件开发，例如跟踪 &lt;code&gt;bug&lt;/code&gt; 反馈、管理 &lt;code&gt;sprint&lt;/code&gt;、发布新版本等等，这些都需要借助网页端或者其它的不同工具来完成。&lt;/p&gt;
&lt;p&gt;于是，&lt;code&gt;Raycast&lt;/code&gt; 正是为了解决他们的困扰而创建，尽可能将常用的管理开发、内部会议、任务规划等内容集成在一起，腾出更多地时间放在编写代码上。简单过一下 &lt;code&gt;Raycast&lt;/code&gt; 目前所能实现的核心功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 &lt;code&gt;macOS&lt;/code&gt; 上启动程序或者搜索文件，相当于聚焦；&lt;/li&gt;
&lt;li&gt;在 &lt;code&gt;GitHub&lt;/code&gt;、&lt;code&gt;Jira&lt;/code&gt; 中创建、搜索和关闭 &lt;code&gt;issues&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;批准、合并和关闭 &lt;code&gt;GitHub&lt;/code&gt; 的拉取请求；&lt;/li&gt;
&lt;li&gt;调用 &lt;code&gt;Zoom&lt;/code&gt; 管理日常会议；&lt;/li&gt;
&lt;li&gt;支持快捷设置日程、待办事项以及其它诸多系统设置；&lt;/li&gt;
&lt;li&gt;支持脚本扩展；&lt;/li&gt;
&lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://img.hi-linux.com/staticfile/raycast-terminal-tool-01-20220428092254655-2022-04-28-iV3UcV.png&quot; alt=&quot;Raycast 下一代快捷启动器&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Alfred" scheme="https://www.hi-linux.com/tags/Alfred/"/>
    
      <category term="Raycast" scheme="https://www.hi-linux.com/tags/Raycast/"/>
    
  </entry>
  
  <entry>
    <title>如何快速的自建 DoH ( DNS over HTTPS) 服务</title>
    <link href="https://www.hi-linux.com/posts/34709.html"/>
    <id>https://www.hi-linux.com/posts/34709.html</id>
    <published>2022-04-19T01:00:00.000Z</published>
    <updated>2022-04-19T04:29:49.665Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="1-前言">1. 前言</span></h2><p>DoH（DNS over HTTPS），顾名思义，使用HTTPS协议执行DNS查询，除了最常用的UDP外，还有DoT（DNS over TLS），DNS over HTTP（服务提供商自定义）等方案，对比如下：</p><table><thead><tr><th style="text-align:left">协议</th><th style="text-align:left">标准</th><th style="text-align:left">描述</th></tr></thead><tbody><tr><td style="text-align:left">DNS over HTTPS</td><td style="text-align:left"><a href="https://datatracker.ietf.org/doc/html/rfc8484" target="_blank" rel="noopener">RFC8484</a></td><td style="text-align:left">使用TLS加密的HTTP/2执行DNS查询</td></tr><tr><td style="text-align:left">DNS over TLS</td><td style="text-align:left"><a href="https://datatracker.ietf.org/doc/html/rfc7858" target="_blank" rel="noopener">RFC7858</a></td><td style="text-align:left">使用TLS加密的TCP执行DNS查询</td></tr><tr><td style="text-align:left">DNS over HTTP</td><td style="text-align:left">服务提供商自定义</td><td style="text-align:left">使用自定义加密的HTTP/1.1执行DNS查询</td></tr></tbody></table><p>移动端的DNS优化已经有很多实践，最常见的是DNS over HTTP，通过加密的HTTP请求规避运营商对DNS的UDP包劫持，从而优化App访问服务器的延迟。但这个方案并没有形成统一的标准，通常需要内嵌DNS服务提供商的SDK，通过访问固定的BGP或任播IP获取DNS响应。</p><a id="more"></a><p>大概是意识到DNS在移动互联网中的扮演越来越重要的角色，在DoT和DoH的规范相继推出后，许多DNS服务提供商都跟进了部署，国内的阿里云、DNSPod，国外的谷歌、Cloudflare等目前已经推出了免费的DoT和DoH服务。</p><p>客户端方面，常用的Chrome、FireFox已经支持了自定义DoH服务器，macOS、iOS也可通过配置文件设置系统范围的默认DoH服务器。</p><p>笔者也正好有一个自定义DNS的需求：</p><ol><li>需要针对一些域名的DNS查询仅返回IPv4记录</li><li>使用的某某路由器系统的自定义DNS服务仅支持设置UDP和DoH</li><li>UDP模式默认使用53端口，不可修改，UDP包容易遭受干扰</li><li>DoH可自定义域名、端口且使用HTTP2作为传输协议，稳定性更强</li></ol><p>综上，只有自建DoH服务了，于是就有了下面的折腾，最后测试时发现这个傻瓜路由器系统只支持一些特定的DoH服务商如阿里云DNS、DNSPod等，不支持自建的DoH服务。</p><h2><span id="2-部署方案">2. 部署方案</span></h2><p>DoH本质上就是一个HTTP请求，只是目前协议定义要求启用TLS与HTTP/2。最初没有跑通coredns的DoH时，使用了nginx作为前端转发DoH请求到doh-server，然后doh-server使用本地的coredns服务作为上游。</p><p>最近再仔细研究了下文档，发现coredns已经支持了DoH服务，可直接对外暴露服务，或者通过nginx转发来复用已经部署好的web服务。</p><h3><span id="21-nginx-doh-server-coredns">2.1 nginx + doh-server + coredns</span></h3><p><a href="https://github.com/m13253/dns-over-https" target="_blank" rel="noopener">https://github.com/m13253/dns-over-https</a> 是一个提供 DNS over HTTP 的服务，需要一个web前端和一个DNS后端，可用的docker镜像地址为：<a href="https://hub.docker.com/r/satishweb/doh-server" target="_blank" rel="noopener">satishweb/doh-server</a>，使用doh-server时，DNS请求流转如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HTTP Service -&gt; doh-server -&gt; DNS Server</span><br></pre></td></tr></table></figure><p>RFC8484中指定使用/dns-query路径作为默认查询路径，因此只需要将该路径前缀的请求转发到doh-server即可，如下：</p><p><strong>nginx配置（已配置好TLS与HTTP2）</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 443 ssl http2 fastopen&#x3D;256 reuseport;</span><br><span class="line">    listen [::]:443 ssl http2 fastopen&#x3D;256 reuseport;</span><br><span class="line">    server_name doh.wbuntu.com</span><br><span class="line">    ...</span><br><span class="line">    location &#x2F;dns-query &#123;</span><br><span class="line">    proxy_redirect off;</span><br><span class="line">    proxy_http_version 1.1;</span><br><span class="line">    proxy_set_header Host $http_host;</span><br><span class="line">    # show real IP</span><br><span class="line">    proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">    proxy_pass http:&#x2F;&#x2F;127.0.0.1:8053;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>doh-server</strong></p><p>使用hostNetwork模式启动服务，监听8053端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart unless-stopped --network host --name doh-server \</span><br><span class="line">  -e UPSTREAM_DNS_SERVER&#x3D;&quot;udp:127.0.0.1:53&quot; \</span><br><span class="line">  -e DOH_HTTP_PREFIX&#x3D;&quot;&#x2F;dns-query&quot; \</span><br><span class="line">  -e DOH_SERVER_LISTEN&#x3D;&quot;127.0.0.1:8053&quot; \</span><br><span class="line">  -e DOH_SERVER_TIMEOUT&#x3D;&quot;10&quot; \</span><br><span class="line">  -e DOH_SERVER_TRIES&#x3D;&quot;3&quot; \</span><br><span class="line">  -e DOH_SERVER_VERBOSE&#x3D;&quot;true&quot; \</span><br><span class="line">  satishweb&#x2F;doh-server</span><br></pre></td></tr></table></figure><p><strong>coredns</strong></p><p>coredns配置文件如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ tree &#x2F;etc&#x2F;coredns&#x2F;</span><br><span class="line">&#x2F;etc&#x2F;coredns&#x2F;</span><br><span class="line">└── Corefile</span><br><span class="line"></span><br><span class="line">0 directories, 1 files</span><br><span class="line">➜  cat &#x2F;etc&#x2F;coredns&#x2F;Corefile</span><br><span class="line">.:53 &#123;</span><br><span class="line">    bind 127.0.0.1</span><br><span class="line">    forward . 1.1.1.1 1.0.0.1</span><br><span class="line">    log</span><br><span class="line">    errors</span><br><span class="line">    cache</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用hostNetwork模式启动服务，监听53端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart unless-stopped --network host --name coredns \</span><br><span class="line">  -v &#x2F;etc&#x2F;coredns:&#x2F;etc&#x2F;coredns \</span><br><span class="line">  coredns&#x2F;coredns \</span><br><span class="line">  -conf &#x2F;etc&#x2F;coredns&#x2F;Corefile</span><br></pre></td></tr></table></figure><p>服务启动后，我们可以得到一个自定义的DoH服务：<a href="https://doh.wbuntu.com/dns-query" target="_blank" rel="noopener">https://doh.wbuntu.com/dns-query</a></p><h3><span id="22-coredns">2.2 coredns</span></h3><p>目前coredns支持作为DoH服务端，不支持连接上游DoH服务器，上游服务器可使用UDP和DoT。</p><p>直接对外暴露服务需要使用有效的TLS证书，coredns配置文件及证书位置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ tree &#x2F;etc&#x2F;coredns&#x2F;</span><br><span class="line">&#x2F;etc&#x2F;coredns&#x2F;</span><br><span class="line">├── Corefile</span><br><span class="line">├── tls.crt</span><br><span class="line">└── tls.key</span><br><span class="line"></span><br><span class="line">0 directories, 3 files</span><br><span class="line">➜  cat &#x2F;etc&#x2F;coredns&#x2F;Corefile</span><br><span class="line">https:&#x2F;&#x2F;.:443 &#123;</span><br><span class="line">    tls &#x2F;etc&#x2F;coredns&#x2F;tls.crt &#x2F;etc&#x2F;coredns&#x2F;tls.key</span><br><span class="line">    bind 0.0.0.0</span><br><span class="line">    forward . 1.1.1.1 1.0.0.1</span><br><span class="line">    log</span><br><span class="line">    errors</span><br><span class="line">    cache</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用hostNetwork模式启动服务，监听443端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart unless-stopped --network host --name coredns \</span><br><span class="line">  -v &#x2F;etc&#x2F;coredns:&#x2F;etc&#x2F;coredns \</span><br><span class="line">  coredns&#x2F;coredns \</span><br><span class="line">  -conf &#x2F;etc&#x2F;coredns&#x2F;Corefile</span><br></pre></td></tr></table></figure><p>服务启动后，我们可以得到一个自定义的DoH服务：<a href="https://doh.wbuntu.com/dns-query" target="_blank" rel="noopener">https://doh.wbuntu.com/dns-query</a></p><h3><span id="23-nginx-coredns">2.3 nginx + coredns</span></h3><p>直接暴露coredns服务到公网需要占用端口，coredns在未配置TLS证书时，可使用nginx作为前端来复用web服务，如下：</p><p><strong>nginx配置（已配置好TLS与HTTP2）</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 443 ssl http2 fastopen&#x3D;256 reuseport;</span><br><span class="line">    listen [::]:443 ssl http2 fastopen&#x3D;256 reuseport;</span><br><span class="line">    server_name doh.wbuntu.com</span><br><span class="line">    ...</span><br><span class="line">    location &#x2F;dns-query &#123;</span><br><span class="line">    proxy_redirect off;</span><br><span class="line">    proxy_http_version 1.1;</span><br><span class="line">    proxy_set_header Host $http_host;</span><br><span class="line">    # show real IP</span><br><span class="line">    proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">    proxy_pass http:&#x2F;&#x2F;127.0.0.1:8053;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>coredns</strong></p><p>coredns配置文件如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ tree &#x2F;etc&#x2F;coredns&#x2F;</span><br><span class="line">&#x2F;etc&#x2F;coredns&#x2F;</span><br><span class="line">└── Corefile</span><br><span class="line"></span><br><span class="line">0 directories, 1 files</span><br><span class="line">➜  cat &#x2F;etc&#x2F;coredns&#x2F;Corefile</span><br><span class="line">https:&#x2F;&#x2F;.:8053 &#123;</span><br><span class="line">    bind 127.0.0.1</span><br><span class="line">    forward . 1.1.1.1 1.0.0.1</span><br><span class="line">    log</span><br><span class="line">    errors</span><br><span class="line">    cache</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>使用hostNetwork模式启动服务，监听8053端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --restart unless-stopped --network host --name coredns \</span><br><span class="line">  -v &#x2F;etc&#x2F;coredns:&#x2F;etc&#x2F;coredns \</span><br><span class="line">  coredns&#x2F;coredns \</span><br><span class="line">  -conf &#x2F;etc&#x2F;coredns&#x2F;Corefile</span><br></pre></td></tr></table></figure><p>服务启动后，我们可以得到一个自定义的DoH服务：<a href="https://doh.wbuntu.com/dns-query" target="_blank" rel="noopener">https://doh.wbuntu.com/dns-query</a></p><h2><span id="3-测试">3. 测试</span></h2><p>使用谷歌浏览器配置DoH服务：Settings -&gt; Secutiry and Privacy -&gt; Secutiry -&gt; Advanced -&gt; Use secure DNS</p><p><img src="https://img.hi-linux.com/staticfile/chrome-doh-settings-20220412111328635-2022-04-12-e3XoP6.png" alt></p><p>使用Go代码测试：<a href="https://github.com/mikumaycry/example/blob/main/2021/doh/main.go" target="_blank" rel="noopener">github.com/mikumaycry/example/blob/main/2021/doh/main.go</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">        &quot;encoding&#x2F;base64&quot;</span><br><span class="line">        &quot;fmt&quot;</span><br><span class="line">        &quot;github.com&#x2F;miekg&#x2F;dns&quot;</span><br><span class="line">        &quot;io&#x2F;ioutil&quot;</span><br><span class="line">        &quot;net&#x2F;http&quot;</span><br><span class="line">        &quot;os&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">       query :&#x3D; dns.Msg&#123;&#125;</span><br><span class="line">       query.SetQuestion(&quot;www.google.com.&quot;, dns.TypeA)</span><br><span class="line">       msg, _ :&#x3D; query.Pack()</span><br><span class="line">       b64 :&#x3D; base64.RawURLEncoding.EncodeToString(msg)</span><br><span class="line">       resp, err :&#x3D; http.Get(&quot;https:&#x2F;&#x2F;doh.wbuntu.com&#x2F;dns-query?dns&#x3D;&quot; + b64)</span><br><span class="line">       if err !&#x3D; nil &#123;</span><br><span class="line">            fmt.Printf(&quot;Send query error, err:%v\n&quot;, err)</span><br><span class="line">            os.Exit(1)</span><br><span class="line">       &#125;</span><br><span class="line">       defer resp.Body.Close()</span><br><span class="line">       bodyBytes, _ :&#x3D; ioutil.ReadAll(resp.Body)</span><br><span class="line">       response :&#x3D; dns.Msg&#123;&#125;</span><br><span class="line">       response.Unpack(bodyBytes)</span><br><span class="line">       fmt.Printf(&quot;Dns answer is :%v\n&quot;, response.String())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>本文转载自：「 Wbuntu 的博客 」，原文：<a href="https://wbuntu.com/deploy-a-doh-service/" target="_blank" rel="noopener">https://wbuntu.com/deploy-a-doh-service/</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-前言&quot;&gt;1. 前言&lt;/h2&gt;
&lt;p&gt;DoH（DNS over HTTPS），顾名思义，使用HTTPS协议执行DNS查询，除了最常用的UDP外，还有DoT（DNS over TLS），DNS over HTTP（服务提供商自定义）等方案，对比如下：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;协议&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;标准&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;描述&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;DNS over HTTPS&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc8484&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RFC8484&lt;/a&gt;&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;使用TLS加密的HTTP/2执行DNS查询&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;DNS over TLS&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;&lt;a href=&quot;https://datatracker.ietf.org/doc/html/rfc7858&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;RFC7858&lt;/a&gt;&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;使用TLS加密的TCP执行DNS查询&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;DNS over HTTP&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;服务提供商自定义&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;使用自定义加密的HTTP/1.1执行DNS查询&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;移动端的DNS优化已经有很多实践，最常见的是DNS over HTTP，通过加密的HTTP请求规避运营商对DNS的UDP包劫持，从而优化App访问服务器的延迟。但这个方案并没有形成统一的标准，通常需要内嵌DNS服务提供商的SDK，通过访问固定的BGP或任播IP获取DNS响应。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="DNS" scheme="https://www.hi-linux.com/tags/DNS/"/>
    
  </entry>
  
  <entry>
    <title>24 个 常见的 Docker 疑难杂症处理技巧</title>
    <link href="https://www.hi-linux.com/posts/42601.html"/>
    <id>https://www.hi-linux.com/posts/42601.html</id>
    <published>2022-04-06T01:00:00.000Z</published>
    <updated>2022-04-06T02:24:56.310Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><blockquote><p><strong>这里主要是为了记录在使用 Docker 的时候遇到的问题及其处理解决方法。</strong></p></blockquote><h2><span id="docker-迁移存储目录">Docker 迁移存储目录</span></h2><blockquote><p><strong>默认情况系统会将 Docker 容器存放在 /var/lib/docker 目录下</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 今天通过监控系统，发现公司其中一台服务器的磁盘快慢，随即上去看了下，发现 <code>/var/lib/docker</code> 这个目录特别大。由上述原因，我们都知道，在 <code>/var/lib/docker</code> 中存储的都是相关于容器的存储，所以也不能随便的将其删除掉。</li><li>那就准备迁移 <code>docker</code> 的存储目录吧，或者对 <code>/var</code> 设备进行扩容来达到相同的目的。更多关于 <code>dockerd</code> 的详细参数，请点击查看 <a href="https://docs.docker.com/engine/reference/commandline/dockerd/" target="_blank" rel="noopener"><strong>官方文档</strong></a> 地址。</li><li>但是需要注意的一点就是，尽量不要用软链， 因为一些 <code>docker</code> 容器编排系统不支持这样做，比如我们所熟知的 <code>k8s</code> 就在内。</li></ul><a id="more"></a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 发现容器启动不了了</span></span><br><span class="line">ERROR：cannot  create temporary directory!</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看系统存储情况</span></span><br><span class="line">$ du -h --max-depth=1</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法 1] 添加软链接</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.停止docker服务</span></span><br><span class="line">$ sudo systemctl stop docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.开始迁移目录</span></span><br><span class="line">$ sudo mv /var/lib/docker /data/</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.添加软链接</span></span><br><span class="line">$ sudo ln -s /data/docker /var/lib/docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.启动docker服务</span></span><br><span class="line">$ sudo systemctl start docker</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法 2] 改动 docker 配置文件</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [方式一] 改动docker启动配置文件</span></span><br><span class="line">$ sudo vim /lib/systemd/system/docker.service</span><br><span class="line">ExecStart=/usr/bin/dockerd --graph=/data/docker/</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># [方式二] 改动docker启动配置文件</span></span><br><span class="line">$ sudo vim /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"live-restore"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"graph"</span>: [ <span class="string">"/data/docker/"</span> ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>[操作注意事项]</strong> 在迁移 <code>docker</code> 目录的时候注意使用的命令，要么使用 <code>mv</code> 命令直接移动，要么使用 <code>cp</code> 命令复制文件，但是需要注意同时复制文件权限和对应属性，不然在使用的时候可能会存在权限问题。如果容器中，也是使用 <code>root</code> 用户，则不会存在该问题，但是也是需要按照正确的操作来迁移目录。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用mv命令</span></span><br><span class="line">$ sudo mv /var/lib/docker /data/docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用cp命令</span></span><br><span class="line">$ sudo cp -arv /data/docker /data2/docker</span><br></pre></td></tr></table></figure><ul><li>下图中，就是因为启动的容器使用的是普通用户运行进程的，且在运行当中需要使用 <code>/tmp</code> 目录，结果提示没有权限。在我们导入容器镜像的时候，其实是会将容器启动时需要的各个目录的权限和属性都赋予了。如果我们直接是 <code>cp</code> 命令单纯复制文件内容的话，就会出现属性不一致的情况，同时还会有一定的安全问题。</li></ul><p><img src="https://img.hi-linux.com/staticfile/docker-have-some-trouble-3-20220329161818563-2022-03-29-j0jRk4.png" alt="Docker迁移存储目录"></p><h2><span id="docker-设备空间不足">Docker 设备空间不足</span></h2><blockquote><p><a href="https://stackoverflow.com/questions/50140939/increase-docker-container-size-from-default-10gb-on-rhel7/52971594#52971594" target="_blank" rel="noopener"><strong>Increase Docker container size from default 10GB on rhel7.</strong></a></p></blockquote><ul><li><strong>[问题起因一]</strong> 容器在导入或者启动的时候，如果提示磁盘空间不足的，那么多半是真的因为物理磁盘空间真的有问题导致的。如下所示，我们可以看到 <code>/</code> 分区确实满了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看物理磁盘空间</span></span><br><span class="line">$ df -Th</span><br><span class="line">Filesystem    Size    Used    Avail    Use%    Mounted on</span><br><span class="line">/dev/vda1      40G     40G       0G    100%    /</span><br><span class="line">tmpfs         7.8G       0     7.8G      0%    /dev/shm</span><br><span class="line">/dev/vdb1     493G    289G     179G     62%    /mnt</span><br></pre></td></tr></table></figure><ul><li>如果发现真的是物理磁盘空间满了的话，就需要查看到底是什么占据了如此大的空间，导致因为容器没有空间无法启动。其中，<code>docker</code> 自带的命令就是一个很好的能够帮助我们发现问题的工具。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看基本信息</span></span><br><span class="line"><span class="comment"># 硬件驱动使用的是devicemapper，空间池为docker-252</span></span><br><span class="line"><span class="comment"># 磁盘可用容量仅剩16.78MB，可用供我们使用</span></span><br><span class="line">$ docker info</span><br><span class="line">Containers: 1</span><br><span class="line">Images: 28</span><br><span class="line">Storage Driver: devicemapper</span><br><span class="line"> Pool Name: docker-252:1-787932-pool</span><br><span class="line"> Pool Blocksize: 65.54 kB</span><br><span class="line"> Backing Filesystem: extfs</span><br><span class="line"> Data file: /dev/loop0</span><br><span class="line"> Metadata file: /dev/loop1</span><br><span class="line"> Data Space Used: 1.225 GB</span><br><span class="line"> Data Space Total: 107.4 GB</span><br><span class="line"> Data Space Available: 16.78 MB</span><br><span class="line"> Metadata Space Used: 2.073 MB</span><br><span class="line"> Metadata Space Total: 2.147 GB</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 通过查看信息，我们知道正是因为 <code>docker</code> 可用的磁盘空间不足，所以导致启动的时候没有足够的空间进行加载启动镜像。解决的方法也很简单，第一就是清理无效数据文件释放磁盘空间(<strong>清除日志</strong>)，第二就是修改 <code>docker</code> 数据的存放路径(<strong>大分区</strong>)。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示哪些容器目录具有最大的日志文件</span></span><br><span class="line">$ du -d1 -h /var/lib/docker/containers | sort -h</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除您选择的容器日志文件的内容</span></span><br><span class="line">$ cat /dev/null &gt; /var/lib/docker/containers/container_id/container_log_name</span><br></pre></td></tr></table></figure><ul><li><strong>[问题起因二]</strong> 显然我遇到的不是上一种情况，而是在启动容器的时候，容器启动之后不久就显示是 <code>unhealthy</code> 的状态，通过如下日志发现，原来是复制配置文件启动的时候，提示磁盘空间不足。</li><li>后面发现是因为 <code>CentOS7</code> 的系统使用的 <code>docker</code> 容器默认的创建大小就是 <code>10G</code> 而已，然而我们使用的容器却超过了这个限制，导致无法启动时提示空间不足。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2019-08-16 11:11:15,816 INFO spawned: <span class="string">'app-demo'</span> with pid 835</span><br><span class="line">2019-08-16 11:11:16,268 INFO exited: app (<span class="built_in">exit</span> status 1; not expected)</span><br><span class="line">2019-08-16 11:11:17,270 INFO gave up: app entered FATAL state, too many start retries too quickly</span><br><span class="line">cp: cannot create regular file <span class="string">'/etc/supervisor/conf.d/grpc-app-demo.conf'</span>: No space left on device</span><br><span class="line">cp: cannot create regular file <span class="string">'/etc/supervisor/conf.d/grpc-app-demo.conf'</span>: No space left on device</span><br><span class="line">cp: cannot create regular file <span class="string">'/etc/supervisor/conf.d/grpc-app-demo.conf'</span>: No space left on device</span><br><span class="line">cp: cannot create regular file <span class="string">'/etc/supervisor/conf.d/grpc-app-demo.conf'</span>: No space left on device</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法 1] 改动 docker 启动配置文件</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /etc/docker/daemon.json</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"live-restore"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"storage-opt"</span>: [ <span class="string">"dm.basesize=20G"</span> ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法 2] 改动 systemctl 的 docker 启动文件</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.stop the docker service</span></span><br><span class="line">$ sudo systemctl stop docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.rm exised container</span></span><br><span class="line">$ sudo rm -rf /var/lib/docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.edit your docker service file</span></span><br><span class="line">$ sudo vim /usr/lib/systemd/system/docker.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.find the execution line</span></span><br><span class="line">ExecStart=/usr/bin/dockerd</span><br><span class="line">and change it to:</span><br><span class="line">ExecStart=/usr/bin/dockerd --storage-opt dm.basesize=20G</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.start docker service again</span></span><br><span class="line">$ sudo systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.reload daemon</span></span><br><span class="line">$ sudo systemctl daemon-reload</span><br></pre></td></tr></table></figure><ul><li><strong>[问题起因三]</strong> 还有一种情况也会让容器无法启动，并提示磁盘空间不足，但是使用命令查看发现并不是因为物理磁盘真的不足导致的。而是，因为对于分区的 <code>inode</code> 节点数满了导致的。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 报错信息</span></span><br><span class="line">No space left on device</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 因为 <code>ext3</code> 文件系统使用 <code>inode table</code> 存储 <code>inode</code> 信息，而 <code>xfs</code> 文件系统使用 <code>B+ tree</code> 来进行存储。考虑到性能问题，默认情况下这个 <code>B+ tree</code> 只会使用前 <code>1TB</code> 空间，当这 <code>1TB</code> 空间被写满后，就会导致无法写入 <code>inode</code> 信息，报磁盘空间不足的错误。我们可以在 <code>mount</code> 时，指定 <code>inode64</code> 即可将这个 <code>B+ tree</code> 使用的空间扩展到整个文件系统。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看系统的inode节点使用情况</span></span><br><span class="line">$ sudo df -i</span><br><span class="line"></span><br><span class="line"><span class="comment"># 尝试重新挂载</span></span><br><span class="line">$ sudo mount -o remount -o noatime,nodiratime,inode64,nobarrier /dev/vda1</span><br></pre></td></tr></table></figure><ul><li><strong>[补充知识]</strong> 文件储存在硬盘上，硬盘的最小存储单位叫做 <strong>扇区</strong>(<code>Sector</code>)。每个扇区储存 <code>512</code> 字节(相当于<code>0.5KB</code>)。操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个<strong>块</strong>(<code>block</code>)。这种由多个扇区组成的<strong>块</strong>，是文件存取的最小单位。<strong>块</strong>的大小，最常见的是<code>4KB</code>，即连续八个 <code>sector</code> 组成一个 <code>block</code> 块。文件数据都储存在<strong>块</strong>中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做<strong>索引节点</strong>(<code>inode</code>)。每一个文件都有对应的 <code>inode</code>，里面包含了除了文件名以外的所有文件信息。</li><li><code>inode</code> 也会消耗硬盘空间，所以硬盘格式化的时候，操作系统自动将硬盘分成两个区域。一个是数据区，存放文件数据；另一个是 <code>inode</code> 区(<code>inode table</code>)，存放 <code>inode</code> 所包含的信息。每个 <code>inode</code> 节点的大小，一般是 <code>128</code> 字节或 <code>256</code> 字节。<code>inode</code> 节点的总数，在格式化时就给定，一般是每<code>1KB</code>或每<code>2KB</code>就设置一个 <code>inode</code> 节点。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个节点信息的内容</span></span><br><span class="line">$ <span class="built_in">stat</span> check_port_live.sh</span><br><span class="line">  File: check_port_live.sh</span><br><span class="line">  Size: 225           Blocks: 8          IO Block: 4096   regular file</span><br><span class="line">Device: 822h/2082d    Inode: 99621663    Links: 1</span><br><span class="line">Access: (0755/-rwxr-xr-x)  Uid: ( 1006/  escape)   Gid: ( 1006/  escape)</span><br><span class="line">Access: 2019-07-29 14:59:59.498076903 +0800</span><br><span class="line">Modify: 2019-07-29 14:59:59.498076903 +0800</span><br><span class="line">Change: 2019-07-29 23:20:27.834866649 +0800</span><br><span class="line"> Birth: -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 磁盘的inode使用情况</span></span><br><span class="line">$ df -i</span><br><span class="line">Filesystem                 Inodes   IUsed     IFree IUse% Mounted on</span><br><span class="line">udev                     16478355     801  16477554    1% /dev</span><br><span class="line">tmpfs                    16487639    2521  16485118    1% /run</span><br><span class="line">/dev/sdc2               244162560 4788436 239374124    2% /</span><br><span class="line">tmpfs                    16487639       5  16487634    1% /dev/shm</span><br></pre></td></tr></table></figure><h2><span id="docker-缺共享链接库">Docker 缺共享链接库</span></h2><blockquote><p><strong>Docker 命令需要对/tmp 目录下面有访问权限</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 给系统安装完 <code>compose</code> 之后，查看版本的时候，提示缺少一个名为 <code>libz.so.1</code> 的共享链接库。第一反应就是，是不是系统少安装那个软件包导致的。随即，搜索了一下，将相关的依赖包都给安装了，却还是提示同样的问题。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 提示错误信息</span></span><br><span class="line">$ docker-compose --version</span><br><span class="line">error <span class="keyword">while</span> loading shared libraries: libz.so.1: failed to map segment from shared object: Operation not permitted</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 后来发现，是因为系统中 <code>docker</code> 没有对 <code>/tmp</code> 目录的访问权限导致，需要重新将其挂载一次，就可以解决了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 重新挂载</span></span><br><span class="line">$ sudo mount /tmp -o remount,<span class="built_in">exec</span></span><br></pre></td></tr></table></figure><h2><span id="docker-容器文件损坏">Docker 容器文件损坏</span></h2><blockquote><p><strong>对 dockerd 的配置有可能会影响到系统稳定</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 容器文件损坏，经常会导致容器无法操作。正常的 <code>docker</code> 命令已经无法操控这台容器了，无法关闭、重启、删除。正巧，前天就需要这个的问题，主要的原因是因为重新对 <code>docker</code> 的默认容器进行了重新的分配限制导致的。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 操作容器遇到类似的错误</span></span><br><span class="line">b<span class="string">'devicemapper: Error running deviceCreate (CreateSnapDeviceRaw) dm_task_run failed'</span></span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 可以通过以下操作将容器删除/重建。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1.关闭docker</span></span><br><span class="line">$ sudo systemctl stop docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.删除容器文件</span></span><br><span class="line">$ sudo rm -rf /var/lib/docker/containers</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.重新整理容器元数据</span></span><br><span class="line">$ sudo thin_check /var/lib/docker/devicemapper/devicemapper/metadata</span><br><span class="line">$ sudo thin_check --clear-needs-check-flag /var/lib/docker/devicemapper/devicemapper/metadata</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.重启docker</span></span><br><span class="line">$ sudo systemctl start docker</span><br></pre></td></tr></table></figure><h2><span id="docker-容器优雅重启">Docker 容器优雅重启</span></h2><blockquote><p><strong>不停止服务器上面运行的容器，重启 dockerd 服务是多么好的一件事</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 默认情况下，当 <code>Docker</code> 守护程序终止时，它会关闭正在运行的容器。从 <code>Docker-ce 1.12</code> 开始，可以在配置文件中添加 <code>live-restore</code> 参数，以便在守护程序变得不可用时容器保持运行。需要注意的是 <code>Windows</code> 平台暂时还是不支持该参数的配置。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Keep containers alive during daemon downtime</span></span><br><span class="line">$ sudo vim /etc/docker/daemon.yaml</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"live-restore"</span>: <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在守护进程停机期间保持容器存活</span></span><br><span class="line">$ sudo dockerd --live-restore</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只能使用reload重载</span></span><br><span class="line"><span class="comment"># 相当于发送SIGHUP信号量给dockerd守护进程</span></span><br><span class="line">$ sudo systemctl reload docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 但是对应网络的设置需要restart才能生效</span></span><br><span class="line">$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 可以通过以下操作将容器删除/重建。</li></ul><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># /etc/docker/daemon.yaml</span><br><span class="line">&#123;</span><br><span class="line">    "registry-mirrors": ["https://vec0xydj.mirror.aliyuncs.com"],  # 配置获取官方镜像的仓库地址</span><br><span class="line">    "experimental": true,  # 启用实验功能</span><br><span class="line">    "default-runtime": "nvidia",  # 容器的默认OCI运行时(默认为runc)</span><br><span class="line">    "live-restore": true,  # 重启dockerd服务的时候容易不终止</span><br><span class="line">    "runtimes": &#123;  # 配置容器运行时</span><br><span class="line">        "nvidia": &#123;</span><br><span class="line">            "path": "/usr/bin/nvidia-container-runtime",</span><br><span class="line">            "runtimeArgs": []</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    "default-address-pools": [  # 配置容器使用的子网地址池</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">"scope"</span>: <span class="string">"local"</span>,</span><br><span class="line">            <span class="attr">"base"</span>:<span class="string">"172.17.0.0/12"</span>,</span><br><span class="line">            <span class="attr">"size"</span>:<span class="number">24</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"default-address-pools"</span> : [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="string">"base"</span> : <span class="string">"172.240.0.0/16"</span>,</span><br><span class="line">      <span class="string">"size"</span> : 24</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="docker-容器无法删除">Docker 容器无法删除</span></h2><blockquote><p><strong>找不到对应容器进程是最吓人的</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 今天遇到 <code>docker</code> 容器无法停止/终止/删除，以为这个容器可能又出现了 <code>dockerd</code> 守护进程托管的情况，但是通过 <code>ps -ef &lt;container id&gt;</code> 无法查到对应的运行进程。哎，后来开始开始查 <code>supervisor</code> 以及 <code>Dockerfile</code> 中的进程，都没有。这种情况的可能原因是容器启动之后，主机因任何原因重新启动并且没有优雅地终止容器。剩下的文件现在阻止你重新生成旧名称的新容器，因为系统认为旧容器仍然存在。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除容器</span></span><br><span class="line">$ sudo docker rm -f f8e8c3..</span><br><span class="line">Error response from daemon: Conflict, cannot remove the default name of the container</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 找到 <code>/var/lib/docker/containers/</code> 下的对应容器的文件夹，将其删除，然后重启一下 <code>dockerd</code> 即可。我们会发现，之前无法删除的容器没有了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除容器文件</span></span><br><span class="line">$ sudo rm -rf /var/lib/docker/containers/f8e8c3...65720</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启服务</span></span><br><span class="line">$ sudo systemctl restart docker.service</span><br></pre></td></tr></table></figure><h2><span id="docker-容器中文异常">Docker 容器中文异常</span></h2><blockquote><p><strong>容器存在问题话，记得优先在官网查询</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 今天登陆之前部署的 <code>MySQL</code> 数据库查询，发现使用 <code>SQL</code> 语句无法查询中文字段，即使直接输入中文都没有办法显示。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看容器支持的字符集</span></span><br><span class="line">root@b18f56aa1e15:<span class="comment"># locale -a</span></span><br><span class="line">C</span><br><span class="line">C.UTF-8</span><br><span class="line">POSIX</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> <code>Docker</code> 部署的 <code>MySQL</code> 系统使用的是 <code>POSIX</code> 字符集。然而 <code>POSIX</code> 字符集是不支持中文的，而 <code>C.UTF-8</code> 是支持中文的只要把系统中的环境 <code>LANG</code> 改为 <code>&quot;C.UTF-8&quot;</code> 格式即可解决问题。同理，在 <code>K8S</code> 进入 <code>pod</code> 不能输入中文也可用此方法解决。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 临时解决</span></span><br><span class="line">docker <span class="built_in">exec</span> -it some-mysql env LANG=C.UTF-8 /bin/bash</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 永久解决</span></span><br><span class="line">docker run --name some-mysql \</span><br><span class="line">    -e MYSQL_ROOT_PASSWORD=my-secret-pw \</span><br><span class="line">    -d mysql:tag --character-set-server=utf8mb4 \</span><br><span class="line">    --collation-server=utf8mb4_unicode_ci</span><br></pre></td></tr></table></figure><h2><span id="docker-容器网络互通">Docker 容器网络互通</span></h2><blockquote><p><strong>了解 Docker 的四种网络模型</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 在本机部署 <code>Nginx</code> 容器想代理本机启动的 <code>Python</code> 后端服务程序，但是对代码服务如下的配置，结果访问的时候一直提示 <code>502</code> 错误。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动Nginx服务</span></span><br><span class="line">$ docker run -d -p 80:80 <span class="variable">$PWD</span>:/etc/nginx nginx</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    ...</span><br><span class="line">    location &#x2F;api &#123;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;localhost:8080</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 后面发现是因为 <code>nginx.conf</code> 配置文件中的 <code>localhost</code> 配置的有问题，由于 <code>Nginx</code> 是在容器中运行，所以 <code>localhost</code> 为容器中的 <code>localhost</code>，而非本机的 <code>localhost</code>，所以导致无法访问。</li><li>可以将 <code>nginx.conf</code> 中的 <code>localhost</code> 改为宿主机的 <code>IP</code> 地址，就可以解决 <code>502</code> 的错误。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询宿主机IP地址 =&gt; 172.17.0.1</span></span><br><span class="line">$ ip addr show docker0</span><br><span class="line">docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default</span><br><span class="line">    link/ether 02:42:d5:4c:f2:1e brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 172.17.0.1/16 scope global docker0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::42:d5ff:fe4c:f21e/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    ...</span><br><span class="line">    location &#x2F;api &#123;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;172.17.0.1:8080</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>当容器使用 <code>host</code> 网络时，容器与宿主共用网络，这样就能在容器中访问宿主机网络，那么容器的 <code>localhost</code> 就是宿主机的 <code>localhost</code> 了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 服务的启动方式有所改变(没有映射出来端口)</span></span><br><span class="line"><span class="comment"># 因为本身与宿主机共用了网络，宿主机暴露端口等同于容器中暴露端口</span></span><br><span class="line">$ docker run -d -p 80:80 --network=host <span class="variable">$PWD</span>:/etc/nginx nginxx</span><br></pre></td></tr></table></figure><h2><span id="docker-容器总线错误">Docker 容器总线错误</span></h2><blockquote><p><strong>总线错误看到的时候还是挺吓人了</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 在 <code>docker</code> 容器中运行程序的时候，提示 <code>bus error</code> 错误。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 总线报错</span></span><br><span class="line">$ inv app.user_op --name=zhangsan</span><br><span class="line">Bus error (core dumped)</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 原因是在 <code>docker</code> 运行的时候，<code>shm</code> 分区设置太小导致 <code>share memory</code> 不够。不设置 <code>--shm-size</code> 参数时，<code>docker</code> 给容器默认分配的 <code>shm</code> 大小为 <code>64M</code>，导致程序启动时不足。具体原因还是因为安装 <code>pytorch</code> 包导致了，多进程跑任务的时候，<code>docker</code> 容器分配的共享内存太小，导致 <code>torch</code> 要在 <code>tmpfs</code> 上面放模型数据用于子线程的 <a href="https://github.com/pytorch/pytorch/issues/2244" target="_blank" rel="noopener">共享不足</a>，就出现报错了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 问题原因</span></span><br><span class="line">root@18...35:/opt/app<span class="comment"># df -TH</span></span><br><span class="line">Filesystem     Type     Size  Used Avail Use% Mounted on</span><br><span class="line">overlay        overlay  2.0T  221G  1.4T   3% /</span><br><span class="line">tmpfs          tmpfs     68M     0   68M   0% /dev</span><br><span class="line">shm            tmpfs     68M   41k   68M   1% /dev/shm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动docker的时候加上--shm-size参数(单位为b,k,m或g)</span></span><br><span class="line">$ docker run -it --rm --shm-size=200m pytorch/pytorch:latest</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在docker-compose添加对应配置</span></span><br><span class="line">$ shm_size: <span class="string">'2gb'</span></span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 还有一种情况就是容器内的磁盘空间不足，也会导致 <code>bus error</code> 这样的报错，所以如果出现了，清除多余文件和目录或者分配一个大的磁盘空间，就可以解决了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 磁盘空间不足</span></span><br><span class="line">$ df -Th</span><br><span class="line">Filesystem     Type     Size  Used Avail Use% Mounted on</span><br><span class="line">overlay        overlay    1T    1T    0G 100% /</span><br><span class="line">shm            tmpfs     64M   24K   64M   1% /dev/shm</span><br></pre></td></tr></table></figure><h2><span id="docker-nfs-挂载报错">Docker NFS 挂载报错</span></h2><blockquote><p><strong>NFS 挂载之后容器程序使用异常为内核版本太低导致的</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 我们将服务部署到 <code>openshift</code> 集群中，启动服务调用资源文件的时候，报错信息如下所示。从报错信息中，得知是在 <code>Python3</code> 程序执行 <code>read_file()</code> 读取文件的内容，给文件加锁的时候报错了。但是奇怪的是，本地调试的时候发现服务都是可以正常运行的，文件加锁也是没问题的。后来发现，在 <code>openshift</code> 集群中使用的是 <code>NFS</code> 挂载的共享磁盘。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 报错信息</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">    ......</span><br><span class="line">    File <span class="string">"xxx/utils/storage.py"</span>, line 34, <span class="keyword">in</span> xxx.utils.storage.LocalStorage.read_file</span><br><span class="line">OSError: [Errno 9] Bad file descriptor</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 文件加锁代码</span><br><span class="line">...</span><br><span class="line">    with open(self.mount(path), &#39;rb&#39;) as fileobj:</span><br><span class="line">        fcntl.flock(fileobj, fcntl.LOCK_EX)</span><br><span class="line">        data &#x3D; fileobj.read()</span><br><span class="line">    return data</span><br><span class="line">...</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 从下面的信息得知，要在 <code>Linux</code> 中使用 <code>flock()</code> 的话，就需要升级内核版本到 <code>2.6.11+</code> 才行。后来才发现，这实际上是由 <code>RedHat</code> 內核中的一个错误引起的，并在 <code>kernel-3.10.0-693.18.1.el7</code> 版本中得到修复。 所以对于 <code>NFSv3</code> 和 <code>NFSv4</code> 服务而已，就需要升级 <code>Linux</code> 内核版本才能够解决这个问题。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://t.codebug.vip/questions-930901.htm</span></span><br><span class="line">$ In Linux kernels up to 2.6.11, flock() does not lock files over NFS (i.e.,</span><br><span class="line">the scope of locks was limited to the <span class="built_in">local</span> system). [...] Since Linux 2.6.12,</span><br><span class="line">NFS clients support flock() locks by emulating them as byte-range locks on the entire file.</span><br></pre></td></tr></table></figure><h2><span id="docker-使用默认网段">Docker 使用默认网段</span></h2><blockquote><p><strong>启动的容器网络无法相互通信，很是奇怪！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 我们在使用 <code>Docker</code> 启动服务的时候，发现有时候服务之前可以相互连通，而有时启动的多个服务之前却出现了无法访问的情况。究其原因，发现原来是因为使用的内部私有地址网段不一致导致的。有的服务启动到了 <code>172.17 - 172.31</code> 的网段，有的服务跑到了 <code>192.169.0 - 192.168.224</code> 的网段，这样导致服务启动之后出现无法访问的情况(默认情况下，有下面这个两个网段可供其使用)。</li></ul><p><img src="https://img.hi-linux.com/staticfile/docker-have-some-trouble-2-2022-03-29-DlFT7v.png" alt="Docker默认使用网段"></p><ul><li><strong>[解决方法]</strong> 上述问题的处理方式，就是手动指定 <code>Docker</code> 服务的启动网段，二选一就可以了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看docker容器配置</span></span><br><span class="line">$ cat /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"registry-mirrors"</span>: [<span class="string">"https://vec0xydj.mirror.aliyuncs.com"</span>],</span><br><span class="line">    <span class="string">"default-address-pools"</span>:[&#123;<span class="string">"base"</span>:<span class="string">"172.17.0.0/12"</span>, <span class="string">"size"</span>:24&#125;],</span><br><span class="line">    <span class="string">"experimental"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"default-runtime"</span>: <span class="string">"nvidia"</span>,</span><br><span class="line">    <span class="string">"live-restore"</span>: <span class="literal">true</span>,</span><br><span class="line">    <span class="string">"runtimes"</span>: &#123;</span><br><span class="line">        <span class="string">"nvidia"</span>: &#123;</span><br><span class="line">            <span class="string">"path"</span>: <span class="string">"/usr/bin/nvidia-container-runtime"</span>,</span><br><span class="line">            <span class="string">"runtimeArgs"</span>: []</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="docker-服务启动串台">Docker 服务启动串台</span></h2><blockquote><p><strong>使用 docker-compose 命令各自启动两组服务，发现服务会串台！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 在两个不同名称的目录目录下面，使用 <code>docker-compose</code> 来启动服务，发现当 <code>A</code> 组服务启动完毕之后，再启动 <code>B</code> 组服务的时候，发现 <code>A</code> 组当中对应的一部分服务又重新启动了一次，这就非常奇怪了！因为这个问题的存在会导致，<code>A</code> 组服务和 <code>B</code> 组服务无法同时启动。之前还以为是工具的 <code>Bug</code>，后来请教了 <strong>“上峰”</strong>，才知道了原因，恍然大悟。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 服务目录结构如下所示</span></span><br><span class="line">A: /data1/app/docker-compose.yml</span><br><span class="line">B: /data2/app/docker-compose.yml</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 发现 <code>A</code> 和 <code>B</code> 两组服务会串台的原因，原来是 <code>docker-compose</code> 会给启动的容器加 <code>label</code> 标签，然后根据这些 <code>label</code> 标签来识别和判断对应的容器服务是由谁启动的、谁来管理的，等等。而这里，我们需要关注的 <code>label</code> 变量是 <code>com.docker.compose.project</code>，其对应的值是使用启动配置文件的目录的最底层子目录名称，即上面的 <code>app</code> 就是对应的值。我们可以发现， <code>A</code> 和 <code>B</code> 两组服务对应的值都是 <code>app</code>，所以启动的时候被认为是同一个，这就出现了上述的问题。如果需要深入了解的话，可以去看对应源代码。</li></ul><p><img src="https://img.hi-linux.com/staticfile/docker-have-some-trouble-4-20220329161418940-2022-03-29-Aqsd14.png" alt="Docker服务启动串台"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以将目录结构调整为如下所示</span></span><br><span class="line">A: /data/app1/docker-compose.yml</span><br><span class="line">B: /data/app2/docker-compose.yml</span><br><span class="line"></span><br><span class="line">A: /data1/app-old/docker-compose.yml</span><br><span class="line">B: /data2/app-new/docker-compose.yml</span><br></pre></td></tr></table></figure><ul><li>或者使用 <code>docker-compose</code> 命令提供的参数 <code>-p</code> 手动指定标签，来规避该问题的发生。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定项目项目名称</span></span><br><span class="line">$ docker-compose -f ./docker-compose.yml -p app1 up -d</span><br></pre></td></tr></table></figure><h2><span id="docker-命令调用报错">Docker 命令调用报错</span></h2><blockquote><p><strong>在编写脚本的时候常常会执行 docker 相关的命令，但是需要注意使用细节！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> <code>CI</code> 更新环境执行了一个脚本，但是脚本执行过程中报错了，如下所示。通过对应的输出信息，可以看到提示说正在执行的设备不是一个 <code>tty</code>。</li></ul><p><img src="https://img.hi-linux.com/staticfile/docker-have-some-trouble-5-2022-03-29-3hwPWX.png" alt="Docker命令调用报错"></p><ul><li>随即，查看了脚本发现报错地方是执行了一个 <code>exec</code> 的 <code>docker</code> 命令，大致如下所示。很奇怪的是，手动执行或直接调脚本的时候，怎么都是没有问题的，但是等到 <code>CI</code> 调用的时候怎么都是有问题。后来好好看下，下面这个命令，注意到 <code>-it</code> 这个参数了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 脚本调用docker命令</span></span><br><span class="line">docker <span class="built_in">exec</span> -it &lt;container_name&gt; psql -Upostgres ......</span><br></pre></td></tr></table></figure><ul><li>我们可以一起看下 <code>exec</code> 命令的这两个参数，自然就差不多理解了。</li></ul><table><thead><tr><th style="text-align:left">编号</th><th style="text-align:left">参数</th><th style="text-align:left">解释说明</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:left"><code>-i</code>/<code>-interactive</code></td><td style="text-align:left">即使没有附加也保持 STDIN 打开；如果你需要执行命令则需要开启这个选项</td></tr><tr><td style="text-align:left">2</td><td style="text-align:left"><code>-t</code>/<code>–tty</code></td><td style="text-align:left">分配一个伪终端进行执行；一个连接用户的终端与容器 stdin 和 stdout 的桥梁</td></tr></tbody></table><ul><li><strong>[解决方法]</strong> <code>docker exec</code> 的参数 <code>-t</code> 是指 <code>Allocate a pseudo-TTY</code> 的意思，而 <code>CI</code> 在执行 <code>job</code> 的时候并不是在 <code>TTY</code> 终端中执行，所以 <code>-t</code> 这个参数会报错。同时在 『<a href="https://stackoverflow.com/questions/43099116/error-the-input-device-is-not-a-tty" target="_blank" rel="noopener">stackoverflow</a>』也有人给出原因，可以自行查看。</li></ul><p><img src="https://img.hi-linux.com/staticfile/docker-have-some-trouble-6-20220329162948314-2022-03-29-ECCkYH.png" alt="Docker命令调用报错"></p><h2><span id="docker-定时任务异常">Docker 定时任务异常</span></h2><blockquote><p><strong>在 Crontab 定时任务中也存在 Docker 命令执行异常的情况！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 今天发现了一个问题，就是在备份 <code>Mysql</code> 数据库的时候，使用 <code>docker</code> 容器进行备份，然后使用 <code>Crontab</code> 定时任务来触发备份。但是发现备份的 <code>MySQL</code> 数据库居然是空的，但是手动执行对应命令切是好的，很奇怪。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Crontab定时任务</span></span><br><span class="line">0 */6 * * * \</span><br><span class="line">    docker <span class="built_in">exec</span> -it &lt;container_name&gt; sh -c \</span><br><span class="line">        <span class="string">'exec mysqldump --all-databases -uroot -ppassword ......'</span></span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 后来发现是因为执行的 <code>docker</code> 命令多个 <code>-i</code> 导致的。因为 <code>Crontab</code> 命令执行的时候，并不是交互式的，所以需要把这个去掉才可以。总结就是，如果你需要回显的话则需要 <code>-t</code> 选项，如果需要交互式会话则需要 <code>-i</code> 选项。</li></ul><table><thead><tr><th style="text-align:left">编号</th><th style="text-align:left">参数</th><th style="text-align:left">解释说明</th></tr></thead><tbody><tr><td style="text-align:left">1</td><td style="text-align:left"><code>-i</code>/<code>-interactive</code></td><td style="text-align:left">即使没有附加也保持 STDIN 打开；如果你需要执行命令则需要开启这个选项</td></tr><tr><td style="text-align:left">2</td><td style="text-align:left"><code>-t</code>/<code>–tty</code></td><td style="text-align:left">分配一个伪终端进行执行；一个连接用户的终端与容器 stdin 和 stdout 的桥梁</td></tr></tbody></table><h2><span id="docker-变量使用引号">Docker 变量使用引号</span></h2><blockquote><p><strong>compose 里边环境变量带不带引号的问题！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 使用过 <code>compose</code> 的朋友可能都遇到过，在编写启服务启动配置文件的时候，添加环境变量时到底是使用单引号、双引号还是不使用引号的问题？时间长了，我们可能会将三者混用，认为其效果是一样的。但是后来，发现的坑越来越多，才发现其越来越隐晦。</li><li>反正我是遇到过很多问题，都是因为添加引号导致的服务启动异常的，后来得出的结论就是一律不使引号。裸奔，体验前所未有的爽快！直到现在看到了 <code>Github</code> 中对应的 <a href="https://github.com/docker/compose/issues/2854" target="_blank" rel="noopener">issus</a> 之后，才终于破案了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在Compose中进行引用TEST_VAR变量，无法找到</span></span><br><span class="line">TEST_VAR=<span class="string">"test"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Compose中进行引用TEST_VAR变量，可以找到</span></span><br><span class="line">TEST_VAR=<span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 后来发现docker本身其实已经正确地处理了引号的使用</span></span><br><span class="line">docker run -it --rm -e TEST_VAR=<span class="string">"test"</span> <span class="built_in">test</span>:latest</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 得到的结论就是，因为 <code>Compose</code> 解析 <code>yaml</code> 配置文件，发现引号也进行了解释包装。这就导致原本的 <code>TEST_VAR=&quot;test&quot;</code> 被解析成了 <code>'TEST_VAR=&quot;test&quot;'</code>，所以我们在引用的时候就无法获取到对应的值。现在解决方法就是，不管是我们直接在配置文件添加环境变量或者使用 <code>env_file</code> 配置文件，能不使用引号就不适用引号。</li><li>需要注意的是环境变量配置的是日志格式的话(<code>2022-01-01</code>)，如果使用的是 <code>Python</code> 的 <code>yaml.load</code> 模块的话，会被当做是 <code>date</code> 类型的，这是如果希望保持原样信息的话，可以使用 <code>'</code>/<code>&quot;</code> 引起来将其变成字符串格式的。</li></ul><h2><span id="docker-删除镜像报错">Docker 删除镜像报错</span></h2><blockquote><p><strong>无法删除镜像，归根到底还是有地方用到了！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 清理服器磁盘空间的时候，删除某个镜像的时候提示如下信息。提示需要强制删除，但是发现及时执行了强制删除依旧没有效果。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除镜像</span></span><br><span class="line">$ docker rmi 3ccxxxx2e862</span><br><span class="line">Error response from daemon: conflict: unable to delete 3ccxxxx2e862 (cannot be forced) - image has dependent child images</span><br><span class="line"></span><br><span class="line"><span class="comment"># 强制删除</span></span><br><span class="line">$ dcoker rmi -f 3ccxxxx2e862</span><br><span class="line">Error response from daemon: conflict: unable to delete 3ccxxxx2e862 (cannot be forced) - image has dependent child images</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 后来才发现，出现这个原因主要是因为 <code>TAG</code>，即存在其他镜像引用了这个镜像。这里我们可以使用如下命令查看对应镜像文件的依赖关系，然后根据对应 <code>TAG</code> 来删除镜像。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询依赖 - image_id表示镜像名称</span></span><br><span class="line">$ docker image inspect --format=<span class="string">'&#123;&#123;.RepoTags&#125;&#125; &#123;&#123;.Id&#125;&#125; &#123;&#123;.Parent&#125;&#125;'</span> $(docker image ls -q --filter since=&lt;image_id&gt;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据TAG删除镜像</span></span><br><span class="line">$ docker rmi -f c565xxxxc87f</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除悬空镜像</span></span><br><span class="line">$ docker rmi $(docker images --filter <span class="string">"dangling=true"</span> -q --no-trunc)</span><br></pre></td></tr></table></figure><h2><span id="docker-普通用户切换">Docker 普通用户切换</span></h2><blockquote><p><strong>切换 Docker 启动用户的话，还是需要注意下权限问题的！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 我们知道在 <code>Docker</code> 容器里面使用 <code>root</code> 用户的话，是不安全的，很容易出现越权的安全问题，所以一般情况下，我们都会使用普通用户来代替 <code>root</code> 进行服务的启动和管理的。今天给一个服务切换用户的时候，发现 <code>Nginx</code> 服务一直无法启动，提示如下权限问题。因为对应的配置文件也没有配置 <code>var</code> 相关的目录，无奈 🤷‍♀ ！️</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nginx报错信息</span></span><br><span class="line">nginx: [alert] could not open error <span class="built_in">log</span> file: open() <span class="string">"/var/log/nginx/error.log"</span> failed (13: Permission denied)</span><br><span class="line">2020/11/12 15:25:47 [emerg] 23<span class="comment">#23: mkdir() "/var/cache/nginx/client_temp" failed (13: Permission denied)</span></span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 后来发现还是 <code>nginx.conf</code> 配置文件，配置的有问题，需要将 <code>Nginx</code> 服务启动时候需要的文件都配置到一个无权限的目录，即可解决。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">user  www-data;</span><br><span class="line">worker_processes  1;</span><br><span class="line"></span><br><span class="line">error_log  &#x2F;data&#x2F;logs&#x2F;master_error.log warn;</span><br><span class="line">pid        &#x2F;dev&#x2F;shm&#x2F;nginx.pid;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    include       &#x2F;etc&#x2F;nginx&#x2F;mime.types;</span><br><span class="line">    default_type  application&#x2F;octet-stream;</span><br><span class="line"></span><br><span class="line">    gzip               on;</span><br><span class="line">    sendfile           on;</span><br><span class="line">    tcp_nopush         on;</span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line"></span><br><span class="line">    client_body_temp_path  &#x2F;tmp&#x2F;client_body;</span><br><span class="line">    fastcgi_temp_path      &#x2F;tmp&#x2F;fastcgi_temp;</span><br><span class="line">    proxy_temp_path        &#x2F;tmp&#x2F;proxy_temp;</span><br><span class="line">    scgi_temp_path         &#x2F;tmp&#x2F;scgi_temp;</span><br><span class="line">    uwsgi_temp_path        &#x2F;tmp&#x2F;uwsgi_temp;</span><br><span class="line"></span><br><span class="line">    include &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;*.conf;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="docker-绑定到-ipv6-上">Docker 绑定到 IPv6 上</span></h2><blockquote><p><strong>Docker 服务在启动的时候，将地址绑定到 IPv6 地址上面了，提示报错信息！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 物理机器更新了对应补丁之后，重启了服务，导致原本可以正常启动的 <code>docker-compose</code> 服务提示如下报错信息。不清楚是否修改了操作系统的相关配置，还是对应 <code>docker</code> 进行的其他方面的配置，比如修改 <code>/etc/docker/daemon.json</code> 或者 <code>docker</code> 的 <code>service</code> 启动文件。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Docker的报错信息</span></span><br><span class="line">docker run -p 80:80 nginx:alpine succeeds. Previously, this was failing with Error \</span><br><span class="line">starting userland proxy: listen tcp6 [::]:80: socket: address family not supported by protocol.</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 通过如上所示的报错信息，可以看到服务的启动端口绑定到了 <code>tcp6</code> 上面了，但是对应的 <code>socket</code> 发现系统本身并不支持。这时，我们一看下对应的操作系统 <code>ipv6</code> 的设置，发现系统禁用了，所有的 <code>ipv6</code> 地址。需要了解的朋友，可以参考 <a href="https://github.com/moby/moby/pull/42322" target="_blank" rel="noopener">fix port forwarding with ipv6.disable=1</a> 和 <a href="https://github.com/moby/moby/issues/42288" target="_blank" rel="noopener">cannot start if ipv6 is disabled on host</a> 这两个 <code>issus</code> 来获取更多信息。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 操作系统配置</span></span><br><span class="line">$ cat /etc/sysctl.conf | grep ipv6</span><br><span class="line">net.ipv6.conf.all.disable_ipv6=1</span><br></pre></td></tr></table></figure><ul><li><strong>[方法一]</strong> 最为简单的解决方法，就是在 <code>docker-compose.yml</code> 文件中，手动指定将对应服务的端口绑定到 <code>ipv4</code> 上面，如下所示。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  app:</span><br><span class="line">    restart: on-failure</span><br><span class="line">    container_name: app_web</span><br><span class="line">    image: app:latest</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;0.0.0.0:80:80&#x2F;tcp&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - &quot;.&#x2F;app_web:&#x2F;data&quot;</span><br><span class="line">    networks:</span><br><span class="line">      - app_network</span><br><span class="line"></span><br><span class="line">networks:</span><br><span class="line">  app_network:</span><br></pre></td></tr></table></figure><ul><li><strong>[方法二]</strong> 或者修改 <code>/etc/docker/daemon.json</code> 文件，在配置中，阻止 <code>Docker</code> 错误的将端口映射到 <code>IPv6</code> 上，即可达到同样的效果，且不用再次修改多个服务的启动配置文件了。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 修改配置</span><br><span class="line">$ vim &#x2F;etc&#x2F;docker&#x2F;daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  &quot;ipv6&quot;: false,</span><br><span class="line">  &quot;fixed-cidr-v6&quot;: &quot;2001:db8:1::&#x2F;64&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 重启服务</span><br><span class="line">$ systemctl reload docker</span><br></pre></td></tr></table></figure><ul><li><strong>[方法三]</strong> <code>Docker</code> 默认情况下会同时将端口映射于 <code>IPv4</code> 与 <code>IPv6</code> 两者上，而且有的时候会出现只绑定到了 <code>IPv6</code>，导致服务无法正常访问的情况。现在通用的始终还是 <code>IPv4</code> 地址，因此最简单的做法就是关闭 <code>IPv6</code> 地址。详细的配置，可以参考 <a href="https://github.com/moby/moby/issues/2174" target="_blank" rel="noopener">Port redirecting binding to IPv6 but not IPv4 interfaces</a> 这个 <code>issus</code> 地址。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改系统配置</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'1'</span> &gt; /proc/sys/net/ipv6/conf/lo/disable_ipv6</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'1'</span> &gt; /proc/sys/net/ipv6/conf/lo/disable_ipv6</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'1'</span> &gt; /proc/sys/net/ipv6/conf/all/disable_ipv6</span><br><span class="line"><span class="built_in">echo</span> <span class="string">'1'</span> &gt; /proc/sys/net/ipv6/conf/default/disable_ipv6</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启网络</span></span><br><span class="line">$ /etc/init.d/networking restart</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最后检测是否已关闭IPv6</span></span><br><span class="line">ip addr show | grep net6</span><br></pre></td></tr></table></figure><h2><span id="docker-容器启动超时">Docker 容器启动超时</span></h2><blockquote><p><strong>Docker 服务在启动的时候，提示超时，被直接终止了！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 使用 <code>docker-compose</code> 启动容器的时候，等待了很久的时候(大约 <code>2-3</code> 分钟左右)，之后提示如下信息。通过阅读信息内容，可以看到是因为超时导致的，提示可以通过设置环境变量，加大超时的时间。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose up -d</span><br><span class="line">ERROR: <span class="keyword">for</span> xxx  UnixHTTPConnectionPool(host=<span class="string">'localhost'</span>, port=None): Read timed out. (<span class="built_in">read</span> timeout=70)</span><br><span class="line">ERROR: An HTTP request took too long to complete. Retry with --verbose to obtain debug information.</span><br><span class="line">If you encounter this issue regularly because of slow network conditions, consider setting COMPOSE_HTTP_TIMEOUT to a higher value (current value: 60).</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 按照提示设置的环境变量之后，再次启动发现确实可以正常启动了，但是还是能够感觉到有些慢。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/profile</span><br><span class="line"><span class="built_in">export</span> COMPOSE_HTTP_TIMEOUT=500</span><br><span class="line"><span class="built_in">export</span> DOCKER_CLIENT_TIMEOUT=500</span><br></pre></td></tr></table></figure><ul><li>排除了下启动流程，因为容器启动有映射目录到容器里面且目录大小比较大，所以怀疑是因为 <code>i/o</code> 导致的。随即使用 <code>iotop</code> 命令查看服务器目前的 <code>i/o</code> 情况，发现存在很多个 <code>rg</code> 命令，且都处于 <code>100%</code> 左右。查了下，发现是 <code>vscode</code> 远程服务器启动的搜索目录结构的进程，西八，有些坑呀！</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo iotop</span><br><span class="line"> 4269 be/4 escape     15.64 K/s    0.00 B/s  0.00 % 98.36 % rg --files --hidden</span><br><span class="line"> 4270 be/4 escape     28.15 K/s    0.00 B/s  0.00 % 97.46 % rg --files --hidden</span><br><span class="line"> 4272 be/4 escape     31.27 K/s    0.00 B/s  0.00 % 97.39 % rg --files --hidden</span><br><span class="line"> 4276 be/4 escape     34.40 K/s    0.00 B/s  0.00 % 96.98 % rg --files --hidden</span><br></pre></td></tr></table></figure><h2><span id="docker-端口网络限制">Docker 端口网络限制</span></h2><blockquote><p><strong>如果发现服务都一切正常，但是无法无法访问的话，则多为网络问题！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 启用服务之后，登录跳转发现直接 <code>502</code> 报错了。排除了配置等相关原因都没有任何问题(做过相关测试)，这就非常奇怪了！</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署服务架构</span></span><br><span class="line">nginx(80) -&gt; web1(8080)</span><br><span class="line">          -&gt; web2(8081)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 报错信息如下所示</span></span><br><span class="line">nginx connect() failed (113: No route to host) <span class="keyword">while</span> connecting to upstream</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 根据错误信息可知，是因为没有路由到指定的 <code>host</code> 导致了，随即看了下防火墙是开着的，看了日志发现被过滤掉了，西八！问题找到了，现在需要做的就是，要么添加防火墙规则，要么关闭防火墙。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 检查开放的端口</span></span><br><span class="line">$ sudo firewall-cmd --permanent --zone=public --list-ports</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开启需要路由的端口</span></span><br><span class="line">$ sudo firewall-cmd --permanent --zone=public --add-port=8080/tcp</span><br><span class="line">$ sudo firewall-cmd --permanent --zone=public --add-port=8081/tcp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置立即生效</span></span><br><span class="line">firewall-cmd --reload</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 关闭防火墙</span></span><br><span class="line">$ sudo systemctl stop firewalld.service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 禁用自启动</span></span><br><span class="line">$ sudo systemctl <span class="built_in">disable</span> firewalld.service</span><br></pre></td></tr></table></figure><h2><span id="docker-无法获取镜像">Docker 无法获取镜像</span></h2><blockquote><p><strong>新初始化的机器，无法获取私有仓库的镜像文件！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 机器初始化之后，使用如下命令登录私有 <code>docker</code> 仓库，发现提示无法获取对应镜像，但是在其他机器上面获取该镜像就可以执行成功，这就非常奇怪了！</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 登录私有仓库</span></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">'123456'</span> | docker login -u escape --password-stdin docker.escapelife.site</span><br><span class="line"></span><br><span class="line"><span class="comment"># 异常信息提示</span></span><br><span class="line">$ sudo docker pull docker.escapelife.site/app:0.10</span><br><span class="line">Error response from daemon: manifest <span class="keyword">for</span> docker.escapelife.site/app:0.10 not found: manifest unknown: manifest unknown</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 太坑了，我还以为我发现某个隐藏的 <code>bug</code> 了，可劲的排查，最后发现，原来是自己镜像包名字写错了，应该写成 <code>0.0.10</code> 的，自己却写成了 <code>0.10</code>。这里，纪念一下，以后碰到上述报错，那肯定是镜像不存在的。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 登录私有仓库之后会在用户家目录下生成一个docker配置</span></span><br><span class="line"><span class="comment"># 其用来记录docker私有仓库的登录认证信息(是加密过的信息但不安全) =&gt; base64</span></span><br><span class="line">$ cat .docker/config.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"auths"</span>: &#123;</span><br><span class="line">        <span class="string">"docker.escapelife.site"</span>: &#123;</span><br><span class="line">            <span class="string">"auth"</span>: <span class="string">"d00u11Fu22B3355VG2xasE12w=="</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="docker-使容器不退出">Docker 使容器不退出</span></h2><blockquote><p><strong>如何使使用 docker-compose 启动的容器服务 hang 住而不退出</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 有时候我们启动的服务，因为某些问题(<code>bug</code>)导致服务无法正常启动，就会出现容器无限重启(<code>restart: on-failure</code>)的情况，这时就很不利于排除问题。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ docker ps -a</span><br><span class="line">4e6xxx9a4   app:latest   <span class="string">"/xxx/…"</span>   26 seconds ago   Restarting (1) 2 seconds ago</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 这时我们就需要根据，服务构建使用命令来决定是用什么命令来 <code>hang</code> 住服务。卡住的原理，就类似于使用 <code>/bin/bash</code> 进入容器是一样的，这里我就不过多解释了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 类似原理</span></span><br><span class="line">docker run -it --rm --entrypoint=/bin/bash xxx/app:latest</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Command命令</span></span><br><span class="line">tty: <span class="literal">true</span></span><br><span class="line"><span class="built_in">command</span>: tail -f /dev/null</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Entrypoint命令</span></span><br><span class="line">tty: <span class="literal">true</span></span><br><span class="line">entrypoint: tail -f /dev/null</span><br></pre></td></tr></table></figure><ul><li>同理，我们在使用 <code>docker-compose</code> 或者 <code>k8s</code> 平台部署服务的时候，也有时会因为启动问题需要，使启动的服务不直接退出，来手动调试和排查问题原因。所以，我这里记录下其不同部署方式的，暂停方式。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Compose</span><br><span class="line"></span><br><span class="line">version: &quot;3&quot;</span><br><span class="line">services:</span><br><span class="line">  app:</span><br><span class="line">    image: ubuntu:latest</span><br><span class="line">    tty: true</span><br><span class="line">    entrypoint: &#x2F;usr&#x2F;bin&#x2F;tail</span><br><span class="line">    command: &quot;-f &#x2F;dev&#x2F;null&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># K8S</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: ubuntu</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: ubuntu</span><br><span class="line">      image: ubuntu:latest</span><br><span class="line">      command: [&quot;&#x2F;bin&#x2F;bash&quot;, &quot;-c&quot;, &quot;--&quot;]</span><br><span class="line">      args: [&quot;while true; do sleep 30; done;&quot;]</span><br><span class="line">      # command: [&quot;sleep&quot;]</span><br><span class="line">      # args: [&quot;infinity&quot;]</span><br></pre></td></tr></table></figure><h2><span id="docker-不使用默认网段">Docker 不使用默认网段</span></h2><blockquote><p><strong>有些情况，内部规划的网段和可能和 Dockerd 默认的网段有冲突，导致异常出现！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 今天在新机器上面，部署了一整套服务(多台机器)，服务部署完毕之后，通过前置 <code>Nginx</code> 服务发现并不能访问，后置机器开放的端口，发现发到对应端口的请求都没有转发出去。这就比较奇怪了，因为端口控制是已经开通了的，不应该出现不通的情况。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">➜ nc -v 172.16.100.12 8000</span><br><span class="line">nc: connect to 172.16.100.12 port 8000 (tcp) failed: Connection refused</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 发现服务器端口不通，我这里怀疑可能是 <code>dockerd</code> 服务启动导致的，所以我先将服务都停掉，直接在机器上面启动了 <code>Python</code> 的服务端程序(<code>Linux</code> 机器自带 <code>Python2.7.x</code> 的版本)，然后在前置 <code>Nginx</code> 服务发现，端口确实是通的。后来，排除发现是内部服务默认网段和 <code>dockerd</code> 服务启动的默认网段是冲突的，导致重写了机器的防火墙规则，导致出现上述异常的。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ python -m SimpleHTTPServer 8000</span><br><span class="line">Serving HTTP on 0.0.0.0 port 8000 ...</span><br><span class="line"></span><br><span class="line">➜ nc -v 172.16.100.12 8000</span><br><span class="line">Connection to 172.16.100.12 8000 port [tcp/*] succeeded!</span><br></pre></td></tr></table></figure><ul><li>既然问题已经知道了，现在需要做的就是非常简单了：不适用默认网段！通过 <a href="https://docs.mirantis.com/mke/3.4/install/plan-deployment/mcr-considerations/default-address-pools.html" target="_blank" rel="noopener">『mirantis』</a> 里面，我们可以选择进行设置，然后重启服务 <code>dockerd</code> 服务，即可。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 修改配置</span></span><br><span class="line">$ sudo cat /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"default-address-pools"</span>:[&#123;<span class="string">"base"</span>:<span class="string">"192.168.100.0/20"</span>,<span class="string">"size"</span>:24&#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启服务</span></span><br><span class="line">$ sudo systemctl restart docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启动服务验证是否生效</span></span><br><span class="line">$ ip a</span><br><span class="line">$ docker network inspect app | grep Subnet</span><br></pre></td></tr></table></figure><p><img src="https://img.hi-linux.com/staticfile/docker-have-some-trouble-7-2022-03-29-Ddg8C6.png" alt="Docker 不使用默认网段"></p><ul><li>这时，就到了考验我们网络的子网划分的能力了：如何在给定的网段下面合理且高效的进行划分呢？咳咳，确实难倒我了，这时我们可以再这个在线网站上面 <a href="https://www.sojson.com/convert/subnetmask.html" target="_blank" rel="noopener">JSON 在线解析</a> 进行划分，然后选定合理的 <code>base</code> 和 <code>size</code> 就可以了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 报错信息</span></span><br><span class="line">Error response from daemon: could not find an available, non-overlapping IPv4 address pool among the defaults to assign to the network</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照下图我们可以对 pool 进行合理划分</span></span><br><span class="line"><span class="comment"># 给定 10.210.200.0 + 255.255.255.0 的网段来划分子网</span></span><br><span class="line">$ sudo cat /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"default-address-pools"</span>:[&#123;<span class="string">"base"</span>:<span class="string">"10.210.200.0/24"</span>,<span class="string">"size"</span>:28&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>其中，<code>base</code> 告诉我们划分子网的网段是什么(从来开始)，是从前两位(<code>/16</code>)开始，还是第三位开始(<code>/24</code>)呢？而 <code>size</code> 则告诉我们划分的每个子网有多少 <code>IP</code> 地址可以使用呢？从 <code>&quot;10.210.200.0/24&quot;</code> 我们可以知道，该网络下面只有 <code>254</code> 个可用的 <code>IP</code> 地址(直接使用肯定不够)，然后我们需要给 <code>docker</code> 使用，划分每个子网可用 <code>16</code> 个 <code>IP</code> 地址，所以子网就应该写成 <code>28</code> 了。</li></ul><p><img src="https://img.hi-linux.com/staticfile/docker-have-some-trouble-8-2022-03-29-tYKsQu.png" alt="Docker 不使用默认网段"></p><h2><span id="docker-添加私有仓库">Docker 添加私有仓库</span></h2><blockquote><p><strong>有些情况，我们服务器上面需要使用内部私有的容器镜像地址！</strong></p></blockquote><ul><li><strong>[问题起因]</strong> 如果新机器上面需要使用私有仓库的话，但是又没有配置，再获取镜像的时候就会出现如下报错信息。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 拉取/登陆私库时提示</span></span><br><span class="line">$ docker pull 192.168.31.191:5000/nginx:latest</span><br><span class="line">x509: certificate signed by unknown authority</span><br></pre></td></tr></table></figure><ul><li><strong>[解决方法]</strong> 该问题的处理方式很简单，如下所示，配置一下仓库地址，重启服务并登陆私有仓库就可以了。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加配置</span></span><br><span class="line">$ sudo cat /etc/docker/daemon.json</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"insecure-registries"</span>: [<span class="string">"192.168.31.191:5000"</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启docker</span></span><br><span class="line">$ sudo systemctl restart docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新登录即可</span></span><br><span class="line">$ docker login 私库地址 -u 用户名 -p 密码</span><br></pre></td></tr></table></figure><blockquote><p>本文转载自：「 escapelife 的博客 」，原文：<a href="https://tinyurl.com/2p89skum" target="_blank" rel="noopener">https://tinyurl.com/2p89skum</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;这里主要是为了记录在使用 Docker 的时候遇到的问题及其处理解决方法。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Docker-迁移存储目录&quot;&gt;Docker 迁移存储目录&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;默认情况系统会将 Docker 容器存放在 /var/lib/docker 目录下&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;[问题起因]&lt;/strong&gt; 今天通过监控系统，发现公司其中一台服务器的磁盘快慢，随即上去看了下，发现 &lt;code&gt;/var/lib/docker&lt;/code&gt; 这个目录特别大。由上述原因，我们都知道，在 &lt;code&gt;/var/lib/docker&lt;/code&gt; 中存储的都是相关于容器的存储，所以也不能随便的将其删除掉。&lt;/li&gt;
&lt;li&gt;那就准备迁移 &lt;code&gt;docker&lt;/code&gt; 的存储目录吧，或者对 &lt;code&gt;/var&lt;/code&gt; 设备进行扩容来达到相同的目的。更多关于 &lt;code&gt;dockerd&lt;/code&gt; 的详细参数，请点击查看 &lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/dockerd/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;strong&gt;官方文档&lt;/strong&gt;&lt;/a&gt; 地址。&lt;/li&gt;
&lt;li&gt;但是需要注意的一点就是，尽量不要用软链， 因为一些 &lt;code&gt;docker&lt;/code&gt; 容器编排系统不支持这样做，比如我们所熟知的 &lt;code&gt;k8s&lt;/code&gt; 就在内。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
  </entry>
  
</feed>
