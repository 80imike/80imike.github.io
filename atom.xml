<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>奇妙的 Linux 世界</title>
  
  <subtitle>种一棵树最好的时间是十年前，其次是现在。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.hi-linux.com/"/>
  <updated>2021-01-29T05:23:50.361Z</updated>
  <id>https://www.hi-linux.com/</id>
  
  <author>
    <name>Mike</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>5 分钟学懂 SSH 隧道技术.md</title>
    <link href="https://www.hi-linux.com/posts/28120.html"/>
    <id>https://www.hi-linux.com/posts/28120.html</id>
    <published>2021-01-29T01:00:00.000Z</published>
    <updated>2021-01-29T05:23:50.361Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="概述">概述</span></h2><p>本文将介绍一些关于 SSH 隧道技术的小技巧，并且给出一个网络拓扑图更好地解释在做的东西。</p><h2><span id="网络基础">网络基础</span></h2><p>假设我的网络情况是这样的一个简化 Topo：</p><p><img src="https://img.hi-linux.com/staticfile/Z3NXLU-2020-12-27-aC86ok.jpg" alt="Z3NXLU-2020-12-27-aC86ok"></p><p>图 1：简化网络拓扑</p><p>那么这里有几种可能的操作，分别是：</p><ul><li>从 Home PC 直接 SSH Office PC<ul><li>从 Office PC SSH 到 Home PC 同理</li></ul></li><li>从 Home PC 直接 SSH 到 VPS<ul><li>从 Office PC SSH 到 VPS 同理</li></ul></li><li>从 VPS SSH 到 Home（Office） PC</li></ul><p>在这几个场景中，最简单实现的就是：<strong>从 Home PC SSH 到 VPS</strong>，这也是我们平时最常见的一个操作。只所以我们可以轻松地实现这个原理，是因为 VPS 拥有公网 IP，我们可以直接从 Home PC 中路由到 VPS，反之，因为我们得 Home PC 没有公网 IP，所以 VPS 无法直接 SSH 到我们的 Home PC。</p><a id="more"></a><p>那么这里就会有同学有想法了，那么我们的 Home PC 能不能有公网 IP，答案肯定是可以的，从上图中可以看到，简单来说，PC 和 VPS 的差异就在于 PC 还接了一次路由器，那么如果直接将 PC 介入 ISP 网络中，是不是就可以拥有公网 IP 了。这个问题很复杂，答案是可能是，这依赖于你的 ISP 如果提供网络给你，在以前 ADSL 时代，基本上都是可以提供公网 IP 的，但是，随着光纤的普及，即使你不使用路由器，你也不是直接介入 ISP 的骨干网了，而是 ISP 的一个分线器，例如 FTTB 系列。所以，这里设想让 PC（路由器） 拥有公网 IP 的想法不具有通用性，加上即使有，ISP 可能也会限制一些端口，因此用的也不多。</p><h2><span id="ssh-隧道">SSH 隧道</span></h2><p>所以，终于进入到本文的主题了，既然没法直接连接到 PC，那么可不可以反过来，让 PC 自己主动来发起请求，这就是 SSH 隧道的原理了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@liqiang.io]# ssh -qngfNTR 9999:localhost:8888 root@192.168.29.48</span><br></pre></td></tr></table></figure><h3><span id="命令行快速使用">命令行快速使用</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@liqiang.io]# cat ~&#x2F;.ssh&#x2F;config</span><br><span class="line">Host jump</span><br><span class="line">  HostName 10.0.0.102</span><br><span class="line">  Port 22</span><br><span class="line">  User root</span><br><span class="line">  IdentityFile &#x2F;root&#x2F;.ssh&#x2F;id_rsa</span><br><span class="line">  ForwardAgent yes</span><br><span class="line">Host 10.0.0.87</span><br><span class="line">  HostName 10.0.0.87</span><br><span class="line">  ProxyJump jump</span><br><span class="line">  User zhangsan</span><br></pre></td></tr></table></figure><ul><li>首先需要确保可以直接登录跳板机：<code>10.0.0.102</code>；</li><li><code>10.0.0.87</code> 是通过 <code>10.0.0.102</code> 这个跳板机访问的；</li></ul><h3><span id="tips">Tips</span></h3><h4><span id="反向隧道只监听-localhost">反向隧道只监听 localhost</span></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@liqiang.io]# cat &#x2F;etc&#x2F;ssh&#x2F;sshd_config</span><br><span class="line">GatewayPorts &#x3D; yes</span><br></pre></td></tr></table></figure><h4><span id="代理-udp">代理 UDP</span></h4><h5><span id="反向代理">反向代理</span></h5><ul><li>本地机器：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@liqiang.io]# yum install -y nc</span><br><span class="line">[root@liqiang.io]# mkfifo &#x2F;tmp&#x2F;fifo</span><br><span class="line">[root@liqiang.io]# nc -l -p 1162 &lt; &#x2F;tmp&#x2F;fifo | nc -u localhost 1163 &gt; &#x2F;tmp&#x2F;fifo</span><br></pre></td></tr></table></figure><p>这里的意思是说监听机器上的 1162 端口发出来的数据，然后以 UDP 的形式将数据发送给 <code>localhost:1163</code>。</p><ul><li>远程机器</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@liqiang.io]# mkfifo &#x2F;tmp&#x2F;fifo</span><br><span class="line">[root@liqiang.io]# nc -l -u -p 1163 &lt; &#x2F;tmp&#x2F;fifo | nc localhost 1162 &gt; &#x2F;tmp&#x2F;fifo</span><br></pre></td></tr></table></figure><p>这里的意思是在机器上监听 1163 端口的 UDP 协议，然后将收到的 UDP 流量通过 TCP 链路 <code>localhost:1162</code> 发送出去。</p><ul><li><p>注意要点</p><ul><li>必须先在本地执行完命令之后再去远程服务器执行，不然，服务器这边会因为没有数据而导致无法正常运行；</li><li>在使用过程中我遇到了一个问题就是这种方式只能接收一次 UDP，第二次就无法接收成功了；</li><li>这是反向代理的例子，正向代理反过来即是。</li></ul></li></ul><h3><span id="参考文档">参考文档</span></h3><ul><li><a href="https://superuser.com/questions/588591/how-to-make-a-ssh-tunnel-publicly-accessible" target="_blank" rel="noopener">How to make a SSH tunnel publicly accessible?</a></li><li><a href="http://zarb.org/~gc/html/udp-in-ssh-tunneling.html" target="_blank" rel="noopener">Performing UDP tunneling through an SSH connection</a></li><li><a href="https://serverfault.com/questions/912017/how-to-use-ansible-openstack-modules-with-a-ssh-socks-proxy" target="_blank" rel="noopener">How to use ansible openstack modules with a ssh socks proxy</a></li><li><a href="https://liqiang.io/post/tunnel-with-ssh-91701268?lang=US_EN" target="_blank" rel="noopener">SSH Tunneling Technology Guide</a></li></ul><blockquote><p>本文转载自：「格物致知」，原文：<a href="https://liqiang.io/post/562b8522%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://liqiang.io/post/562b8522，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;概述&quot;&gt;概述&lt;/h2&gt;
&lt;p&gt;本文将介绍一些关于 SSH 隧道技术的小技巧，并且给出一个网络拓扑图更好地解释在做的东西。&lt;/p&gt;
&lt;h2 id=&quot;网络基础&quot;&gt;网络基础&lt;/h2&gt;
&lt;p&gt;假设我的网络情况是这样的一个简化 Topo：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img.hi-linux.com/staticfile/Z3NXLU-2020-12-27-aC86ok.jpg&quot; alt=&quot;Z3NXLU-2020-12-27-aC86ok&quot;&gt;&lt;/p&gt;
&lt;p&gt;图 1：简化网络拓扑&lt;/p&gt;
&lt;p&gt;那么这里有几种可能的操作，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从 Home PC 直接 SSH Office PC
&lt;ul&gt;
&lt;li&gt;从 Office PC SSH 到 Home PC 同理&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;从 Home PC 直接 SSH 到 VPS
&lt;ul&gt;
&lt;li&gt;从 Office PC SSH 到 VPS 同理&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;从 VPS SSH 到 Home（Office） PC&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在这几个场景中，最简单实现的就是：&lt;strong&gt;从 Home PC SSH 到 VPS&lt;/strong&gt;，这也是我们平时最常见的一个操作。只所以我们可以轻松地实现这个原理，是因为 VPS 拥有公网 IP，我们可以直接从 Home PC 中路由到 VPS，反之，因为我们得 Home PC 没有公网 IP，所以 VPS 无法直接 SSH 到我们的 Home PC。&lt;/p&gt;
    
    </summary>
    
    
      <category term="SSH" scheme="https://www.hi-linux.com/categories/SSH/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="SSH" scheme="https://www.hi-linux.com/tags/SSH/"/>
    
  </entry>
  
  <entry>
    <title>使用 K3s 快速上手 Kubernetes 集群指南</title>
    <link href="https://www.hi-linux.com/posts/42885.html"/>
    <id>https://www.hi-linux.com/posts/42885.html</id>
    <published>2021-01-26T01:00:00.000Z</published>
    <updated>2021-01-26T01:09:29.397Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><blockquote><p><strong>k3s: 带你尝鲜史上最轻量 Kubernetes 发行版!</strong></p></blockquote><p>我们都知道 <code>Kubernetes</code> 是一个容器编排平台，可以用来管理我们的容器集群。但是如果我们只是作为学习来使用的话，<code>Kubernetes</code> 未免有些太重了，有些人的本机估计都没有办法来运行完整的 <code>3</code> 实例(一个 <code>master</code>，两个 <code>agent</code>)的集群环境。虽然网上也有使用 <code>vagrant</code> 和 <code>machine</code> 的部署方式，但是使用和配置还是比较复杂的。而 <a href="https://k3s.io/" target="_blank" rel="noopener"><code>k3s</code></a> 就是为了解决上述问题，应运而生的。</p><h2><span id="1-项目介绍">1. 项目介绍</span></h2><blockquote><p><strong>我们首先需要了解该项目的适用场景和功能特点！</strong></p></blockquote><p>了解或使用过 <code>Kubernetes</code> 的用户来说，肯定都听说过 <code>Rancher</code> 这个开源产品，其同样也是一款开源的企业级 <code>Kubernetes</code> 管理平台，可以极为出色地安装和管理<code>Kubernetes</code> 集群。而 <code>k3s</code> 这款轻量级的 <code>Kubernetes</code> 发行版，也是该公司创建和维护的。</p><p><code>k3s</code> 同样还是一款完全通过 <code>CNCF</code> 认证的 <code>Kubernetes</code> 发行版，这意味着我们可以通过编写 <code>YAML</code> 来对完整版的 <code>Kubernetes</code> 进行操作，并且它们也将适用于 <code>k3s</code> 集群。并且，其完全实现了 <code>Kubernetes</code> 提供的所有 <code>API</code> 接口，我们可以自由的通过接口来操作 <code>Kubernetes</code> 了。创建 <code>k3s</code> 项目的主旨是为了打造一个非常非常轻量级的 <code>Kubernetes</code> 发行版，主要适用于下面这些方面：</p><ul><li>Edge</li><li>IoT</li><li>CI</li><li>Development</li><li>ARM</li><li>Embedding K8s</li><li>Situations where a PhD in K8s clusterology is infeasible</li></ul><a id="more"></a><p><code>k3s</code> 将安装 <code>Kubernetes</code> 所需的一切打包进仅有 <code>XXMB</code> 大小的二进制文件中。并且，为了减少运行 <code>k8s</code> 所需的内存，删除了很多不必要的驱动程序，并用附加组件对其进行替换。这样，它只需要极低的资源就可以运行且安装所需的时间也非常短，因此它能够运行在树莓派等设备上面，即 <code>master</code> 和 <code>agent</code> 运行在一起的模式。</p><ul><li><strong>裁剪功能</strong><ul><li>过时的功能和非默认功能</li><li>过时的功能和非默认功能 <code>Alpha</code> 功能</li><li>过时的功能和非默认功能内置的云提供商插件</li><li>过时的功能和非默认功能内置的存储驱动</li><li>过时的功能和非默认功能 <code>Docker</code></li></ul></li></ul><table><thead><tr><th style="text-align:left">PROTOCOL</th><th style="text-align:left">PORT</th><th style="text-align:left">SOURCE</th><th style="text-align:left">DESCRIPTION</th></tr></thead><tbody><tr><td style="text-align:left">TCP</td><td style="text-align:left">6443</td><td style="text-align:left">K3s agent nodes</td><td style="text-align:left">Kubernetes API</td></tr><tr><td style="text-align:left">UDP</td><td style="text-align:left">8472</td><td style="text-align:left">K3s server and agent nodes</td><td style="text-align:left">Required only for Flannel VXLAN</td></tr><tr><td style="text-align:left">TCP</td><td style="text-align:left">10250</td><td style="text-align:left">K3s server and agent nodes</td><td style="text-align:left">kubelet</td></tr></tbody></table><ul><li><strong>项目特点</strong><ul><li>使用 <code>SQLite</code> 作为默认数据存储替代 <code>etcd</code>，但 <code>etcd</code> 仍然是支持的</li><li>内置了 <code>local storage provider</code>、<code>service load balancer</code> 等</li><li>所有 <code>k8s</code> 控制组件如 <code>api-server</code>、<code>scheduler</code> 等封装成为一个精简二进制程序，单进程即可运行</li><li>删除内置插件，比如 <code>cloudprovider</code> 插件和存储插件等</li><li>减少外部依赖，操作系统只需要安装较新的内核以及支持 <code>cgroup</code> 即可</li></ul></li></ul><table><thead><tr><th style="text-align:left">DEPLOYMENT SIZE</th><th style="text-align:left">NODES</th><th style="text-align:left">VCPUS</th><th style="text-align:left">RAM</th></tr></thead><tbody><tr><td style="text-align:left">Small</td><td style="text-align:left">Up to 10</td><td style="text-align:left">2</td><td style="text-align:left">4 GB</td></tr><tr><td style="text-align:left">Medium</td><td style="text-align:left">Up to 100</td><td style="text-align:left">4</td><td style="text-align:left">8 GB</td></tr><tr><td style="text-align:left">Large</td><td style="text-align:left">Up to 250</td><td style="text-align:left">8</td><td style="text-align:left">16 GB</td></tr><tr><td style="text-align:left">X-Large</td><td style="text-align:left">Up to 500</td><td style="text-align:left">16</td><td style="text-align:left">32 GB</td></tr><tr><td style="text-align:left">XX-Large</td><td style="text-align:left">500+</td><td style="text-align:left">32</td><td style="text-align:left">64 GB</td></tr></tbody></table><ul><li><strong>缺点不足</strong><ul><li>因为在高可用的场景中，其没有办法做到或很难做到。所以如果你要进行大型的集群部署，那么我建议你选择使用 <code>K8s</code> 来安装部署。如果你处于边缘计算等小型部署的场景或仅仅需要部署一些非核心集群进行开发/测试，那么选择 <code>k3s</code> 则是性价比更高的选择。</li><li>在单个 <code>master</code> 的 <code>k3s</code> 中，默认使用的是 <code>SQLite</code> 数据库存储数据的，这对于小型数据库十分友好，但是如果遭受重击，那么 <code>SQLite</code> 将成为主要痛点。但是，<code>Kubernetes</code> 控制平面中发生的更改更多是与频繁更新部署、调度 <code>Pod</code> 等有关，因此对于小型开发/测试集群而言，数据库不会造成太大负载。</li></ul></li></ul><p>当然如果想学习 <code>k8s</code>，而又不想折腾 <code>k8s</code> 的繁琐安装部署，完全可以使用 <code>k3s</code> 代替 <code>k8s</code>，<code>k3s</code> 包含了 <code>k8s</code> 的所有基础功能，而 <code>k8s</code> 附加功能其实大多数情况也用不到。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This won't take long ...</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check for Ready node, takes maybe 30 seconds</span></span><br><span class="line">$ k3s kubectl get node</span><br></pre></td></tr></table></figure><h2><span id="2-项目架构">2. 项目架构</span></h2><blockquote><p><strong>下图是官方网站上面提供的其项目架构的图示！</strong></p></blockquote><p><code>k3s</code> 安装包中已经包含了 <code>containerd</code>、<code>Flannel</code>、<code>CoreDNS</code> 组件，非常方便地一键式安装，不需要额外安装 <code>Docker</code>、<code>Flannel</code> 等组件。</p><ul><li><strong>Architecture</strong></li></ul><p><img src="https://img.hi-linux.com/staticfile/kubernetes-k3s-tool-1-2021-01-25-HvdtLG.png" alt="k3s加速k8s集群学习"></p><ul><li><strong>Single-server Setup with an Embedded DB</strong></li></ul><p><img src="https://img.hi-linux.com/staticfile/kubernetes-k3s-tool-2-2021-01-25-aYogJb.png" alt="k3s加速k8s集群学习"></p><ul><li><strong>High-Availability K3s Server with an External DB</strong></li></ul><p><img src="https://img.hi-linux.com/staticfile/kubernetes-k3s-tool-3-20210125124058021-2021-01-25-VaTr6m.png" alt="k3s加速k8s集群学习"></p><ul><li><strong>Fixed Registration Address for Agent Nodes</strong></li></ul><p><img src="https://img.hi-linux.com/staticfile/kubernetes-k3s-tool-4-20210125124110871-2021-01-25-GnyXwG.svg" alt="k3s加速k8s集群学习"></p><h2><span id="3-安装方式">3. 安装方式</span></h2><blockquote><p><strong>安装，原来如此简单！</strong></p></blockquote><ul><li><strong>[1] 快速使用 =&gt; 使用安装脚本</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部署一套k3s单节点环境(all in one)</span></span><br><span class="line"><span class="comment"># 安装脚本可以将k3s注册到systemd或openrc里面并其实作为服务运行</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | sh -</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装完成之后即可执行对应命令</span></span><br><span class="line"><span class="comment"># kubeconfig配置文件/etc/rancher/k3s/k3s.yaml</span></span><br><span class="line"><span class="comment"># kubectl、crictl、k3s-killall.sh、k3s-uninstall.sh</span></span><br><span class="line">$ sudo kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加更多的node节点</span></span><br><span class="line"><span class="comment"># K3S_URL: 为api-server服务的URL地址</span></span><br><span class="line"><span class="comment"># k3S_TOKEN: 为node注册token字符串</span></span><br><span class="line"><span class="comment"># K3S_TOKEN: 在master节点的/var/lib/rancher/k3s/server/node-token路径下</span></span><br><span class="line">$ curl -sfL https://get.k3s.io | K3S_URL=https://myserver:6443 K3S_TOKEN=XXX sh -</span><br></pre></td></tr></table></figure><ul><li><strong>[2] 源码安装 =&gt; 使用二进制包</strong></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 下载k3s二进制包</span></span><br><span class="line">https://github.com/rancher/k3s/releases/latest</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行master节点服务(/etc/rancher/k3s/k3s.yaml)</span></span><br><span class="line">$ sudo k3s server &amp;</span><br><span class="line">$ sudo k3s kubectl get nodes</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在另一台机器添加节点信息到主节点</span></span><br><span class="line">$ sudo k3s agent --server https://myserver:6443 --token <span class="variable">$&#123;NODE_TOKEN&#125;</span></span><br></pre></td></tr></table></figure><h2><span id="4-使用方式">4. 使用方式</span></h2><blockquote><p><strong>像使用 k8s 一样使用 k3s 命令！</strong></p></blockquote><p><code>k3s</code> 安装之后内置了一个 <code>kubectl</code> 的子命令，我们通过执行 <code>k3s kubectl</code> 命令来调用它，其功能和使用方式都和 <code>k8s</code> 的 <code>kubectl</code> 命令是一致。为了我们更加方便的使用，可以设置一个 <code>alias</code> 别名或者创建一个软连接达到命令的无缝使用。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建alias别名</span></span><br><span class="line">$ <span class="built_in">alias</span> kubectl=<span class="string">'k3s kubectl'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建软连接</span></span><br><span class="line">$ ln -sf /usr/bin/kubectl /usr/<span class="built_in">local</span>/bin/k3s</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置kubectl命令补全</span></span><br><span class="line">$ <span class="built_in">source</span> &lt;(kubectl completion bash)</span><br></pre></td></tr></table></figure><p>配置完成之后，就可以使用 <code>kubectl</code> 来操作集群机器了。通过运行如下命令，可以查看 <code>kube-system</code> 名称空间中运行的 <code>pod</code> 列表。我们发现并没有运行 <code>apiserver</code>、<code>scheduler</code>、<code>kube-proxy</code> 以及 <code>flannel</code> 等组件，因为这些都已经内嵌到了 <code>k3s</code> 进程中了。另外 <code>k3s</code> 已经给我们默认部署运行了 <code>traefik ingress</code>、<code>metrics-server</code> 等服务，不需要再额外安装了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看kube-system运行的pod列表</span></span><br><span class="line">$ kubectl get pod -n kube-system</span><br><span class="line">NAME                                      READY   STATUS      RESTARTS   AGE</span><br><span class="line">metrics-server-6d123c7b5-4qppl            1/1     Running     0          70m</span><br><span class="line"><span class="built_in">local</span>-path-provisioner-58f123bdfd-8l4hn   1/1     Running     0          70m</span><br><span class="line">helm-install-traefik-pltbs                1/1     Running     0          70m</span><br><span class="line">coredns-6c62348b64-b9qcl                  1/1     Running     0          70m</span><br><span class="line">svclb-traefik-223g2                       2/2     Running     0          70m</span><br><span class="line">traefik-7b81234c8-xk237                   1/1     Running     0          70m</span><br></pre></td></tr></table></figure><p><code>k3s</code> 默认没有使用 <code>Docker</code> 作为容器的运行环境，而是使用了内置的 <code>contained</code>，可以使用 <code>crictl</code> 子命令与 <code>CRI</code> 进行交互。当然，我们也可以采用创建 <code>alias</code> 别名的方式达到命令的无缝使用。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建alias别名</span></span><br><span class="line">$ <span class="built_in">alias</span> docker=<span class="string">'k3s crictl'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 配置docker命令补全</span></span><br><span class="line">$ <span class="built_in">source</span> &lt;(docker completion)</span><br><span class="line">$ complete -F _cli_bash_autocomplete docker</span><br></pre></td></tr></table></figure><p>这样的话，我们就可以使用 <code>docker</code> 命令来查看机器上运行的容器了。我们发现下面命令输出中，多了 <code>ATTEMPT</code> 以及 <code>POD ID</code> 这样的字段，这是 <code>CRI</code> 所特有的，而真正的 <code>docker</code> 命令并没有。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过docker查看运行的容器</span></span><br><span class="line">$ docker  ps</span><br><span class="line">CONTAINER    IMAGE      CREATED    STATE      NAME              ATTEMPT    POD ID</span><br><span class="line">d8a...5      aa7...1    1min       Running    traefik           0          799...c</span><br><span class="line">1ec...f      897...f    1min       Running    lb-port-443       0          457...d</span><br><span class="line">021...1      897...f    1min       Running    lb-port-80        0          407...d</span><br><span class="line">089...0      c4d...b    1min       Running    coredns           0          423...d</span><br><span class="line">ac0...0      9dd...1    1min       Running    metrics-server    0          f6f...6</span><br></pre></td></tr></table></figure><p>安装和配置完成服务之后，还有下面只是是需要我们知道的(配合上<code>k9s</code>效果更佳！)：</p><ul><li>网络<ul><li>因为 <code>k3s</code> 已经内置了 <code>Traefik</code> 组件，不需要再单独安装 <code>ingress controller</code> 了，直接创建 <code>Ingress</code> 即可。其中 <code>192.168.xxx.xxx</code> 为 <code>master</code> 节点的 <code>IP</code>，由于我们没有 <code>DNS</code> 解析，因此可以通过配置 <code>/etc/hosts</code> 文件进行静态配置，之后就可以通过域名来访问我们的服务了。</li></ul></li><li>网络<ul><li>因为 <code>k3s</code> 已经内置了 <code>Flannel</code> 网络插件，默认使用 <code>VXLAN</code> 后端，默认 <code>IP</code> 段为 <code>10.42.0.0/16</code>。内置的 <code>Flannel</code> 除了 <code>VXLAN</code> 还支持 <code>ipsec</code>、<code>host-gw</code> 以及 <code>wireguard</code>。当然除了默认的 <code>Flannel</code>，<code>k3s</code> 还支持其他 <code>CNI</code>，如 <code>Canal</code>、<code>Calico</code> 等。</li></ul></li><li>存储<ul><li><code>k3s</code> 删除了 <code>k8s</code> 内置 <code>cloud provider</code> 以及 <code>storage</code> 插件，内置了 <code>Local Path Provider</code> 来提供存储。而内置 <code>local path</code> 存储，只能单机使用，不支持跨主机使用，也不支持存储的高可用。可以通过使用外部的存储插件解决 k3s 存储问题，比如 <code>Longhorn</code> 云原生分布式块存储系统。</li></ul></li></ul><p><img src="https://img.hi-linux.com/staticfile/kubernetes-k3s-tool-6-2021-01-25-bNK40D.png" alt="k3s加速k8s集群学习"></p><p><img src="https://img.hi-linux.com/staticfile/kubernetes-k3s-tool-5-20210125124122545-2021-01-25-NfTdh9.png" alt="k3s加速k8s集群学习"></p><h2><span id="5-参考链接">5. 参考链接</span></h2><blockquote><p><strong>送人玫瑰，手有余香！</strong></p></blockquote><ul><li><a href="https://rancher.com/docs/k3s/latest/en/" target="_blank" rel="noopener">K3s - Lightweight Kubernetes</a></li><li><a href="https://www.infoq.cn/article/0c7viUfLrxOZeh7qlRBT" target="_blank" rel="noopener">轻量级 Kubernetes k3s 初探</a></li><li><a href="https://juejin.im/post/5dbf8e73f265da4d25054fX76" target="_blank" rel="noopener">K8s 还是 k3s？This is a question</a></li></ul><blockquote><p>本文转载自：「 Escape 的博客 」，原文：<a href="https://tinyurl.com/yyhuczrr" target="_blank" rel="noopener">https://tinyurl.com/yyhuczrr</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;k3s: 带你尝鲜史上最轻量 Kubernetes 发行版!&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我们都知道 &lt;code&gt;Kubernetes&lt;/code&gt; 是一个容器编排平台，可以用来管理我们的容器集群。但是如果我们只是作为学习来使用的话，&lt;code&gt;Kubernetes&lt;/code&gt; 未免有些太重了，有些人的本机估计都没有办法来运行完整的 &lt;code&gt;3&lt;/code&gt; 实例(一个 &lt;code&gt;master&lt;/code&gt;，两个 &lt;code&gt;agent&lt;/code&gt;)的集群环境。虽然网上也有使用 &lt;code&gt;vagrant&lt;/code&gt; 和 &lt;code&gt;machine&lt;/code&gt; 的部署方式，但是使用和配置还是比较复杂的。而 &lt;a href=&quot;https://k3s.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;&lt;code&gt;k3s&lt;/code&gt;&lt;/a&gt; 就是为了解决上述问题，应运而生的。&lt;/p&gt;
&lt;h2 id=&quot;1-项目介绍&quot;&gt;1. 项目介绍&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;我们首先需要了解该项目的适用场景和功能特点！&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;了解或使用过 &lt;code&gt;Kubernetes&lt;/code&gt; 的用户来说，肯定都听说过 &lt;code&gt;Rancher&lt;/code&gt; 这个开源产品，其同样也是一款开源的企业级 &lt;code&gt;Kubernetes&lt;/code&gt; 管理平台，可以极为出色地安装和管理&lt;code&gt;Kubernetes&lt;/code&gt; 集群。而 &lt;code&gt;k3s&lt;/code&gt; 这款轻量级的 &lt;code&gt;Kubernetes&lt;/code&gt; 发行版，也是该公司创建和维护的。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;k3s&lt;/code&gt; 同样还是一款完全通过 &lt;code&gt;CNCF&lt;/code&gt; 认证的 &lt;code&gt;Kubernetes&lt;/code&gt; 发行版，这意味着我们可以通过编写 &lt;code&gt;YAML&lt;/code&gt; 来对完整版的 &lt;code&gt;Kubernetes&lt;/code&gt; 进行操作，并且它们也将适用于 &lt;code&gt;k3s&lt;/code&gt; 集群。并且，其完全实现了 &lt;code&gt;Kubernetes&lt;/code&gt; 提供的所有 &lt;code&gt;API&lt;/code&gt; 接口，我们可以自由的通过接口来操作 &lt;code&gt;Kubernetes&lt;/code&gt; 了。创建 &lt;code&gt;k3s&lt;/code&gt; 项目的主旨是为了打造一个非常非常轻量级的 &lt;code&gt;Kubernetes&lt;/code&gt; 发行版，主要适用于下面这些方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Edge&lt;/li&gt;
&lt;li&gt;IoT&lt;/li&gt;
&lt;li&gt;CI&lt;/li&gt;
&lt;li&gt;Development&lt;/li&gt;
&lt;li&gt;ARM&lt;/li&gt;
&lt;li&gt;Embedding K8s&lt;/li&gt;
&lt;li&gt;Situations where a PhD in K8s clusterology is infeasible&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
      <category term="K3s" scheme="https://www.hi-linux.com/tags/K3s/"/>
    
  </entry>
  
  <entry>
    <title>使用 etcdadm 三分钟内快速搭建一个生产级别的高可用 etcd 集群</title>
    <link href="https://www.hi-linux.com/posts/28648.html"/>
    <id>https://www.hi-linux.com/posts/28648.html</id>
    <published>2021-01-25T01:00:00.000Z</published>
    <updated>2021-01-25T04:35:10.264Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="介绍">介绍</span></h2><p>在搭建 Kubernetes 集群的过程中首先要搞定 Etcd 集群，虽然说 kubeadm 工具已经提供了默认和 master 节点绑定的 Etcd 集群自动搭建方式，但是我个人一直是手动将 Etcd 集群搭建在宿主机；<strong>因为这个玩意太重要了，毫不夸张的说 kubernetes 所有组件崩溃我们都能在一定时间以后排查问题恢复，但是一旦 Etcd 集群没了那么 Kubernetes 集群也就真没了。</strong></p><p>在很久以前我创建了 <a href="https://github.com/Gozap/edep" target="_blank" rel="noopener">edep</a> 工具来实现 Etcd 集群的辅助部署，再后来由于我们的底层系统偶合了 Ubuntu，所以创建了 <a href="https://github.com/mritd/etcd-deb" target="_blank" rel="noopener">etcd-deb</a> 项目来自动打 deb 包来直接安装；最近逛了一下 Kubernetes 的相关项目，发现跟我的 edep 差不多的项目 <a href="https://github.com/kubernetes-sigs/etcdadm" target="_blank" rel="noopener">etcdadm</a>，试了一下 “真香”。</p><h2><span id="安装">安装</span></h2><p><a href="https://github.com/kubernetes-sigs/etcdadm" target="_blank" rel="noopener">etcdadm</a> 项目是使用 go 编写的，所以很明显只有一个二进制下载下来就能用:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;github.com&#x2F;kubernetes-sigs&#x2F;etcdadm&#x2F;releases&#x2F;download&#x2F;v0.1.3&#x2F;etcdadm-linux-amd64</span><br><span class="line">chmod +x etcdadm-linux-amd64</span><br></pre></td></tr></table></figure><a id="more"></a><h2><span id="使用">使用</span></h2><h3><span id="31-启动引导节点">3.1、启动引导节点</span></h3><p>类似 kubeadm 一样，etcdadm 也是先启动第一个节点，然后后续节点直接 join 即可；第一个节点启动只需要执行 <code>etcdadm init</code> 命令即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">k1.node ➜  ~ .&#x2F;etcdadm-linux-amd64 init</span><br><span class="line">INFO[0000] [install] extracting etcd archive &#x2F;var&#x2F;cache&#x2F;etcdadm&#x2F;etcd&#x2F;v3.3.8&#x2F;etcd-v3.3.8-linux-amd64.tar.gz to &#x2F;tmp&#x2F;etcd664686683</span><br><span class="line">INFO[0001] [install] verifying etcd 3.3.8 is installed in &#x2F;opt&#x2F;bin&#x2F;</span><br><span class="line">INFO[0001] [certificates] creating PKI assets</span><br><span class="line">INFO[0001] creating a self signed etcd CA certificate and key files</span><br><span class="line">[certificates] Generated ca certificate and key.</span><br><span class="line">INFO[0001] creating a new server certificate and key files for etcd</span><br><span class="line">[certificates] Generated server certificate and key.</span><br><span class="line">[certificates] server serving cert is signed for DNS names [k1.node] and IPs [127.0.0.1 172.16.10.21]</span><br><span class="line">INFO[0002] creating a new certificate and key files for etcd peering</span><br><span class="line">[certificates] Generated peer certificate and key.</span><br><span class="line">[certificates] peer serving cert is signed for DNS names [k1.node] and IPs [172.16.10.21]</span><br><span class="line">INFO[0002] creating a new client certificate for the etcdctl</span><br><span class="line">[certificates] Generated etcdctl-etcd-client certificate and key.</span><br><span class="line">INFO[0002] creating a new client certificate for the apiserver calling etcd</span><br><span class="line">[certificates] Generated apiserver-etcd-client certificate and key.</span><br><span class="line">[certificates] valid certificates and keys now exist in &quot;&#x2F;etc&#x2F;etcd&#x2F;pki&quot;</span><br><span class="line">INFO[0006] [health] Checking local etcd endpoint health</span><br><span class="line">INFO[0006] [health] Local etcd endpoint is healthy</span><br><span class="line">INFO[0006] To add another member to the cluster, copy the CA cert&#x2F;key to its certificate dir and run:</span><br><span class="line">INFO[0006]      etcdadm join https:&#x2F;&#x2F;172.16.10.21:2379</span><br></pre></td></tr></table></figure><p>从命令行输出可以看到不同阶段 etcdadm 的相关日志输出；在 <code>init</code> 命令时可以指定一些特定参数来覆盖默认行为，比如版本号、安装目录等:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">k1.node ➜  ~ .&#x2F;etcdadm-linux-amd64 init --help</span><br><span class="line">Initialize a new etcd cluster</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  etcdadm init [flags]</span><br><span class="line"></span><br><span class="line">Flags:</span><br><span class="line">      --certs-dir string                    certificates directory (default &quot;&#x2F;etc&#x2F;etcd&#x2F;pki&quot;)</span><br><span class="line">      --disk-priorities stringArray         Setting etcd disk priority (default [Nice&#x3D;-10,IOSchedulingClass&#x3D;best-effort,IOSchedulingPriority&#x3D;2])</span><br><span class="line">      --download-connect-timeout duration   Maximum time in seconds that you allow the connection to the server to take. (default 10s)</span><br><span class="line">  -h, --help                                help for init</span><br><span class="line">      --install-dir string                  install directory (default &quot;&#x2F;opt&#x2F;bin&#x2F;&quot;)</span><br><span class="line">      --name string                         etcd member name</span><br><span class="line">      --release-url string                  URL used to download etcd (default &quot;https:&#x2F;&#x2F;github.com&#x2F;coreos&#x2F;etcd&#x2F;releases&#x2F;download&quot;)</span><br><span class="line">      --server-cert-extra-sans strings      optional extra Subject Alternative Names for the etcd server signing cert, can be multiple comma separated DNS names or IPs</span><br><span class="line">      --skip-hash-check                     Ignore snapshot integrity hash value (required if copied from data directory)</span><br><span class="line">      --snapshot string                     Etcd v3 snapshot file used to initialize member</span><br><span class="line">      --version string                      etcd version (default &quot;3.3.8&quot;)</span><br><span class="line"></span><br><span class="line">Global Flags:</span><br><span class="line">  -l, --log-level string   set log level for output, permitted values debug, info, warn, error, fatal and panic (default &quot;info&quot;)</span><br></pre></td></tr></table></figure><h3><span id="32-其他节点加入">3.2、其他节点加入</span></h3><p>在首个节点启动完成后，将集群 ca 证书复制到其他节点然后执行 <code>etcdadm join ENDPOINT_ADDRESS</code> 即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"># 复制 ca 证书</span><br><span class="line">k1.node ➜  ~ rsync -avR &#x2F;etc&#x2F;etcd&#x2F;pki&#x2F;ca.* 172.16.10.22:&#x2F;</span><br><span class="line">root@172.16.10.22&#39;s password:</span><br><span class="line">sending incremental file list</span><br><span class="line">&#x2F;etc&#x2F;etcd&#x2F;</span><br><span class="line">&#x2F;etc&#x2F;etcd&#x2F;pki&#x2F;</span><br><span class="line">&#x2F;etc&#x2F;etcd&#x2F;pki&#x2F;ca.crt</span><br><span class="line">&#x2F;etc&#x2F;etcd&#x2F;pki&#x2F;ca.key</span><br><span class="line"></span><br><span class="line">sent 2,932 bytes  received 67 bytes  856.86 bytes&#x2F;sec</span><br><span class="line">total size is 2,684  speedup is 0.89</span><br><span class="line"></span><br><span class="line"># 执行 join</span><br><span class="line">k2.node ➜  ~ .&#x2F;etcdadm-linux-amd64 join https:&#x2F;&#x2F;172.16.10.21:2379</span><br><span class="line">INFO[0000] [certificates] creating PKI assets</span><br><span class="line">INFO[0000] creating a self signed etcd CA certificate and key files</span><br><span class="line">[certificates] Using the existing ca certificate and key.</span><br><span class="line">INFO[0000] creating a new server certificate and key files for etcd</span><br><span class="line">[certificates] Generated server certificate and key.</span><br><span class="line">[certificates] server serving cert is signed for DNS names [k2.node] and IPs [172.16.10.22 127.0.0.1]</span><br><span class="line">INFO[0000] creating a new certificate and key files for etcd peering</span><br><span class="line">[certificates] Generated peer certificate and key.</span><br><span class="line">[certificates] peer serving cert is signed for DNS names [k2.node] and IPs [172.16.10.22]</span><br><span class="line">INFO[0000] creating a new client certificate for the etcdctl</span><br><span class="line">[certificates] Generated etcdctl-etcd-client certificate and key.</span><br><span class="line">INFO[0001] creating a new client certificate for the apiserver calling etcd</span><br><span class="line">[certificates] Generated apiserver-etcd-client certificate and key.</span><br><span class="line">[certificates] valid certificates and keys now exist in &quot;&#x2F;etc&#x2F;etcd&#x2F;pki&quot;</span><br><span class="line">INFO[0001] [membership] Checking if this member was added</span><br><span class="line">INFO[0001] [membership] Member was not added</span><br><span class="line">INFO[0001] Removing existing data dir &quot;&#x2F;var&#x2F;lib&#x2F;etcd&quot;</span><br><span class="line">INFO[0001] [membership] Adding member</span><br><span class="line">INFO[0001] [membership] Checking if member was started</span><br><span class="line">INFO[0001] [membership] Member was not started</span><br><span class="line">INFO[0001] [membership] Removing existing data dir &quot;&#x2F;var&#x2F;lib&#x2F;etcd&quot;</span><br><span class="line">INFO[0001] [install] extracting etcd archive &#x2F;var&#x2F;cache&#x2F;etcdadm&#x2F;etcd&#x2F;v3.3.8&#x2F;etcd-v3.3.8-linux-amd64.tar.gz to &#x2F;tmp&#x2F;etcd315786364</span><br><span class="line">INFO[0003] [install] verifying etcd 3.3.8 is installed in &#x2F;opt&#x2F;bin&#x2F;</span><br><span class="line">INFO[0006] [health] Checking local etcd endpoint health</span><br><span class="line">INFO[0006] [health] Local etcd endpoint is healthy</span><br></pre></td></tr></table></figure><h2><span id="细节分析">细节分析</span></h2><h3><span id="41-默认配置">4.1、默认配置</span></h3><p>在目前 etcdadm 尚未支持配置文件，目前所有默认配置存放在 <a href="https://github.com/kubernetes-sigs/etcdadm/blob/master/constants/constants.go#L22" target="_blank" rel="noopener">constants.go</a> 中，这里面包含了默认安装位置、systemd 配置、环境变量配置等，限于篇幅请自行查看代码；下面简单介绍一些一些刚须的配置:</p><h4><span id="411-etcdctl">4.1.1、etcdctl</span></h4><p>etcdctl 默认安装在 <code>/opt/bin</code> 目录下，同时你会发现该目录下还存在一个 <code>etcdctl.sh</code> 脚本，<strong>这个脚本将会自动读取 etcdctl 配置文件(<code>/etc/etcd/etcdctl.env</code>)，所以推荐使用这个脚本来替代 etcdctl 命令。</strong></p><h4><span id="412-数据目录">4.1.2、数据目录</span></h4><p>默认的数据目录存储在 <code>/var/lib/etcd</code> 目录，目前 etcdadm 尚未提供任何可配置方式，当然你可以自己改源码。</p><h4><span id="423-配置文件">4.2.3、配置文件</span></h4><p>配置文件总共有两个，一个是 <code>/etc/etcd/etcdctl.env</code> 用于 <code>/opt/bin/etcdctl.sh</code> 读取；另一个是 <code>/etc/etcd/etcd.env</code> 用于 systemd 读取并启动 etcd server。</p><h3><span id="42-join-流程">4.2、Join 流程</span></h3><blockquote><p>其实很久以前由于我自己部署方式导致了我一直以来理解的一个错误，我一直以为 etcd server 证书要包含所有 server 地址，当然这个想法是怎么来的我也不知道，但是当我看了以下 Join 操作源码以后突然意识到 “为什么要包含所有？包含当前 server 不就行了么。”；当然对于 HTTPS 证书的理解一直是明白的，但是很奇怪就是不知道怎么就产生了这个想法(哈哈，我自己都觉的不可思议)…</p></blockquote><ul><li>由于预先拷贝了 ca 证书，所以 join 开始前 etcdadm 使用这个 ca 证书会签发自己需要的所有证书。</li><li>接下来 etcdadmin 通过 etcdctl-etcd-client 证书创建 client，然后调用 <code>MemberAdd</code> 添加新集群</li><li>最后老套路下载安装+启动就完成了</li></ul><h3><span id="43-目前不足">4.3、目前不足</span></h3><p>目前 etcdadm 虽然已经基本生产可用，但是仍有些不足的地方:</p><ul><li>不支持配置文件，很多东西无法定制</li><li>join 加入集群是在内部 api 完成，并未持久化到物理配置文件，后续重建可能忘记节点 ip</li><li>集群证书目前不支持自动续期，默认证书为 1 年很容易过期</li><li>下载动作调用了系统命令(curl)依赖性有点强</li><li>日志格式有点不友好，比如 level 和日期</li></ul><blockquote><p>本文转载自：「 Bleem 」，原文：<a href="https://tinyurl.com/y64xjpvf" target="_blank" rel="noopener">https://tinyurl.com/y64xjpvf</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;在搭建 Kubernetes 集群的过程中首先要搞定 Etcd 集群，虽然说 kubeadm 工具已经提供了默认和 master 节点绑定的 Etcd 集群自动搭建方式，但是我个人一直是手动将 Etcd 集群搭建在宿主机；&lt;strong&gt;因为这个玩意太重要了，毫不夸张的说 kubernetes 所有组件崩溃我们都能在一定时间以后排查问题恢复，但是一旦 Etcd 集群没了那么 Kubernetes 集群也就真没了。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在很久以前我创建了 &lt;a href=&quot;https://github.com/Gozap/edep&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;edep&lt;/a&gt; 工具来实现 Etcd 集群的辅助部署，再后来由于我们的底层系统偶合了 Ubuntu，所以创建了 &lt;a href=&quot;https://github.com/mritd/etcd-deb&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;etcd-deb&lt;/a&gt; 项目来自动打 deb 包来直接安装；最近逛了一下 Kubernetes 的相关项目，发现跟我的 edep 差不多的项目 &lt;a href=&quot;https://github.com/kubernetes-sigs/etcdadm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;etcdadm&lt;/a&gt;，试了一下 “真香”。&lt;/p&gt;
&lt;h2 id=&quot;安装&quot;&gt;安装&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/kubernetes-sigs/etcdadm&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;etcdadm&lt;/a&gt; 项目是使用 go 编写的，所以很明显只有一个二进制下载下来就能用:&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;wget https:&amp;#x2F;&amp;#x2F;github.com&amp;#x2F;kubernetes-sigs&amp;#x2F;etcdadm&amp;#x2F;releases&amp;#x2F;download&amp;#x2F;v0.1.3&amp;#x2F;etcdadm-linux-amd64&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;chmod +x etcdadm-linux-amd64&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Etcd" scheme="https://www.hi-linux.com/categories/etcd/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Etcd" scheme="https://www.hi-linux.com/tags/Etcd/"/>
    
  </entry>
  
  <entry>
    <title>重磅 | 微众银行开源一款生产级云原生容器平台 Dockin</title>
    <link href="https://www.hi-linux.com/posts/64715.html"/>
    <id>https://www.hi-linux.com/posts/64715.html</id>
    <published>2021-01-20T01:00:00.000Z</published>
    <updated>2021-01-22T03:22:55.539Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>今天给大家推荐的这个开源项目是来自于微众银行开源的一个重量级的生产级云原生容器平台 <strong>Dockin</strong>。</p><p><strong>Dockin</strong> 是微众银行开源的生产级容器平台，提供了一整套私有云容器化的落地方案。涵盖 <strong>Kubernetes</strong> 集群管理、应用管理、网络、运维工具、开放 API 等组件，用户可以自由搭配使用，定制自己的容器平台。开源版本从他们生产环境中剥离出来，经过了金融级生产环境的严格验证，是私有化部署的较好方案。</p><blockquote><p>开源项目地址：<strong><a href="https://github.com/WeBankFinTech/Dockin" target="_blank" rel="noopener">https://github.com/WeBankFinTech/Dockin</a></strong></p></blockquote><a id="more"></a><p>这个开源项目着力于生产级、高可用、安全性、云原生一体化、可定制，适用于传统 IT 向云原生转型。</p><p><img src="https://img.hi-linux.com/staticfile/640-2021-01-19-QkXPga.jpg" alt="图片"></p><p>云原生下容器化的价值：</p><p><img src="https://img.hi-linux.com/staticfile/640-20210119134445965-2021-01-19-c9uFZC.jpg" alt="图片"></p><p>目前，已经开源的组件有四个：</p><ul><li>Dockin-CNI</li></ul><p>一款支持固定 IP 的网络插件（Dockin-CNI） 基于 kubernetes 的 CNI 网络插件，支持固定 IP，支持多网卡。</p><p>地址：<a href="https://github.com/WeBankFinTech/Dockin-CNI" target="_blank" rel="noopener">https://github.com/WeBankFinTech/Dockin-CNI</a></p><ul><li>Dockin-Ops</li></ul><p>一套安全的运维编排服务（Dockin-Ops） Dockin 运维管理系统是安全的运维管理服务，优化 exec 执行性能，支持命令权限管理。</p><p>地址：<a href="https://github.com/WeBankFinTech/Dockin-Ops" target="_blank" rel="noopener">https://github.com/WeBankFinTech/Dockin-Ops</a></p><ul><li>Dockin-Installer</li></ul><p>一套离线 Kubernetes 集群安装器（Dockin-Installer） Dockin 平台安装器，快速部署 Docker、高可用 kubernetes 集群、ETCD 集群，生产级参数调优。全离线安装，不需要连外网，支持十年的证书续订、ETCD 备份恢复。</p><p>地址：<a href="https://github.com/WeBankFinTech/Dockin-Installer" target="_blank" rel="noopener">https://github.com/WeBankFinTech/Dockin-Installer</a></p><ul><li>Dockin-RM</li></ul><p>一款应用资源管理系统（Dockin-RM） Dockin 容器项目资源管理器，是应用定义和容器实例管理的核心模块，提供容器分配、回收、查询等功能。</p><p>地址：<a href="https://github.com/WeBankFinTech/Dockin-RM" target="_blank" rel="noopener">https://github.com/WeBankFinTech/Dockin-RM</a></p><p><img src="https://img.hi-linux.com/staticfile/640-20210119134458014-2021-01-19-AN5L3k.jpg" alt="图片"></p><p>后续开源的组件还有：</p><ul><li>静态应用管理方案</li><li>镜像管理系统</li><li>平台管理台</li><li>高可用监控服务</li><li>统一 API 网关</li><li>智能调度系统</li><li>通用 Operator 套件</li></ul><p>各位 Kubernets YAML 工程师，这个开源项目是不是很香呢？</p></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天给大家推荐的这个开源项目是来自于微众银行开源的一个重量级的生产级云原生容器平台 &lt;strong&gt;Dockin&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dockin&lt;/strong&gt; 是微众银行开源的生产级容器平台，提供了一整套私有云容器化的落地方案。涵盖 &lt;strong&gt;Kubernetes&lt;/strong&gt; 集群管理、应用管理、网络、运维工具、开放 API 等组件，用户可以自由搭配使用，定制自己的容器平台。开源版本从他们生产环境中剥离出来，经过了金融级生产环境的严格验证，是私有化部署的较好方案。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;开源项目地址：&lt;strong&gt;&lt;a href=&quot;https://github.com/WeBankFinTech/Dockin&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/WeBankFinTech/Dockin&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
      <category term="Dockin" scheme="https://www.hi-linux.com/tags/Dockin/"/>
    
  </entry>
  
  <entry>
    <title>Percona MySQL Server 部署指南</title>
    <link href="https://www.hi-linux.com/posts/8700.html"/>
    <id>https://www.hi-linux.com/posts/8700.html</id>
    <published>2021-01-18T01:00:00.000Z</published>
    <updated>2021-01-18T09:47:24.722Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="一-版本信息">一、版本信息</span></h2><p>目前采用 MySQL fork 版本 Percona Server 5.7.28，监控方面选择 Percona Monitoring and Management 2.1.0，对应监控 Client 版本为 2.1.0</p><h2><span id="二-percona-server-安装">二、Percona Server 安装</span></h2><p>为保证兼容以及稳定性，MySQL 宿主机系统选择 CentOS 7，Percona Server 安装方式为 rpm 包，安装后由 Systemd 守护</p><h3><span id="21-下载安装包">2.1、下载安装包</span></h3><p>安装包下载地址为 <a href="https://www.percona.com/downloads/Percona-Server-5.7/LATEST/%EF%BC%8C%E4%B8%8B%E8%BD%BD%E6%97%B6%E9%80%89%E6%8B%A9" target="_blank" rel="noopener">https://www.percona.com/downloads/Percona-Server-5.7/LATEST/，下载时选择</a> <code>Download All Packages Together</code>，下载后是所有组件全量的压缩 tar 包。</p><h3><span id="22-安装前准备">2.2、安装前准备</span></h3><p>针对 CentOS 7 系统，安装前升级所有系统组件库，执行 <code>yum update</code> 既可；大部份 <strong>CentOS 7 安装后可能会附带 <code>mariadb-libs</code> 包，这个包会默认创建一些配置文件，导致后面的 Percona Server 无法覆盖它(例如 <code>/etc/my.cnf</code>)，所以安装 Percona Server 之前需要卸载它 <code>yum remove mariadb-libs</code></strong></p><p>针对于数据存储硬盘，目前统一为 SSD 硬盘，挂载点为 <code>/data</code>，挂载方式可以采用 <code>fstab</code>、<code>systemd-mount</code>，分区格式目前采用 <code>xfs</code> 格式。</p><p><strong>SSD 优化有待补充…</strong></p><h3><span id="23-安装-percona-server">2.3、安装 Percona Server</span></h3><p>Percona Server tar 包解压后会有 9 个 rpm 包，实际安装时只需要安装其中 4 个既可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install Percona-Server-client-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-server-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-shared-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-shared-compat-57-5.7.28-31.1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><a id="more"></a><h3><span id="24-安装后调整">2.4、安装后调整</span></h3><h4><span id="241-硬盘调整">2.4.1、硬盘调整</span></h4><p>目前 MySQL 数据会统一存放到 <code>/data</code> 目录下，所以需要将单独的数据盘挂载到 <code>/data</code> 目录；<strong>如果是 SSD 硬盘还需要调整系统 I/O 调度器等其他优化。</strong></p><h4><span id="242-目录预创建">2.4.2、目录预创建</span></h4><p>Percona Server 安装完成后，由于配置调整原因，还会用到一些其他的数据目录，这些目录可以预先创建并授权</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;var&#x2F;log&#x2F;mysql &#x2F;data&#x2F;mysql_tmp</span><br><span class="line">chown -R mysql:mysql &#x2F;var&#x2F;log&#x2F;mysql &#x2F;data&#x2F;mysql_tmp</span><br></pre></td></tr></table></figure><p><code>/var/log/mysql</code> 目录用来存放 MySQL 相关的日志(不包括 binlog)，<code>/data/mysql_tmp</code> 用来存放 MySQL 运行时产生的缓存文件。</p><h4><span id="243-文件描述符调整">2.4.3、文件描述符调整</span></h4><p>由于 rpm 安装的 Percona Server 会采用 Systemd 守护，所以如果想修改文件描述符配置应当调整 Systemd 配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;mysqld.service</span><br><span class="line"></span><br><span class="line"># Sets open_files_limit</span><br><span class="line"># 注意 infinity &#x3D; 65536</span><br><span class="line">LimitCORE &#x3D; infinity</span><br><span class="line">LimitNOFILE &#x3D; infinity</span><br><span class="line">LimitNPROC &#x3D; infinity</span><br></pre></td></tr></table></figure><p>然后执行 <code>systemctl daemon-reload</code> 重载既可。</p><h4><span id="244-配置文件调整">2.4.4、配置文件调整</span></h4><p>Percona Server 安装完成后也会生成 <code>/etc/my.cnf</code> 配置文件，不过不建议直接修改该文件；修改配置文件需要进入到 <code>/etc/percona-server.conf.d/</code> 目录调整相应配置；以下为配置样例(<strong>生产环境 mysqld 配置需要优化调整</strong>)</p><p><strong>mysql.cnf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[mysql]</span><br><span class="line">auto-rehash</span><br><span class="line">default_character_set&#x3D;utf8mb4</span><br></pre></td></tr></table></figure><p><strong>mysqld.cnf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"># Percona Server template configuration</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">#</span><br><span class="line"># Remove leading # and set to the amount of RAM for the most important data</span><br><span class="line"># cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.</span><br><span class="line"># innodb_buffer_pool_size &#x3D; 128M</span><br><span class="line">#</span><br><span class="line"># Remove leading # to turn on a very important data integrity option: logging</span><br><span class="line"># changes to the binary log between backups.</span><br><span class="line"># log_bin</span><br><span class="line">#</span><br><span class="line"># Remove leading # to set options mainly useful for reporting servers.</span><br><span class="line"># The server defaults are faster for transactions and fast SELECTs.</span><br><span class="line"># Adjust sizes as needed, experiment to find the optimal values.</span><br><span class="line"># join_buffer_size &#x3D; 128M</span><br><span class="line"># sort_buffer_size &#x3D; 2M</span><br><span class="line"># read_rnd_buffer_size &#x3D; 2M</span><br><span class="line">port&#x3D;3306</span><br><span class="line">datadir&#x3D;&#x2F;data&#x2F;mysql</span><br><span class="line">socket&#x3D;&#x2F;data&#x2F;mysql&#x2F;mysql.sock</span><br><span class="line">pid_file&#x3D;&#x2F;data&#x2F;mysql&#x2F;mysqld.pid</span><br><span class="line"></span><br><span class="line"># 服务端编码</span><br><span class="line">character_set_server&#x3D;utf8mb4</span><br><span class="line"># 服务端排序</span><br><span class="line">collation_server&#x3D;utf8mb4_general_ci</span><br><span class="line"># 强制使用 utf8mb4 编码集，忽略客户端设置</span><br><span class="line">skip_character_set_client_handshake&#x3D;1</span><br><span class="line"># 日志输出到文件</span><br><span class="line">log_output&#x3D;FILE</span><br><span class="line"># 开启常规日志输出</span><br><span class="line">general_log&#x3D;1</span><br><span class="line"># 常规日志输出文件位置</span><br><span class="line">general_log_file&#x3D;&#x2F;var&#x2F;log&#x2F;mysql&#x2F;mysqld.log</span><br><span class="line"># 错误日志位置</span><br><span class="line">log_error&#x3D;&#x2F;var&#x2F;log&#x2F;mysql&#x2F;mysqld-error.log</span><br><span class="line"># 记录慢查询</span><br><span class="line">slow_query_log&#x3D;1</span><br><span class="line"># 慢查询时间(大于 1s 被视为慢查询)</span><br><span class="line">long_query_time&#x3D;1</span><br><span class="line"># 慢查询日志文件位置</span><br><span class="line">slow_query_log_file&#x3D;&#x2F;var&#x2F;log&#x2F;mysql&#x2F;mysqld-slow.log</span><br><span class="line"># 临时文件位置</span><br><span class="line">tmpdir&#x3D;&#x2F;data&#x2F;mysql_tmp</span><br><span class="line"># 线程池缓存(refs https:&#x2F;&#x2F;my.oschina.net&#x2F;realfighter&#x2F;blog&#x2F;363853)</span><br><span class="line">thread_cache_size&#x3D;30</span><br><span class="line"># The number of open tables for all threads.(refs https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;5.7&#x2F;en&#x2F;server-system-variables.html#sysvar_table_open_cache)</span><br><span class="line">table_open_cache&#x3D;16384</span><br><span class="line"># 文件描述符(此处修改不生效，请修改 systemd service 配置) </span><br><span class="line"># refs https:&#x2F;&#x2F;www.percona.com&#x2F;blog&#x2F;2017&#x2F;10&#x2F;12&#x2F;open_files_limit-mystery&#x2F;</span><br><span class="line"># refs https:&#x2F;&#x2F;www.cnblogs.com&#x2F;wxxjianchi&#x2F;p&#x2F;10370419.html</span><br><span class="line">#open_files_limit&#x3D;65535</span><br><span class="line"># 表定义缓存(5.7 以后自动调整)</span><br><span class="line"># refs https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;5.6&#x2F;en&#x2F;server-system-variables.html#sysvar_table_definition_cache</span><br><span class="line"># refs http:&#x2F;&#x2F;mysql.taobao.org&#x2F;monthly&#x2F;2015&#x2F;08&#x2F;10&#x2F;</span><br><span class="line">#table_definition_cache&#x3D;16384</span><br><span class="line">sort_buffer_size&#x3D;1M</span><br><span class="line">join_buffer_size&#x3D;1M</span><br><span class="line"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><br><span class="line">read_buffer_size&#x3D;1M</span><br><span class="line">read_rnd_buffer_size&#x3D;1M</span><br><span class="line"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><br><span class="line">key_buffer_size&#x3D;32M</span><br><span class="line"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><br><span class="line">bulk_insert_buffer_size&#x3D;16M</span><br><span class="line"># myisam_sort_buffer_size 与 sort_buffer_size 区别请参考(https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;7871027&#x2F;myisam-sort-buffer-size-vs-sort-buffer-size)</span><br><span class="line">myisam_sort_buffer_size&#x3D;64M</span><br><span class="line"># 内部内存临时表大小</span><br><span class="line">tmp_table_size&#x3D;32M</span><br><span class="line"># 用户创建的 MEMORY 表最大大小(tmp_table_size 受此值影响)</span><br><span class="line">max_heap_table_size&#x3D;32M</span><br><span class="line"># 开启查询缓存</span><br><span class="line">query_cache_type&#x3D;1</span><br><span class="line"># 查询缓存大小</span><br><span class="line">query_cache_size&#x3D;32M</span><br><span class="line"># sql mode</span><br><span class="line">sql_mode&#x3D;&#39;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&#39;</span><br><span class="line"></span><br><span class="line">########### Network ###########</span><br><span class="line"># 最大连接数(该参数受到最大文件描述符影响，如果不生效请检查最大文件描述符设置)</span><br><span class="line"># refs https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;39976756&#x2F;the-max-connections-in-mysql-5-7</span><br><span class="line">max_connections&#x3D;1500</span><br><span class="line"># mysql 堆栈内暂存的链接数量</span><br><span class="line"># 当短时间内链接数量超过 max_connections 时，部分链接会存储在堆栈内，存储数量受此参数控制</span><br><span class="line">back_log&#x3D;256</span><br><span class="line"># 最大链接错误，针对于 client 主机，超过此数量的链接错误将会导致 mysql server 针对此主机执行锁定(禁止链接 ERROR 1129 )</span><br><span class="line"># 此错误计数仅在 mysql 链接握手失败才会计算，一般出现问题时都是网络故障</span><br><span class="line"># refs https:&#x2F;&#x2F;www.cnblogs.com&#x2F;kerrycode&#x2F;p&#x2F;8405862.html</span><br><span class="line">max_connect_errors&#x3D;100000</span><br><span class="line"># mysql server 允许的最大数据包大小</span><br><span class="line">max_allowed_packet&#x3D;64M</span><br><span class="line"># 交互式客户端链接超时(30分钟自动断开)</span><br><span class="line">interactive_timeout&#x3D;1800</span><br><span class="line"># 非交互式链接超时时间(10分钟)</span><br><span class="line"># 如果客户端有连接池，则需要协商此参数(refs https:&#x2F;&#x2F;database.51cto.com&#x2F;art&#x2F;201909&#x2F;603519.htm)</span><br><span class="line">wait_timeout&#x3D;600</span><br><span class="line"># 跳过外部文件系统锁定</span><br><span class="line"># If you run multiple servers that use the same database directory (not recommended), </span><br><span class="line"># each server must have external locking enabled.</span><br><span class="line"># refs https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;5.7&#x2F;en&#x2F;external-locking.html</span><br><span class="line">skip_external_locking&#x3D;1</span><br><span class="line"># 跳过链接的域名解析(开启此选项后 mysql 用户授权的 host 方式失效)</span><br><span class="line">skip_name_resolve&#x3D;0</span><br><span class="line"># 禁用主机名缓存，每次都会走 DNS</span><br><span class="line">host_cache_size&#x3D;0</span><br><span class="line"></span><br><span class="line">########### REPL ###########</span><br><span class="line"># 开启 binlog</span><br><span class="line">log_bin&#x3D;mysql-bin</span><br><span class="line"># 作为从库时，同步信息依然写入 binlog，方便此从库再作为其他从库的主库</span><br><span class="line">log_slave_updates&#x3D;1</span><br><span class="line"># server id，默认为 ipv4 地址去除第一段</span><br><span class="line"># eg: 172.16.10.11 &#x3D;&gt; 161011</span><br><span class="line">server_id&#x3D;161011</span><br><span class="line"># 每次次事务 binlog 刷新到磁盘</span><br><span class="line"># refs http:&#x2F;&#x2F;liyangliang.me&#x2F;posts&#x2F;2014&#x2F;03&#x2F;innodb_flush_log_at_trx_commit-and-sync_binlog&#x2F;</span><br><span class="line">sync_binlog&#x3D;100</span><br><span class="line"># binlog 格式(refs https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;33504555)</span><br><span class="line">binlog_format&#x3D;row</span><br><span class="line"># binlog 自动清理时间</span><br><span class="line">expire_logs_days&#x3D;10</span><br><span class="line"># 开启 relay-log，一般作为 slave 时开启</span><br><span class="line">relay_log&#x3D;mysql-replay</span><br><span class="line"># 主从复制时跳过 test 库</span><br><span class="line">replicate_ignore_db&#x3D;test</span><br><span class="line"># 每个 session binlog 缓存</span><br><span class="line">binlog_cache_size&#x3D;4M</span><br><span class="line"># binlog 滚动大小</span><br><span class="line">max_binlog_size&#x3D;1024M</span><br><span class="line"># GTID 相关(refs https:&#x2F;&#x2F;keithlan.github.io&#x2F;2016&#x2F;06&#x2F;23&#x2F;gtid&#x2F;)</span><br><span class="line">#gtid_mode&#x3D;1</span><br><span class="line">#enforce_gtid_consistency&#x3D;1</span><br><span class="line"></span><br><span class="line">########### InnoDB ###########</span><br><span class="line"># 永久表默认存储引擎</span><br><span class="line">default_storage_engine&#x3D;InnoDB</span><br><span class="line"># 系统表空间数据文件大小(初始化为 1G，并且自动增长)</span><br><span class="line">innodb_data_file_path&#x3D;ibdata1:1G:autoextend</span><br><span class="line"># InnoDB 缓存池大小</span><br><span class="line"># innodb_buffer_pool_size 必须等于 innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances，或者是其整数倍</span><br><span class="line"># refs https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;5.7&#x2F;en&#x2F;innodb-buffer-pool-resize.html</span><br><span class="line"># refs https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;60089484</span><br><span class="line">innodb_buffer_pool_size&#x3D;7680M</span><br><span class="line">innodb_buffer_pool_instances&#x3D;10</span><br><span class="line">innodb_buffer_pool_chunk_size&#x3D;128M</span><br><span class="line"># InnoDB 强制恢复(refs https:&#x2F;&#x2F;www.askmaclean.com&#x2F;archives&#x2F;mysql-innodb-innodb_force_recovery.html)</span><br><span class="line">innodb_force_recovery&#x3D;0</span><br><span class="line"># InnoDB buffer 预热(refs http:&#x2F;&#x2F;www.dbhelp.net&#x2F;2017&#x2F;01&#x2F;12&#x2F;mysql-innodb-buffer-pool-warmup.html)</span><br><span class="line">innodb_buffer_pool_dump_at_shutdown&#x3D;1</span><br><span class="line">innodb_buffer_pool_load_at_startup&#x3D;1</span><br><span class="line"># InnoDB 日志组中的日志文件数</span><br><span class="line">innodb_log_files_in_group&#x3D;2</span><br><span class="line"># InnoDB redo 日志大小</span><br><span class="line"># refs https:&#x2F;&#x2F;www.percona.com&#x2F;blog&#x2F;2017&#x2F;10&#x2F;18&#x2F;chose-mysql-innodb_log_file_size&#x2F;</span><br><span class="line">innodb_log_file_size&#x3D;256MB</span><br><span class="line"># 缓存还未提交的事务的缓冲区大小</span><br><span class="line">innodb_log_buffer_size&#x3D;16M</span><br><span class="line"># InnoDB 在事务提交后的日志写入频率</span><br><span class="line"># refs http:&#x2F;&#x2F;liyangliang.me&#x2F;posts&#x2F;2014&#x2F;03&#x2F;innodb_flush_log_at_trx_commit-and-sync_binlog&#x2F;</span><br><span class="line">innodb_flush_log_at_trx_commit&#x3D;2</span><br><span class="line"># InnoDB DML 操作行级锁等待时间</span><br><span class="line"># 超时返回 ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction</span><br><span class="line"># refs https:&#x2F;&#x2F;ningyu1.github.io&#x2F;site&#x2F;post&#x2F;75-mysql-lock-wait-timeout-exceeded&#x2F;</span><br><span class="line">innodb_lock_wait_timeout&#x3D;30</span><br><span class="line"># InnoDB 行级锁超时是否回滚整个事务，默认为 OFF 仅回滚上一条语句</span><br><span class="line"># 此时应用程序可以接受到错误后选择是否继续提交事务(并没有违反 ACID 原子性)</span><br><span class="line"># refs https:&#x2F;&#x2F;www.cnblogs.com&#x2F;hustcat&#x2F;archive&#x2F;2012&#x2F;11&#x2F;18&#x2F;2775487.html</span><br><span class="line">#innodb_rollback_on_timeout&#x3D;ON</span><br><span class="line"># InnoDB 数据写入磁盘的方式，具体见博客文章</span><br><span class="line"># refs https:&#x2F;&#x2F;www.cnblogs.com&#x2F;gomysql&#x2F;p&#x2F;3595806.html</span><br><span class="line">innodb_flush_method&#x3D;O_DIRECT</span><br><span class="line"># InnoDB 缓冲池脏页刷新百分比</span><br><span class="line"># refs https:&#x2F;&#x2F;dbarobin.com&#x2F;2015&#x2F;08&#x2F;29&#x2F;mysql-optimization-under-ssd</span><br><span class="line">innodb_max_dirty_pages_pct&#x3D;50</span><br><span class="line"># InnoDB 每秒执行的写IO量</span><br><span class="line"># refs https:&#x2F;&#x2F;www.centos.bz&#x2F;2016&#x2F;11&#x2F;mysql-performance-tuning-15-config-item&#x2F;#10.INNODB_IO_CAPACITY,%20INNODB_IO_CAPACITY_MAX</span><br><span class="line">innodb_io_capacity&#x3D;500</span><br><span class="line">innodb_io_capacity_max&#x3D;1000</span><br><span class="line"># 请求并发 InnoDB 线程数</span><br><span class="line"># refs https:&#x2F;&#x2F;www.cnblogs.com&#x2F;xinysu&#x2F;p&#x2F;6439715.html#_lab2_1_0</span><br><span class="line">innodb_thread_concurrency&#x3D;60</span><br><span class="line"># 再使用多个 InnoDB 表空间时，允许打开的最大 &quot;.ibd&quot; 文件个数，不设置默认 300，</span><br><span class="line"># 并且取与 table_open_cache 相比较大的一个，此选项独立于 open_files_limit</span><br><span class="line"># refs https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;5.7&#x2F;en&#x2F;innodb-parameters.html#sysvar_innodb_open_files</span><br><span class="line">innodb_open_files&#x3D;65535</span><br><span class="line"># 每个 InnoDB 表都存储在独立的表空间(.ibd)中</span><br><span class="line">innodb_file_per_table&#x3D;1</span><br><span class="line"># 事务级别(可重复读，会出幻读)</span><br><span class="line">transaction_isolation&#x3D;REPEATABLE-READ</span><br><span class="line"># 是否在搜索和索引扫描中使用间隙锁(gap locking)，不建议使用未来将删除</span><br><span class="line">innodb_locks_unsafe_for_binlog&#x3D;0</span><br><span class="line"># InnoDB 后台清理线程数，更大的值有助于 DML 执行性能，&gt;&#x3D; 5.7.8 默认为 4</span><br><span class="line">innodb_purge_threads&#x3D;4</span><br></pre></td></tr></table></figure><p><strong>mysqld_safe.cnf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#</span><br><span class="line"># The Percona Server 5.7 configuration file.</span><br><span class="line">#</span><br><span class="line"># One can use all long options that the program supports.</span><br><span class="line"># Run program with --help to get a list of available options and with</span><br><span class="line"># --print-defaults to see which it would actually understand and use.</span><br><span class="line">#</span><br><span class="line"># For explanations see</span><br><span class="line"># http:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;mysql&#x2F;en&#x2F;server-system-variables.html</span><br><span class="line"></span><br><span class="line">[mysqld_safe]</span><br><span class="line">pid-file &#x3D; &#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.pid</span><br><span class="line">socket   &#x3D; &#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.sock</span><br><span class="line">nice     &#x3D; 0</span><br></pre></td></tr></table></figure><p><strong>mysqldump.cnf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[mysqldump]</span><br><span class="line">quick</span><br><span class="line">default-character-set&#x3D;utf8mb4</span><br><span class="line">max_allowed_packet&#x3D;256M</span><br></pre></td></tr></table></figure><h3><span id="25-启动">2.5、启动</span></h3><p>配置文件调整完成后启动既可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start mysqld</span><br></pre></td></tr></table></figure><p>启动完成后默认 root 密码会自动生成，通过 <code>grep 'temporary password' /var/log/mysql/*</code> 查看默认密码；获得默认密码后可以通过 <code>mysqladmin -S /data/mysql/mysql.sock -u root -p password</code> 修改 root 密码。</p><h2><span id="三-percona-monitoring-and-management">三、Percona Monitoring and Management</span></h2><p>数据库创建成功后需要增加 pmm 监控，后续将会通过监控信息来调优数据库，所以数据库监控必不可少。</p><h3><span id="31-安装前准备">3.1、安装前准备</span></h3><p>pmm 监控需要使用特定用户来监控数据信息，所以需要预先为 pmm 创建用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">USE mysql;</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;pmm&#39;@&#39;%&#39; IDENTIFIED BY &#39;pmm12345&#39; WITH GRANT OPTION;</span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><h3><span id="32-安装-pmm-server">3.2、安装 PMM Server</span></h3><p>pmm server 端推荐直接使用 docker 启动，以下为样例 docker compose</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">version: &#39;3.7&#39;</span><br><span class="line">services:</span><br><span class="line">  pmm:</span><br><span class="line">    image: percona&#x2F;pmm-server:2.1.0</span><br><span class="line">    container_name: pmm</span><br><span class="line">    restart: always</span><br><span class="line">    volumes:</span><br><span class="line">      - data:&#x2F;srv</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;80:80&quot;</span><br><span class="line">      - &quot;443:443&quot;</span><br><span class="line">volumes:</span><br><span class="line">  data:</span><br></pre></td></tr></table></figure><p><strong>如果想要自定义证书，请将证书复制到 volume 内的 nginx 目录下，自定义证书需要以下证书文件</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pmmserver.node ➜ tree</span><br><span class="line">.</span><br><span class="line">├── ca-certs.pem</span><br><span class="line">├── certificate.conf  # 此文件是 pmm 默认生成自签证书的配置文件，不需要关注</span><br><span class="line">├── certificate.crt</span><br><span class="line">├── certificate.key</span><br><span class="line">└── dhparam.pem</span><br></pre></td></tr></table></figure><p><strong>pmm server 启动后访问 <code>http(s)://IP_ADDRESS</code> 既可进入 granafa 面板，默认账户名和密码都是 <code>admin</code></strong></p><h3><span id="33-安装-pmm-client">3.3、安装 PMM Client</span></h3><p>PMM Client 同样采用 rpm 安装，下载地址 <a href="https://www.percona.com/downloads/pmm2/%EF%BC%8C%E5%BD%93%E5%89%8D%E9%87%87%E7%94%A8%E6%9C%80%E6%96%B0%E7%9A%84" target="_blank" rel="noopener">https://www.percona.com/downloads/pmm2/，当前采用最新的</a> 2.1.0 版本；rpm 下载完成后直接 <code>yum install</code> 既可。</p><p>rpm 安装完成后使用 <code>pmm-admin</code> 命令配置服务端地址，并添加当前 mysql 实例监控</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 配置服务端地址</span><br><span class="line">pmm-admin config --server-url https:&#x2F;&#x2F;admin:admin@pmm.mysql.node 172.16.0.11 generic mysql</span><br><span class="line"># 配置当前 mysql 实例</span><br><span class="line">pmm-admin add mysql --username&#x3D;pmm --password&#x3D;pmm12345 mysql 172.16.0.11:3306</span><br></pre></td></tr></table></figure><p>完成后稍等片刻既可在 pmm server 端的 granafa 中看到相关数据。</p><h2><span id="四-数据导入">四、数据导入</span></h2><p>从原始数据库 dump 相关库，并导入到新数据库既可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># dump</span><br><span class="line">mysqldump -h 172.16.1.10 -u root -p --master-data&#x3D;2 --routines --triggers --single_transaction --databases DATABASE_NAME &gt; dump.sql</span><br><span class="line"># load</span><br><span class="line">mysql -S &#x2F;data&#x2F;mysql&#x2F;mysql.sock -u root -p &lt; dump.sql</span><br></pre></td></tr></table></figure><p>数据导入后重建业务用户既可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">USE mysql;</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;test_user&#39;@&#39;%&#39; IDENTIFIED BY &#39;test_user&#39; WITH GRANT OPTION;</span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><h2><span id="五-数据备份">五、数据备份</span></h2><h3><span id="51-安装-xtrabackup">5.1、安装 xtrabackup</span></h3><p>目前数据备份采用 Perconra xtrabackup 工具，xtrabackup 可以实现高速、压缩带增量的备份；xtrabackup 安装同样采用 rpm 方式，下载地址为 <a href="https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/%EF%BC%8C%E4%B8%8B%E8%BD%BD%E5%AE%8C%E6%88%90%E5%90%8E%E6%89%A7%E8%A1%8C" target="_blank" rel="noopener">https://www.percona.com/downloads/Percona-XtraBackup-2.4/LATEST/，下载完成后执行</a> <code>yum install</code> 既可</p><h3><span id="52-备份工具">5.2、备份工具</span></h3><p>目前备份工具开源在 <a href="https://github.com/gozap/mybak" target="_blank" rel="noopener">GitHub</a> 上，每次全量备份会写入 <code>.full-backup</code> 文件，增量备份会写入 <code>.inc-backup</code> 文件</p><h3><span id="53-配置-systemd">5.3、配置 systemd</span></h3><p>为了使备份自动运行，目前将定时任务配置到 systemd 中，由 systemd 调度并执行；以下为相关 systemd 配置文件</p><p><strong>mysql-backup-full.service</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description&#x3D;mysql full backup</span><br><span class="line">After&#x3D;network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">Restart&#x3D;on-failure</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;mybak --backup-dir &#x2F;data&#x2F;mysql_backup --prefix mysql full</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure><p><strong>mysql-backup-inc.service</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description&#x3D;mysql incremental backup</span><br><span class="line">After&#x3D;network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">Restart&#x3D;on-failure</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;mybak --backup-dir &#x2F;data&#x2F;mysql_backup --prefix mysql inc</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure><p><strong>mysql-backup-compress.service</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description&#x3D;mysql backup compress</span><br><span class="line">After&#x3D;network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type&#x3D;simple</span><br><span class="line">Restart&#x3D;on-failure</span><br><span class="line">ExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;mybak --backup-dir &#x2F;data&#x2F;mysql_backup --prefix mysql compress --clean</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;multi-user.target</span><br></pre></td></tr></table></figure><p><strong>mysql-backup-full.timer</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description&#x3D;mysql weekly full backup</span><br><span class="line"># 备份之前依赖相关目录的挂载</span><br><span class="line">After&#x3D;data.mount</span><br><span class="line">After&#x3D;data-mysql_backup.mount</span><br><span class="line"></span><br><span class="line">[Timer]</span><br><span class="line"># 目前每周日一个全量备份</span><br><span class="line">OnCalendar&#x3D;Sun *-*-* 3:00</span><br><span class="line">Persistent&#x3D;true</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;timers.target</span><br></pre></td></tr></table></figure><p><strong>mysql-backup-inc.timer</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description&#x3D;mysql weekly full backup</span><br><span class="line">After&#x3D;data.mount</span><br><span class="line">After&#x3D;data-mysql_backup.mount</span><br><span class="line"></span><br><span class="line">[Timer]</span><br><span class="line"># 每天三个增量备份</span><br><span class="line">OnCalendar&#x3D;*-*-* 9:00</span><br><span class="line">OnCalendar&#x3D;*-*-* 13:00</span><br><span class="line">OnCalendar&#x3D;*-*-* 18:00</span><br><span class="line">Persistent&#x3D;true</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;timers.target</span><br></pre></td></tr></table></figure><p><strong>mysql-backup-compress.timer</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description&#x3D;mysql weekly backup compress</span><br><span class="line"># 备份之前依赖相关目录的挂载</span><br><span class="line">After&#x3D;data.mount</span><br><span class="line">After&#x3D;data-mysql_backup.mount</span><br><span class="line"></span><br><span class="line">[Timer]</span><br><span class="line"># 目前每周日一个全量备份，自动压缩后同时完成清理</span><br><span class="line">OnCalendar&#x3D;Sun *-*-* 5:00</span><br><span class="line">Persistent&#x3D;true</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy&#x3D;timers.target</span><br></pre></td></tr></table></figure><p>创建好相关文件后启动相关定时器既可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp *.timer *.service &#x2F;lib&#x2F;systemd&#x2F;system</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl start mysql-backup-full.timer mysql-backup-inc.timer mysql-backup-compress.timer</span><br><span class="line">systemctl enable mysql-backup-full.timer mysql-backup-inc.timer mysql-backup-compress.timer</span><br></pre></td></tr></table></figure><h2><span id="六-数据恢复">六、数据恢复</span></h2><h3><span id="61-全量备份恢复">6.1、全量备份恢复</span></h3><p>针对于全量备份，只需要按照官方文档的还原顺序进行还原既可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 由于备份时进行了压缩，所以先解压备份文件</span><br><span class="line">xtrabackup --decompress --parallel 4 --target-dir &#x2F;data&#x2F;mysql_backup&#x2F;mysql-20191205230502</span><br><span class="line"># 执行预处理</span><br><span class="line">xtrabackup --prepare --target-dir &#x2F;data&#x2F;mysql_backup&#x2F;mysql-20191205230502</span><br><span class="line"># 执行恢复(恢复时自动根据 my.cnf 将数据覆盖到 data 数据目录)</span><br><span class="line">xtrabackup --copy-back --target-dir &#x2F;data&#x2F;mysql_backup&#x2F;mysql-20191205230502</span><br><span class="line"># 修复数据目录权限</span><br><span class="line">chown -R mysql:mysql &#x2F;data&#x2F;mysql</span><br><span class="line"># 启动 mysql</span><br><span class="line">systemctl start mysqld</span><br></pre></td></tr></table></figure><h3><span id="62-增量备份恢复">6.2、增量备份恢复</span></h3><p>对于增量备份恢复，其与全量备份恢复的根本区别在于: <strong>对于非最后一个增量文件的预处理必须使用 <code>--apply-log-only</code> 选项防止运行回滚阶段的处理</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 对所有备份文件进行解压处理</span><br><span class="line">for dir in &#96;ls&#96;; do xtrabackup --decompress --parallel 4 --target-dir $dir; done</span><br><span class="line"># 对全量备份文件执行预处理(注意增加 --apply-log-only 选项)</span><br><span class="line">xtrabackup --prepare --apply-log-only --target-dir &#x2F;data&#x2F;mysql_backup&#x2F;mysql-20191205230502</span><br><span class="line"># 对非最后一个增量备份执行预处理</span><br><span class="line">xtrabackup --prepare --apply-log-only --target-dir &#x2F;data&#x2F;mysql_backup&#x2F;mysql-20191205230502 --incremental-dir &#x2F;data&#x2F;mysql_backup&#x2F;mysql-inc-20191206230802</span><br><span class="line"># 对最后一个增量备份执行预处理(不需要 --apply-log-only)</span><br><span class="line">xtrabackup --prepare --target-dir &#x2F;data&#x2F;mysql_backup&#x2F;mysql-20191205230502 --incremental-dir &#x2F;data&#x2F;mysql_backup&#x2F;mysql-inc-20191207031005</span><br><span class="line"># 执行恢复(恢复时自动根据 my.cnf 将数据覆盖到 data 数据目录)</span><br><span class="line">xtrabackup --copy-back --target-dir &#x2F;data&#x2F;mysql_backup&#x2F;mysql-20191205230502</span><br><span class="line"># 修复数据目录权限</span><br><span class="line">chown -R mysql:mysql &#x2F;data&#x2F;mysql</span><br><span class="line"># 启动 mysql</span><br><span class="line">systemctl start mysqld</span><br></pre></td></tr></table></figure><h3><span id="63-创建-slave">6.3、创建 slave</span></h3><p>针对 xtrabackup 备份的数据可以直接恢复成 slave 节点，具体步骤如下:</p><p>首先将备份文件复制到目标机器，然后执行解压(默认备份工具采用 lz4 压缩)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xtrabackup --decompress --target-dir&#x3D;xxxxxx</span><br></pre></td></tr></table></figure><p>解压完成后执行预处理操作(<strong>在执行预处理之前请确保 slave 机器上相关配置文件与 master 相同，并且处理好数据目录存放等</strong>)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xtrabackup --user&#x3D;root --password&#x3D;xxxxxxx --prepare --target-dir&#x3D;xxxx</span><br></pre></td></tr></table></figure><p>预处理成功后便可执行恢复，以下命令将自动读取 <code>my.cnf</code> 配置，自动识别数据目录位置并将数据文件移动到该位置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xtrabackup --move-back --target-dir&#x3D;xxxxx</span><br></pre></td></tr></table></figure><p>所由准备就绪后需要进行权限修复</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown -R mysql:mysql MYSQL_DATA_DIR</span><br></pre></td></tr></table></figure><p>最后在 mysql 内启动 slave 既可，slave 信息可通过从数据备份目录的 <code>xtrabackup_binlog_info</code> 中获取</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 获取备份 POS 信息</span><br><span class="line">cat xxxxxx&#x2F;xtrabackup_binlog_info</span><br><span class="line"></span><br><span class="line"># 创建 slave 节点</span><br><span class="line">CHANGE MASTER TO</span><br><span class="line">    MASTER_HOST&#x3D;&#39;192.168.2.48&#39;,</span><br><span class="line">    MASTER_USER&#x3D;&#39;repl&#39;,</span><br><span class="line">    MASTER_PASSWORD&#x3D;&#39;xxxxxxx&#39;,</span><br><span class="line">    MASTER_LOG_FILE&#x3D;&#39;mysql-bin.000005&#39;,</span><br><span class="line">    MASTER_LOG_POS&#x3D;52500595;</span><br><span class="line"></span><br><span class="line"># 启动 slave</span><br><span class="line">start slave;</span><br><span class="line">show slave status \G;</span><br></pre></td></tr></table></figure><h2><span id="七-生产处理">七、生产处理</span></h2><h3><span id="71-数据目录">7.1、数据目录</span></h3><p>目前生产环境数据目录位置调整到 <code>/home/mysql</code>，所以目录权限处理也要做对应调整</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;var&#x2F;log&#x2F;mysql &#x2F;home&#x2F;mysql_tmp</span><br><span class="line">chown -R mysql:mysql &#x2F;var&#x2F;log&#x2F;mysql &#x2F;home&#x2F;mysql_tmp</span><br></pre></td></tr></table></figure><h3><span id="72-配置文件">7.2、配置文件</span></h3><p>生产环境目前节点配置如下</p><ul><li>CPU: <code>Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz</code></li><li>RAM: <code>128G</code></li></ul><p>所以配置文件也需要做相应的优化调整</p><p><strong>mysql.cnf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[mysql]</span><br><span class="line">auto-rehash</span><br><span class="line">default_character_set&#x3D;utf8mb4</span><br></pre></td></tr></table></figure><p><strong>mysqld.cnf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br></pre></td><td class="code"><pre><span class="line"># Percona Server template configuration</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">#</span><br><span class="line"># Remove leading # and set to the amount of RAM for the most important data</span><br><span class="line"># cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.</span><br><span class="line"># innodb_buffer_pool_size &#x3D; 128M</span><br><span class="line">#</span><br><span class="line"># Remove leading # to turn on a very important data integrity option: logging</span><br><span class="line"># changes to the binary log between backups.</span><br><span class="line"># log_bin</span><br><span class="line">#</span><br><span class="line"># Remove leading # to set options mainly useful for reporting servers.</span><br><span class="line"># The server defaults are faster for transactions and fast SELECTs.</span><br><span class="line"># Adjust sizes as needed, experiment to find the optimal values.</span><br><span class="line"># join_buffer_size &#x3D; 128M</span><br><span class="line"># sort_buffer_size &#x3D; 2M</span><br><span class="line"># read_rnd_buffer_size &#x3D; 2M</span><br><span class="line">port&#x3D;3306</span><br><span class="line">datadir&#x3D;&#x2F;home&#x2F;mysql&#x2F;mysql</span><br><span class="line">socket&#x3D;&#x2F;home&#x2F;mysql&#x2F;mysql&#x2F;mysql.sock</span><br><span class="line">pid_file&#x3D;&#x2F;home&#x2F;mysql&#x2F;mysql&#x2F;mysqld.pid</span><br><span class="line"></span><br><span class="line"># 服务端编码</span><br><span class="line">character_set_server&#x3D;utf8mb4</span><br><span class="line"># 服务端排序</span><br><span class="line">collation_server&#x3D;utf8mb4_general_ci</span><br><span class="line"># 强制使用 utf8mb4 编码集，忽略客户端设置</span><br><span class="line">skip_character_set_client_handshake&#x3D;1</span><br><span class="line"># 日志输出到文件</span><br><span class="line">log_output&#x3D;FILE</span><br><span class="line"># 开启常规日志输出</span><br><span class="line">general_log&#x3D;1</span><br><span class="line"># 常规日志输出文件位置</span><br><span class="line">general_log_file&#x3D;&#x2F;var&#x2F;log&#x2F;mysql&#x2F;mysqld.log</span><br><span class="line"># 错误日志位置</span><br><span class="line">log_error&#x3D;&#x2F;var&#x2F;log&#x2F;mysql&#x2F;mysqld-error.log</span><br><span class="line"># 记录慢查询</span><br><span class="line">slow_query_log&#x3D;1</span><br><span class="line"># 慢查询时间(大于 1s 被视为慢查询)</span><br><span class="line">long_query_time&#x3D;1</span><br><span class="line"># 慢查询日志文件位置</span><br><span class="line">slow_query_log_file&#x3D;&#x2F;var&#x2F;log&#x2F;mysql&#x2F;mysqld-slow.log</span><br><span class="line"># 临时文件位置</span><br><span class="line">tmpdir&#x3D;&#x2F;home&#x2F;mysql&#x2F;mysql_tmp</span><br><span class="line"># The number of open tables for all threads.(refs https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;5.7&#x2F;en&#x2F;server-system-variables.html#sysvar_table_open_cache)</span><br><span class="line">table_open_cache&#x3D;16384</span><br><span class="line"># 文件描述符(此处修改不生效，请修改 systemd service 配置) </span><br><span class="line"># refs https:&#x2F;&#x2F;www.percona.com&#x2F;blog&#x2F;2017&#x2F;10&#x2F;12&#x2F;open_files_limit-mystery&#x2F;</span><br><span class="line"># refs https:&#x2F;&#x2F;www.cnblogs.com&#x2F;wxxjianchi&#x2F;p&#x2F;10370419.html</span><br><span class="line">#open_files_limit&#x3D;65535</span><br><span class="line"># 表定义缓存(5.7 以后自动调整)</span><br><span class="line"># refs https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;5.6&#x2F;en&#x2F;server-system-variables.html#sysvar_table_definition_cache</span><br><span class="line"># refs http:&#x2F;&#x2F;mysql.taobao.org&#x2F;monthly&#x2F;2015&#x2F;08&#x2F;10&#x2F;</span><br><span class="line">#table_definition_cache&#x3D;16384</span><br><span class="line">sort_buffer_size&#x3D;1M</span><br><span class="line">join_buffer_size&#x3D;1M</span><br><span class="line"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><br><span class="line">read_buffer_size&#x3D;1M</span><br><span class="line">read_rnd_buffer_size&#x3D;1M</span><br><span class="line"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><br><span class="line">key_buffer_size&#x3D;32M</span><br><span class="line"># MyiSAM 引擎专用(内部临时磁盘表可能会用)</span><br><span class="line">bulk_insert_buffer_size&#x3D;16M</span><br><span class="line"># myisam_sort_buffer_size 与 sort_buffer_size 区别请参考(https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;7871027&#x2F;myisam-sort-buffer-size-vs-sort-buffer-size)</span><br><span class="line">myisam_sort_buffer_size&#x3D;64M</span><br><span class="line"># 内部内存临时表大小</span><br><span class="line">tmp_table_size&#x3D;32M</span><br><span class="line"># 用户创建的 MEMORY 表最大大小(tmp_table_size 受此值影响)</span><br><span class="line">max_heap_table_size&#x3D;32M</span><br><span class="line"># 开启查询缓存</span><br><span class="line">query_cache_type&#x3D;1</span><br><span class="line"># 查询缓存大小</span><br><span class="line">query_cache_size&#x3D;32M</span><br><span class="line"># sql mode</span><br><span class="line">sql_mode&#x3D;&#39;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&#39;</span><br><span class="line"></span><br><span class="line">########### Network ###########</span><br><span class="line"># 最大连接数(该参数受到最大文件描述符影响，如果不生效请检查最大文件描述符设置)</span><br><span class="line"># refs https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;39976756&#x2F;the-max-connections-in-mysql-5-7</span><br><span class="line">max_connections&#x3D;1500</span><br><span class="line"># mysql 堆栈内暂存的链接数量</span><br><span class="line"># 当短时间内链接数量超过 max_connections 时，部分链接会存储在堆栈内，存储数量受此参数控制</span><br><span class="line">back_log&#x3D;256</span><br><span class="line"># 最大链接错误，针对于 client 主机，超过此数量的链接错误将会导致 mysql server 针对此主机执行锁定(禁止链接 ERROR 1129 )</span><br><span class="line"># 此错误计数仅在 mysql 链接握手失败才会计算，一般出现问题时都是网络故障</span><br><span class="line"># refs https:&#x2F;&#x2F;www.cnblogs.com&#x2F;kerrycode&#x2F;p&#x2F;8405862.html</span><br><span class="line">max_connect_errors&#x3D;100000</span><br><span class="line"># mysql server 允许的最大数据包大小</span><br><span class="line">max_allowed_packet&#x3D;64M</span><br><span class="line"># 交互式客户端链接超时(30分钟自动断开)</span><br><span class="line">interactive_timeout&#x3D;1800</span><br><span class="line"># 非交互式链接超时时间(10分钟)</span><br><span class="line"># 如果客户端有连接池，则需要协商此参数(refs https:&#x2F;&#x2F;database.51cto.com&#x2F;art&#x2F;201909&#x2F;603519.htm)</span><br><span class="line">wait_timeout&#x3D;28800</span><br><span class="line"># 跳过外部文件系统锁定</span><br><span class="line"># If you run multiple servers that use the same database directory (not recommended), </span><br><span class="line"># each server must have external locking enabled.</span><br><span class="line"># refs https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;5.7&#x2F;en&#x2F;external-locking.html</span><br><span class="line">skip_external_locking&#x3D;1</span><br><span class="line"># 跳过链接的域名解析(开启此选项后 mysql 用户授权的 host 方式失效)</span><br><span class="line">skip_name_resolve&#x3D;0</span><br><span class="line"># 禁用主机名缓存，每次都会走 DNS</span><br><span class="line">host_cache_size&#x3D;0</span><br><span class="line"></span><br><span class="line">########### REPL ###########</span><br><span class="line"># 开启 binlog</span><br><span class="line">log_bin&#x3D;mysql-bin</span><br><span class="line"># 作为从库时，同步信息依然写入 binlog，方便此从库再作为其他从库的主库</span><br><span class="line">log_slave_updates&#x3D;1</span><br><span class="line"># server id，默认为 ipv4 地址去除第一段</span><br><span class="line"># eg: 192.168.2.48 &#x3D;&gt; 168248</span><br><span class="line">server_id&#x3D;168248</span><br><span class="line"># 每 n 次事务 binlog 刷新到磁盘</span><br><span class="line"># refs http:&#x2F;&#x2F;liyangliang.me&#x2F;posts&#x2F;2014&#x2F;03&#x2F;innodb_flush_log_at_trx_commit-and-sync_binlog&#x2F;</span><br><span class="line">sync_binlog&#x3D;100</span><br><span class="line"># binlog 格式(refs https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;33504555)</span><br><span class="line">binlog_format&#x3D;row</span><br><span class="line"># binlog 自动清理时间</span><br><span class="line">expire_logs_days&#x3D;20</span><br><span class="line"># 开启 relay-log，一般作为 slave 时开启</span><br><span class="line">relay_log&#x3D;mysql-replay</span><br><span class="line"># 主从复制时跳过 test 库</span><br><span class="line">replicate_ignore_db&#x3D;test</span><br><span class="line"># 每个 session binlog 缓存</span><br><span class="line">binlog_cache_size&#x3D;4M</span><br><span class="line"># binlog 滚动大小</span><br><span class="line">max_binlog_size&#x3D;1024M</span><br><span class="line"># GTID 相关(refs https:&#x2F;&#x2F;keithlan.github.io&#x2F;2016&#x2F;06&#x2F;23&#x2F;gtid&#x2F;)</span><br><span class="line">#gtid_mode&#x3D;1</span><br><span class="line">#enforce_gtid_consistency&#x3D;1</span><br><span class="line"></span><br><span class="line">########### InnoDB ###########</span><br><span class="line"># 永久表默认存储引擎</span><br><span class="line">default_storage_engine&#x3D;InnoDB</span><br><span class="line"># 系统表空间数据文件大小(初始化为 1G，并且自动增长)</span><br><span class="line">innodb_data_file_path&#x3D;ibdata1:1G:autoextend</span><br><span class="line"># InnoDB 缓存池大小(资源充足，为所欲为)</span><br><span class="line"># innodb_buffer_pool_size 必须等于 innodb_buffer_pool_chunk_size*innodb_buffer_pool_instances，或者是其整数倍</span><br><span class="line"># refs https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;5.7&#x2F;en&#x2F;innodb-buffer-pool-resize.html</span><br><span class="line"># refs https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;60089484</span><br><span class="line">innodb_buffer_pool_size&#x3D;61440M</span><br><span class="line">innodb_buffer_pool_instances&#x3D;16</span><br><span class="line"># 默认 128M</span><br><span class="line">innodb_buffer_pool_chunk_size&#x3D;128M</span><br><span class="line"># InnoDB 强制恢复(refs https:&#x2F;&#x2F;www.askmaclean.com&#x2F;archives&#x2F;mysql-innodb-innodb_force_recovery.html)</span><br><span class="line">innodb_force_recovery&#x3D;0</span><br><span class="line"># InnoDB buffer 预热(refs http:&#x2F;&#x2F;www.dbhelp.net&#x2F;2017&#x2F;01&#x2F;12&#x2F;mysql-innodb-buffer-pool-warmup.html)</span><br><span class="line">innodb_buffer_pool_dump_at_shutdown&#x3D;1</span><br><span class="line">innodb_buffer_pool_load_at_startup&#x3D;1</span><br><span class="line"># InnoDB 日志组中的日志文件数</span><br><span class="line">innodb_log_files_in_group&#x3D;2</span><br><span class="line"># InnoDB redo 日志大小</span><br><span class="line"># refs https:&#x2F;&#x2F;www.percona.com&#x2F;blog&#x2F;2017&#x2F;10&#x2F;18&#x2F;chose-mysql-innodb_log_file_size&#x2F;</span><br><span class="line">innodb_log_file_size&#x3D;256MB</span><br><span class="line"># 缓存还未提交的事务的缓冲区大小</span><br><span class="line">innodb_log_buffer_size&#x3D;16M</span><br><span class="line"># InnoDB 在事务提交后的日志写入频率</span><br><span class="line"># refs http:&#x2F;&#x2F;liyangliang.me&#x2F;posts&#x2F;2014&#x2F;03&#x2F;innodb_flush_log_at_trx_commit-and-sync_binlog&#x2F;</span><br><span class="line">innodb_flush_log_at_trx_commit&#x3D;2</span><br><span class="line"># InnoDB DML 操作行级锁等待时间</span><br><span class="line"># 超时返回 ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction</span><br><span class="line"># refs https:&#x2F;&#x2F;ningyu1.github.io&#x2F;site&#x2F;post&#x2F;75-mysql-lock-wait-timeout-exceeded&#x2F;</span><br><span class="line">innodb_lock_wait_timeout&#x3D;30</span><br><span class="line"># InnoDB 行级锁超时是否回滚整个事务，默认为 OFF 仅回滚上一条语句</span><br><span class="line"># 此时应用程序可以接受到错误后选择是否继续提交事务(并没有违反 ACID 原子性)</span><br><span class="line"># refs https:&#x2F;&#x2F;www.cnblogs.com&#x2F;hustcat&#x2F;archive&#x2F;2012&#x2F;11&#x2F;18&#x2F;2775487.html</span><br><span class="line">#innodb_rollback_on_timeout&#x3D;ON</span><br><span class="line"># InnoDB 数据写入磁盘的方式，具体见博客文章</span><br><span class="line"># refs https:&#x2F;&#x2F;www.cnblogs.com&#x2F;gomysql&#x2F;p&#x2F;3595806.html</span><br><span class="line">innodb_flush_method&#x3D;O_DIRECT</span><br><span class="line"># InnoDB 缓冲池脏页刷新百分比</span><br><span class="line"># refs https:&#x2F;&#x2F;dbarobin.com&#x2F;2015&#x2F;08&#x2F;29&#x2F;mysql-optimization-under-ssd</span><br><span class="line">innodb_max_dirty_pages_pct&#x3D;50</span><br><span class="line"># InnoDB 每秒执行的写IO量</span><br><span class="line"># refs https:&#x2F;&#x2F;www.centos.bz&#x2F;2016&#x2F;11&#x2F;mysql-performance-tuning-15-config-item&#x2F;#10.INNODB_IO_CAPACITY,%20INNODB_IO_CAPACITY_MAX</span><br><span class="line"># refs https:&#x2F;&#x2F;www.alibabacloud.com&#x2F;blog&#x2F;testing-io-performance-with-sysbench_594709</span><br><span class="line">innodb_io_capacity&#x3D;8000</span><br><span class="line">innodb_io_capacity_max&#x3D;16000</span><br><span class="line"># 请求并发 InnoDB 线程数</span><br><span class="line"># refs https:&#x2F;&#x2F;www.cnblogs.com&#x2F;xinysu&#x2F;p&#x2F;6439715.html#_lab2_1_0</span><br><span class="line">innodb_thread_concurrency&#x3D;0</span><br><span class="line"># 再使用多个 InnoDB 表空间时，允许打开的最大 &quot;.ibd&quot; 文件个数，不设置默认 300，</span><br><span class="line"># 并且取与 table_open_cache 相比较大的一个，此选项独立于 open_files_limit</span><br><span class="line"># refs https:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;refman&#x2F;5.7&#x2F;en&#x2F;innodb-parameters.html#sysvar_innodb_open_files</span><br><span class="line">innodb_open_files&#x3D;65535</span><br><span class="line"># 每个 InnoDB 表都存储在独立的表空间(.ibd)中</span><br><span class="line">innodb_file_per_table&#x3D;1</span><br><span class="line"># 事务级别(可重复读，会出幻读)</span><br><span class="line">transaction_isolation&#x3D;REPEATABLE-READ</span><br><span class="line"># 是否在搜索和索引扫描中使用间隙锁(gap locking)，不建议使用未来将删除</span><br><span class="line">innodb_locks_unsafe_for_binlog&#x3D;0</span><br><span class="line"># InnoDB 后台清理线程数，更大的值有助于 DML 执行性能，&gt;&#x3D; 5.7.8 默认为 4</span><br><span class="line">innodb_purge_threads&#x3D;4</span><br></pre></td></tr></table></figure><p><strong>mysqld_safe.cnf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#</span><br><span class="line"># The Percona Server 5.7 configuration file.</span><br><span class="line">#</span><br><span class="line"># One can use all long options that the program supports.</span><br><span class="line"># Run program with --help to get a list of available options and with</span><br><span class="line"># --print-defaults to see which it would actually understand and use.</span><br><span class="line">#</span><br><span class="line"># For explanations see</span><br><span class="line"># http:&#x2F;&#x2F;dev.mysql.com&#x2F;doc&#x2F;mysql&#x2F;en&#x2F;server-system-variables.html</span><br><span class="line"></span><br><span class="line">[mysqld_safe]</span><br><span class="line">pid-file &#x3D; &#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.pid</span><br><span class="line">socket   &#x3D; &#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.sock</span><br><span class="line">nice     &#x3D; 0</span><br></pre></td></tr></table></figure><p><strong>mysqldump.cnf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[mysqldump]</span><br><span class="line">quick</span><br><span class="line">default-character-set&#x3D;utf8mb4</span><br><span class="line">max_allowed_packet&#x3D;256M</span><br></pre></td></tr></table></figure><h2><span id="八-常用诊断">八、常用诊断</span></h2><h3><span id="81-动态配置-diff">8.1、动态配置 diff</span></h3><p>mysql 默认允许在实例运行后使用 <code>set global VARIABLES=VALUE</code> 的方式动态调整一些配置，这可能导致在运行一段时间后(运维动态修改)实例运行配置和配置文件中配置不一致；所以<strong>建议定期 diff 运行时配置与配置文件配置差异，防制特殊情况下 mysql 重启后运行期配置丢失</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pt-config-diff &#x2F;etc&#x2F;percona-server.conf.d&#x2F;mysqld.cnf h&#x3D;127.0.0.1 --user root --ask-pass --report-width 100</span><br><span class="line">Enter MySQL password:</span><br><span class="line">2 config differences</span><br><span class="line">Variable                  &#x2F;etc&#x2F;percona-server.conf.d&#x2F;mysqld.cnf mysql47.test.com</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">innodb_max_dirty_pages... 50                                    50.000000</span><br><span class="line">skip_name_resolve         0                                     ON</span><br></pre></td></tr></table></figure><h3><span id="82-配置优化建议">8.2、配置优化建议</span></h3><p>Percona Toolkit 提供了一个诊断工具，用于对 mysql 内的配置进行扫描并给出优化建议，在初始化时可以使用此工具评估 mysql 当前配置的具体情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">pt-variable-advisor 127.0.0.1 --user root --ask-pass | grep -v &#39;^$&#39;</span><br><span class="line">Enter password: </span><br><span class="line"></span><br><span class="line"># WARN delay_key_write: MyISAM index blocks are never flushed until necessary.</span><br><span class="line"># WARN innodb_flush_log_at_trx_commit-1: InnoDB is not configured in strictly ACID mode.</span><br><span class="line"># NOTE innodb_max_dirty_pages_pct: The innodb_max_dirty_pages_pct is lower than the default.</span><br><span class="line"># WARN max_connections: If the server ever really has more than a thousand threads running, then the system is likely to spend more time scheduling threads than really doing useful work.</span><br><span class="line"># NOTE read_buffer_size-1: The read_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><br><span class="line"># NOTE read_rnd_buffer_size-1: The read_rnd_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><br><span class="line"># NOTE sort_buffer_size-1: The sort_buffer_size variable should generally be left at its default unless an expert determines it is necessary to change it.</span><br><span class="line"># NOTE innodb_data_file_path: Auto-extending InnoDB files can consume a lot of disk space that is very difficult to reclaim later.</span><br><span class="line"># WARN myisam_recover_options: myisam_recover_options should be set to some value such as BACKUP,FORCE to ensure that table corruption is noticed.</span><br><span class="line"># WARN sync_binlog: Binary logging is enabled, but sync_binlog isn&#39;t configured so that every transaction is flushed to the binary log for durability.</span><br></pre></td></tr></table></figure><h3><span id="83-死锁诊断">8.3、死锁诊断</span></h3><p>使用 pt-deadlock-logger 工具可以诊断当前的死锁状态，以下为对死锁检测的测试</p><p>首先创建测试数据库和表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 创建测试库</span><br><span class="line">CREATE DATABASE dbatest CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;</span><br><span class="line"># 切换到测试库并建立测试表</span><br><span class="line">USE dbatest;</span><br><span class="line">CREATE TABLE IF NOT EXISTS test (id INT AUTO_INCREMENT PRIMARY KEY, value VARCHAR(255), createtime TIMESTAMP DEFAULT CURRENT_TIMESTAMP) ENGINE&#x3D;INNODB;</span><br></pre></td></tr></table></figure><p>在一个其他终端上开启 pt-deadlock-logger 检测</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pt-deadlock-logger 127.0.0.1 --user root --ask-pass --tab</span><br></pre></td></tr></table></figure><p>检测开启后进行死锁测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 插入两条测试数据</span><br><span class="line">INSERT INTO test(value) VALUES(&#39;test1&#39;);</span><br><span class="line">INSERT INTO test(value) VALUES(&#39;test2&#39;);</span><br><span class="line"># 在两个终端下进行交叉事务</span><br><span class="line"></span><br><span class="line"># 统一关闭自动提交</span><br><span class="line">terminal_1 # SET AUTOCOMMIT &#x3D; 0;</span><br><span class="line">terminal_2 # SET AUTOCOMMIT &#x3D; 0;</span><br><span class="line"></span><br><span class="line"># 交叉事务，终端 1 先更新第一条数据，终端 2 先更新第二条数据</span><br><span class="line">terminal_1 # BEGIN;</span><br><span class="line">terminal_1 # UPDATE test set value&#x3D;&#39;x1&#39; where id&#x3D;1;</span><br><span class="line">terminal_2 # BEGIN;</span><br><span class="line">terminal_2 # UPDATE test set value&#x3D;&#39;x2&#39; where id&#x3D;2;</span><br><span class="line"></span><br><span class="line"># 此后终端 1 再尝试更新第二条数据，终端 2 再尝试更新第一条数据；造成等待互向释放锁的死锁</span><br><span class="line">terminal_1 # UPDATE test set value&#x3D;&#39;lock2&#39; where id&#x3D;2;</span><br><span class="line">terminal_2 # UPDATE test set value&#x3D;&#39;lock1&#39; where id&#x3D;1;</span><br><span class="line"></span><br><span class="line"># 此时由于开启了 mysql innodb 的死锁自动检测机制，会导致终端 2 弹出错误</span><br><span class="line">ERROR 1213 (40001): Deadlock found when trying to get lock; try restarting transaction</span><br><span class="line"></span><br><span class="line"># 同时 pt-deadlock-logger 有日志输出</span><br><span class="line">server  ts      thread  txn_id  txn_time        user    hostname    ip      db      tbl     idx     lock_type       lock_mode       wait_hold       victim  query</span><br><span class="line">127.0.0.1       2019-12-24T14:57:10     87      0       52      root            127.0.0.1       dbatest test    PRIMARY RECORD  X       w       0       UPDATE test set value&#x3D;&#39;lock2&#39; where id&#x3D;2</span><br><span class="line">127.0.0.1       2019-12-24T14:57:10     89      0       41      root            127.0.0.1       dbatest test    PRIMARY RECORD  X       w       1       UPDATE test set value&#x3D;&#39;lock1&#39; where id&#x3D;1</span><br></pre></td></tr></table></figure><h3><span id="84-查看-io-详情">8.4、查看 IO 详情</span></h3><p>不同于 <code>iostat</code>，<code>pt-diskstats</code> 提供了更加详细的 IO 详情统计，并且据有交互式处理，执行一下命令将会实时检测 IO 状态</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pt-diskstats --show-timestamps</span><br></pre></td></tr></table></figure><p>其中几个关键值含义如下(更详细的请参考官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/pt-diskstats.html#output" target="_blank" rel="noopener">https://www.percona.com/doc/percona-toolkit/LATEST/pt-diskstats.html#output</a>)</p><ul><li>rd_s: 每秒平均读取次数。这是发送到基础设备的 IO 请求数。通常，此数量少于应用程序发出的逻辑IO请求的数量。更多请求可能已排队到块设备，但是其中一些请求通常在发送到磁盘之前先进行合并。</li><li>rd_avkb: 读取的平均大小，以千字节为单位。</li><li>rd_mb_s: 每秒读取的平均兆字节数。</li><li>rd_mrg: 在发送到物理设备之前在队列调度程序中合并在一起的读取请求的百分比。</li><li>rd_rt: 读取操作的平均响应时间(以毫秒为单位)；这是端到端响应时间，包括在队列中花费的时间。这是发出 IO 请求的应用程序看到的响应时间，而不是块设备下的物理磁盘的响应时间。</li><li>busy: 设备至少有一个请求 wall-clock 时间的比例；等同于 <code>iostat</code> 的 <code>％util</code>。</li><li>in_prg: 正在进行的请求数。与读写并发是从可靠数字中生成的平均值不同，该数字是一个时样本，您可以看到它可能表示请求峰值，而不是真正的长期平均值。如果此数字很大，则从本质上讲意味着设备高负载运行。</li><li>ios_s: 物理设备的平均吞吐量，以每秒 IO 操作(IOPS)为单位。此列显示基础设备正在处理的总 IOPS；它是 rd_s 和 wr_s 的总和。</li><li>qtime: 平均排队时间；也就是说，请求在发送到物理设备之前在设备调度程序队列中花费的时间。</li><li>stime: 平均服务时间；也就是说，请求完成在队列中的等待之后，物理设备处理请求的时间。</li></ul><h3><span id="85-重复索引优化">8.5、重复索引优化</span></h3><p>pt-duplicate-key-checker 工具提供了对数据库重复索引和外键的自动查找功能，工具使用如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">pt-duplicate-key-checker 127.0.0.1 --user root --ask-pass</span><br><span class="line">Enter password:</span><br><span class="line"></span><br><span class="line"># A software update is available:</span><br><span class="line"># ########################################################################</span><br><span class="line"># aaaaaa.aaaaaa_audit</span><br><span class="line"># ########################################################################</span><br><span class="line"></span><br><span class="line"># index_linkId is a duplicate of unique_linkId</span><br><span class="line"># Key definitions:</span><br><span class="line">#   KEY &#96;index_linkId&#96; (&#96;link_id&#96;)</span><br><span class="line">#   UNIQUE KEY &#96;unique_linkId&#96; (&#96;link_id&#96;),</span><br><span class="line"># Column types:</span><br><span class="line">#         &#96;link_id&#96; bigint(20) not null comment &#39;bdid&#39;</span><br><span class="line"># To remove this duplicate index, execute:</span><br><span class="line">ALTER TABLE &#96;aaaaaa.aaaaaa_audit&#96; DROP INDEX &#96;index_linkId&#96;;</span><br><span class="line"></span><br><span class="line"># ########################################################################</span><br><span class="line"># Summary of indexes</span><br><span class="line"># ########################################################################</span><br><span class="line"></span><br><span class="line"># Size Duplicate Indexes   927420</span><br><span class="line"># Total Duplicate Indexes  3</span><br><span class="line"># Total Indexes            847</span><br></pre></td></tr></table></figure><h3><span id="86-表统计">8.6、表统计</span></h3><p>pt-find 是一个很方便的表查找统计工具，默认的一些选项可以实现批量查找符合条件的表，甚至执行一些 SQL 处理命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 批量查找大于 5G 的表，并排序</span><br><span class="line">pt-find --host 127.0.0.1 --user root --ask-pass --tablesize +5G | sort -rn</span><br><span class="line">Enter password: </span><br><span class="line"></span><br><span class="line">&#96;rss_service&#96;.&#96;test_feed_news&#96;</span><br><span class="line">&#96;db_log_history&#96;.&#96;test_mobile_click_201912&#96;</span><br><span class="line">&#96;db_log_history&#96;.&#96;test_mobile_click_201911&#96;</span><br><span class="line">&#96;db_log_history&#96;.&#96;test_mobile_click_201910&#96;</span><br><span class="line">&#96;test_dix&#96;.&#96;test_user_messages&#96;</span><br><span class="line">&#96;test_dix&#96;.&#96;test_user_link_history&#96;</span><br><span class="line">&#96;test_dix&#96;.&#96;test_mobile_click&#96;</span><br><span class="line">&#96;test_dix&#96;.&#96;test_message&#96;</span><br><span class="line">&#96;test_dix&#96;.&#96;test_link_votes&#96;</span><br><span class="line">&#96;test_dix&#96;.&#96;test_links_mobile_content&#96;</span><br><span class="line">&#96;test_dix&#96;.&#96;test_links&#96;</span><br><span class="line">&#96;test_dix&#96;.&#96;test_comment_votes&#96;</span><br><span class="line">&#96;test_dix&#96;.&#96;test_comments&#96;</span><br></pre></td></tr></table></figure><p>如果想要定制输出可以采用 <code>--printf</code> 选项</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">pt-find --host 127.0.0.1 --user root --ask-pass --tablesize +5G --printf &quot;%T\t%D.%N\n&quot; | sort -rn</span><br><span class="line">Enter password: </span><br><span class="line"></span><br><span class="line">13918404608     &#96;test_dix&#96;.&#96;test_links_mobile_content&#96;</span><br><span class="line">13735231488     &#96;test_dix&#96;.&#96;test_comment_votes&#96;</span><br><span class="line">12633227264     &#96;test_dix&#96;.&#96;test_user_messages&#96;</span><br><span class="line">12610174976     &#96;test_dix&#96;.&#96;test_user_link_history&#96;</span><br><span class="line">10506305536     &#96;test_dix&#96;.&#96;test_links&#96;</span><br><span class="line">9686745088      &#96;test_dix&#96;.&#96;test_message&#96;</span><br><span class="line">9603907584      &#96;rss_service&#96;.&#96;test_feed_news&#96;</span><br><span class="line">9004122112      &#96;db_log_history&#96;.&#96;test_mobile_click_201910&#96;</span><br><span class="line">8919007232      &#96;test_dix&#96;.&#96;test_comments&#96;</span><br><span class="line">8045707264      &#96;db_log_history&#96;.&#96;test_mobile_click_201912&#96;</span><br><span class="line">7855915008      &#96;db_log_history&#96;.&#96;test_mobile_click_201911&#96;</span><br><span class="line">6099566592      &#96;test_dix&#96;.&#96;test_mobile_click&#96;</span><br><span class="line">5892898816      &#96;test_dix&#96;.&#96;test_link_votes&#96;</span><br></pre></td></tr></table></figure><p><strong>遗憾的是目前 <code>printf</code> 格式来源与 Perl 的 <code>sprintf</code> 函数，所以支持格式有限，不过简单的格式定制已经基本实现，复杂的建议通过 awk 处理</strong>；其他的可选参数具体参考官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/pt-find.html" target="_blank" rel="noopener">https://www.percona.com/doc/percona-toolkit/LATEST/pt-find.html</a></p><h3><span id="87-其他命令">8.7、其他命令</span></h3><p>迫于篇幅，其他更多的高级命令请自行查阅官方文档 <a href="https://www.percona.com/doc/percona-toolkit/LATEST/index.html" target="_blank" rel="noopener">https://www.percona.com/doc/percona-toolkit/LATEST/index.html</a></p><blockquote><p>本文转载自：「 Bleem 」，原文：<a href="https://tinyurl.com/y5snvvlp" target="_blank" rel="noopener">https://tinyurl.com/y5snvvlp</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、版本信息&quot;&gt;一、版本信息&lt;/h2&gt;
&lt;p&gt;目前采用 MySQL fork 版本 Percona Server 5.7.28，监控方面选择 Percona Monitoring and Management 2.1.0，对应监控 Client 版本为 2.1.0&lt;/p&gt;
&lt;h2 id=&quot;二、Percona-Server-安装&quot;&gt;二、Percona Server 安装&lt;/h2&gt;
&lt;p&gt;为保证兼容以及稳定性，MySQL 宿主机系统选择 CentOS 7，Percona Server 安装方式为 rpm 包，安装后由 Systemd 守护&lt;/p&gt;
&lt;h3 id=&quot;2-1、下载安装包&quot;&gt;2.1、下载安装包&lt;/h3&gt;
&lt;p&gt;安装包下载地址为 &lt;a href=&quot;https://www.percona.com/downloads/Percona-Server-5.7/LATEST/%EF%BC%8C%E4%B8%8B%E8%BD%BD%E6%97%B6%E9%80%89%E6%8B%A9&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.percona.com/downloads/Percona-Server-5.7/LATEST/，下载时选择&lt;/a&gt; &lt;code&gt;Download All Packages Together&lt;/code&gt;，下载后是所有组件全量的压缩 tar 包。&lt;/p&gt;
&lt;h3 id=&quot;2-2、安装前准备&quot;&gt;2.2、安装前准备&lt;/h3&gt;
&lt;p&gt;针对 CentOS 7 系统，安装前升级所有系统组件库，执行 &lt;code&gt;yum update&lt;/code&gt; 既可；大部份 &lt;strong&gt;CentOS 7 安装后可能会附带 &lt;code&gt;mariadb-libs&lt;/code&gt; 包，这个包会默认创建一些配置文件，导致后面的 Percona Server 无法覆盖它(例如 &lt;code&gt;/etc/my.cnf&lt;/code&gt;)，所以安装 Percona Server 之前需要卸载它 &lt;code&gt;yum remove mariadb-libs&lt;/code&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;针对于数据存储硬盘，目前统一为 SSD 硬盘，挂载点为 &lt;code&gt;/data&lt;/code&gt;，挂载方式可以采用 &lt;code&gt;fstab&lt;/code&gt;、&lt;code&gt;systemd-mount&lt;/code&gt;，分区格式目前采用 &lt;code&gt;xfs&lt;/code&gt; 格式。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SSD 优化有待补充…&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;2-3、安装-Percona-Server&quot;&gt;2.3、安装 Percona Server&lt;/h3&gt;
&lt;p&gt;Percona Server tar 包解压后会有 9 个 rpm 包，实际安装时只需要安装其中 4 个既可&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;yum install Percona-Server-client-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-server-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-shared-57-5.7.28-31.1.el7.x86_64.rpm Percona-Server-shared-compat-57-5.7.28-31.1.el7.x86_64.rpm&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="MySQL" scheme="https://www.hi-linux.com/categories/MySQL/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="MySQL" scheme="https://www.hi-linux.com/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>Caddy 2.0 简明教程</title>
    <link href="https://www.hi-linux.com/posts/29316.html"/>
    <id>https://www.hi-linux.com/posts/29316.html</id>
    <published>2021-01-11T01:00:00.000Z</published>
    <updated>2021-01-11T08:08:38.585Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Caddy 是一个 Go 编写的 Web 服务器，类似于 Nginx，Caddy 提供了更加强大的功能，随着 v2 版本发布 Caddy 已经可以作为中小型站点 Web 服务器的另一个选择；相较于 Nginx 来说使用 Caddy 的优势如下:</p><ul><li>自动的 HTTPS 证书申请(ACME HTTP/DNS 挑战)</li><li>自动证书续期以及 OCSP stapling 等</li><li>更高的安全性包括但不限于 TLS 配置以及内存安全等</li><li>友好且强大的配置文件支持</li><li>支持 API 动态调整配置(有木有人可以搞个 Dashboard？)</li><li>支持 HTTP3(QUIC)</li><li>支持动态后端，例如连接 Consul、作为 k8s ingress 等</li><li>后端多种负载策略以及健康检测等</li><li>本身 Go 编写，高度模块化的系统方便扩展(CoreDNS 基于 Caddy1 开发)</li><li>……</li></ul><p>就目前来说，Caddy 对于我个人印象唯一的缺点就是性能没有 Nginx 高，但是这是个仁者见仁智者见智的问题；相较于提供的这些便利性，在性能可接受的情况下完全有理由切换到 Caddy。</p><a id="more"></a><h2><span id="编译-caddy2">编译 Caddy2</span></h2><blockquote><p>注意: 在 Caddy1 时代，Caddy 官方发布的预编译二进制文件是不允许进行商业使用的，Caddy2 以后已经全部切换到 Apache 2.0 License，具体请参考 <a href="https://github.com/caddyserver/caddy/issues/2786" target="_blank" rel="noopener">issue#2786</a>。</p></blockquote><p>在默认情况下 Caddy2 官方提供了预编译的二进制文件，以及<a href="https://caddyserver.com/download" target="_blank" rel="noopener">自定义 build 下载页面</a>，不过对于需要集成一些第三方插件时，我们仍需采用官方提供的 <a href="https://github.com/caddyserver/xcaddy" target="_blank" rel="noopener">xcaddy</a> 来进行自行编译；以下为具体的编译过程:</p><h3><span id="11-golang-环境安装">1.1、Golang 环境安装</span></h3><blockquote><p><strong>本部分编译环境默认为 Ubuntu 20.04 系统，同时使用 root 用户，其他环境请自行调整相关目录以及配置；编译时自行处理好科学上网相关配置，也可以直接用国外 VPS 服务器编译。</strong></p></blockquote><p>首先下载 go 语言的 SDK 压缩包，其他平台可以从 <a href="https://golang.org/dl/" target="_blank" rel="noopener">https://golang.org/dl/</a> 下载对应的压缩包:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;golang.org&#x2F;dl&#x2F;go1.15.6.linux-amd64.tar.gz</span><br></pre></td></tr></table></figure><p>下载完成后解压并配置相关变量:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># 解压</span><br><span class="line">tar -zxvf go1.15.6.linux-amd64.tar.gz</span><br><span class="line"></span><br><span class="line"># 移动到任意目录</span><br><span class="line">mkdir -p &#x2F;opt&#x2F;devtools</span><br><span class="line">mv go &#x2F;opt&#x2F;devtools&#x2F;go</span><br><span class="line"></span><br><span class="line"># 创建 go 相关目录</span><br><span class="line">mkdir -p $&#123;HOME&#125;&#x2F;gopath&#x2F;&#123;src,bin,pkg&#125;</span><br><span class="line"></span><br><span class="line"># 调整变量配置，将以下变量加入到 shell 初始化配置中</span><br><span class="line"># bash 用户请编辑 ~&#x2F;.bashrc</span><br><span class="line"># zsh 用户请编辑 ~&#x2F;.zshrc</span><br><span class="line">export GOROOT&#x3D;&#39;&#x2F;opt&#x2F;devtools&#x2F;go&#39;</span><br><span class="line">export GOPATH&#x3D;&quot;$&#123;HOME&#125;&#x2F;gopath&quot;</span><br><span class="line">export GOPROXY&#x3D;&#39;https:&#x2F;&#x2F;goproxy.cn&#39; # 如果已经解决了科学上网问题，GOPROXY 变量可以删除，否则可能会起反作用</span><br><span class="line">export PATH&#x3D;&quot;$&#123;GOROOT&#125;&#x2F;bin:$&#123;GOPATH&#125;&#x2F;bin:$&#123;PATH&#125;&quot;</span><br><span class="line"></span><br><span class="line"># 让配置生效</span><br><span class="line"># bash 用户替换成 ~&#x2F;.basrc</span><br><span class="line"># 重新退出登录也可以</span><br><span class="line">source ~&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>配置完成后，应该在命令行执行 <code>go version</code> 有成功返回:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bleem ➜ ~ go version</span><br><span class="line">go version go1.15.6 linux&#x2F;amd64</span><br></pre></td></tr></table></figure><h3><span id="12-安装-xcaddy">1.2、安装 xcaddy</span></h3><p>按照官方文档直接命令行执行 <code>go get -u github.com/caddyserver/xcaddy/cmd/xcaddy</code> 安装即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bleem ➜ ~ go get -u github.com&#x2F;caddyserver&#x2F;xcaddy&#x2F;cmd&#x2F;xcaddy</span><br><span class="line">go: downloading github.com&#x2F;caddyserver&#x2F;xcaddy v0.1.7</span><br><span class="line">go: found github.com&#x2F;caddyserver&#x2F;xcaddy&#x2F;cmd&#x2F;xcaddy in github.com&#x2F;caddyserver&#x2F;xcaddy v0.1.7</span><br><span class="line">go: downloading github.com&#x2F;Masterminds&#x2F;semver&#x2F;v3 v3.1.0</span><br><span class="line">go: github.com&#x2F;Masterminds&#x2F;semver&#x2F;v3 upgrade &#x3D;&gt; v3.1.1</span><br><span class="line">go: downloading github.com&#x2F;Masterminds&#x2F;semver&#x2F;v3 v3.1.1</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><p>安装完成后应当在命令行可以直接执行 <code>xcaddy</code> 命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># xcaddy 并没有提供完善的命令行支持，所以 &#96;--help&#96; 报错很正常</span><br><span class="line">bleem ➜  ~ xcaddy --help</span><br><span class="line">go: cannot match &quot;all&quot;: working directory is not part of a module</span><br><span class="line">2021&#x2F;01&#x2F;07 12:15:56 [ERROR] exec [go list -m -f&#x3D;&#123;&#123;if .Replace&#125;&#125;&#123;&#123;.Path&#125;&#125; &#x3D;&gt; &#123;&#123;.Replace&#125;&#125;&#123;&#123;end&#125;&#125; all]: exit status 1:</span><br></pre></td></tr></table></figure><h3><span id="13-编译-caddy2">1.3、编译 Caddy2</span></h3><p>编译之前系统需要安装 <code>jq</code>、<code>curl</code>、<code>git</code> 命令，没有的请使用 <code>apt install -y curl git jq</code> 命令安装；</p><p>自行编译的目的是增加第三方插件方便使用，其中官方列出的插件可以从 <a href="https://caddyserver.com/download" target="_blank" rel="noopener">Download</a> 页面获取到:</p><p><a href="https://cdn.oss.link/markdown/XkJZ2R1609993722272.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/XkJZ2R1609993722272-2021-01-10-yYthRl.png" alt="XkJZ2R1609993722272"></a></p><p>其他插件可以从 GitHub 上寻找或者自行编写，整理好这些插件列表以后只需要使用 <code>xcaddy</code> 编译即可:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 获取最新版本号，其实直接去 GitHub realse 页复制以下就行</span><br><span class="line"># 这里转化为脚本是为了方便自动化</span><br><span class="line">export version&#x3D;$(curl -s &quot;https:&#x2F;&#x2F;api.github.com&#x2F;repos&#x2F;caddyserver&#x2F;caddy&#x2F;releases&#x2F;latest&quot; | jq -r .tag_name)</span><br><span class="line"></span><br><span class="line"># 使用 xcaddy 编译</span><br><span class="line">xcaddy build $&#123;version&#125; --output .&#x2F;caddy_$&#123;version&#125; \</span><br><span class="line">        --with github.com&#x2F;abiosoft&#x2F;caddy-exec \</span><br><span class="line">        --with github.com&#x2F;caddy-dns&#x2F;cloudflare \</span><br><span class="line">        --with github.com&#x2F;caddy-dns&#x2F;dnspod \</span><br><span class="line">        --with github.com&#x2F;caddy-dns&#x2F;duckdns \</span><br><span class="line">        --with github.com&#x2F;caddy-dns&#x2F;gandi \</span><br><span class="line">        --with github.com&#x2F;caddy-dns&#x2F;route53 \</span><br><span class="line">        --with github.com&#x2F;greenpau&#x2F;caddy-auth-jwt \</span><br><span class="line">        --with github.com&#x2F;greenpau&#x2F;caddy-auth-portal \</span><br><span class="line">        --with github.com&#x2F;greenpau&#x2F;caddy-trace \</span><br><span class="line">        --with github.com&#x2F;hairyhenderson&#x2F;caddy-teapot-module \</span><br><span class="line">        --with github.com&#x2F;kirsch33&#x2F;realip \</span><br><span class="line">        --with github.com&#x2F;porech&#x2F;caddy-maxmind-geolocation \</span><br><span class="line">        --with github.com&#x2F;caddyserver&#x2F;format-encoder \</span><br><span class="line">        --with github.com&#x2F;mholt&#x2F;caddy-webdav</span><br></pre></td></tr></table></figure><p>编译过程日志如下所示，稍等片刻后将会生成编译好的二进制文件:</p><p><a href="https://cdn.oss.link/markdown/Kr2tG61609993987722.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/Kr2tG61609993987722-2021-01-10-dHDkEg.png" alt="Kr2tG61609993987722"></a></p><p>编译成功后可以通过 <code>list-modules</code> 子命令查看被添加的插件是否成功编译到了 caddy 中:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">bleem ➜  ~ .&#x2F;caddy_v2.3.0 list-modules</span><br><span class="line">admin.api.load</span><br><span class="line">admin.api.metrics</span><br><span class="line">caddy.adapters.caddyfile</span><br><span class="line">caddy.listeners.tls</span><br><span class="line">caddy.logging.encoders.console</span><br><span class="line">caddy.logging.encoders.filter</span><br><span class="line">caddy.logging.encoders.filter.delete</span><br><span class="line">caddy.logging.encoders.filter.ip_mask</span><br><span class="line">caddy.logging.encoders.formatted</span><br><span class="line">caddy.logging.encoders.json</span><br><span class="line">caddy.logging.encoders.logfmt</span><br><span class="line">caddy.logging.encoders.single_field</span><br><span class="line">caddy.logging.writers.discard</span><br><span class="line">caddy.logging.writers.file</span><br><span class="line">caddy.logging.writers.net</span><br><span class="line">caddy.logging.writers.stderr</span><br><span class="line">caddy.logging.writers.stdout</span><br><span class="line">caddy.storage.file_system</span><br><span class="line">dns.providers.cloudflare</span><br><span class="line">dns.providers.dnspod</span><br><span class="line">dns.providers.duckdns</span><br><span class="line">dns.providers.gandi</span><br><span class="line">dns.providers.route53</span><br><span class="line">exec</span><br><span class="line">http</span><br><span class="line">http.authentication.hashes.bcrypt</span><br><span class="line">http.authentication.hashes.scrypt</span><br><span class="line">http.authentication.providers.http_basic</span><br><span class="line">http.authentication.providers.jwt</span><br><span class="line">......</span><br></pre></td></tr></table></figure><h2><span id="安装-caddy2">安装 Caddy2</span></h2><h3><span id="21-宿主机安装">2.1、宿主机安装</span></h3><p>宿主机安装 Caddy2 需要使用 systemd 进行守护，幸运的是 Caddy2 官方提供了各种平台的安装包以及 <a href="https://github.com/caddyserver/dist" target="_blank" rel="noopener">systemd 配置文件仓库</a>；目前推荐的方式是直接采用包管理器安装标准版本的 Caddy2，然后替换自编译的可执行文件:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 安装标准版本 Caddy2</span><br><span class="line">sudo apt install -y debian-keyring debian-archive-keyring apt-transport-https</span><br><span class="line">curl -1sLf &#39;https:&#x2F;&#x2F;dl.cloudsmith.io&#x2F;public&#x2F;caddy&#x2F;stable&#x2F;cfg&#x2F;gpg&#x2F;gpg.155B6D79CA56EA34.key&#39; | sudo apt-key add -</span><br><span class="line">curl -1sLf &#39;https:&#x2F;&#x2F;dl.cloudsmith.io&#x2F;public&#x2F;caddy&#x2F;stable&#x2F;cfg&#x2F;setup&#x2F;config.deb.txt?distro&#x3D;debian&amp;version&#x3D;any-version&#39; | sudo tee -a &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;caddy-stable.list</span><br><span class="line">sudo apt update</span><br><span class="line">sudo apt install caddy</span><br><span class="line"></span><br><span class="line"># 替换二进制文件</span><br><span class="line">systemctl stop caddy</span><br><span class="line">rm -f &#x2F;usr&#x2F;bin&#x2F;caddy</span><br><span class="line">mv .&#x2F;caddy_v2.3.0 &#x2F;usr&#x2F;bin&#x2F;caddy</span><br></pre></td></tr></table></figure><h3><span id="22-docker-安装">2.2、Docker 安装</span></h3><p>Docker 用户可以通过 Dockerfile 自行编译 image，目前我编写了一个基于 xcaddy 的 <a href="https://github.com/mritd/dockerfile/blob/master/caddy/Dockerfile" target="_blank" rel="noopener">Dockerfile</a>，如果有其他插件需要集成自行修改重新编译即可；当前 Dockerfile 预编译的镜像已经推送到了 <a href="https://hub.docker.com/repository/docker/mritd/caddy" target="_blank" rel="noopener">Docker Hub</a> 中，镜像名称为 <code>mritd/caddy</code>。</p><h2><span id="配置-caddy2">配置 Caddy2</span></h2><p>Caddy2 的配置文件核心采用 json，但是 json 可读性不强，所以官方维护了一个转换器，抽象出称之为 Caddyfile 的新配置格式；关于 Caddyfile 的完整语法请查看官方文档 <a href="https://caddyserver.com/docs/caddyfile%EF%BC%8C%E6%9C%AC%E6%96%87%E4%BB%85%E5%81%9A%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E7%9A%84%E6%A0%B7%E4%BE%8B%E3%80%82" target="_blank" rel="noopener">https://caddyserver.com/docs/caddyfile，本文仅做一些基本使用的样例。</a></p><h3><span id="31-配置片段">3.1、配置片段</span></h3><p>Caddyfile 支持类似代码中 function 一样的配置片段，**这些配置片段可以在任意位置被 <code>import</code>，同时可以接受参数，**以下为配置片断示例:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 括号内为片段名称，可以自行定义</span><br><span class="line">(TLS) &#123;</span><br><span class="line">    protocols tls1.2 tls1.3</span><br><span class="line">    ciphers TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 在任意位置可以引用此片段从而达到配置复用</span><br><span class="line">import TLS</span><br></pre></td></tr></table></figure><h3><span id="32-配置模块化">3.2、配置模块化</span></h3><p><code>import</code> 指令除了支持引用配置片段以外，还支持引用外部文件，同时支持通配符，有了这个命令以后我们就可以方便的将配置文件进行模块化处理:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 引用外部的 &#x2F;etc&#x2F;caddy&#x2F;*.caddy</span><br><span class="line">import &#x2F;etc&#x2F;caddy&#x2F;*.caddy</span><br></pre></td></tr></table></figure><h3><span id="33-站点配置">3.3、站点配置</span></h3><p>针对于站点域名配置，Caddyfile 比较自由化，其格式如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">地址 &#123;</span><br><span class="line">    站点配置</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于这个 “地址” 接受多种格式，以下都为合法的地址格式:</p><ul><li><code>localhost</code></li><li><code>example.com</code></li><li><code>:443</code></li><li><code>http://example.com</code></li><li><code>localhost:8080</code></li><li><code>127.0.0.1</code></li><li><code>[::1]:2015</code></li><li><code>example.com/foo/*</code></li><li><code>*.example.com</code></li><li><code>http://</code></li></ul><h3><span id="34-环境变量">3.4、环境变量</span></h3><p>Caddyfile 支持直接引用系统环境变量，通过此功能可以将一些敏感信息从配置文件中剔除:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 引用环境变量 GANDI_API_TOKEN</span><br><span class="line">dns gandi &#123;$GANDI_API_TOKEN&#125;</span><br></pre></td></tr></table></figure><h3><span id="35-配置片段参数支持">3.5、配置片段参数支持</span></h3><p>针对于配置片段，Caddyfile 还支持类似于函数代码的参数支持，通过参数支持可以让外部引用时动态修改配置信息:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">(LOG) &#123;</span><br><span class="line">    log &#123;</span><br><span class="line">        format json  &#123;</span><br><span class="line">            time_format &quot;iso8601&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        # &quot;&#123;args.0&#125;&quot; 引用传入的第一个参数，此处用于动态传入日志文件名称</span><br><span class="line">        output file &quot;&#123;args.0&#125;&quot; &#123;</span><br><span class="line">            roll_size 100mb</span><br><span class="line">            roll_keep 3</span><br><span class="line">            roll_keep_for 7d</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 引用片段</span><br><span class="line">import LOG &quot;&#x2F;data&#x2F;logs&#x2F;mritd.com.log&quot;</span><br></pre></td></tr></table></figure><h3><span id="36-自动证书申请">3.6、自动证书申请</span></h3><p>在启动 Caddy2 之前，如果目标域名(例如: <code>www.example.com</code>)已经解析到了本机，那么 Caddy2 启动后会尝试自动通过 ACME HTTP 挑战申请证书；如果期望使用 DNS 的方式申请证书则需要其他 DNS 插件支持，比如上面编译的 <code>--with github.com/caddy-dns/gandi</code> 为 gandi 服务商的 DNS 插件；关于使用 DNS 挑战的配置编写方式需要具体去看其插件文档，目前 gandi 的配置如下:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tls &#123;</span><br><span class="line">dns gandi &#123;env.GANDI_API_TOKEN&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置完成后 Caddy2 会通过 ACME DNS 挑战申请证书，<strong>值得注意的是即使通过 DNS 申请证书默认也不会申请泛域名证书，如果想要调整这种细节配置请使用 json 配置或管理 API。</strong></p><h3><span id="37-完整模块化配置样例">3.7、完整模块化配置样例</span></h3><p>了解了以上基础配置信息，我们就可以实际编写一个站点配置了；以下为本站的 Caddy 配置样例:</p><p>目录结构:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">caddy</span><br><span class="line">├── Caddyfile</span><br><span class="line">├── mritd.com.caddy</span><br><span class="line">└── mritd.me.caddy</span><br></pre></td></tr></table></figure><h4><span id="371-caddyfile">3.7.1、Caddyfile</span></h4><p><strong>Caddyfile 主要包含一些通用的配置，并将其抽到配置片段中，类似与 nginx 的 <code>nginx.conf</code> 主配置；在最后部分通过 <code>import</code> 关键字引入其他具体站点配置，类似 nginx 的 <code>vhost</code> 配置。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">(LOG) &#123;</span><br><span class="line">    log &#123;</span><br><span class="line">        # 日志格式参考 https:&#x2F;&#x2F;github.com&#x2F;caddyserver&#x2F;format-encoder 插件文档</span><br><span class="line">        format formatted &quot;[&#123;ts&#125;] &#123;request&gt;remote_addr&#125; &#123;request&gt;proto&#125; &#123;request&gt;method&#125; &lt;- &#123;status&#125; -&gt; &#123;request&gt;host&#125; &#123;request&gt;uri&#125; &#123;request&gt;headers&gt;User-Agent&gt;[0]&#125;&quot;  &#123;</span><br><span class="line">            time_format &quot;iso8601&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        output file &quot;&#123;args.0&#125;&quot; &#123;</span><br><span class="line">            roll_size 100mb</span><br><span class="line">            roll_keep 3</span><br><span class="line">            roll_keep_for 7d</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">(TLS) &#123;</span><br><span class="line">    # TLS 配置采用 https:&#x2F;&#x2F;mozilla.github.io&#x2F;server-side-tls&#x2F;ssl-config-generator&#x2F; 生成，SSL Labs 评分 A+</span><br><span class="line">    protocols tls1.2 tls1.3</span><br><span class="line">    ciphers TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">(HSTS) &#123;</span><br><span class="line">    # HSTS (63072000 seconds)</span><br><span class="line">    header &#x2F; Strict-Transport-Security &quot;max-age&#x3D;63072000&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">(ACME_GANDI) &#123;</span><br><span class="line">    # 从环境变量获取 GANDI_API_TOKEN</span><br><span class="line">    dns gandi &#123;$GANDI_API_TOKEN&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 聚合上面的配置片段为新的片段</span><br><span class="line">(COMMON_CONFIG) &#123;</span><br><span class="line">    # 压缩支持</span><br><span class="line">    encode zstd gzip</span><br><span class="line"></span><br><span class="line">    # TLS 配置</span><br><span class="line">    tls &#123;</span><br><span class="line">        import TLS</span><br><span class="line">        import ACME_GANDI</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    # HSTS</span><br><span class="line">    import HSTS</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 开启 HTTP3 实验性支持</span><br><span class="line">&#123;</span><br><span class="line">    servers :443 &#123;</span><br><span class="line">        protocol &#123;</span><br><span class="line">            experimental_http3</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 引入其他具体的站点配置</span><br><span class="line">import &#x2F;etc&#x2F;caddy&#x2F;*.caddy</span><br></pre></td></tr></table></figure><h4><span id="372-mritdcomcaddy">3.7.2、mritd.com.caddy</span></h4><p><strong><code>mritd.com.caddy</code> 为主站点配置，主站点配置内主要编写一些路由规则，TLS 等都从配置片段引入，这样可以保持统一。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">www.mritd.com &#123;</span><br><span class="line">    # 重定向到 mritd.com(默认 302)</span><br><span class="line">    redir https:&#x2F;&#x2F;mritd.com&#123;uri&#125;</span><br><span class="line"></span><br><span class="line">    # 日志</span><br><span class="line">    import LOG &quot;&#x2F;data&#x2F;logs&#x2F;mritd.com.log&quot;</span><br><span class="line"></span><br><span class="line">    # TLS、HSTS、ACME 等通用配置</span><br><span class="line">    import COMMON_CONFIG</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mritd.com &#123;</span><br><span class="line">    # 路由</span><br><span class="line">    route &#x2F;* &#123;</span><br><span class="line">        reverse_proxy mritd_com:80</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    # 日志</span><br><span class="line">    import LOG &quot;&#x2F;data&#x2F;logs&#x2F;mritd.com.log&quot;</span><br><span class="line"></span><br><span class="line">    # TLS、HSTS、ACME 等通用配置</span><br><span class="line">    import COMMON_CONFIG</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4><span id="373-mritdmecaddy">3.7.3、mritd.me.caddy</span></h4><p><strong><code>mritd.me.caddy</code> 为老站点配置，目前主要将其 301 到新站点即可。</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">www.mritd.me &#123;</span><br><span class="line">    # 重定向到 mritd.com</span><br><span class="line">    # 最后的 &quot;code&quot; 支持三种参数</span><br><span class="line">    # temporary &#x3D;&gt; 302</span><br><span class="line">    # permanent &#x3D;&gt; 301</span><br><span class="line">    # html &#x3D;&gt; HTML document redirect</span><br><span class="line">    redir https:&#x2F;&#x2F;mritd.com&#123;uri&#125; permanent</span><br><span class="line"></span><br><span class="line">    # 日志</span><br><span class="line">    import LOG &quot;&#x2F;data&#x2F;logs&#x2F;mritd.com.log&quot;</span><br><span class="line"></span><br><span class="line">    # TLS、HSTS、ACME 等通用配置</span><br><span class="line">    import COMMON_CONFIG</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mritd.me &#123;</span><br><span class="line">    # 重定向</span><br><span class="line">    redir https:&#x2F;&#x2F;mritd.com&#123;uri&#125; permanent</span><br><span class="line"></span><br><span class="line">    # 日志</span><br><span class="line">    import LOG &quot;&#x2F;data&#x2F;logs&#x2F;mritd.com.log&quot;</span><br><span class="line"></span><br><span class="line">    # TLS、HSTS、ACME 等通用配置</span><br><span class="line">    import COMMON_CONFIG</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2><span id="启动与重载">启动与重载</span></h2><p>配置文件编写完成后，通过 <code>systemctl start caddy</code> 可启动 caddy 服务器；每次配置修改后可以通过 <code>systemctl reload caddy</code> 进行配置重载，重载期间 caddy 不会重启(实际上调用 <code>caddy reload</code> 命令)，<strong>当配置文件书写错误时，重载只会失败，不会影响正在运行的 caddy 服务器。</strong></p><h2><span id="总结">总结</span></h2><p>本文只是列举了一些简单的 Caddy 使用样例，在强大的插件配合下，Caddy 可以实现各种 “神奇” 的功能，这些功能依赖于复杂的 Caddy 配置，Caddy 配置需要仔细阅读<a href="https://caddyserver.com/docs/caddyfile/directives" target="_blank" rel="noopener">官方文档</a>，关于 Caddyfile 的每个配置段在文档中都有详细的描述。</p><p>值得一提的是 Caddy 本身内置了丰富的插件，例如内置 “file_server”、内置各种负载均衡策略等，这些插件组合在一起可以实现一些复杂的功能；Caddy 是采用 go 编写的，官方也给出了详细的<a href="https://caddyserver.com/docs/extending-caddy" target="_blank" rel="noopener">开发文档</a>，相较于 Nginx 来说通过 Lua 或者 C 来开发编写插件来说，Caddy 的插件开发上手要容易得多；Caddy 本身针对数据存储、动态后端、配置文件转换等都内置了扩展接口，这为有特定需求的扩展开发打下了良好基础。</p><p>最终总结，综合来看目前 Caddy2 的性能损失可接受的情况下，相较于 Nginx 绝对是个绝佳选择，各种新功能都能够满足现代化 Web 站点的需求，真香警告。</p><blockquote><p>本文转载自：「 Bleem 」，原文：<a href="https://tinyurl.com/y5frng86" target="_blank" rel="noopener">https://tinyurl.com/y5frng86</a> ，版权归原作者所有。欢迎投稿，投稿邮箱: <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Caddy 是一个 Go 编写的 Web 服务器，类似于 Nginx，Caddy 提供了更加强大的功能，随着 v2 版本发布 Caddy 已经可以作为中小型站点 Web 服务器的另一个选择；相较于 Nginx 来说使用 Caddy 的优势如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自动的 HTTPS 证书申请(ACME HTTP/DNS 挑战)&lt;/li&gt;
&lt;li&gt;自动证书续期以及 OCSP stapling 等&lt;/li&gt;
&lt;li&gt;更高的安全性包括但不限于 TLS 配置以及内存安全等&lt;/li&gt;
&lt;li&gt;友好且强大的配置文件支持&lt;/li&gt;
&lt;li&gt;支持 API 动态调整配置(有木有人可以搞个 Dashboard？)&lt;/li&gt;
&lt;li&gt;支持 HTTP3(QUIC)&lt;/li&gt;
&lt;li&gt;支持动态后端，例如连接 Consul、作为 k8s ingress 等&lt;/li&gt;
&lt;li&gt;后端多种负载策略以及健康检测等&lt;/li&gt;
&lt;li&gt;本身 Go 编写，高度模块化的系统方便扩展(CoreDNS 基于 Caddy1 开发)&lt;/li&gt;
&lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;就目前来说，Caddy 对于我个人印象唯一的缺点就是性能没有 Nginx 高，但是这是个仁者见仁智者见智的问题；相较于提供的这些便利性，在性能可接受的情况下完全有理由切换到 Caddy。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Caddy" scheme="https://www.hi-linux.com/categories/Caddy/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Caddy" scheme="https://www.hi-linux.com/tags/Caddy/"/>
    
  </entry>
  
  <entry>
    <title>终端共享神器 Tmate 简明教程</title>
    <link href="https://www.hi-linux.com/posts/37112.html"/>
    <id>https://www.hi-linux.com/posts/37112.html</id>
    <published>2021-01-05T01:00:00.000Z</published>
    <updated>2021-01-05T01:29:13.206Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>在 Unix/Linux 上工作，最常用的就是 Terminal。那么，如何将你的 Terminal 共享给别人一起协同工作（你帮助别人解决问题或者请别人帮助你解决问题）呢？很简单，使用终端共享神器 <a href="https://tmate.io/" target="_blank" rel="noopener">tmate</a>。</p><h2><span id="1-刚性需求">1. 刚性需求</span></h2><p>绝大多数人都不是万能的，总有需要他人现场指导或提供帮助的时候。那么，在无法面对面交谈的时候（尤其是在新冠病毒肆虐的特殊时期），把你的终端 (Terminal) 共享出去，就可以实现即时且所见即所得的 1:1 协助。同样地，基于 Terminal 的结对编程或 Code Review 也迫切需要终端共享。</p><h2><span id="2-基本流程">2. 基本流程</span></h2><h3><span id="21-安装-tmate">2.1 安装 tmate</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo dnf install tmate # &lt;&lt;&lt; Fedora</span><br><span class="line">$ sudo yum install tmate # &lt;&lt;&lt; CentOS</span><br><span class="line">$ sudo apt install tmate # &lt;&lt;&lt; Ubuntu</span><br></pre></td></tr></table></figure><h3><span id="22-启动-tmate">2.2 启动 tmate</span></h3><p>假定 A 现在需要 B 的帮助，于是，A 在他的终端 (Terminal) 上键入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A$ tmate</span><br></pre></td></tr></table></figure><p>然后就可以看到类似下图的界面：</p><p><img src="https://img2018.cnblogs.com/blog/1094457/202002/1094457-20200229195401211-1496580497.png" alt="img"></p><p>现在 A 通过即时通讯软件（如 IRC）将  &quot;<strong>ssh session:&quot;</strong> 后面的那串消息 “<strong>ssh <a href="mailto:3vRvL79HMtjmBetF37REVvU4z@sfo2.tmate.io">3vRvL79HMtjmBetF37REVvU4z@sfo2.tmate.io</a></strong>” 发送给B，然后等待 B 的连接。</p><p>**注意：**如果 A 没有 SSH key ,需要事先创建一个，命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A$ ssh-keygen</span><br></pre></td></tr></table></figure><a id="more"></a><h3><span id="23-通过-ssh-进行连接">2.3 通过 ssh 进行连接</span></h3><p>B 在其终端输入：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">B$ ssh 3vRvL79HMtjmBetF37REVvU4z@sfo2.tmate.io</span><br></pre></td></tr></table></figure><p>然后 B 和 A 就共享了同一个 Terminal。无论是 A 还是 B 都可以操作该 Terminal。</p><h3><span id="24-关闭连接">2.4 关闭连接</span></h3><p>A 在他的终端 (Terminal) 上键入 exit 即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A$ exit</span><br></pre></td></tr></table></figure><h2><span id="3-基本原理">3. 基本原理</span></h2><p>tmate 即 teammates，是 tmux 的一个分支，并且和 tmux 使用相同的配置信息 (i.e:tmate 可与 tmux 共享 ~/.tmux.conf )。**tmate 不仅是一个终端多路复用器，而且具有即时分享终端的能力。**它允许在单个屏幕中创建并操控多个终端，同时这些终端还能与其他人分享。总的来说，tmux 支持的窗口 (window) 和窗格 (pane) 功能，tmate 都支持。tmate 的基本工作原理如下：</p><ul><li>运行 tmate 时，会在后台创建一个连接到 <a href="http://tmate.io" target="_blank" rel="noopener">tmate.io</a>(由 tmate 开发者维护的后台服务器)的 ssh 连接；</li><li><a href="http://tmate.io" target="_blank" rel="noopener">tmate.io</a> 服务器的 ssh 密钥通过 DH 交换进行校验；</li><li>客户端通过本地 ssh 密钥进行认证；</li><li>连接创建后，本地 tmux 服务器会生成一个 150 位(不可猜测的随机字符)会话令牌；</li><li>队友能通过用户提供的 SSH 会话 ID 连接到 <a href="http://tmate.io" target="_blank" rel="noopener">tmate.io</a>。</li></ul><h2><span id="4-常见命令">4. 常见命令</span></h2><h3><span id="41-显示连接信息">4.1 显示连接信息</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ tmate show-messages</span><br><span class="line">Sat Feb 29 20:32:31 2020 [tmate] Connecting to master.tmate.io...</span><br><span class="line">Sat Feb 29 20:32:37 2020 [tmate] Note: clear your terminal before sharing readonly access</span><br><span class="line">Sat Feb 29 20:32:37 2020 [tmate] web session read only: https:&#x2F;&#x2F;tmate.io&#x2F;t&#x2F;ro-59nhrEMMpr8fvYEfW3LbU69r9</span><br><span class="line">Sat Feb 29 20:32:37 2020 [tmate] ssh session read only: ssh ro-59nhrEMMpr8fvYEfW3LbU69r9@nyc1.tmate.io</span><br><span class="line">Sat Feb 29 20:32:37 2020 [tmate] web session: https:&#x2F;&#x2F;tmate.io&#x2F;t&#x2F;2VFPtcBNnhaNRGWmKgKZH3zfn</span><br><span class="line">Sat Feb 29 20:32:37 2020 [tmate] ssh session: ssh 2VFPtcBNnhaNRGWmKgKZH3zfn@nyc1.tmate.io</span><br></pre></td></tr></table></figure><h3><span id="42-分离接入查看">4.2 分离/接入/查看</span></h3><ul><li>指定 socket 文件启动</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tmate -S &#x2F;tmp&#x2F;foo.sock</span><br></pre></td></tr></table></figure><ul><li>分离</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tmate detach</span><br></pre></td></tr></table></figure><ul><li>接入</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tmate -S &#x2F;tmp&#x2F;foo.sock attach</span><br></pre></td></tr></table></figure><ul><li>查看</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ tmate -S &#x2F;tmp&#x2F;foo.sock ls</span><br><span class="line">0: 2 windows (created Sat Feb 29 20:40:02 2020) [144x35]</span><br></pre></td></tr></table></figure><ul><li>关闭会话</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tmate -S &#x2F;tmp&#x2F;foo.sock kill-session</span><br></pre></td></tr></table></figure><h2><span id="5-更多连接方式">5. 更多连接方式</span></h2><p>tmate 支持 4 种连接方式，ssh、ssh-ro、web 和 web-ro。其中，ssh、web 支持读写访问，ssh-ro、web-ro 支持只读访问。下面就是 web 只读访问方式的截图。</p><p><img src="https://img.hi-linux.com/staticfile/1094457-20200229205228053-1804451785-2020-12-31-0fSzAI.png" alt="img"></p><p><strong>注意：</strong> tmate 启动之后，过几分钟后再执行 tmate show-messages 就会失效，那么需要重新获取连接信息的话，可以使用下面的脚本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> 1 #!&#x2F;bin&#x2F;bash</span><br><span class="line"> 2 </span><br><span class="line"> 3 function usage</span><br><span class="line"> 4 &#123;</span><br><span class="line"> 5         echo &quot;Usage: $1 &lt;sock&gt; [sshrw|webrw|sshro|webro]&quot; &gt;&amp;2</span><br><span class="line"> 6 &#125;</span><br><span class="line"> 7 </span><br><span class="line"> 8 tmate_sock&#x3D;$1</span><br><span class="line"> 9 msg_type&#x3D;$&#123;2:-&quot;sshrw&quot;&#125;</span><br><span class="line">10 [[ -z $tmate_sock ]] &amp;&amp; usage $0 &amp;&amp; exit 1</span><br><span class="line">11 </span><br><span class="line">12 case $msg_type in</span><br><span class="line">13         &quot;sshrw&quot;) tmate -S $tmate_sock display -p &#39;#&#123;tmate_ssh&#125;&#39;    ;;</span><br><span class="line">14         &quot;sshro&quot;) tmate -S $tmate_sock display -p &#39;#&#123;tmate_ssh_ro&#125;&#39; ;;</span><br><span class="line">15         &quot;webrw&quot;) tmate -S $tmate_sock display -p &#39;#&#123;tmate_web&#125;&#39;    ;;</span><br><span class="line">16         &quot;webro&quot;) tmate -S $tmate_sock display -p &#39;#&#123;tmate_web_ro&#125;&#39; ;;</span><br><span class="line">17         *) usage $0; exit 1; ;;</span><br><span class="line">18 esac</span><br><span class="line">19 exit $?</span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ .&#x2F;foo.sh &#x2F;tmp&#x2F;foo.sock sshrw</span><br><span class="line">ssh JHELdz9a3EvTcL5w5beVMvwde@sfo2.tmate.io</span><br><span class="line">$ .&#x2F;foo.sh &#x2F;tmp&#x2F;foo.sock sshro</span><br><span class="line">ssh ro-Nv7fk2YT3urVEAEFCSexx7XHw@sfo2.tmate.io</span><br><span class="line">$ .&#x2F;foo.sh &#x2F;tmp&#x2F;foo.sock webrw</span><br><span class="line">https:&#x2F;&#x2F;tmate.io&#x2F;t&#x2F;JHELdz9a3EvTcL5w5beVMvwde</span><br><span class="line">$ .&#x2F;foo.sh &#x2F;tmp&#x2F;foo.sock webro</span><br><span class="line">https:&#x2F;&#x2F;tmate.io&#x2F;t&#x2F;ro-Nv7fk2YT3urVEAEFCSexx7XHw</span><br></pre></td></tr></table></figure><blockquote><p>新版本的 Tmate，可以用 tmate -F 随时显示相关连接信息。</p></blockquote><h2><span id="6访问控制">6.访问控制</span></h2><p>通常情况下，鉴于 tmate 生成的共享链接(ssh or web)在提供给他人访问的时候无需任何安全验证，而且此连接存储在 <a href="http://tmate.io" target="_blank" rel="noopener">tmate.io</a> 的服务器上，所以在使用此功能的时候请保持谨慎。</p><ul><li>第一，只把共享链接发送给你所信任的人知晓;</li><li>第二，如无必要，请仅仅发送只读链接；</li><li>第三，一旦共享结束，请及时关闭会话。</li></ul><p>那么，如何实现访问控制呢？</p><ul><li><p>将你所信任的人 B 的公钥加入到你 (A) 的 ~/.ssh/authorized_keys 文件中，</p></li><li><p>启动 tmate 使用如下命令：</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">A$ tmate -a ~&#x2F;.ssh&#x2F;authorized_keys</span><br></pre></td></tr></table></figure><p>这样，只有B才能通过 ssh 访问你的共享链接。陌生人 C 的公钥没有保存到 A 的 ~/.ssh/authorized_keys 文件中，于是会出现如下类似的拒绝访问信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">C$ ssh ERayHQKUPZkhtVufjjFTvtfjC@sgp1.tmate.io</span><br><span class="line">ERayHQKUPZkhtVufjjFTvtfjC@sgp1.tmate.io: Permission denied (publickey).</span><br></pre></td></tr></table></figure><p>有关访问控制的更多内容，请访问 <a href="https://tmate.io/" target="_blank" rel="noopener">https://tmate.io/</a> 的 <strong>Access control</strong> 一节。</p><p><strong>特别提示：</strong> 一旦使用了基于文件  authorized_keys 的访问控制，就不能使用基于 web 的连接方式，也就是说，只能使用 ssh 进行连接。</p><h2><span id="7-参考资料">7. 参考资料</span></h2><ol><li><a href="https://www.ostechnix.com/tmate-share-terminal-instantly-anyone-anywhere/" target="_blank" rel="noopener">Tmate – Share Your Terminal Instantly To Anyone From Anywhere</a></li><li><a href="https://linuxhandbook.com/tmate/" target="_blank" rel="noopener">tmate: Instantly Share Terminal Session With Other Linux Users</a></li><li><a href="https://www.2daygeek.com/tmate-instantly-share-your-terminal-session-to-anyone-in-seconds/" target="_blank" rel="noopener">tmate – To share your terminal session instantly with anyone in few seconds</a></li><li><a href="https://linux.cn/article-9096-1.html" target="_blank" rel="noopener">tmate:秒级分享你的终端会话</a></li></ol><blockquote><p>本文转载自：「veli 的博客」，原文：<a href="https://tinyurl.com/y7rnvrkt%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://tinyurl.com/y7rnvrkt，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Unix/Linux 上工作，最常用的就是 Terminal。那么，如何将你的 Terminal 共享给别人一起协同工作（你帮助别人解决问题或者请别人帮助你解决问题）呢？很简单，使用终端共享神器 &lt;a href=&quot;https://tmate.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;tmate&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&quot;1-刚性需求&quot;&gt;1. 刚性需求&lt;/h2&gt;
&lt;p&gt;绝大多数人都不是万能的，总有需要他人现场指导或提供帮助的时候。那么，在无法面对面交谈的时候（尤其是在新冠病毒肆虐的特殊时期），把你的终端 (Terminal) 共享出去，就可以实现即时且所见即所得的 1:1 协助。同样地，基于 Terminal 的结对编程或 Code Review 也迫切需要终端共享。&lt;/p&gt;
&lt;h2 id=&quot;2-基本流程&quot;&gt;2. 基本流程&lt;/h2&gt;
&lt;h3 id=&quot;2-1-安装-tmate&quot;&gt;2.1 安装 tmate&lt;/h3&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo dnf install tmate # &amp;lt;&amp;lt;&amp;lt; Fedora&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo yum install tmate # &amp;lt;&amp;lt;&amp;lt; CentOS&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo apt install tmate # &amp;lt;&amp;lt;&amp;lt; Ubuntu&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h3 id=&quot;2-2-启动-tmate&quot;&gt;2.2 启动 tmate&lt;/h3&gt;
&lt;p&gt;假定 A 现在需要 B 的帮助，于是，A 在他的终端 (Terminal) 上键入：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;A$ tmate&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后就可以看到类似下图的界面：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://img2018.cnblogs.com/blog/1094457/202002/1094457-20200229195401211-1496580497.png&quot; alt=&quot;img&quot;&gt;&lt;/p&gt;
&lt;p&gt;现在 A 通过即时通讯软件（如 IRC）将  &amp;quot;&lt;strong&gt;ssh session:&amp;quot;&lt;/strong&gt; 后面的那串消息 “&lt;strong&gt;ssh &lt;a href=&quot;mailto:3vRvL79HMtjmBetF37REVvU4z@sfo2.tmate.io&quot;&gt;3vRvL79HMtjmBetF37REVvU4z@sfo2.tmate.io&lt;/a&gt;&lt;/strong&gt;” 发送给B，然后等待 B 的连接。&lt;/p&gt;
&lt;p&gt;**注意：**如果 A 没有 SSH key ,需要事先创建一个，命令如下：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;A$ ssh-keygen&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="工具" scheme="https://www.hi-linux.com/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>一文读懂开源日志管理方案 ELK 和 EFK 的区别</title>
    <link href="https://www.hi-linux.com/posts/50197.html"/>
    <id>https://www.hi-linux.com/posts/50197.html</id>
    <published>2020-12-31T01:00:00.000Z</published>
    <updated>2020-12-31T03:37:06.152Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="前言">前言</span></h2><p>主流的 ELK (Elasticsearch, <code>Logstash</code>, Kibana) 目前已经转变为 EFK (Elasticsearch, <code>Filebeat</code> or <code>Fluentd</code>, Kibana) 比较重，对于容器云的日志方案业内也普遍推荐采用 Fluentd，我们一起来看下从 ELK 到 EFK 发生了哪些变化，与此同时我也推荐大家了解下 Grafana Loki</p><h2><span id="elk-和-efk-概述">ELK 和 EFK 概述</span></h2><p>随着现在各种软件系统的复杂度越来越高，特别是部署到云上之后，再想登录各个节点上查看各个模块的 log，基本是不可行了。因为不仅效率低下，而且有时由于安全性，不可能让工程师直接访问各个物理节点。而且现在大规模的软件系统基本都采用集群的部署方式，意味着对每个 service，会启动多个完全一样的 POD 对外提供服务，每个 container 都会产生自己的 log，仅从产生的 log 来看，你根本不知道是哪个 POD 产生的，这样对查看分布式的日志更加困难。</p><p>所以在云时代，需要一个收集并分析 log 的解决方案。首先需要将分布在各个角落的 log 收集到一个集中的地方，方便查看。收集了之后，还可以进行各种统计分析，甚至用流行的大数据或 maching learning 的方法进行分析。当然，对于传统的软件部署方式，也需要这样的 log 的解决方案，不过本文主要从云的角度来介绍。</p><p>ELK 就是这样的解决方案，而且基本就是事实上的标准。ELK 是三个开源项目的首字母缩写，如下：</p><blockquote><p>E: Elasticsearch</p><p>L: Logstash</p><p>K: Kibana</p></blockquote><a id="more"></a><p>Logstash 的主要作用是收集分布在各处的 log 并进行处理；Elasticsearch 则是一个集中存储 log 的地方，更重要的是它是一个全文检索以及分析的引擎，它能让用户以近乎实时的方式来查看、分析海量的数据。Kibana 则是为 Elasticsearch 开发的前端 GUI，让用户可以很方便的以图形化的接口查询 Elasticsearch 中存储的数据，同时也提供了各种分析的模块，比如构建 dashboard 的功能。</p><p>我个人认为将 ELK 中的 L 理解成 Logging Agent 更合适。Elasticsearch 和 Kibana 基本就是存储、检索和分析 log 的标准方案，而 Logstash 则并不是唯一的收集 log 的方案，Fluentd 和 Filebeats 也能用于收集 log。所以现在网上有 ELK，EFK 之类的缩写。</p><p>一般采用的架构如下图所示。通常一个小型的 cluster 有三个节点，在这三个节点上可能会运行几十个甚至上百个容器。而我们只需要在每个节点上启动一个 logging agent 的实例（在 kubernetes 中就是 DaemonSet 的概念）即可。</p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201103151412.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201103151412-20201227171752168-2020-12-27-adWSyk.png" alt="img"></a></p><h2><span id="filebeats-logstash-fluentd-三者的区别和联系">Filebeats、Logstash、Fluentd 三者的区别和联系</span></h2><p>这里有必要对 Filebeats、Logstash 和 Fluentd 三者之间的联系和区别做一个简要的说明。Filebeats 是一个轻量级的收集本地 log 数据的方案，官方对 Filebeats 的说明如下。可以看出 Filebeats 功能比较单一，它仅仅只能收集本地的 log，但并不能对收集到的 Log 做什么处理，所以通常 Filebeats 通常需要将收集到的 log 发送到 Logstash 做进一步的处理。</p><p>Filebeat is a log data shipper for local files. Installed as an agent on your servers, Filebeat monitors the log directories or specific log files, tails the files, and forwards them either to Elasticsearch or Logstash for indexing</p><p>Logstash 和 Fluentd 都具有收集并处理 log 的能力，网上有很多关于二者的对比，提供一个写得比较好的文章链接如下。功能上二者旗鼓相当，但 Logstash 消耗更多的 memory，对此 Logstash 的解决方案是使用 Filebeats 从各个叶子节点上收集 log，当然 Fluentd 也有对应的 Fluent Bit。</p><p><a href="https://logz.io/blog/fluentd-Logstash/" target="_blank" rel="noopener">https://logz.io/blog/fluentd-Logstash/</a></p><p>另外一个重要的区别是 Fluentd 抽象性做得更好，对用户屏蔽了底层细节的繁琐。作者的原话如下：</p><p>Fluentd’s approach is more declarative whereas Logstash’s method is procedural. For programmers trained in procedural programming, Logstash’s configuration can be easier to get started. On the other hand, Fluentd’s tag-based routing allows complex routing to be expressed cleanly.</p><p>虽然作者说是要中立的对二者（Logstash 和 Fluentd）进行对比，但实际上偏向性很明显了：）。本文也主要基于 Fluentd 进行介绍，不过总体思路都是相通的。</p><p>额外说一点，Filebeats、Logstash、Elasticsearch 和 Kibana 是属于同一家公司的开源项目，官方文档如下：</p><p><a href="https://www.elastic.co/guide/index.html" target="_blank" rel="noopener">https://www.elastic.co/guide/index.html</a></p><p>Fluentd 则是另一家公司的开源项目，官方文档如下：</p><p><a href="https://docs.fluentd.org/" target="_blank" rel="noopener">https://docs.fluentd.org</a></p><h2><span id="关于-elk">关于 ELK</span></h2><h3><span id="elk-简介">ELK 简介</span></h3><p>ELK 是 Elastic 公司提供的一套完整的日志收集以及展示的解决方案，是三个产品的首字母缩写，分别是 Elasticsearch、Logstash 和 Kibana。</p><p><img src="https://img.hi-linux.com/staticfile/yNzE7m-2020-12-27-R7llen.jpg" alt="yNzE7m-2020-12-27-R7llen"></p><ul><li>Elasticsearch 是实时全文搜索和分析引擎，提供搜集、分析、存储数据三大功能</li><li>Logstash 是一个用来搜集、分析、过滤日志的工具</li><li>Kibana 是一个基于 Web 的图形界面，用于搜索、分析和可视化存储在 Elasticsearch 指标中的日志数据</li></ul><h3><span id="elk-日志处理流程">ELK 日志处理流程</span></h3><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102143528.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102143528-20201227171753070-2020-12-27-g8BWUq.png" alt="img"></a></p><p>上图展示了在 Docker 环境下，一个典型的 ELK 方案下的日志收集处理流程：</p><ul><li>Logstash 从各个 Docker 容器中提取日志信息</li><li>Logstash 将日志转发到 Elasticsearch 进行索引和保存</li><li>Kibana 负责分析和可视化日志信息</li></ul><p>由于 Logstash 在数据收集上并不出色，而且作为 Agent，其性能并不达标。基于此，Elastic 发布了 beats 系列轻量级采集组件。</p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102143611.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102143611-20201227171753780-2020-12-27-oIzFLy.png" alt="img"></a></p><p>这里我们要实践的 Beat 组件是 Filebeat，Filebeat 是构建于 beats 之上的，应用于日志收集场景的实现，用来替代 Logstash Forwarder 的下一代 Logstash 收集器，是为了更快速稳定轻量低耗地进行收集工作，它可以很方便地与 Logstash 还有直接与 Elasticsearch 进行对接。</p><p>本次实验直接使用 Filebeat 作为 Agent，它会收集我们在第一篇《Docker logs &amp; logging driver》中介绍的 json-file 的 log 文件中的记录变动，并直接将日志发给 Elasticsearch 进行索引和保存，其处理流程变为下图，你也可以认为它可以称作 EFK。</p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102143641.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102143641-20201227171755527-2020-12-27-H0tuM8.png" alt="img"></a></p><h2><span id="elk-套件的安装">ELK 套件的安装</span></h2><p>本次实验我们采用 Docker 方式部署一个最小规模的 ELK 运行环境，当然，实际环境中我们或许需要考虑高可用和负载均衡。</p><p>首先拉取一下 sebp/elk 这个集成镜像，这里选择的 tag 版本是 latest：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull sebp&#x2F;elk:latest</span><br></pre></td></tr></table></figure><p>注：由于其包含了整个 ELK 方案，所以需要耐心等待一会。</p><p>通过以下命令使用 sebp/elk 这个集成镜像启动运行 ELK：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -d --name elk \</span><br><span class="line">    -p 5601:5601 \</span><br><span class="line">    -p 9200:9200 \</span><br><span class="line">    -p 5044:5044 \</span><br><span class="line">    sebp&#x2F;elk:latest</span><br></pre></td></tr></table></figure><p>运行完成之后就可以先访问一下 <code>http://192.168.4.31:5601</code> 看看 Kibana 的效果：</p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102143944.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102143944-20201227171757659-2020-12-27-FSjdYL.png" alt="img"></a></p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102144305.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102144305-20201227171759040-2020-12-27-jbdgcS.png" alt="img"></a></p><p>当然，目前没有任何可以显示的 ES 的索引和数据，再访问一下 <code>http://192.168.4.31:9200</code> 看看 Elasticsearch 的 API 接口是否可用：</p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102144319.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102144319-20201227171800893-2020-12-27-8CsFqr.png" alt="img"></a></p><blockquote><p>注意：</p></blockquote><p>如果启动过程中发现一些错误，导致 ELK 容器无法启动，可以参考 <a href="https://www.cnblogs.com/zhi-leaf/p/8484337.html" target="_blank" rel="noopener">《ElasticSearch 启动常见错误》</a> 一文。如果你的主机内存低于 4G，建议增加配置设置 ES 内存使用大小，以免启动不了。例如下面增加的配置，限制 ES 内存使用最大为 1G：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -d --name elk \</span><br><span class="line">    -p 5601:5601 \</span><br><span class="line">    -p 9200:9200 \</span><br><span class="line">    -p 5044:5044 \</span><br><span class="line">    -e ES_MIN_MEM&#x3D;512m \</span><br><span class="line">    -e ES_MAX_MEM&#x3D;1024m \</span><br><span class="line">    sebp&#x2F;elk:latest</span><br></pre></td></tr></table></figure><p>若启动容器的时候提示 <code>max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</code> 请参考</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 编辑 sysctl.con</span><br><span class="line">vi &#x2F;etc&#x2F;sysctl.conf</span><br><span class="line"># 添加下面配置</span><br><span class="line">vm.max_map_count&#x3D;655360</span><br><span class="line"># 然后执行命令</span><br><span class="line">sysctl -p</span><br></pre></td></tr></table></figure><h2><span id="filebeat-配置">Filebeat 配置</span></h2><h3><span id="安装-filebeat">安装 Filebeat</span></h3><p><a href="https://www.elastic.co/downloads/beats/filebeat" target="_blank" rel="noopener">Download Filebeat</a></p><p>这里我们通过 rpm 的方式下载 Filebeat，注意这里下载和我们 ELK 对应的版本（ELK 是 7.6.1，这里也是下载 7.6.1，避免出现错误）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;downloads&#x2F;beats&#x2F;filebeat&#x2F;filebeat-7.6.1-x86_64.rpm</span><br><span class="line">rpm -ivh filebeat-7.6.1-x86_64.rpm</span><br></pre></td></tr></table></figure><h3><span id="配置-filebeat">配置 Filebeat</span></h3><p>这里我们需要告诉 Filebeat 要监控哪些日志文件 及 将日志发送到哪里去，因此我们需要修改一下 Filebeat 的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nano &#x2F;etc&#x2F;filebeat&#x2F;filebeat.yml</span><br></pre></td></tr></table></figure><p>要修改的内容为：</p><p>（1）监控哪些日志？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">filebeat.inputs:</span><br><span class="line"></span><br><span class="line"># Each - is an input. Most options can be set at the input level, so</span><br><span class="line"># you can use different inputs for various configurations.</span><br><span class="line"># Below are the input specific configurations.</span><br><span class="line"></span><br><span class="line">- type: log</span><br><span class="line"></span><br><span class="line">  # Change to true to enable this input configuration.</span><br><span class="line">  enabled: true</span><br><span class="line"></span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.</span><br><span class="line">  paths:</span><br><span class="line">    - &#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers&#x2F;*&#x2F;*.log</span><br></pre></td></tr></table></figure><p>这里指定 paths：<code>/var/lib/docker/containers/*/*.log</code>，另外需要注意的是将 <code>enabled</code> 设为 <code>true</code>。</p><p>（2）将日志发到哪里？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#-------------------------- Elasticsearch output ------------------------------</span><br><span class="line">output.elasticsearch:</span><br><span class="line">  # Array of hosts to connect to.</span><br><span class="line">  hosts: [&quot;192.168.4.31:9200&quot;]</span><br><span class="line"></span><br><span class="line">  # Optional protocol and basic auth credentials.</span><br><span class="line">  #protocol: &quot;https&quot;</span><br><span class="line">  #username: &quot;elastic&quot;</span><br><span class="line">  #password: &quot;changeme&quot;</span><br></pre></td></tr></table></figure><p>这里指定直接发送到 Elasticsearch，配置一下 ES 的接口地址即可。</p><blockquote><p>注意：如果要发到 Logstash，请使用后面这段配置，将其取消注释进行相关配置即可：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#----------------------------- Logstash output --------------------------------</span><br><span class="line">#output.Logstash:</span><br><span class="line">  # The Logstash hosts</span><br><span class="line">  #hosts: [&quot;localhost:9200&quot;]</span><br><span class="line"></span><br><span class="line">  # Optional SSL. By default is off.</span><br><span class="line">  # List of root certificates for HTTPS server verifications</span><br><span class="line">  #ssl.certificate_authorities: [&quot;&#x2F;etc&#x2F;pki&#x2F;root&#x2F;ca.pem&quot;]</span><br><span class="line"></span><br><span class="line">  # Certificate for SSL client authentication</span><br><span class="line">  #ssl.certificate: &quot;&#x2F;etc&#x2F;pki&#x2F;client&#x2F;cert.pem&quot;</span><br><span class="line"></span><br><span class="line">  # Client Certificate Key</span><br><span class="line">  #ssl.key: &quot;&#x2F;etc&#x2F;pki&#x2F;client&#x2F;cert.key&quot;</span><br></pre></td></tr></table></figure><h3><span id="启动-filebeat">启动 Filebeat</span></h3><p>由于 Filebeat 在安装时已经注册为 systemd 的服务，所以只需要直接启动即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start filebeat</span><br></pre></td></tr></table></figure><p>设置开机启动：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl enable filebeat</span><br></pre></td></tr></table></figure><p>检查 Filebeat 启动状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status filebeat</span><br></pre></td></tr></table></figure><blockquote><p>上述操作总结为脚本为：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;downloads&#x2F;beats&#x2F;filebeat&#x2F;filebeat-7.6.1-x86_64.rpm</span><br><span class="line">rpm -ivh filebeat-7.6.1-x86_64.rpm</span><br><span class="line">echo &quot;please input elk host_ip&quot;</span><br><span class="line">read host_ip</span><br><span class="line"></span><br><span class="line">sed -i &quot;s&#x2F;  enabled: false&#x2F;  enabled: true&#x2F;g&quot; &#x2F;etc&#x2F;filebeat&#x2F;filebeat.yml</span><br><span class="line">sed -i &quot;s&#x2F;\&#x2F;var\&#x2F;log\&#x2F;\*.log&#x2F;\&#x2F;var\&#x2F;lib\&#x2F;docker\&#x2F;containers\&#x2F;\*\&#x2F;\*.log&#x2F;g&quot; &#x2F;etc&#x2F;filebeat&#x2F;filebeat.yml</span><br><span class="line">sed -i &quot;s&#x2F;localhost:9200&#x2F;$&#123;host_ip&#125;:9200&#x2F;g&quot; &#x2F;etc&#x2F;filebeat&#x2F;filebeat.yml</span><br><span class="line"></span><br><span class="line">systemctl start filebeat</span><br><span class="line">systemctl enable filebeat</span><br><span class="line">systemctl status filebeat</span><br></pre></td></tr></table></figure><h2><span id="kibana-配置">Kibana 配置</span></h2><p>接下来我们就要告诉 Kibana，要查询和分析 Elasticsearch 中的哪些日志，因此需要配置一个 Index Pattern。从 Filebeat 中我们知道 Index 是 filebeat-timestamp 这种格式，因此这里我们定义 Index Pattern 为 <code>filebeat-*</code></p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102153056.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102153056-20201227171801874-2020-12-27-A9dfZY.png" alt="img"></a></p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102153110.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102153110-20201227171803343-2020-12-27-dgHCab.png" alt="img"></a></p><p>点击 Next Step，这里我们选择 Time Filter field name 为 @timestamp：</p><p><img src="https://img.hi-linux.com/staticfile/7oRfXU-2020-12-27-vCn40M.jpg" alt="7oRfXU-2020-12-27-vCn40M"></p><p>单击 Create index pattern 按钮，即可完成配置。</p><p>这时我们单击 Kibana 左侧的 Discover 菜单，即可看到容器的日志信息啦：</p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102153138.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102153138-20201227171805109-2020-12-27-8XwjPe.png" alt="img"></a></p><p>仔细看看细节，我们关注一下 message 字段：</p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102153155.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102153155-20201227171805571-2020-12-27-CapfYV.png" alt="img"></a></p><p>可以看到，我们重点要关注的是 message，因此我们也可以筛选一下只看这个字段的信息：</p><p><img src="https://img.hi-linux.com/staticfile/fu2Q1N-2020-12-27-F6JxmO.jpg" alt="fu2Q1N-2020-12-27-F6JxmO"></p><p><img src="https://img.hi-linux.com/staticfile/MufL8A-2020-12-27-21oHVH.jpg" alt="MufL8A-2020-12-27-21oHVH"></p><p>这里只是朴素的展示了导入 ELK 的日志信息，实际上 ELK 还有很多很丰富的玩法，例如分析聚合、炫酷 Dashboard 等等。笔者在这里也是初步使用，就介绍到这里啦。</p><h2><span id="fluentd-引入">Fluentd 引入</span></h2><h3><span id="关于-fluentd">关于 Fluentd</span></h3><p>前面我们采用的是 Filebeat 收集 Docker 的日志信息，基于 Docker 默认的 json-file 这个 logging driver，这里我们改用 Fluentd 这个开源项目来替换 json-file 收集容器的日志。</p><p>Fluentd 是一个开源的数据收集器，专为处理数据流设计，使用 JSON 作为数据格式。它采用了插件式的架构，具有高可扩展性高可用性，同时还实现了高可靠的信息转发。Fluentd 也是云原生基金会 (CNCF) 的成员项目之一，遵循 Apache 2 License 协议，其 GitHub 地址为：<a href="https://github.com/fluent/fluentd/%E3%80%82Fluentd" target="_blank" rel="noopener">https://github.com/fluent/fluentd/。Fluentd</a> 与 Logstash 相比，比占用内存更少、社区更活跃，两者的对比可以参考这篇文章<a href="https://logz.io/blog/fluentd-Logstash/" target="_blank" rel="noopener">《Fluentd vs Logstash》</a>。</p><p>因此，整个日志收集与处理流程变为下图，我们用 Filebeat 将 Fluentd 收集到的日志转发给 Elasticsearch。</p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102153447.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102153447-20201227171808047-2020-12-27-LU6D7k.png" alt="img"></a></p><p>当然，我们也可以使用 Fluentd 的插件（<code>fluent-plugin-elasticsearch</code>）直接将日志发送给 Elasticsearch，可以根据自己的需要替换掉 Filebeat，从而形成 Fluentd =&gt; Elasticsearch =&gt; Kibana 的架构，也称作 EFK。</p><h3><span id="运行-fluentd">运行 Fluentd</span></h3><p>这里我们通过容器来运行一个 Fluentd 采集器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -d --name fluentd \</span><br><span class="line">    -p 24224:24224 \</span><br><span class="line">    -p 24224:24224&#x2F;udp \</span><br><span class="line">    -v &#x2F;etc&#x2F;fluentd&#x2F;log:&#x2F;fluentd&#x2F;log \</span><br><span class="line">    fluent&#x2F;fluentd:latest</span><br></pre></td></tr></table></figure><p>默认 Fluentd 会使用 24224 端口，其日志会收集在我们映射的路径下。</p><p>此外，我们还需要修改 Filebeat 的配置文件，将 / etc/fluentd/log 加入监控目录下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D; Filebeat inputs &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">filebeat.inputs:</span><br><span class="line"></span><br><span class="line"># Each - is an input. Most options can be set at the input level, so</span><br><span class="line"># you can use different inputs for various configurations.</span><br><span class="line"># Below are the input specific configurations.</span><br><span class="line"></span><br><span class="line">- type: log</span><br><span class="line"></span><br><span class="line">  # Change to true to enable this input configuration.</span><br><span class="line">  enabled: true</span><br><span class="line"></span><br><span class="line">  # Paths that should be crawled and fetched. Glob based paths.</span><br><span class="line">  paths:</span><br><span class="line">    - &#x2F;etc&#x2F;fluentd&#x2F;log&#x2F;*.log</span><br></pre></td></tr></table></figure><p>添加监控配置之后，需要重新 restart 一下 filebeat：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart filebeat</span><br></pre></td></tr></table></figure><h3><span id="运行测试容器">运行测试容器</span></h3><p>为了验证效果，这里我们 Run 两个容器，并分别制定其 log-dirver 为 fluentd：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">docker run -d \</span><br><span class="line">    --log-driver&#x3D;fluentd \</span><br><span class="line">    --log-opt fluentd-address&#x3D;localhost:24224 \</span><br><span class="line">    --log-opt tag&#x3D;&quot;test-docker-A&quot; \</span><br><span class="line">    busybox sh -c &#39;while true; do echo &quot;This is a log message from container A&quot;; sleep 10; done;&#39;</span><br><span class="line"></span><br><span class="line">docker run -d \</span><br><span class="line">    --log-driver&#x3D;fluentd \</span><br><span class="line">    --log-opt fluentd-address&#x3D;localhost:24224 \</span><br><span class="line">    --log-opt tag&#x3D;&quot;test-docker-B&quot; \</span><br><span class="line">    busybox sh -c &#39;while true; do echo &quot;This is a log message from container B&quot;; sleep 10; done;&#39;</span><br></pre></td></tr></table></figure><p>这里通过指定容器的 log-driver，以及为每个容器设立了 tag，方便我们后面验证查看日志。</p><h3><span id="验证-efk-效果">验证 EFK 效果</span></h3><p>这时再次进入 Kibana 中查看日志信息，便可以通过刚刚设置的 tag 信息筛选到刚刚添加的容器的日志信息了：</p><p><a href="https://raw.githubusercontent.com/wsgzao/storage-public/master/img/20201102153744.png" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/20201102153744-20201227171809122-2020-12-27-2mV6oe.png" alt="img"></a></p><h2><span id="模拟日志生成压力测试工具">模拟日志生成压力测试工具</span></h2><ol><li><p><a href="https://github.com/elastic/rally" target="_blank" rel="noopener">https://github.com/elastic/rally</a></p></li><li><p><a href="https://pypi.org/project/log-generator/" target="_blank" rel="noopener">https://pypi.org/project/log-generator/</a></p></li><li><p><a href="https://github.com/mingrammer/flog" target="_blank" rel="noopener">https://github.com/mingrammer/flog</a></p></li></ol><h2><span id="小结">小结</span></h2><p>本文从 ELK 的基本组成入手，介绍了 ELK 的基本处理流程，以及从 0 开始搭建了一个 ELK 环境，演示了基于 Filebeat 收集容器日志信息的案例。然后，通过引入 Fluentd 这个开源数据收集器，演示了如何基于 EFK 的日志收集案例。当然，ELK/EFK 有很多的知识点，笔者也还只是初步使用，希望未来能够分享更多的实践总结。</p><h2><span id="参考文章">参考文章</span></h2><ol><li><p><a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html" target="_blank" rel="noopener">Elasticsearch Reference</a></p></li><li><p><a href="https://www.cnblogs.com/edisonchou/p/docker_logs_study_summary_part2.html" target="_blank" rel="noopener">开源日志管理方案 ELK/EFK</a></p></li><li><p><a href="https://ld246.com/article/1588145447021" target="_blank" rel="noopener">开源日志管理方案 ELK/EFK</a></p></li><li><p><a href="http://www.uml.org.cn/bigdata/202002193.asp?artid=22965" target="_blank" rel="noopener">ELK 构建云时代的 logging 解决方案</a></p></li><li><p><a href="https://www.qikqiak.com/post/install-efk-stack-on-k8s/" target="_blank" rel="noopener">在 Kubernetes 上搭建 EFK 日志收集系统</a></p></li><li><p><a href="https://qhh.me/2019/09/05/Kubernetes-%E5%9F%BA%E4%BA%8E-EFK-%E6%8A%80%E6%9C%AF%E6%A0%88%E7%9A%84%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%AE%9E%E8%B7%B5/" target="_blank" rel="noopener">Kubernetes 基于 EFK 技术栈的日志收集实践</a></p></li></ol><blockquote><p>本文转载自：「HelloDog」，原文：<a href="https://wsgzao.github.io/post/efk/%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://wsgzao.github.io/post/efk/，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;主流的 ELK (Elasticsearch, &lt;code&gt;Logstash&lt;/code&gt;, Kibana) 目前已经转变为 EFK (Elasticsearch, &lt;code&gt;Filebeat&lt;/code&gt; or &lt;code&gt;Fluentd&lt;/code&gt;, Kibana) 比较重，对于容器云的日志方案业内也普遍推荐采用 Fluentd，我们一起来看下从 ELK 到 EFK 发生了哪些变化，与此同时我也推荐大家了解下 Grafana Loki&lt;/p&gt;
&lt;h2 id=&quot;ELK-和-EFK-概述&quot;&gt;ELK 和 EFK 概述&lt;/h2&gt;
&lt;p&gt;随着现在各种软件系统的复杂度越来越高，特别是部署到云上之后，再想登录各个节点上查看各个模块的 log，基本是不可行了。因为不仅效率低下，而且有时由于安全性，不可能让工程师直接访问各个物理节点。而且现在大规模的软件系统基本都采用集群的部署方式，意味着对每个 service，会启动多个完全一样的 POD 对外提供服务，每个 container 都会产生自己的 log，仅从产生的 log 来看，你根本不知道是哪个 POD 产生的，这样对查看分布式的日志更加困难。&lt;/p&gt;
&lt;p&gt;所以在云时代，需要一个收集并分析 log 的解决方案。首先需要将分布在各个角落的 log 收集到一个集中的地方，方便查看。收集了之后，还可以进行各种统计分析，甚至用流行的大数据或 maching learning 的方法进行分析。当然，对于传统的软件部署方式，也需要这样的 log 的解决方案，不过本文主要从云的角度来介绍。&lt;/p&gt;
&lt;p&gt;ELK 就是这样的解决方案，而且基本就是事实上的标准。ELK 是三个开源项目的首字母缩写，如下：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;E: Elasticsearch&lt;/p&gt;
&lt;p&gt;L: Logstash&lt;/p&gt;
&lt;p&gt;K: Kibana&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Elasticsearch" scheme="https://www.hi-linux.com/categories/Elasticsearch/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
      <category term="Elasticsearch" scheme="https://www.hi-linux.com/tags/Elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Dashboard 2.10 尝鲜记</title>
    <link href="https://www.hi-linux.com/posts/39524.html"/>
    <id>https://www.hi-linux.com/posts/39524.html</id>
    <published>2020-12-29T01:00:00.000Z</published>
    <updated>2020-12-29T05:17:06.131Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="简介">简介</span></h2><p>Kubernetes Dashboard 是 Kubernetes 集群的基于 Web 的通用 UI。它允许用户管理在群集中运行的应用程序并对其进行故障排除，以及管理群集本身。最近推出了 v2.1.0 版本，这里在 Kubernetes 中部署一下，尝试看看新版本咋样。</p><h2><span id="兼容性">兼容性</span></h2><table><thead><tr><th style="text-align:left">Kubernetes版本</th><th style="text-align:left">1.17</th><th style="text-align:left">1.18</th><th style="text-align:left">1.19</th><th style="text-align:left">1.20</th><th style="text-align:left"></th></tr></thead><tbody><tr><td style="text-align:left">兼容性</td><td style="text-align:left">？</td><td style="text-align:left">？</td><td style="text-align:left">？</td><td style="text-align:left">✓</td><td style="text-align:left"></td></tr></tbody></table><ul><li>✕ 不支持的版本范围。</li><li>✓ 完全支持的版本范围。</li><li>? 由于Kubernetes API版本之间的重大更改，某些功能可能无法在仪表板中正常运行。</li></ul><h2><span id="部署-kubernetes-dashboard">部署 Kubernetes Dashboard</span></h2><blockquote><p>注意：如果 “kube-system” 命名空间已经存在 Kubernetes-Dashboard 相关资源，请换成别的 Namespace。</p></blockquote><a id="more"></a><h3><span id="系统环境">系统环境</span></h3><ul><li>Kubernetes 版本：1.20.1</li><li>kubernetes-dashboard 版本：v2.1.0</li></ul><h3><span id="部署文件">部署文件</span></h3><p>完整部署文件 Github 地址：<a href="https://github.com/my-dlq/blog-example/tree/master/kubernetes/kubernetes-dashboard2.1.0-deploy" target="_blank" rel="noopener">https://github.com/my-dlq/blog-example/tree/master/kubernetes/kubernetes-dashboard2.1.0-deploy</a></p><h3><span id="1-dashboard-rbac">1、Dashboard RBAC</span></h3><h4><span id="创建-dashboard-rbac-部署文件">创建 Dashboard RBAC 部署文件</span></h4><p><strong>k8s-dashboard-rbac.yaml</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> <span class="string">[""]</span></span><br><span class="line">    <span class="attr">resources:</span> <span class="string">["secrets"]</span></span><br><span class="line">    <span class="attr">resourceNames:</span> <span class="string">["kubernetes-dashboard-key-holder",</span> <span class="string">"kubernetes-dashboard-certs"</span><span class="string">,</span> <span class="string">"kubernetes-dashboard-csrf"</span><span class="string">]</span></span><br><span class="line">    <span class="attr">verbs:</span> <span class="string">["get",</span> <span class="string">"update"</span><span class="string">,</span> <span class="string">"delete"</span><span class="string">]</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> <span class="string">[""]</span></span><br><span class="line">    <span class="attr">resources:</span> <span class="string">["configmaps"]</span></span><br><span class="line">    <span class="attr">resourceNames:</span> <span class="string">["kubernetes-dashboard-settings"]</span></span><br><span class="line">    <span class="attr">verbs:</span> <span class="string">["get",</span> <span class="string">"update"</span><span class="string">]</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> <span class="string">[""]</span></span><br><span class="line">    <span class="attr">resources:</span> <span class="string">["services"]</span></span><br><span class="line">    <span class="attr">resourceNames:</span> <span class="string">["heapster",</span> <span class="string">"dashboard-metrics-scraper"</span><span class="string">]</span></span><br><span class="line">    <span class="attr">verbs:</span> <span class="string">["proxy"]</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> <span class="string">[""]</span></span><br><span class="line">    <span class="attr">resources:</span> <span class="string">["services/proxy"]</span></span><br><span class="line">    <span class="attr">resourceNames:</span> <span class="string">["heapster",</span> <span class="string">"http:heapster:"</span><span class="string">,</span> <span class="string">"https:heapster:"</span><span class="string">,</span> <span class="string">"dashboard-metrics-scraper"</span><span class="string">,</span> <span class="string">"http:dashboard-metrics-scraper"</span><span class="string">]</span></span><br><span class="line">    <span class="attr">verbs:</span> <span class="string">["get"]</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">apiGroups:</span> <span class="string">["metrics.k8s.io"]</span></span><br><span class="line">    <span class="attr">resources:</span> <span class="string">["pods",</span> <span class="string">"nodes"</span><span class="string">]</span></span><br><span class="line">    <span class="attr">verbs:</span> <span class="string">["get",</span> <span class="string">"list"</span><span class="string">,</span> <span class="string">"watch"</span><span class="string">]</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">    <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><h4><span id="部署-dashboard-rbac">部署 Dashboard RBAC</span></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f k8s-dashboard-rbac.yaml</span><br></pre></td></tr></table></figure><h3><span id="2-创建-configmap-secret">2、创建 ConfigMap、Secret</span></h3><h4><span id="创建-dashboard-config-amp-secret-部署文件">创建 Dashboard Config &amp; Secret 部署文件</span></h4><p><strong>k8s-dashboard-configmap-secret.yaml</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard-certs</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard-csrf</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">csrf:</span> <span class="string">""</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard-key-holder</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard-settings</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure><h4><span id="部署-dashboard-config-amp-secret">部署 Dashboard Config &amp; Secret</span></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f k8s-dashboard-configmap-secret.yaml</span><br></pre></td></tr></table></figure><h3><span id="3-kubernetes-dashboard">3、kubernetes-dashboard</span></h3><h4><span id="创建-dashboard-deploy-部署文件">创建 Dashboard Deploy 部署文件</span></h4><p><strong>k8s-dashboard-deploy.yaml</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Dashboard Service</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">443</span></span><br><span class="line">      <span class="attr">nodePort:</span> <span class="number">30001</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">8443</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment">## Dashboard Deployment</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">kubernetesui/dashboard:v2.1.0</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br><span class="line">            <span class="attr">readOnlyRootFilesystem:</span> <span class="literal">true</span></span><br><span class="line">            <span class="attr">runAsUser:</span> <span class="number">1001</span></span><br><span class="line">            <span class="attr">runAsGroup:</span> <span class="number">2001</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8443</span></span><br><span class="line">              <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">          <span class="attr">args:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--auto-generate-certificates</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">--namespace=kube-system</span>          <span class="comment">#设置为当前部署的Namespace</span></span><br><span class="line">          <span class="attr">resources:</span></span><br><span class="line">            <span class="attr">limits:</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="string">1000m</span></span><br><span class="line">              <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line">            <span class="attr">requests:</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="string">1000m</span></span><br><span class="line">              <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line">          <span class="attr">livenessProbe:</span></span><br><span class="line">            <span class="attr">httpGet:</span></span><br><span class="line">              <span class="attr">scheme:</span> <span class="string">HTTPS</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">              <span class="attr">port:</span> <span class="number">8443</span></span><br><span class="line">            <span class="attr">initialDelaySeconds:</span> <span class="number">30</span></span><br><span class="line">            <span class="attr">timeoutSeconds:</span> <span class="number">30</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kubernetes-dashboard-certs</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/certs</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tmp-volume</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/tmp</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">localtime</span></span><br><span class="line">              <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">              <span class="attr">mountPath:</span> <span class="string">/etc/localtime</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kubernetes-dashboard-certs</span></span><br><span class="line">          <span class="attr">secret:</span></span><br><span class="line">            <span class="attr">secretName:</span> <span class="string">kubernetes-dashboard-certs</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tmp-volume</span></span><br><span class="line">          <span class="attr">emptyDir:</span> <span class="string">&#123;&#125;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">localtime</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">File</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/etc/localtime</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">          <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br></pre></td></tr></table></figure><h4><span id="部署-dashboard-deploy">部署 Dashboard Deploy</span></h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f k8s-dashboard-deploy.yaml</span><br></pre></td></tr></table></figure><h3><span id="4-创建-kubernetes-metrics-scraper">4、创建 kubernetes-metrics-scraper</span></h3><h4><span id="创建-dashboard-metrics-部署文件">创建 Dashboard Metrics 部署文件</span></h4><p><strong>k8s-dashboard-metrics.yaml</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Dashboard Metrics Service</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">dashboard-metrics-scraper</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">dashboard-metrics-scraper</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">8000</span></span><br><span class="line">      <span class="attr">targetPort:</span> <span class="number">8000</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">dashboard-metrics-scraper</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="comment">## Dashboard Metrics Deployment</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">k8s-app:</span> <span class="string">dashboard-metrics-scraper</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">dashboard-metrics-scraper</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">revisionHistoryLimit:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">k8s-app:</span> <span class="string">dashboard-metrics-scraper</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">k8s-app:</span> <span class="string">dashboard-metrics-scraper</span></span><br><span class="line">      <span class="attr">annotations:</span></span><br><span class="line">        <span class="attr">seccomp.security.alpha.kubernetes.io/pod:</span> <span class="string">'runtime/default'</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">serviceAccountName:</span> <span class="string">kubernetes-dashboard</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">dashboard-metrics-scraper</span></span><br><span class="line">          <span class="attr">image:</span> <span class="string">kubernetesui/metrics-scraper:v1.0.6</span></span><br><span class="line">          <span class="attr">securityContext:</span></span><br><span class="line">            <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br><span class="line">            <span class="attr">readOnlyRootFilesystem:</span> <span class="literal">true</span></span><br><span class="line">            <span class="attr">runAsUser:</span> <span class="number">1001</span></span><br><span class="line">            <span class="attr">runAsGroup:</span> <span class="number">2001</span></span><br><span class="line">          <span class="attr">ports:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8000</span></span><br><span class="line">              <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">          <span class="attr">resources:</span></span><br><span class="line">            <span class="attr">limits:</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="string">1000m</span></span><br><span class="line">              <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line">            <span class="attr">requests:</span></span><br><span class="line">              <span class="attr">cpu:</span> <span class="string">1000m</span></span><br><span class="line">              <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line">          <span class="attr">livenessProbe:</span></span><br><span class="line">            <span class="attr">httpGet:</span></span><br><span class="line">              <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">              <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">              <span class="attr">port:</span> <span class="number">8000</span></span><br><span class="line">            <span class="attr">initialDelaySeconds:</span> <span class="number">30</span></span><br><span class="line">            <span class="attr">timeoutSeconds:</span> <span class="number">30</span></span><br><span class="line">          <span class="attr">volumeMounts:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/tmp</span></span><br><span class="line">            <span class="attr">name:</span> <span class="string">tmp-volume</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">localtime</span></span><br><span class="line">            <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">            <span class="attr">mountPath:</span> <span class="string">/etc/localtime</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">tmp-volume</span></span><br><span class="line">          <span class="attr">emptyDir:</span> <span class="string">&#123;&#125;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">localtime</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">type:</span> <span class="string">File</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/etc/localtime</span></span><br><span class="line">      <span class="attr">nodeSelector:</span></span><br><span class="line">        <span class="attr">"beta.kubernetes.io/os":</span> <span class="string">linux</span></span><br><span class="line">      <span class="attr">tolerations:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">          <span class="attr">effect:</span> <span class="string">NoSchedule</span></span><br></pre></td></tr></table></figure><h4><span id="部署-dashboard-metrics">部署 Dashboard Metrics</span></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f k8s-dashboard-metrics.yaml</span><br></pre></td></tr></table></figure><h3><span id="5-创建访问的-serviceaccount">5、创建访问的 ServiceAccount</span></h3><p>创建一个绑定 admin 权限的 ServiceAccount，获取其 Token 用于访问看板。</p><h4><span id="创建-dashboard-serviceaccount-部署文件">创建 Dashboard ServiceAccount 部署文件</span></h4><p><strong>k8s-dashboard-token.yaml</strong></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">admin</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">rbac.authorization.kubernetes.io/autoupdate:</span> <span class="string">"true"</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cluster-admin</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">admin</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">admin</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">kubernetes.io/cluster-service:</span> <span class="string">"true"</span></span><br><span class="line">    <span class="attr">addonmanager.kubernetes.io/mode:</span> <span class="string">Reconcile</span></span><br></pre></td></tr></table></figure><h4><span id="部署访问的-serviceaccount">部署访问的 ServiceAccount</span></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f k8s-dashboard-token.yaml</span><br></pre></td></tr></table></figure><h4><span id="获取-token">获取 Token</span></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe secret/$(kubectl get secret -n kube-system |grep admin|awk <span class="string">'&#123;print $1&#125;'</span>) -n kube-system</span><br></pre></td></tr></table></figure><p><strong>token：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1iNGo0aCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjkwMTQzMWYxLTVmNGItMTFlOS05Mjg3LTAwMGMyOWQ5ODY5NyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTphZG1pbiJ9.iwE1UdhB78FgXZJh4ByyOZVNh7M1l2CmOOevihOrY9tl_Z5sf3i_04CA33xA2LAMg7WNVYPjGB7vszBlkQyDGw0H5kJzIfL1YnR0JeLQkNk3v9TLyRqKJA2n8pxmJQIJP1xq0OPRGOfcA_n_c5qESs9QFHejVc5vABim8VBGX-pefKoJVXgu3r4w8gr1ORn4l5-LtHdQjSz3Dys7HwZo71fX2aLQR5bOPurkFKXqymcUoBYpWVsf-0cyN7hLRO-x-Z1i-uVpdM8ClpYSHv49eoDJePrcWpRp-Ryq6SNpGhiqCjjifEQAVHbr36QSAx8I1aamqLcpA0Da2qnunw52JA</span><br></pre></td></tr></table></figure><h2><span id="登录新版本-dashboard-查看">登录新版本 Dashboard 查看</span></h2><p>​    本人的 Kubernetes 集群地址为”192.168.2.11”并且在 Service 中设置了 NodePort 端口为 30001 和类型为 NodePort 方式访问 Dashboard ，所以访问地址：<a href="https://192.168.2.11:30001/" target="_blank" rel="noopener">https://192.168.2.11:30001</a> 进入 Kubernetes Dashboard 页面，然后输入上一步中创建的 ServiceAccount 的 Token 进入 Dashboard，可以看到新的 Dashboard。</p><p><img src="https://img.hi-linux.com/staticfile/fVBrYD-2020-12-29-OYaZah.jpg" alt="fVBrYD-2020-12-29-OYaZah"></p><p>​    跟上一个版本比较，整体资源的显示位置，增加对 1.20 版本的支持等：</p><p><img src="https://img.hi-linux.com/staticfile/VFwBYI-2020-12-29-89sGmV.jpg" alt="VFwBYI-2020-12-29-89sGmV"></p><h2><span id="部署-metrics-server-为-dashboard-提供指标数据">部署 Metrics Server 为 Dashboard 提供指标数据</span></h2><p>Dashboard 已经部署完成，不过登录 Dashboard 后可以看到：</p><p><a href="https://mydlq-club.oss-cn-beijing.aliyuncs.com/images/kubernetes-dashboard-1007.png?x-oss-process=style/shuiyin" target="_blank" rel="noopener"><img src="https://img.hi-linux.com/staticfile/shuiyin-20201228124013774-2020-12-28-fuSFKv.png" alt="img"></a></p><p>这些栏数据显示都是空，这是由于 Dashboard 的指标部署需要从 Metrics Server 中获取，Dashboard 该版本另一个组件 kubernetes-metrics-scraper 就是用于从 Metrics Server 获取指标的适配器。之前我们已经部署 kubernetes-metrics-scraper 组件，接下来只要再部署 Metrics Server 组件就能获取系统指标数据，供 Dashboard 绘制图形，部署 Metrics Server 可以参考：</p><ul><li><a href="http://www.mydlq.club/article/77/" target="_blank" rel="noopener">Kubernetes 部署 Metrics Server 获取集群指标数据</a></li></ul><p>当按照上面部署完成后，等一段时间，再刷新 Dashboard 界面，可以观察到如下界面：</p><p><img src="https://img.hi-linux.com/staticfile/RW4Yyd-2020-12-29-zQ64cm.jpg" alt="RW4Yyd-2020-12-29-zQ64cm"></p><p><img src="https://img.hi-linux.com/staticfile/IIZh3v-2020-12-29-Zgf3DB.jpg" alt="IIZh3v-2020-12-29-Zgf3DB"></p><blockquote><p>本文转载自：「小豆丁个人博客」，原文：<a href="http://www.mydlq.club/article/99/%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">http://www.mydlq.club/article/99/，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;简介&quot;&gt;简介&lt;/h2&gt;
&lt;p&gt;Kubernetes Dashboard 是 Kubernetes 集群的基于 Web 的通用 UI。它允许用户管理在群集中运行的应用程序并对其进行故障排除，以及管理群集本身。最近推出了 v2.1.0 版本，这里在 Kubernetes 中部署一下，尝试看看新版本咋样。&lt;/p&gt;
&lt;h2 id=&quot;兼容性&quot;&gt;兼容性&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:left&quot;&gt;Kubernetes版本&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;1.17&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;1.18&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;1.19&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;1.20&lt;/th&gt;
&lt;th style=&quot;text-align:left&quot;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:left&quot;&gt;兼容性&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;？&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;？&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;？&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;✓&lt;/td&gt;
&lt;td style=&quot;text-align:left&quot;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;ul&gt;
&lt;li&gt;✕ 不支持的版本范围。&lt;/li&gt;
&lt;li&gt;✓ 完全支持的版本范围。&lt;/li&gt;
&lt;li&gt;? 由于Kubernetes API版本之间的重大更改，某些功能可能无法在仪表板中正常运行。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;部署-Kubernetes-Dashboard&quot;&gt;部署 Kubernetes Dashboard&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;注意：如果 “kube-system” 命名空间已经存在 Kubernetes-Dashboard 相关资源，请换成别的 Namespace。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>手把手教你深度定制 Kubernetes Nginx Ingress 错误提示页面</title>
    <link href="https://www.hi-linux.com/posts/22262.html"/>
    <id>https://www.hi-linux.com/posts/22262.html</id>
    <published>2020-12-28T01:00:00.000Z</published>
    <updated>2020-12-28T04:34:42.903Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><blockquote><p>错误页面是发生错误时显示的网页。 错误页面会警告用户发生的错误类型，并可能为用户提供解决问题的步骤的建议。 除了在未样式化的网页上提供错误信息的基本页面之外，还可以使用可以设计为具有额外功能和样式外观的自定义错误页面。 这些设置可以在服务器上更改。 许多服务器提供了可用于生成自定义错误页面的实用程序。</p><p>引文参考：<a href="https://www.netinbag.com/cn/internet/what-are-error-pages.html" target="_blank" rel="noopener">https://www.netinbag.com/cn/internet/what-are-error-pages.html</a></p></blockquote><h2><span id="1-错误页面状态码">1、错误页面状态码</span></h2><p>网站运行过程中难免出现问题，为用户抛出一个错误页面，常见的错误页面包含<code>403</code>、<code>404</code>、<code>500</code>、<code>502</code>、<code>503</code>、<code>504</code>状态码，这些常见的错误页面状态码的含义如下</p><ul><li>403 Forbidden</li><li>404 Not Found</li><li>500 Internal Server Eroor</li><li>502 Bad Gateway</li><li>503 Service Unavailable</li><li>504 Gateway Timeout</li></ul><a id="more"></a><h2><span id="2-在k8s中模拟错误页面">2、在k8s中模拟错误页面</span></h2><p>本文中涉及到的的<code>k8s集群</code>版本、<code>Ingress nginx</code>版本如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl version</span></span><br><span class="line">Client Version: version.Info&#123;Major:"1", Minor:"15", GitVersion:"v1.15.0", GitCommit:"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529", GitTreeState:"clean", BuildDate:"2019-06-19T16:40:16Z", GoVersion:"go1.12.5", Compiler:"gc", Platform:"linux/amd64"&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:"1", Minor:"15", GitVersion:"v1.15.0", GitCommit:"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529", GitTreeState:"clean", BuildDate:"2019-06-19T16:32:14Z", GoVersion:"go1.12.5", Compiler:"gc", Platform:"linux/amd64"&#125;</span><br><span class="line"><span class="meta">#</span><span class="bash"> POD_NAME=$(kubectl get pods -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx -o jsonpath=<span class="string">'&#123;.items[0].metadata.name&#125;'</span>)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> kubectl <span class="built_in">exec</span> -it <span class="variable">$POD_NAME</span> -n ingress-nginx -- /nginx-ingress-controller --version</span></span><br><span class="line">-------------------------------------------------------------------------------</span><br><span class="line">NGINX Ingress controller</span><br><span class="line">  Release:    0.25.0</span><br><span class="line">  Build:      git-1387f7b7e</span><br><span class="line">  Repository: https://github.com/kubernetes/ingress-nginx</span><br><span class="line">-------------------------------------------------------------------------------</span><br></pre></td></tr></table></figure><p>对于错误页面状态码，为了方便，这里模拟出<code>404</code>和<code>503</code>两个错误状态码页面</p><ul><li>404页面</li></ul><p>解析一个不存在的域名到<code>Ingress controller</code>所在的节点，进行访问，页面如下</p><p><img src="https://img.hi-linux.com/staticfile/pH3Yqo-2020-12-27-O6ybjO.jpg" alt="pH3Yqo-2020-12-27-O6ybjO"></p><p>这里对<code>Ingress nginx</code>做了版本号的隐藏，返回了默认的<code>404 Not Found</code>(页面未找到)</p><ul><li>503页面</li></ul><p>在<code>k8s</code>中创建一个如下的<code>Ingress</code>资源</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Ingress</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">example-ingress</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">rules:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">host:</span> <span class="string">example.bar.com</span></span><br><span class="line">      <span class="attr">http:</span></span><br><span class="line">        <span class="attr">paths:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">backend:</span></span><br><span class="line">              <span class="attr">serviceName:</span> <span class="string">nginx-service</span></span><br><span class="line">              <span class="attr">servicePort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>同样将对应的域名解析到<code>Ingress controller</code>所在的节点进行访问，由于该<code>Ingress</code>的后端并没有对应的<code>nginx-service</code>，因此会返回默认的<code>503</code>(服务暂时不可用)</p><p><img src="https://img.hi-linux.com/staticfile/YpwQZG-2020-12-27-NxbEl6.jpg" alt="YpwQZG-2020-12-27-NxbEl6"></p><h2><span id="3-默认后端错误页面">3、默认后端错误页面</span></h2><p>很多时候我们虽然隐藏了<code>Ingress nginx</code>的版本号，但直接返回状态码还是不够友好。一些网站都会有自定义的较友好、美观的错误页面或跳转到公益页面等。</p><p>如何定制错误页面？在网址的域名<code>dns</code>被正确解析而不是未注册或被劫持的情况下，简单来说可以根据网络访问链路分为以下两种情况：</p><ul><li>域名通过<code>CNAME</code>解析到<code>cdn</code></li></ul><p>如果网站前面用到了类似阿里云提供的<code>CDN</code>加速、全站加速等服务，域名通过<code>CNAME</code>解析到<code>CDN</code>，<code>CDN</code>再配置关联的域名。这种情况下错误页面的定义都可以直接在<code>CDN</code>控制台进行配置。如下图所示，指定状态码对应的页面即可。</p><p><img src="https://img.hi-linux.com/staticfile/Hjr9qs-2020-12-27-Y5ZyqI.jpg" alt="Hjr9qs-2020-12-27-Y5ZyqI"></p><ul><li>域名通过<code>A</code>记录解析到<code>LB</code>或者真实服务器</li></ul><p>如果网站域名通过<code>A</code>记录解析到<code>LB</code>或者真实服务器，而<code>LB</code>或者真实服务器不做任何处理，那么将返回上面所示的错误状态码页面。对于<code>k8s</code>中通过<code>Ingress nginx</code>暴露的服务来说，可以在<code>Ingress-controller</code>配置默认后端错误页面。</p><p>可以参照官方的<a href="https://github.com/kubernetes/ingress-nginx/tree/nginx-0.25.0/docs/examples/customization/custom-errors" target="_blank" rel="noopener">文档说明</a>，配置流程如下。</p><h3><span id="31-部署默认后端">3.1 部署默认后端</span></h3><p><code>Ingress nginx</code>提供了默认的自定义后端供用户使用，<code>yaml</code>如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-errors</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">nginx-errors</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">nginx-errors</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">ingress-nginx</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">8080</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-errors</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">nginx-errors</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app.kubernetes.io/name:</span> <span class="string">nginx-errors</span></span><br><span class="line">      <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">ingress-nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app.kubernetes.io/name:</span> <span class="string">nginx-errors</span></span><br><span class="line">        <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">ingress-nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-error-server</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">quay.io/kubernetes-ingress-controller/custom-error-pages-amd64:0.3</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8080</span></span><br><span class="line">        <span class="comment"># Setting the environment variable DEBUG we can see the headers sent </span></span><br><span class="line">        <span class="comment"># by the ingress controller to the backend in the client response.</span></span><br><span class="line">        <span class="comment"># env:</span></span><br><span class="line">        <span class="comment"># - name: DEBUG</span></span><br><span class="line">        <span class="comment">#   value: "true"</span></span><br></pre></td></tr></table></figure><p>保证镜像可用的情况下，直接创建对应资源即可</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl create -f custom-default-backend.yaml</span></span><br><span class="line">service "nginx-errors" created</span><br><span class="line">deployment.apps "nginx-errors" created</span><br></pre></td></tr></table></figure><p>检查创建的资源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl get deploy,svc</span></span><br><span class="line">NAME                           DESIRED   CURRENT   READY     AGE</span><br><span class="line">deployment.apps/nginx-errors   1         1         1         10s</span><br><span class="line"></span><br><span class="line">NAME                   TYPE        CLUSTER-IP  EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service/nginx-errors   ClusterIP   10.0.0.12   &lt;none&gt;        80/TCP    10s</span><br></pre></td></tr></table></figure><h3><span id="32-配置启动参数">3.2 配置启动参数</span></h3><p>修改<code>Ingress controller</code>控制器的启动参数，加入以下配置，通过<code>--default-backend</code>标志的值设置为新创建的错误后端的名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl -n ingress-nginx edit ds nginx-ingress-controller</span></span><br><span class="line">...</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - args:</span><br><span class="line">        - /nginx-ingress-controller</span><br><span class="line">        - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">        - --publish-service=$(POD_NAMESPACE)/ingress-nginx</span><br><span class="line">        - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">        - --default-backend-service=ingress-nginx/nginx-errors  # 添加此行</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3><span id="33-修改configmap">3.3 修改configmap</span></h3><p>修改对应的<code>configmap</code>指定要关联到默认后端服务的服务状态码，意味着如果状态码是配置项中的值，那么返回给客户端浏览器的就是默认后端服务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl -n ingress-nginx edit configmap nginx-configuration</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  custom-http-errors: 403,404,500,502,503,504 # 添加此行</span><br></pre></td></tr></table></figure><h3><span id="34-测试">3.4 测试</span></h3><p>通过终端命令访问上面<code>404</code>和<code>503</code>页面的两个域名</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">  ingress-nginx curl example.bar.com                              </span></span><br><span class="line">5xx html                                                                                                                                                                        #  ingress-nginx curl example.foo.com                              </span><br><span class="line">&lt;span&gt;The page you're looking for could not be found.&lt;/span&gt;</span><br><span class="line"><span class="meta">#</span><span class="bash">  自定义Accept标头</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  ingress-nginx curl -H <span class="string">'Accept: application/json'</span> example.foo.com</span></span><br><span class="line">&#123; "message": "The page you're looking for could not be found" &#125;</span><br></pre></td></tr></table></figure><p>可以看到默认后端将<code>404</code>状态码返回了字符串，<code>503</code>返回了<code>5xx html</code>的字符串。缺点在于这样的情况如果用浏览器进行访问，仅仅是一个字符串文本甚至无法正常显示，因此需要重新定义这个默认后端服务，提供友好的界面返回。</p><h2><span id="4-自定义错误页面">4、自定义错误页面</span></h2><h3><span id="41-剖析请求与关键">4.1 剖析请求与关键</span></h3><p>如下图所示，<code>Ingress Controller</code>控制器的工作原理，简单来说，将控制器理解为一个监听器，通过不断地监听 <code>kube-apiserver</code>，实时的感知后端 <code>Service</code>和<code>Pod</code>的变化，当得到这些信息变化后，<code>Ingress Controller</code>再结合<code>Ingress</code>的配置，更新反向代理负载均衡器，从而达到服务发现的作用。<code>Ingress-nginx</code>的最终目标是构造<code>nginx.conf</code>这样的配置文件，主要用途是在配置文件有任何变更后都需要重新加载 <code>nginx</code>。</p><p><img src="https://img.hi-linux.com/staticfile/fHCwV4-2020-12-27-uFOoUZ.jpg" alt="fHCwV4-2020-12-27-uFOoUZ"></p><p>通过上面创建<code>ingress</code>资源，以及配置控制器启动参数和<code>configmap</code>，进入到<code>nginx-ingress-controller</code>的<code>pod</code>中查看配置(文件内容很多，可以导出或过滤查看)。</p><p>会看到将状态码关联了自定义的默认后端</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl -n ingress-nginx <span class="built_in">exec</span> -it nginx-ingress-controller-2rrsw bash</span></span><br><span class="line">www-data@k8s-qa-node-03:/etc/nginx$ grep "error_page" nginx.conf -C 10</span><br><span class="line">        </span><br><span class="line">        ssl_ecdh_curve auto;</span><br><span class="line">        </span><br><span class="line">        proxy_intercept_errors on;</span><br><span class="line">        </span><br><span class="line">        error_page 404 = @custom_upstream-default-backend_404;</span><br><span class="line">        error_page 500 = @custom_upstream-default-backend_500;</span><br><span class="line">        error_page 502 = @custom_upstream-default-backend_502;</span><br><span class="line">        error_page 503 = @custom_upstream-default-backend_503;</span><br><span class="line">        error_page 504 = @custom_upstream-default-backend_504;</span><br><span class="line">        </span><br><span class="line">        proxy_ssl_session_reuse on;</span><br><span class="line">        </span><br><span class="line">        upstream upstream_balancer &#123;</span><br><span class="line">                server 0.0.0.1; # placeholder</span><br></pre></td></tr></table></figure><p>过滤出上面创建的域名<code>example.bar.com</code>相关配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># start server example.bar.com</span></span></span><br><span class="line">server &#123;</span><br><span class="line">server_name example.bar.com ;</span><br><span class="line"></span><br><span class="line">listen 80;</span><br><span class="line"></span><br><span class="line">listen [::]:80;</span><br><span class="line"></span><br><span class="line">set $proxy_upstream_name "-";</span><br><span class="line">set $pass_access_scheme $scheme;</span><br><span class="line">set $pass_server_port $server_port;</span><br><span class="line">set $best_http_host $http_host;</span><br><span class="line">set $pass_port $pass_server_port;</span><br><span class="line">...</span><br><span class="line">location @custom_upstream-default-backend_404 &#123;</span><br><span class="line">internal;</span><br><span class="line"></span><br><span class="line">proxy_intercept_errors off;</span><br><span class="line"></span><br><span class="line">proxy_set_header       X-Code             404;</span><br><span class="line">proxy_set_header       X-Format           $http_accept;</span><br><span class="line">proxy_set_header       X-Original-URI     $request_uri;</span><br><span class="line">proxy_set_header       X-Namespace        $namespace;</span><br><span class="line">proxy_set_header       X-Ingress-Name     $ingress_name;</span><br><span class="line">proxy_set_header       X-Service-Name     $service_name;</span><br><span class="line">proxy_set_header       X-Service-Port     $service_port;</span><br><span class="line">proxy_set_header       X-Request-ID       $req_id;</span><br><span class="line">proxy_set_header       Host               $best_http_host;</span><br><span class="line"></span><br><span class="line">set $proxy_upstream_name upstream-default-backend;</span><br><span class="line"></span><br><span class="line">rewrite                (.*) / break;</span><br><span class="line"></span><br><span class="line">proxy_pass            http://upstream_balancer;</span><br><span class="line">log_by_lua_block &#123;</span><br><span class="line"></span><br><span class="line">monitor.call()</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">location @custom_upstream-default-backend_500 &#123;</span><br><span class="line">internal;</span><br><span class="line"></span><br><span class="line">proxy_intercept_errors off;</span><br><span class="line"></span><br><span class="line">proxy_set_header       X-Code             500;</span><br><span class="line">proxy_set_header       X-Format           $http_accept;</span><br><span class="line">proxy_set_header       X-Original-URI     $request_uri;</span><br><span class="line">proxy_set_header       X-Namespace        $namespace;</span><br><span class="line">proxy_set_header       X-Ingress-Name     $ingress_name;</span><br><span class="line">proxy_set_header       X-Service-Name     $service_name;</span><br><span class="line">proxy_set_header       X-Service-Port     $service_port;</span><br><span class="line">proxy_set_header       X-Request-ID       $req_id;</span><br><span class="line">proxy_set_header       Host               $best_http_host;</span><br><span class="line"></span><br><span class="line">set $proxy_upstream_name upstream-default-backend;</span><br><span class="line"></span><br><span class="line">rewrite                (.*) / break;</span><br><span class="line"></span><br><span class="line">proxy_pass            http://upstream_balancer;</span><br><span class="line">log_by_lua_block &#123;</span><br><span class="line"></span><br><span class="line">monitor.call()</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>这个<code>server</code>中关于默认后端的配置内容是<strong>关键信息</strong>(踩坑发现，后面只有用到这里的相关配置才能达到最终目标，否则无法判断)。</p><p>可以看到，在传递默认后端时，设置了多个请求头字段，其中<code>X-Code</code>即状态码正是所需要的，这里意味着将控制器返回的对应状态码，例如<code>500</code>定义在了<code>X-Code中</code>。如果自定义一个默认后端来取代官方的默认后端，就可以通过<code>X-Code</code>这个特定的头部来判断实现不同的状态码从而返回不同的自定义错误页面。</p><p>关于<code>X-code</code>早期的版本可能会不生效，<a href="https://github.com/kubernetes/ingress-nginx/issues/2281" target="_blank" rel="noopener">issue参考</a></p><h3><span id="42-构建自定义后端">4.2 构建自定义后端</span></h3><p>自定义后端页面可以理解成就是简单的静态页面，这里可以通过熟悉的<code>nginx</code>来构建这样的自定义后端。即通过手动编译安装<code>nginx</code>，并打包好自定义错误页面、配置文件成一个<code>docker</code>镜像。</p><p>镜像中<code>nginx.conf</code>的<strong>关键配置</strong></p><ul><li>利用上面提到的<code>X-code</code>特定头部进行原始状态码的判断。</li><li><code>nginx</code>不支持嵌套的<code>if</code>判断以及逻辑运算，因此通过设置<code>flag</code>变量标记的形式实现不同状态码的判断返回，如果列出的状态码都不匹配，将状态码设置为返回<code>404</code>。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">    server &#123;</span><br><span class="line">        listen 80;</span><br><span class="line">...</span><br><span class="line">        root   /data/www/error;</span><br><span class="line">        error_page  403  /403.html;</span><br><span class="line">        error_page  404  /404.html;</span><br><span class="line">        error_page  500  /500.html;</span><br><span class="line">        error_page  502  /502.html;</span><br><span class="line">        error_page  503  /503.html;</span><br><span class="line">        error_page  504  /504.html;</span><br><span class="line">        location = / &#123;</span><br><span class="line">            set $flag 404;</span><br><span class="line">            if ($http_x_code = "403")&#123;set $flag 403;&#125;</span><br><span class="line">            if ($http_x_code = "404")&#123;set $flag 404;&#125;</span><br><span class="line">            if ($http_x_code = "500")&#123;set $flag 500;&#125;</span><br><span class="line">            if ($http_x_code = "502")&#123;set $flag 502;&#125;</span><br><span class="line">            if ($http_x_code = "503")&#123;set $flag 503;&#125;</span><br><span class="line">            if ($http_x_code = "504")&#123;set $flag 504;&#125;</span><br><span class="line">            if ($flag = "403")&#123;return 403;&#125;</span><br><span class="line">            if ($flag = "404")&#123;return 404;&#125;</span><br><span class="line">            if ($flag = "500")&#123;return 500;&#125;</span><br><span class="line">            if ($flag = "502")&#123;return 502;&#125;</span><br><span class="line">            if ($flag = "503")&#123;return 503;&#125;</span><br><span class="line">            if ($flag = "504")&#123;return 504;&#125;</span><br><span class="line">        &#125;</span><br><span class="line">        location = /403.html &#123;</span><br><span class="line">            internal;</span><br><span class="line">        &#125;</span><br><span class="line">        location = /404.html &#123;</span><br><span class="line">            internal;</span><br><span class="line">        &#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>代码根目录结构</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@docker nginx_error]# tree error/</span><br><span class="line">error/</span><br><span class="line">├── 403.html</span><br><span class="line">├── 404.html</span><br><span class="line">├── 500.html</span><br><span class="line">├── 502.html</span><br><span class="line">├── 503.html</span><br><span class="line">└── 504.html</span><br></pre></td></tr></table></figure><p>这里我已经将制作好的镜像上传到了<code>dockerhub</code>，可以通过以下命令拉取镜像</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull ssgeek/nginx:nginx_error_1.14.2_v1.0</span><br></pre></td></tr></table></figure><h3><span id="43-部署自定义后端">4.3 部署自定义后端</span></h3><p>参照已有模板，重新部署一个新的默认后端</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ssgeek-errors</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">ssgeek-errors</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">ingress-nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">ssgeek-errors</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">ingress-nginx</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ssgeek-errors</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/name:</span> <span class="string">ssgeek-errors</span></span><br><span class="line">    <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">ingress-nginx</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">ingress-nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app.kubernetes.io/name:</span> <span class="string">ssgeek-errors</span></span><br><span class="line">      <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">ingress-nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app.kubernetes.io/name:</span> <span class="string">ssgeek-errors</span></span><br><span class="line">        <span class="attr">app.kubernetes.io/part-of:</span> <span class="string">ingress-nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ssgeek-errors</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">ssgeek/nginx:nginx_error_1.14.2_v1.0</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="comment"># Setting the environment variable DEBUG we can see the headers sent </span></span><br><span class="line">        <span class="comment"># by the ingress controller to the backend in the client response.</span></span><br><span class="line">        <span class="comment"># env:</span></span><br><span class="line">        <span class="comment"># - name: DEBUG</span></span><br><span class="line">        <span class="comment">#   value: "true"</span></span><br></pre></td></tr></table></figure><p>同样的，修改<code>Ingress controller</code>控制器的启动参数，修改关联的<code>service</code>名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> kubectl -n ingress-nginx edit ds nginx-ingress-controller</span></span><br><span class="line">...</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - args:</span><br><span class="line">        - /nginx-ingress-controller</span><br><span class="line">        - --configmap=$(POD_NAMESPACE)/nginx-configuration</span><br><span class="line">        - --tcp-services-configmap=$(POD_NAMESPACE)/tcp-services</span><br><span class="line">        - --udp-services-configmap=$(POD_NAMESPACE)/udp-services</span><br><span class="line">        - --publish-service=$(POD_NAMESPACE)/ingress-nginx</span><br><span class="line">        - --annotations-prefix=nginx.ingress.kubernetes.io</span><br><span class="line">        - --default-backend-service=ingress-nginx/ssgeek-errors  # 修改成自定义的默认后端服务</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3><span id="44-最终测试">4.4 最终测试</span></h3><p>测试效果如下</p><p><video id="video" width="800" height="450" controls="controls" preload="none" poster="https://image.ssgeek.com/20201217-05.png" style="box-sizing: inherit; margin: 0px; padding: 0px; color: rgb(73, 80, 87); font-family: -apple-system, system-ui, &quot;Segoe UI&quot;, Roboto, &quot;PingFang SC&quot;, &quot;Hiragino Sans GB&quot;, &quot;Microsoft YaHei&quot;, Helvetica, Arial, sans-serif, &quot;Apple Color Emoji&quot;, &quot;Segoe UI Emoji&quot;, &quot;Segoe UI Symbol&quot;; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: 400; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(248, 249, 250); text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial;"></video></p><p>到这里，基于<code>k8s Ingress nginx</code>对错误页面的深度定制就完成了。</p><blockquote><p>本文转载自：「山山仙人博客」，原文：<a href="https://tinyurl.com/ybkp2jbw%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://tinyurl.com/ybkp2jbw，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;错误页面是发生错误时显示的网页。 错误页面会警告用户发生的错误类型，并可能为用户提供解决问题的步骤的建议。 除了在未样式化的网页上提供错误信息的基本页面之外，还可以使用可以设计为具有额外功能和样式外观的自定义错误页面。 这些设置可以在服务器上更改。 许多服务器提供了可用于生成自定义错误页面的实用程序。&lt;/p&gt;
&lt;p&gt;引文参考：&lt;a href=&quot;https://www.netinbag.com/cn/internet/what-are-error-pages.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.netinbag.com/cn/internet/what-are-error-pages.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;1、错误页面状态码&quot;&gt;1、错误页面状态码&lt;/h2&gt;
&lt;p&gt;网站运行过程中难免出现问题，为用户抛出一个错误页面，常见的错误页面包含&lt;code&gt;403&lt;/code&gt;、&lt;code&gt;404&lt;/code&gt;、&lt;code&gt;500&lt;/code&gt;、&lt;code&gt;502&lt;/code&gt;、&lt;code&gt;503&lt;/code&gt;、&lt;code&gt;504&lt;/code&gt;状态码，这些常见的错误页面状态码的含义如下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;403 Forbidden&lt;/li&gt;
&lt;li&gt;404 Not Found&lt;/li&gt;
&lt;li&gt;500 Internal Server Eroor&lt;/li&gt;
&lt;li&gt;502 Bad Gateway&lt;/li&gt;
&lt;li&gt;503 Service Unavailable&lt;/li&gt;
&lt;li&gt;504 Gateway Timeout&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>史上最全的 SSH 中文教程 (阮一峰 2020 最新开源力作)</title>
    <link href="https://www.hi-linux.com/posts/11502.html"/>
    <id>https://www.hi-linux.com/posts/11502.html</id>
    <published>2020-12-24T01:00:00.000Z</published>
    <updated>2020-12-24T07:36:55.580Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>最近，阮一峰老师正式发布了他的新作：《SSH 入门教程》，只要你接触过后端开发，或了解计算机相关的一些基础知识，那一定会接触到 <code>SSH</code> 概念。</p><p><code>SSH</code> 是 <code>Linux</code> 系统的登录工具，现在广泛用于服务器登录和各种加密通信、身份验证等场景。对于一个常年跟服务器打交道的人，摸透 <code>SSH</code> 是一件很有必要的事情。</p><p>在《SSH 入门教程》里面，阮一峰老师将会向开发者讲解 <code>SSH</code> 基础知识、客户端、密钥登录、服务器操作、端口转发、证书登录，以及其他几个常用命令（<code>scp</code>、<code>rsync</code>、<code>sftp</code>）的基础使用等内容，也可以当作工具手册日常查询。</p><a id="more"></a><p><strong>本书的大致目录如下:</strong></p><p><img src="https://www.hi-linux.com/img/staticfile/ssh1-2020-12-24-2XAQWe.png" alt></p><p>每个章节下面，均对应着更为详细的内容讲解，以 <code>SSH</code> 客户端一节为例，将着重为开发者讲解 <code>SSH</code>客户端基本用法、连接流程、密钥变更、加密参数等内容：</p><p><img src="https://www.hi-linux.com/img/staticfile/ssh2-2020-12-24-KVVSFN.png" alt></p><p>如果你对 <code>SSH</code> 的发展历史，以及计算机网络技术中的应用还不甚了解或者是想重温一遍 <code>SSH</code> 的知识，推荐你可以好好看下这本书。</p><blockquote><p>文档地址：<strong><a href="https://wangdoc.com/ssh/index.html" target="_blank" rel="noopener">https://wangdoc.com/ssh/index.html</a></strong></p><p>开源项目地址：<strong><a href="https://github.com/wangdoc/ssh-tutorial" target="_blank" rel="noopener">https://github.com/wangdoc/ssh-tutorial</a></strong></p></blockquote><p>如果你需要下载收藏本书，相关的内容及源码，我们已给大家打包好了，只需要在公众号后台回复「<strong>SSH</strong>」，即可免费领取！。</p><p><strong>参考文档：</strong></p><ol><li><a href="https://mp.weixin.qq.com/s/jcKe6SrI-vxXKZv8tHDxBg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/jcKe6SrI-vxXKZv8tHDxBg</a></li><li><a href="https://mp.weixin.qq.com/s/71jJm5iFmi2hcU8oKADNlg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/71jJm5iFmi2hcU8oKADNlg</a></li></ol></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近，阮一峰老师正式发布了他的新作：《SSH 入门教程》，只要你接触过后端开发，或了解计算机相关的一些基础知识，那一定会接触到 &lt;code&gt;SSH&lt;/code&gt; 概念。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SSH&lt;/code&gt; 是 &lt;code&gt;Linux&lt;/code&gt; 系统的登录工具，现在广泛用于服务器登录和各种加密通信、身份验证等场景。对于一个常年跟服务器打交道的人，摸透 &lt;code&gt;SSH&lt;/code&gt; 是一件很有必要的事情。&lt;/p&gt;
&lt;p&gt;在《SSH 入门教程》里面，阮一峰老师将会向开发者讲解 &lt;code&gt;SSH&lt;/code&gt; 基础知识、客户端、密钥登录、服务器操作、端口转发、证书登录，以及其他几个常用命令（&lt;code&gt;scp&lt;/code&gt;、&lt;code&gt;rsync&lt;/code&gt;、&lt;code&gt;sftp&lt;/code&gt;）的基础使用等内容，也可以当作工具手册日常查询。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="SSH" scheme="https://www.hi-linux.com/tags/SSH/"/>
    
  </entry>
  
  <entry>
    <title>Docker Desktop 3.0.0 正式版发布：开始支持补丁增量更新和为 Mac 的 M1 设备引入支持</title>
    <link href="https://www.hi-linux.com/posts/20863.html"/>
    <id>https://www.hi-linux.com/posts/20863.html</id>
    <published>2020-12-22T01:00:00.000Z</published>
    <updated>2020-12-22T07:40:42.614Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>近期 Docker 发布了全新的 Docker Desktop 3.0.0 版本，这个版本采用补丁形式进行增量更新，减少了每次更新的容量。官方还删除稳定和边缘频道，以单一发布串流代替，减少版本之间的混淆。而 Docker Engine 则发布 20.10版本，并开始支持 Cgroups V2。</p><blockquote><p>Docker Desktop 是一个支持 Windows 和 MAC 系统的完整桌面开发环境，包括 Docker App，开发人员工具，Kubernetes 以及与最新版本的 Docker 引擎。Docker Desktop 可以让开发者利用认证的镜像和模板以及自选语言和工具进行快速的容器集群自动构建，利用 Docker Hub 将开发环境部署到安全的存储库，进行持续集成和安全协作。</p></blockquote><p>Docker Desktop 3.0 版本主要更新内容包括：</p><ul><li>Docker 仪表板，可以让用户在一个 UI 界面中访问容器、应用程序和远程镜像;</li><li>适用于 Windows 10 Home 的 Docker 桌面;</li><li>针对 Windows 上的 WSL 2 后端提供了更本地化的集成并大大提高了性能；</li><li>支持 Azure 容器实例和 Amazon Elastic Container Service ;</li><li>与 Snyk 建立合作伙伴关系，以安全扫描本地镜像并显示来自 Docker Hub 的镜像扫描结果；</li><li>Windows 和 Mac 上都加入了新的文件系统；</li><li>针对 Mac 的新款 CPU 进行大量的改进；</li><li>自动增量更新：现在版本更新只需要安装增量软件包（几十 M），并自动在后台完成。</li></ul><a id="more"></a><h2><span id="支持补丁增量更新">支持补丁增量更新</span></h2><p>由于不少使用者反应 Docker Desktop 的更新容量太大，下载和安装需要花费太长时间。因此官方决定改变更新方法，过去 Docker Desktop 的更新发布，都是提供完整的安装包，因此用户每次更新都需要下载数百 MB 的文件，而往后的更新，将会是前一个版本的增量更新，更新下载的容量会降至数十 MB，而且用户也将不用停下手边的工作，才能更新 Docker Desktop ，更新程序会在后台下载并且安装，届时用户只要重新启动软件，就可以开始使用新的版本。</p><p>官方还提到，他们还收到用户对稳定版与边缘版本的抱怨，稳定版的修复代码更新速度太慢，而边缘版本的更新频率又太过频繁，不时还会包含具有破坏性的更新。从稳定版切换到边缘版本，还需要重置容器和镜像。而且稳定版与边缘版使用平行，但又独立的版本号，让使用者难以比较版本的新旧。</p><p>因此从现在开始，Docker Desktop 3.0 将只会有一个发布串流，包含最新的修复代码以及实验性功能，而且皆为累积性更新，解决用户不清楚每个发布版本差异的问题。统一发布串流后，每个使用者都可以选择使用最新的功能，且接收的更新容量不只缩小，系统也会自动应用修复程序。</p><h2><span id="支持-apple-siliconm1设备">支持 Apple Silicon（M1）设备</span></h2><p>Docker Desktop 3.0 最大的变化，就是提供了对 Apple Silicon 设备的支持。如果你想要在 13 英寸的 M1 MacBook Air / Pro 或 Mac mini 上使用 Docker Desktop，现无需担心在体验上有任何妥协。与此同时，Docker 最新预览版也引入了对 Windows Linux 子系统（WSL 2）的 GPU 支持。</p><p>之前 Docker Desktop 仅限于收到邀请的开发者预览，但即日起已向所有用户开放。</p><h2><span id="docker-engine-2010-版本发布">Docker Engine 20.10 版本发布</span></h2><p>另外，官方也发布了 Docker Engine 20.10 版本，重要的更新包括支持 Cgroups V2。Docker 使用了几种基本 Linux 核心功能，以隔离正在执行的程序以及相关的文件，其中一个便是 Cgroups，在 Linux 中，Cgroups 会限制程序使用的资源，包括 CPU、内存和磁盘等，而 Docker 结合 Cgroups 与 Linux 命名空间，来将程序隔离在容器中。Linux 核心在 2016 年加入了 Cgroups V2，强化群组管理，以及无根容器的资源限制支持，而现在 Docker Engine 20.10 也开始支持 Cgroups V2，支持这些更多的管理功能。</p><p>Docker Engine 20.10 还支持以各种日志驱动程序读取 Docker 日志，过去 jsonfile 和 journald 日志程序，支持以 Docker 日志读取容器日志，但是许多第三方日志程序，并不支持 Docker 日志在本地端读取日志，而这对于想要以自动化和标准方式收集日志的使用者，造成许多麻烦，因此从 Docker Engine 20.10 开始，无论配置的日志记录驱动，或是扩充组件，都可以使用 Docker 日志读取容器日志。</p><h2><span id="参考文档">参考文档</span></h2><ol><li><a href="https://www.google.com" target="_blank" rel="noopener">https://www.google.com</a></li><li><a href="https://www.ithome.com.tw/news/141673" target="_blank" rel="noopener">https://www.ithome.com.tw/news/141673</a></li><li><a href="https://www.docker.com/blog/docker-desktop-3-0-0-smaller-faster-releases/" target="_blank" rel="noopener">https://www.docker.com/blog/docker-desktop-3-0-0-smaller-faster-releases/</a></li></ol></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;近期 Docker 发布了全新的 Docker Desktop 3.0.0 版本，这个版本采用补丁形式进行增量更新，减少了每次更新的容量。官方还删除稳定和边缘频道，以单一发布串流代替，减少版本之间的混淆。而 Docker Engine 则发布 20.10版本，并开始支持 Cgroups V2。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Docker Desktop 是一个支持 Windows 和 MAC 系统的完整桌面开发环境，包括 Docker App，开发人员工具，Kubernetes 以及与最新版本的 Docker 引擎。Docker Desktop 可以让开发者利用认证的镜像和模板以及自选语言和工具进行快速的容器集群自动构建，利用 Docker Hub 将开发环境部署到安全的存储库，进行持续集成和安全协作。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Docker Desktop 3.0 版本主要更新内容包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Docker 仪表板，可以让用户在一个 UI 界面中访问容器、应用程序和远程镜像;&lt;/li&gt;
&lt;li&gt;适用于 Windows 10 Home 的 Docker 桌面;&lt;/li&gt;
&lt;li&gt;针对 Windows 上的 WSL 2 后端提供了更本地化的集成并大大提高了性能；&lt;/li&gt;
&lt;li&gt;支持 Azure 容器实例和 Amazon Elastic Container Service ;&lt;/li&gt;
&lt;li&gt;与 Snyk 建立合作伙伴关系，以安全扫描本地镜像并显示来自 Docker Hub 的镜像扫描结果；&lt;/li&gt;
&lt;li&gt;Windows 和 Mac 上都加入了新的文件系统；&lt;/li&gt;
&lt;li&gt;针对 Mac 的新款 CPU 进行大量的改进；&lt;/li&gt;
&lt;li&gt;自动增量更新：现在版本更新只需要安装增量软件包（几十 M），并自动在后台完成。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="Docker" scheme="https://www.hi-linux.com/categories/docker/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>再见 Docker，是时候拥抱下一代容器镜像构建工具 Kaniko 了</title>
    <link href="https://www.hi-linux.com/posts/32725.html"/>
    <id>https://www.hi-linux.com/posts/32725.html</id>
    <published>2020-12-15T01:00:00.000Z</published>
    <updated>2020-12-15T05:12:47.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="1-daemon-less-镜像构建工具">1. daemon-less 镜像构建工具</span></h2><h3><span id="11-什么是-daemon-less-镜像构建工具">1.1  什么是 daemon-less 镜像构建工具</span></h3><p>在 CICD 流程中，经常会涉及镜像构建，常规的做法是使用 Docker in Docker 或者 Docker out of Docker 进行构建。详情可以参考文档：<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247492294&amp;idx=1&amp;sn=6c702bf5b94f6df07b6560403813b27f&amp;chksm=eac6c3efddb14af9b540b88ae9d46fe97c98fe35bfe135c3091e4de5d30d5184d130f94a6f1d&amp;token=692202010&amp;lang=zh_CN#rd" target="_blank" rel="noopener">如何在 Docker 中使用 Docker</a></p><p>实际上，为了避免垄断，促进行业发展，基于 Docker 的镜像格式，早就指定了统一的 OCI 镜像格式规范。也就是说，只需要通过 Dockerfile 得到一个符合 OCI 规范的镜像即可，并没有强制要求谁来做这件事。</p><p>daemon-less 的构建工具，可以不依赖于 Docker Daemon 进行构建镜像。这在 CICD 场景下，具有重要的意义。同时，Kubernetes 正在摆脱 Docker 的影响，构建更加开放的架构生态，CICD 需要与 Docker Daemon 解耦。</p><a id="more"></a><h3><span id="12-daemon-less-的优势">1.2 daemon-less 的优势</span></h3><ul><li>免挂载 sock 文件</li></ul><p>目前，实现 CRI 和 OCI 接口的运行时组件越来越多，例如 cri-o、containerd。这些运行时管理工具，并没有类似 /var/run/docker.sock 的文件用于挂载服务。daemon-less 的构建方式能够对接这些工具。</p><ul><li>更加安全</li></ul><p>这些 daemon-less 工具，通常运行在 userspace ，而不是以 root 特权运行，提供更加安全的运行时。</p><ul><li>更适合 Kubernetes</li></ul><p>通过挂载 /var/run/docker.sock 的方式，使用 Docker Daemon 构建镜像，是一种集中的构建方式。镜像的构建主要交给节点上的 Docker Daemon 来完成。而如果能直接将 Dockerfile 转换成镜像，更加适应 Kubernetes、Serverless 基础设施，构建规模和效率将得到提高。</p><h3><span id="13-相关的开源项目">1.3 相关的开源项目</span></h3><ul><li>Kaniko, Google 主导</li><li>Buildah, Red Hat 主导</li><li>Img, Jessie Frazelle 发起</li></ul><p>从 Star 量上看，Buildah、Img 目前是 3K+，而 Kaniko 达到 7K+，下面以 Kaniko 为例行文。</p><h2><span id="2-kaniko-的工作原理">2. Kaniko 的工作原理</span></h2><p><img src="https://www.hi-linux.com/img/staticfile/2020-12-11-m1ucVx.png" alt></p><p>如上图是 Kaniko 的工作原理图。Kaniko 执行器从 Dockerfile 构建镜像，并将其推送到镜像仓库。主要分为以下几步:</p><ul><li>根据 Dockerfile 中 FROM 描述，解压基础镜像的文件系统</li><li>执行 Dockerfile 中的每条命令，在用户空间建立文件快照</li><li>将变更的文件层添加到基础镜像中，更新镜像的元数据</li><li>推送镜像</li></ul><p>已知的问题</p><ul><li>不支持构建 Windows 镜像</li><li>kaniko 命令只能在官方镜像中运行，不支持其他 Docker 镜像</li><li>不支持 Registry V1 接口</li></ul><h2><span id="3-kaniko-demo">3. Kaniko Demo</span></h2><h3><span id="31-测试-demo-选择">3.1 测试 Demo 选择</span></h3><p>这里选择的是 <a href="https://github.com/traefik/whoami" target="_blank" rel="noopener">https://github.com/traefik/whoami</a> 项目。访问该服务之后，接口会返回访问者的相关信息。</p><p>对项目的要求是，通过 Dockerfile 能够直接编译得到镜像。一共有两个验证点:</p><ul><li>能够构建镜像</li><li>构建的镜像能够运行</li></ul><h3><span id="32-在-docker-上运行-kaniko">3.2 在 Docker 上运行 Kaniko</span></h3><ul><li>生成推送镜像的凭证</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ export AUTH&#x3D;$(echo -n YOUR_USERNAME:YOUR_PASSWORD | base64 )</span><br><span class="line"></span><br><span class="line">$ cat &gt; config.json &lt;&lt;-EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;auths&quot;: &#123;</span><br><span class="line">        &quot;https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;&quot;: &#123;</span><br><span class="line">            &quot;auth&quot;: &quot;$&#123;AUTH&#125;&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>构建镜像</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ docker run \</span><br><span class="line">  --interactive -v &#96;pwd&#96;&#x2F;config.json:&#x2F;kaniko&#x2F;.docker&#x2F;config.json gcr.io&#x2F;kaniko-project&#x2F;executor:latest \</span><br><span class="line">  --context git:&#x2F;&#x2F;github.com&#x2F;traefik&#x2F;whoami \</span><br><span class="line">  --dockerfile Dockerfile \</span><br><span class="line">  --destination&#x3D;shaowenchen&#x2F;kaniko-demo:v1</span><br></pre></td></tr></table></figure><p>参数说明:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- context, 构建需要的上下文。支持多种格式，S3、本地目录、标准输入、Git 仓库等</span><br><span class="line">- dockerfile, Dockerfile 路径</span><br><span class="line">- destination, 构建后推送的镜像地址</span><br></pre></td></tr></table></figure><p>在生产环境，可以配置缓存，用于加速镜像构建。</p><ul><li>查看日志</li></ul><p>执行上一步的命令之后，可以看到如下输出:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">Enumerating objects: 17, done.</span><br><span class="line">Counting objects: 100% (17&#x2F;17), done.</span><br><span class="line">Compressing objects: 100% (13&#x2F;13), done.</span><br><span class="line">Total 157 (delta 3), reused 12 (delta 3), pack-reused 140</span><br><span class="line">INFO[0004] Resolved base name golang:1-alpine to builder</span><br><span class="line">INFO[0004] Retrieving image manifest golang:1-alpine</span><br><span class="line">INFO[0004] Retrieving image golang:1-alpine</span><br><span class="line">INFO[0007] Retrieving image manifest golang:1-alpine</span><br><span class="line">INFO[0007] Retrieving image golang:1-alpine</span><br><span class="line">INFO[0010] No base image, nothing to extract</span><br><span class="line">INFO[0010] Built cross stage deps: map[0:[&#x2F;usr&#x2F;share&#x2F;zoneinfo &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;ca-certificates.crt &#x2F;go&#x2F;whoami&#x2F;whoami]]</span><br><span class="line">INFO[0010] Retrieving image manifest golang:1-alpine</span><br><span class="line">INFO[0010] Retrieving image golang:1-alpine</span><br><span class="line">INFO[0012] Retrieving image manifest golang:1-alpine</span><br><span class="line">INFO[0012] Retrieving image golang:1-alpine</span><br><span class="line">INFO[0015] Executing 0 build triggers</span><br><span class="line">INFO[0015] Unpacking rootfs as cmd RUN apk --no-cache --no-progress add git ca-certificates tzdata make     &amp;&amp; update-ca-certificates     &amp;&amp; rm -rf &#x2F;var&#x2F;cache&#x2F;apk&#x2F;* requires it.</span><br><span class="line">INFO[0035] RUN apk --no-cache --no-progress add git ca-certificates tzdata make     &amp;&amp; update-ca-certificates     &amp;&amp; rm -rf &#x2F;var&#x2F;cache&#x2F;apk&#x2F;*</span><br><span class="line">INFO[0035] Taking snapshot of full filesystem...</span><br><span class="line">INFO[0041] cmd: &#x2F;bin&#x2F;sh</span><br><span class="line">INFO[0041] args: [-c apk --no-cache --no-progress add git ca-certificates tzdata make     &amp;&amp; update-ca-certificates     &amp;&amp; rm -rf &#x2F;var&#x2F;cache&#x2F;apk&#x2F;*]</span><br><span class="line">INFO[0041] Running: [&#x2F;bin&#x2F;sh -c apk --no-cache --no-progress add git ca-certificates tzdata make     &amp;&amp; update-ca-certificates     &amp;&amp; rm -rf &#x2F;var&#x2F;cache&#x2F;apk&#x2F;*]</span><br><span class="line">fetch http:&#x2F;&#x2F;dl-cdn.alpinelinux.org&#x2F;alpine&#x2F;v3.12&#x2F;main&#x2F;x86_64&#x2F;APKINDEX.tar.gz</span><br><span class="line">fetch http:&#x2F;&#x2F;dl-cdn.alpinelinux.org&#x2F;alpine&#x2F;v3.12&#x2F;community&#x2F;x86_64&#x2F;APKINDEX.tar.gz</span><br><span class="line">(1&#x2F;7) Installing nghttp2-libs (1.41.0-r0)</span><br><span class="line">(2&#x2F;7) Installing libcurl (7.69.1-r2)</span><br><span class="line">(3&#x2F;7) Installing expat (2.2.9-r1)</span><br><span class="line">(4&#x2F;7) Installing pcre2 (10.35-r0)</span><br><span class="line">(5&#x2F;7) Installing git (2.26.2-r0)</span><br><span class="line">(6&#x2F;7) Installing make (4.3-r0)</span><br><span class="line">(7&#x2F;7) Installing tzdata (2020c-r1)</span><br><span class="line">Executing busybox-1.31.1-r19.trigger</span><br><span class="line">OK: 25 MiB in 22 packages</span><br><span class="line">WARNING: ca-certificates.crt does not contain exactly one certificate or CRL: skipping</span><br><span class="line">INFO[0042] Taking snapshot of full filesystem...</span><br><span class="line">INFO[0045] WORKDIR &#x2F;go&#x2F;whoami</span><br><span class="line">INFO[0045] cmd: workdir</span><br><span class="line">INFO[0045] Changed working directory to &#x2F;go&#x2F;whoami</span><br><span class="line">INFO[0045] Creating directory &#x2F;go&#x2F;whoami</span><br><span class="line">INFO[0045] Taking snapshot of files...</span><br><span class="line">INFO[0045] COPY go.mod .</span><br><span class="line">INFO[0045] Taking snapshot of files...</span><br><span class="line">INFO[0045] COPY go.sum .</span><br><span class="line">INFO[0045] Taking snapshot of files...</span><br><span class="line">INFO[0045] RUN GO111MODULE&#x3D;on GOPROXY&#x3D;https:&#x2F;&#x2F;proxy.golang.org go mod download</span><br><span class="line">INFO[0045] cmd: &#x2F;bin&#x2F;sh</span><br><span class="line">INFO[0045] args: [-c GO111MODULE&#x3D;on GOPROXY&#x3D;https:&#x2F;&#x2F;proxy.golang.org go mod download]</span><br><span class="line">INFO[0045] Running: [&#x2F;bin&#x2F;sh -c GO111MODULE&#x3D;on GOPROXY&#x3D;https:&#x2F;&#x2F;proxy.golang.org go mod download]</span><br><span class="line">INFO[0045] Taking snapshot of full filesystem...</span><br><span class="line">INFO[0047] COPY . .</span><br><span class="line">INFO[0047] Taking snapshot of files...</span><br><span class="line">INFO[0047] RUN make build</span><br><span class="line">INFO[0047] cmd: &#x2F;bin&#x2F;sh</span><br><span class="line">INFO[0047] args: [-c make build]</span><br><span class="line">INFO[0047] Running: [&#x2F;bin&#x2F;sh -c make build]</span><br><span class="line">CGO_ENABLED&#x3D;0 go build -a --trimpath --installsuffix cgo --ldflags&#x3D;&quot;-s&quot; -o whoami</span><br><span class="line">INFO[0058] Taking snapshot of full filesystem...</span><br><span class="line">INFO[0062] Saving file usr&#x2F;share&#x2F;zoneinfo for later use</span><br><span class="line">INFO[0062] Saving file etc&#x2F;ssl&#x2F;certs&#x2F;ca-certificates.crt for later use</span><br><span class="line">INFO[0062] Saving file go&#x2F;whoami&#x2F;whoami for later use</span><br><span class="line">INFO[0062] Deleting filesystem...</span><br><span class="line">INFO[0063] No base image, nothing to extract</span><br><span class="line">INFO[0063] Executing 0 build triggers</span><br><span class="line">INFO[0063] Unpacking rootfs as cmd COPY --from&#x3D;builder &#x2F;usr&#x2F;share&#x2F;zoneinfo &#x2F;usr&#x2F;share&#x2F;zoneinfo requires it.</span><br><span class="line">INFO[0063] COPY --from&#x3D;builder &#x2F;usr&#x2F;share&#x2F;zoneinfo &#x2F;usr&#x2F;share&#x2F;zoneinfo</span><br><span class="line">INFO[0063] Taking snapshot of files...</span><br><span class="line">INFO[0063] COPY --from&#x3D;builder &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;ca-certificates.crt &#x2F;etc&#x2F;ssl&#x2F;certs&#x2F;</span><br><span class="line">INFO[0063] Taking snapshot of files...</span><br><span class="line">INFO[0063] COPY --from&#x3D;builder &#x2F;go&#x2F;whoami&#x2F;whoami .</span><br><span class="line">INFO[0063] Taking snapshot of files...</span><br><span class="line">INFO[0063] ENTRYPOINT [&quot;&#x2F;whoami&quot;]</span><br><span class="line">INFO[0063] EXPOSE 80</span><br><span class="line">INFO[0063] cmd: EXPOSE</span><br><span class="line">INFO[0063] Adding exposed port: 80&#x2F;tcp</span><br></pre></td></tr></table></figure><p>最后正常退出，构建并推送镜像成功。</p><ul><li>查看构建镜像是否落盘本地</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker images|grep kaniko-demo</span><br></pre></td></tr></table></figure><p>执行完命令，没有任何输出，符合预期。构建之后的镜像，直接被推送到了远程 Registry。</p><ul><li>DockerHub 查看镜像</li></ul><p>在 DockerHub 页面可以查看到推送的镜像:</p><p><img src="https://www.hi-linux.com/img/staticfile/2020-12-11-0ZiuKe.png" alt></p><ul><li>使用镜像，创建容器</li></ul><p>执行命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -d -p 8011:80 shaowenchen&#x2F;kaniko-demo:v1</span><br></pre></td></tr></table></figure><p>验证服务是否正常:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ curl localhost:8011</span><br><span class="line"></span><br><span class="line">Hostname: 6dd22f1e4100</span><br><span class="line">IP: 127.0.0.1</span><br><span class="line">IP: 172.17.0.2</span><br><span class="line">RemoteAddr: 172.17.0.1:40940</span><br><span class="line">GET &#x2F; HTTP&#x2F;1.1</span><br><span class="line">Host: localhost:8011</span><br><span class="line">User-Agent: curl&#x2F;7.29.0</span><br><span class="line">Accept: *&#x2F;*</span><br></pre></td></tr></table></figure><h3><span id="33-在-kubernetes-上运行-kaniko">3.3  在 Kubernetes 上运行 Kaniko</span></h3><ul><li>查看 Kubernetes 版本 - v1.17.9</li></ul><p>对于不同版本的 Kubernetes，下面有些命令会有所差异，需要自行适配。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl version</span><br><span class="line"></span><br><span class="line">Client Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;17&quot;, GitVersion:&quot;v1.17.9&quot;, GitCommit:&quot;4fb7ed12476d57b8437ada90b4f93b17ffaeed99&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-07-15T16:18:16Z&quot;, GoVersion:&quot;go1.13.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:&quot;1&quot;, Minor:&quot;17&quot;, GitVersion:&quot;v1.17.9&quot;, GitCommit:&quot;4fb7ed12476d57b8437ada90b4f93b17ffaeed99&quot;, GitTreeState:&quot;clean&quot;, BuildDate:&quot;2020-07-15T16:10:45Z&quot;, GoVersion:&quot;go1.13.9&quot;, Compiler:&quot;gc&quot;, Platform:&quot;linux&#x2F;amd64&quot;&#125;</span><br></pre></td></tr></table></figure><ul><li>创建命名空间</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create ns kaniko-demo</span><br></pre></td></tr></table></figure><ul><li>创建镜像推送秘钥</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kaniko-demo create secret docker-registry kaniko-secret \</span><br><span class="line">  --docker-server&#x3D;https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F; \</span><br><span class="line">  --docker-username&#x3D;YOUR_USERNAME \</span><br><span class="line">  --docker-password&#x3D;YOUR_PASSWORD \</span><br><span class="line">  --docker-email&#x3D;mail@chenshaowen.com</span><br></pre></td></tr></table></figure><ul><li>创建 kaniko Pod 构建镜像</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ cat &gt; kaniko-builder.yaml &lt;&lt;-EOF</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: kaniko</span><br><span class="line">  namespace: kaniko-demo</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: kaniko</span><br><span class="line">    image: gcr.io&#x2F;kaniko-project&#x2F;executor:latest</span><br><span class="line">    args:</span><br><span class="line">    - &quot;--dockerfile&#x3D;Dockerfile&quot;</span><br><span class="line">    - &quot;--context&#x3D;git:&#x2F;&#x2F;github.com&#x2F;traefik&#x2F;whoami&quot;</span><br><span class="line">    - &quot;--destination&#x3D;shaowenchen&#x2F;kaniko-demo:v2&quot;</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: kaniko-secret</span><br><span class="line">      mountPath: &#x2F;kaniko&#x2F;.docker&#x2F;</span><br><span class="line">    restartPolicy: Never</span><br><span class="line">    volumes:</span><br><span class="line">  - name: kaniko-secret</span><br><span class="line">    secret:</span><br><span class="line">      secretName: kaniko-secret</span><br><span class="line">      items:</span><br><span class="line">          - key: .dockerconfigjson</span><br><span class="line">            path: config.json</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><ul><li>创建 Pod</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f kaniko-builder.yaml</span><br></pre></td></tr></table></figure><ul><li>查看日志</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kaniko-demo logs kaniko</span><br></pre></td></tr></table></figure><p>日志内容与直接在 Docker 中运行类似，这里就不重复贴出。</p><ul><li>查看 DockerHub 页面的镜像</li></ul><p><img src="https://www.hi-linux.com/img/staticfile/2020-12-11-uMSm2v.png" alt></p><ul><li>使用构建的镜像创建负载</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kaniko-demo run kaniko --image&#x3D;shaowenchen&#x2F;kaniko-demo:v2</span><br></pre></td></tr></table></figure><ul><li>暴露服务并查看服务端口</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n  kaniko-demo expose deploy&#x2F;kaniko --type&#x3D;NodePort --port&#x3D;80 --target-port&#x3D;80</span><br><span class="line">$ kubectl -n  kaniko-demo get svc</span><br><span class="line">NAME     TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">kaniko   NodePort   10.233.27.225   &lt;none&gt;        80:30772&#x2F;TCP   29s</span><br></pre></td></tr></table></figure><ul><li>验证构建的镜像能正常使用</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ curl 192.168.13.3:30772</span><br><span class="line"></span><br><span class="line">Hostname: kaniko-5ddbf597b6-5h8k8</span><br><span class="line">IP: 127.0.0.1</span><br><span class="line">IP: 10.233.90.187</span><br><span class="line">RemoteAddr: 192.168.13.3:11443</span><br><span class="line">GET &#x2F; HTTP&#x2F;1.1</span><br><span class="line">Host: 192.168.13.3:30772</span><br><span class="line">User-Agent: curl&#x2F;7.29.0</span><br><span class="line">Accept: *&#x2F;*</span><br></pre></td></tr></table></figure><h2><span id="4-其他关注的问题">4. 其他关注的问题</span></h2><ul><li>支持 ARM 镜像构建</li></ul><p>在最新的版本中，已经可以看到 ARM 的 Kaniko 执行器: <a href="https://github.com/GoogleContainerTools/kaniko/releases/tag/v1.3.0" target="_blank" rel="noopener">https://github.com/GoogleContainerTools/kaniko/releases/tag/v1.3.0</a></p><ul><li>无法给 Git 仓库单独配置秘钥和分支参数</li></ul><p>官方提供的一种方式是直接将秘钥和分支参数拼接在 Git 仓库的 URL 中</p><ul><li>没有提供构建之前的 prehook 命令</li></ul><p>需要在 Dockerfile 中完整描述从源码到镜像的构建过程。最好借助于分阶段构建，改造有些项目中直接 Copy 构建产物的的 Dockerfile 。类似下面的 Dockerfile 不能直接使用:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM java:openjdk-8-jre-alpine</span><br><span class="line"></span><br><span class="line">WORKDIR &#x2F;home</span><br><span class="line"></span><br><span class="line">COPY target&#x2F;*.jar &#x2F;home</span><br><span class="line"></span><br><span class="line">ENTRYPOINT java -jar *.jar</span><br></pre></td></tr></table></figure><h2><span id="5-参考">5. 参考</span></h2><ol><li><a href="https://github.com/GoogleContainerTools/kaniko" target="_blank" rel="noopener">https://github.com/GoogleContainerTools/kaniko</a></li><li><a href="https://github.com/genuinetools/img" target="_blank" rel="noopener">https://github.com/genuinetools/img</a></li><li><a href="https://github.com/containers/buildah" target="_blank" rel="noopener">https://github.com/containers/buildah</a></li></ol><blockquote><p>本文转载自：「陈少文的博客」，原文：<a href="https://tinyurl.com/yyrnttya%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://tinyurl.com/yyrnttya，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-daemon-less-镜像构建工具&quot;&gt;1. daemon-less 镜像构建工具&lt;/h2&gt;
&lt;h3 id=&quot;1-1-什么是-daemon-less-镜像构建工具&quot;&gt;1.1  什么是 daemon-less 镜像构建工具&lt;/h3&gt;
&lt;p&gt;在 CICD 流程中，经常会涉及镜像构建，常规的做法是使用 Docker in Docker 或者 Docker out of Docker 进行构建。详情可以参考文档：&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247492294&amp;amp;idx=1&amp;amp;sn=6c702bf5b94f6df07b6560403813b27f&amp;amp;chksm=eac6c3efddb14af9b540b88ae9d46fe97c98fe35bfe135c3091e4de5d30d5184d130f94a6f1d&amp;amp;token=692202010&amp;amp;lang=zh_CN#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;如何在 Docker 中使用 Docker&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;实际上，为了避免垄断，促进行业发展，基于 Docker 的镜像格式，早就指定了统一的 OCI 镜像格式规范。也就是说，只需要通过 Dockerfile 得到一个符合 OCI 规范的镜像即可，并没有强制要求谁来做这件事。&lt;/p&gt;
&lt;p&gt;daemon-less 的构建工具，可以不依赖于 Docker Daemon 进行构建镜像。这在 CICD 场景下，具有重要的意义。同时，Kubernetes 正在摆脱 Docker 的影响，构建更加开放的架构生态，CICD 需要与 Docker Daemon 解耦。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Docker" scheme="https://www.hi-linux.com/categories/docker/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Kaniko" scheme="https://www.hi-linux.com/tags/Kaniko/"/>
    
  </entry>
  
  <entry>
    <title>几种在 Kubernetes 集群中获取客户端真实 IP 的方法</title>
    <link href="https://www.hi-linux.com/posts/6856.html"/>
    <id>https://www.hi-linux.com/posts/6856.html</id>
    <published>2020-12-15T01:00:00.000Z</published>
    <updated>2020-12-15T05:12:47.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>Kubernetes 依靠 kube-proxy 组件实现 Service 的通信与负载均衡。在这个过程中，由于使用了 SNAT 对源地址进行了转换，导致 Pod 中的服务拿不到真实的客户端 IP 地址信息。本篇主要解答了在 Kubernetes 集群中负载如何获取客户端真实 IP 地址这个问题。</p><h2><span id="1-创建一个后端服务">1. 创建一个后端服务</span></h2><h3><span id="11-服务选择">1.1 服务选择</span></h3><p>这里选择 containous/whoami 作为后端服务镜像。在 Dockerhub 的介绍页面，可以看到访问其 80 端口时，会返回客户端的相关信息。在代码中，我们可以在 Http 头部中拿到这些信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Hostname :  6e0030e67d6a</span><br><span class="line">IP :  127.0.0.1</span><br><span class="line">IP :  ::1</span><br><span class="line">IP :  172.17.0.27</span><br><span class="line">IP :  fe80::42:acff:fe11:1b</span><br><span class="line">GET &#x2F; HTTP&#x2F;1.1</span><br><span class="line">Host: 0.0.0.0:32769</span><br><span class="line">User-Agent: curl&#x2F;7.35.0</span><br><span class="line">Accept: *&#x2F;*</span><br></pre></td></tr></table></figure><a id="more"></a><h3><span id="12-集群环境">1.2 集群环境</span></h3><p>集群有三个节点，一个 master ，两个 worker 节点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get node -o wide</span><br><span class="line">NAME     STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">master   Ready    master   91d   v1.17.9   192.168.13.4   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker:&#x2F;&#x2F;19.3.8</span><br><span class="line">node1    Ready    worker   91d   v1.17.9   192.168.13.5   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker:&#x2F;&#x2F;19.3.8</span><br><span class="line">node2    Ready    worker   91d   v1.17.9   192.168.13.6   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker:&#x2F;&#x2F;19.3.8</span><br></pre></td></tr></table></figure><h3><span id="13-创建服务">1.3 创建服务</span></h3><ul><li>创建命名空间</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create ns realip</span><br></pre></td></tr></table></figure><ul><li>创建负载</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n realip run myservice --image&#x3D;containous&#x2F;whoami</span><br></pre></td></tr></table></figure><ul><li>创建服务</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n realip expose deploy myservice --type&#x3D;NodePort --port&#x3D;80</span><br></pre></td></tr></table></figure><ul><li>查看创建的服务</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n realip get pod,deploy,svc  -o wide</span><br><span class="line"></span><br><span class="line">NAME                           READY   STATUS    RESTARTS   AGE    IP             NODE     NOMINATED NODE   READINESS GATES</span><br><span class="line">pod&#x2F;myservice-fc55d766-9ttxt   1&#x2F;1     Running   0          2m1s   10.233.70.42   master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">NAME                        READY   UP-TO-DATE   AVAILABLE   AGE    CONTAINERS   IMAGES              SELECTOR</span><br><span class="line">deployment.apps&#x2F;myservice   1&#x2F;1     1            1           2m1s   myservice    containous&#x2F;whoami   run&#x3D;myservice</span><br><span class="line"></span><br><span class="line">NAME                TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE   SELECTOR</span><br><span class="line">service&#x2F;myservice   NodePort   10.233.13.66   &lt;none&gt;        80:31509&#x2F;TCP   5s    run&#x3D;myservice</span><br></pre></td></tr></table></figure><ul><li>访问服务</li></ul><p>浏览器打开 Master 节点的 EIP + :31509 时，返回如下内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Hostname: myservice-fc55d766-9ttxt</span><br><span class="line">IP: 127.0.0.1</span><br><span class="line">IP: 10.233.70.42</span><br><span class="line">RemoteAddr: 192.168.13.4:21708</span><br><span class="line">GET &#x2F; HTTP&#x2F;1.1</span><br><span class="line">Host: dev.chenshaowen.com:31509</span><br><span class="line">User-Agent: Chrome&#x2F;86.0.4240.198 Safari&#x2F;537.36</span><br><span class="line">Accept: text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,image&#x2F;avif,image&#x2F;webp,image&#x2F;apng,*&#x2F;*;q&#x3D;0.8,application&#x2F;signed-exchange;v&#x3D;b3;q&#x3D;0.9</span><br><span class="line">Accept-Encoding: gzip, deflate</span><br><span class="line">Accept-Language: zh-CN,zh;q&#x3D;0.9,en;q&#x3D;0.8</span><br><span class="line">Cookie: lang&#x3D;zh;</span><br><span class="line">Dnt: 1</span><br><span class="line">Upgrade-Insecure-Requests: 1</span><br></pre></td></tr></table></figure><p>可以看到 RemoteAddr 是 Master 节点的 IP ，并不是访问客户端的真实 IP 地址。</p><h3><span id="14-准备-ingress-controller-和-lb">1.4 准备 Ingress Controller 和 LB</span></h3><p>这里为了更好模拟生产环境的场景，增加了可能的两个链路节点 Ingress 和 Load Balancer(以下简称 LB) 。</p><ul><li>安装 Ingress Controller</li></ul><p>参考文档: <a href="https://www.chenshaowen.com/blog/install-harbor-using-helm.html#1-%E4%BD%BF%E7%94%A8-Helm-%E5%AE%89%E8%A3%85-Ingress" target="_blank" rel="noopener">使用 Helm 安装 Ingress</a></p><ul><li>准备 LB</li></ul><p>这里使用的是云厂商的 LB，如果是物理机，也可以使用针对物理机的 LB。当然，也可以使用 keepalived + haproxy 替代 LB。</p><h2><span id="2-直接通过-nortport-访问获取真实-ip">2. 直接通过 NortPort 访问获取真实 IP</span></h2><p>在上面的访问中，获取不到客户端真实 IP 的原因是 SNAT 使得访问 SVC 的源 IP 发生了变化。将服务的 externalTrafficPolicy 改为 Local 模式可以解决这个问题。</p><p>执行命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n realip patch svc myservice  -p &#39;&#123;&quot;spec&quot;:&#123;&quot;externalTrafficPolicy&quot;:&quot;Local&quot;&#125;&#125;&#39;</span><br></pre></td></tr></table></figure><p>访问服务，可以得到如下内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Hostname: myservice-fc55d766-9ttxt</span><br><span class="line">IP: 127.0.0.1</span><br><span class="line">IP: 10.233.70.42</span><br><span class="line">RemoteAddr: 139.198.254.11:51326</span><br><span class="line">GET &#x2F; HTTP&#x2F;1.1</span><br><span class="line">Host: dev.chenshaowen.com:31509</span><br><span class="line">User-Agent: hrome&#x2F;86.0.4240.198 Safari&#x2F;537.36</span><br><span class="line">Accept: text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,image&#x2F;avif,image&#x2F;webp,image&#x2F;apng,*&#x2F;*;q&#x3D;0.8,application&#x2F;signed-exchange;v&#x3D;b3;q&#x3D;0.9</span><br><span class="line">Accept-Encoding: gzip, deflate</span><br><span class="line">Accept-Language: zh-CN,zh;q&#x3D;0.9,en;q&#x3D;0.8</span><br><span class="line">Cache-Control: max-age&#x3D;0</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Cookie: lang&#x3D;zh;</span><br><span class="line">Dnt: 1</span><br><span class="line">Upgrade-Insecure-Requests: 1</span><br></pre></td></tr></table></figure><p>Cluster 隐藏了客户端源 IP，可能导致第二跳到另一个节点，但具有良好的整体负载分布。 Local 保留客户端源 IP 并避免 LoadBalancer 和 NodePort 类型服务的第二跳，但存在潜在的不均衡流量传播风险。(Kubernetes 官方解释)</p><p>下面是对比简图：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-realip1.png" alt><br><img src="https://www.hi-linux.com/img/linux/k8s-realip2.png" alt></p><p>当请求落到没有服务 Pod 的节点时，将无法访问。用 curl 访问时，会一直停顿在 TCP_NODELAY , 然后提示超时:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">*   Trying 139.198.112.248...</span><br><span class="line">* TCP_NODELAY set</span><br><span class="line">* Connection failed</span><br><span class="line">* connect to 139.198.112.248 port 31509 failed: Operation timed out</span><br><span class="line">* Failed to connect to 139.198.112.248 port 31509: Operation timed out</span><br><span class="line">* Closing connection 0</span><br></pre></td></tr></table></figure><h2><span id="3-通过-lb-gt-service-访问获取真实-ip">3. 通过 LB -&gt; Service 访问获取真实 IP</span></h2><p>在生产环境，通常会有多个节点同时接收客户端的流量，如果仅使用 Local 模式将会导致服务可访问性变低。引入 LB 的目的是为了利用其探活的特点，仅将流量转发到存在服务 Pod 的节点上。</p><p>如下图可以看到，在服务的 31509 端口仅 master 节点处于活跃状态，流量也仅会导向 master 节点，符合预期。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-realip3.png" alt></p><p>接着继续增加副本数量到 3</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n realip scale deploy myservice --replicas&#x3D;3</span><br><span class="line">$ kubectl -n realip get pod  -o wide</span><br><span class="line">NAME                       READY   STATUS    RESTARTS   AGE     IP              NODE     NOMINATED NODE   READINESS GATES</span><br><span class="line">myservice-fc55d766-9ttxt   1&#x2F;1     Running   0          144m    10.233.70.42    master   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">myservice-fc55d766-f8wbs   1&#x2F;1     Running   0          5m13s   10.233.90.143   node1    &lt;none&gt;           &lt;none&gt;</span><br><span class="line">myservice-fc55d766-nzwzn   1&#x2F;1     Running   0          5m13s   10.233.70.45    master   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><p>遗憾的是，Pod 并没有均匀分布在三个节点，其中有两个处于 master 上。因此 LB 的后端节点也没有完全点亮。如下图:</p><p><img src="https://www.hi-linux.com/img/linux/k8s-realip4.png" alt></p><p>这就需要给 deploy 加上反亲和性的描述。执行命令:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n realip edit deploy myservice</span><br></pre></td></tr></table></figure><p>这里有两种选择。第一种是配置软策略，但不能保证全部 LB 后端点亮，均匀分配到流量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: myservice</span><br><span class="line">    spec:</span><br><span class="line">      affinity:</span><br><span class="line">        podAntiAffinity:</span><br><span class="line">          preferredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">            - weight: 100</span><br><span class="line">              podAffinityTerm:</span><br><span class="line">                labelSelector:</span><br><span class="line">                  matchExpressions:</span><br><span class="line">                    - key: app</span><br><span class="line">                      operator: In</span><br><span class="line">                      values:</span><br><span class="line">                        - myservice</span><br><span class="line">                topologyKey: kubernetes.io&#x2F;hostname</span><br></pre></td></tr></table></figure><p>另一种是配置硬策略，强制 Pod 分配在不同的节点上，但会限制副本数量，也就是 Pod 总数不能超过 Node 总数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: myservice</span><br><span class="line">    spec:</span><br><span class="line">      affinity:</span><br><span class="line">        podAntiAffinity:</span><br><span class="line">          requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">            - labelSelector:</span><br><span class="line">                matchExpressions:</span><br><span class="line">                  - key: app</span><br><span class="line">                    operator: In</span><br><span class="line">                    values:</span><br><span class="line">                      - myservice</span><br><span class="line">              topologyKey: kubernetes.io&#x2F;hostname</span><br></pre></td></tr></table></figure><p>采用硬策略的配置，最终点亮全部后端，如下图:</p><p><img src="https://www.hi-linux.com/img/linux/k8s-realip5.png" alt></p><h2><span id="4-通过-lb-gt-ingress-gt-service-访问获取真实-ip">4. 通过 LB -&gt; Ingress -&gt; Service 访问获取真实 IP</span></h2><p>如果每一个服务都占用一个 LB，成本很高，同时配置不够灵活，每次新增服务时，都需要去 LB 增加新的端口映射。</p><p>还有一种方案是 LB 将 80、443 的流量导给 Ingress Controller，然后将流量转发到 Service，接着达到 Pod 中的服务。</p><p>此时，需要 LB 能做 TCP 层的透传，或者 HTTP 层的带真实 IP 转发，将 Ingress Controller 的 externalTrafficPolicy 设置为 Local 模式，而 Service 可以不必设置为 Local 模式。</p><p>如果想要提高可访问性，同样可以参考上面配置反亲和性，保证在每个后端节点上都有 Ingress Controller 。</p><p>访问服务，可以得到如下内容:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Hostname: myservice-7dcf6b965f-vv6md</span><br><span class="line">IP: 127.0.0.1</span><br><span class="line">IP: 10.233.96.152</span><br><span class="line">RemoteAddr: 10.233.70.68:34334</span><br><span class="line">GET &#x2F; HTTP&#x2F;1.1</span><br><span class="line">Host: realip.dev.chenshaowen.com:30000</span><br><span class="line">User-Agent: Chrome&#x2F;87.0.4280.67 Safari&#x2F;537.36</span><br><span class="line">Accept: text&#x2F;html,application&#x2F;xhtml+xml,application&#x2F;xml;q&#x3D;0.9,image&#x2F;avif,image&#x2F;webp,image&#x2F;apng,*&#x2F;*;q&#x3D;0.8,application&#x2F;signed-exchange;v&#x3D;b3;q&#x3D;0.9</span><br><span class="line">Accept-Encoding: gzip, deflate</span><br><span class="line">Accept-Language: zh-CN,zh;q&#x3D;0.9,en;q&#x3D;0.8</span><br><span class="line">Cache-Control: max-age&#x3D;0</span><br><span class="line">Cookie: _ga&#x3D;GA1.2.896113372.1605489938; _gid&#x3D;GA1.2.863456118.1605830768</span><br><span class="line">Cookie: lang&#x3D;zh;</span><br><span class="line">Upgrade-Insecure-Requests: 1</span><br><span class="line">X-Forwarded-For: 139.198.113.75</span><br><span class="line">X-Forwarded-Host: realip.dev.chenshaowen.com:30000</span><br><span class="line">X-Forwarded-Port: 80</span><br><span class="line">X-Forwarded-Proto: http</span><br><span class="line">X-Original-Uri: &#x2F;</span><br><span class="line">X-Real-Ip: 139.198.113.75</span><br><span class="line">X-Request-Id: 999fa36437a1180eda3160a1b9f495a4</span><br><span class="line">X-Scheme: http</span><br></pre></td></tr></table></figure><p>在 X-Forwarded-For 中，已经能够拿到客户端的真实 IP 信息。</p><p>在有个文档中，我看到强调还需在 Ingress configuration 中增加如下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data:</span><br><span class="line">  compute-full-forwarded-for: &#39;true&#39;</span><br><span class="line">  forwarded-for-header: X-Forwarded-For</span><br><span class="line">  use-forwarded-headers: &#39;true&#39;</span><br><span class="line">  # use-proxy-protocol: &quot;true&quot;</span><br></pre></td></tr></table></figure><p>但这里并没有配置，也能拿到 Forward 字段信息，可能和 Ingress Controller 版本有关。如果在返回中没有 X- 头部时，可以尝试。</p><p>这里贴一下 Ingress 的相关配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl  -n realip get ingress realip -o yaml</span><br><span class="line"></span><br><span class="line">apiVersion: extensions&#x2F;v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubesphere.io&#x2F;creator: admin</span><br><span class="line">  generation: 1</span><br><span class="line">  name: realip</span><br><span class="line">  namespace: realip</span><br><span class="line">  resourceVersion: &quot;38923603&quot;</span><br><span class="line">  selfLink: &#x2F;apis&#x2F;extensions&#x2F;v1beta1&#x2F;namespaces&#x2F;realip&#x2F;ingresses&#x2F;realip</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: realip.dev.chenshaowen.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: myservice</span><br><span class="line">          servicePort: 80</span><br><span class="line">        path: &#x2F;</span><br><span class="line">status:</span><br><span class="line">  loadBalancer: &#123;&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get  svc -n realip</span><br><span class="line">router-realip    NodePort    10.233.39.119   &lt;none&gt;        80:30000&#x2F;TCP                 3h18m</span><br></pre></td></tr></table></figure><p>流量的转发路径:</p><p>LB(80/443) -&gt; Ingress Controller(30000) -&gt; myservice(80) -&gt; myservice-fc55d766-xxxx(80)</p><h2><span id="5-总结">5. 总结</span></h2><p>本文介绍了三种获取真实 IP 的部署方式：</p><ul><li>直接通过 NortPort 访问获取真实 IP</li></ul><p>受制于 Local 模式，可能会导致服务不可访问。需要保证对外提供入口的节点上，必须具有服务的负载。</p><ul><li>通过 LB -&gt; Service 访问获取真实 IP</li></ul><p>利用 LB 的探活能力，能够提高服务的可访问性。适用于服务较少，或者愿意每个服务一个 LB 的场景。</p><ul><li>通过 LB -&gt; Ingress -&gt; Service 访问获取真实 IP</li></ul><p>通过 LB 将 80、443 端口的流量转到 Ingress Controller ，再进行服务分发。但 Ingress Controller 使用 Local 模式，就要求 LB 的每个后端节点都有 Ingress Controller 副本。适用于对外暴露服务数量较多的场景。</p><p>当然也可以组合使用，对于并不需要获取客户端真实 IP 的服务，可以继续使用 Cluster 模式。</p><h2><span id="6-参考文档">6. 参考文档</span></h2><ol><li><a href="https://hub.docker.com/r/containous/whoami" target="_blank" rel="noopener">https://hub.docker.com/r/containous/whoami</a></li><li><a href="https://kubernetes.io/zh/docs/tutorials/services/source-ip/" target="_blank" rel="noopener">https://kubernetes.io/zh/docs/tutorials/services/source-ip/</a></li></ol><blockquote><p>本文转载自：「陈少文的博客」，原文：<a href="https://tinyurl.com/y62ysxro%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://tinyurl.com/y62ysxro，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 依靠 kube-proxy 组件实现 Service 的通信与负载均衡。在这个过程中，由于使用了 SNAT 对源地址进行了转换，导致 Pod 中的服务拿不到真实的客户端 IP 地址信息。本篇主要解答了在 Kubernetes 集群中负载如何获取客户端真实 IP 地址这个问题。&lt;/p&gt;
&lt;h2 id=&quot;1-创建一个后端服务&quot;&gt;1. 创建一个后端服务&lt;/h2&gt;
&lt;h3 id=&quot;1-1-服务选择&quot;&gt;1.1 服务选择&lt;/h3&gt;
&lt;p&gt;这里选择 containous/whoami 作为后端服务镜像。在 Dockerhub 的介绍页面，可以看到访问其 80 端口时，会返回客户端的相关信息。在代码中，我们可以在 Http 头部中拿到这些信息。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Hostname :  6e0030e67d6a&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;IP :  127.0.0.1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;IP :  ::1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;IP :  172.17.0.27&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;IP :  fe80::42:acff:fe11:1b&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;GET &amp;#x2F; HTTP&amp;#x2F;1.1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Host: 0.0.0.0:32769&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;User-Agent: curl&amp;#x2F;7.35.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;Accept: *&amp;#x2F;*&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>CentOS 8 明年正式停止维护，以后再也不会有免费的 RHEL 了</title>
    <link href="https://www.hi-linux.com/posts/58263.html"/>
    <id>https://www.hi-linux.com/posts/58263.html</id>
    <published>2020-12-10T01:00:00.000Z</published>
    <updated>2020-12-21T05:17:44.398Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>CentOS 是 Community Enterprise Operating System（社区企业操作系统）的首字母缩写，是 100％ 重建的 RHEL（红帽企业 Linux）。尽管 RHEL 需要花钱，但 CentOS 是免费的，社区支持的企业 Linux 发行版。擅长 Linux 且不想支付 RHEL 支持费的开发人员和公司总是选择 CentOS 来节省资金并获得企业级软件。</p><p>但是，现在免费乘车已经结束了！红帽宣布，作为 RHEL 8 的重建版本，CentOS Linux 8 将在 2021 年结束。</p><a id="more"></a><h2><span id="centos-项目历史">CentOS 项目历史</span></h2><p>CentOS 项目开始于 2004 年 5 月，称为 CentOS 2，它是从 RHEL 2.1AS（高级服务器）派生而来的。在 Linux 爱好者，网络托管公司，开发人员和 HPC 社区中，它立即受到热烈欢迎。 CentOS 免费提供的企业级软件具有自助功能，而社区支持则由电子邮件列表或在线论坛驱动。当您不再需要支持或培训合同时，这是节省金钱的好方法。</p><h2><span id="什么是-centos-stream">什么是 CentOS Stream</span></h2><p>CentOS 项目以后将重点转移到 CentOS Stream，Centos Stream 是一个滚动发布的 Linux 发行版，它介于 Fedora Linux 的上游开发和 RHEL 的下游开发之间而存在。你可以把 CentOS Streams 当成是用来体验最新红帽系 Linux 特性的一个版本。换句话说，CentOS Stream 是 RHEL 的滚动发行版。它充当 Fedora 和 CentOS 之间的网关。</p><p>明年，我们将把重点从重建 RHEL 的 CentOS Linux 转移到 CentOS Stream。 CentOS Stream 将作为 Red Hat Enterprise Linux 的上游（开发）分支。也就是说，以后，Fedora 依然是第一个上游，但是在 RHEL 发布新版本之后，CentOS Stream 会在它的基础上滚动更新，并将成熟的更新反哺到 RHEL 当中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Fedora Linux ➡️ CentOS Stream ➡️ RHEL</span><br></pre></td></tr></table></figure><p>当然，在 CentOS Linux 8 结束时，你可以考虑迁移到 CentOS Stream 8，它会像传统的 CentOS Linux 版本一样定期更新。但是，切记，这是一个作为 RHEL 中游的滚动发行版，并不太建议你在生产环境中使用。</p><h2><span id="对-centos-7-暂无影响">对 CentOS 7 暂无影响</span></h2><p>CentoS 7 将继续在 RHEL 7 生命周期内 (2024 年底)继续得到支持。因此对 CentOS 7 用户暂时没有影响。</p><h2><span id="结论">结论</span></h2><p>CentOS 的主要优点是与 RHEL 提供 100% 的二进制兼容性。红帽这次的变更肯定会对 CentOS 用户造成影响。</p><p>对此，你会不会选择使用其它发行版呢？欢迎大家留言区积极发表自己的看法！</p><h2><span id="参考文档">参考文档</span></h2><ol><li><a href="https://www.google.com" target="_blank" rel="noopener">https://www.google.com</a></li><li><a href="https://www.cyberciti.biz/linux-news/centos-linux-8-will-end-in-2021-and-shifts-focus-to-centos-stream/" target="_blank" rel="noopener">https://www.cyberciti.biz/linux-news/centos-linux-8-will-end-in-2021-and-shifts-focus-to-centos-stream/</a></li></ol></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;CentOS 是 Community Enterprise Operating System（社区企业操作系统）的首字母缩写，是 100％ 重建的 RHEL（红帽企业 Linux）。尽管 RHEL 需要花钱，但 CentOS 是免费的，社区支持的企业 Linux 发行版。擅长 Linux 且不想支付 RHEL 支持费的开发人员和公司总是选择 CentOS 来节省资金并获得企业级软件。&lt;/p&gt;
&lt;p&gt;但是，现在免费乘车已经结束了！红帽宣布，作为 RHEL 8 的重建版本，CentOS Linux 8 将在 2021 年结束。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/categories/Linux/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="CentOS" scheme="https://www.hi-linux.com/tags/CentOS/"/>
    
  </entry>
  
  <entry>
    <title>推荐一款实时查看 Docker 容器日志的神器 Dozzie</title>
    <link href="https://www.hi-linux.com/posts/40902.html"/>
    <id>https://www.hi-linux.com/posts/40902.html</id>
    <published>2020-12-02T01:00:00.000Z</published>
    <updated>2020-12-02T05:05:10.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>介绍一款使用了几个月的开源小工具，Dozzle。它是一款轻量、简单的容器日志查看工具。</p><p>项目地址：<a href="https://dozzle.dev/" target="_blank" rel="noopener">https://dozzle.dev/</a></p><p>本篇将简单介绍如何使用它，包括如何快速从源码构建它。</p><h2><span id="写在前面">写在前面</span></h2><p>这款工具相比较一些重量级的工具，比如 ELK 系列而言，实在是太轻量了，容器版本不过 10MB 左右大小。</p><p>主要原因是，它不存储和处理日志，仅仅提供实时查看功能，类似我们日常使用 ps、top 一样，使用它可以减少我们在服务器上低效执行 docker logs 或者 docker-compose logs 等类似命令的执行。</p><p><img src="https://www.hi-linux.com/img/linux/dozzle1.png" alt></p><p>先来聊聊一般情况下，怎么用这个小工具。</p><a id="more"></a><h2><span id="一般场景">一般场景</span></h2><p>一般场景下，启动它，提供一个我们可以访问的端口，和其他多数优秀的开源项目一样简单，只需要一条简单的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name dozzle -d --volume&#x3D;&#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock:ro -p 8888:8080 amir20&#x2F;dozzle:latest</span><br></pre></td></tr></table></figure><p>这里和官方文档不同的是，在挂载 docker.sock 的时候，因为我们并不需要去操作该文件，所以建议加上 ro 只读的限制，避免出现一些我们不希望出现的意外情况，尤其是使用 latest 版本的时候。当然，实际使用中还是建议锁定镜像版本，并认真审查项目相关源码。</p><p>当然，考虑到可维护性，我们可以把它写成 compose 配置文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">version: &quot;3&quot;</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  dozzle:</span><br><span class="line">    container_name: dozzle</span><br><span class="line">    image: amir20&#x2F;dozzle:latest</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock:ro</span><br><span class="line">    ports:</span><br><span class="line">      - 8888:8080</span><br></pre></td></tr></table></figure><p>将上面的内容保存为 docker-compose.yml ，使用 docker-compose up 启动程序；或者前文提到的 docker run 命令之后，我们便可以通过浏览器访问 localhost:8888 来查看当前相同机器（容器网卡）内的容器的实时日志，以及已停止容器的历史运行日志了。</p><p>因为容器的盛行，许多时候我们需要同时查阅多个服务的日志状态，Dozzle 支持“分屏”查看，还是很方便的。</p><p><img src="https://www.hi-linux.com/img/linux/dozzle2.png" alt></p><p>当然，还有常用的关键词搜索过滤、文本高亮。</p><p><img src="https://www.hi-linux.com/img/linux/dozzle3.png" alt></p><h2><span id="如何从源码构建应用">如何从源码构建应用</span></h2><p>实际使用过程中，我们可能需要定制代码，来实现一些特别的需求，作者很贴心的提供了多阶段构建脚本，不过估计不少同样身处国内的同学会因为网络客观原因无法顺利构建，这里记录一下如何简单快速的完成应用构建。</p><p>首先是获取源代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;amir20&#x2F;dozzle.git --depth&#x3D;1 --branch&#x3D;master</span><br></pre></td></tr></table></figure><p>使用 --depth 和 --branch 可以有效减少获取代码时的传输量，从而提高你的代码下载速度。</p><p>接着是针对原始的 Dockerfile 进行调整，让 Alpine 、Node 、Go 的相关依赖、软件获取。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">diff --git a&#x2F;Dockerfile b&#x2F;Dockerfile</span><br><span class="line">index 9bce54e..eda3672 100644</span><br><span class="line">--- a&#x2F;Dockerfile</span><br><span class="line">+++ b&#x2F;Dockerfile</span><br><span class="line">@@ -1,5 +1,15 @@</span><br><span class="line"> # Build assets</span><br><span class="line">-FROM node:current-alpine as node</span><br><span class="line">+FROM node:14.15.0-alpine as node</span><br><span class="line">+</span><br><span class="line">+ENV LANG en_US.UTF-8</span><br><span class="line">+ENV LANGUAGE en_US.UTF-8</span><br><span class="line">+ENV LC_ALL&#x3D;en_US.UTF-8</span><br><span class="line">+</span><br><span class="line">+RUN echo &#39;&#39; &gt; &#x2F;etc&#x2F;apk&#x2F;repositories &amp;&amp; \</span><br><span class="line">+    VER&#x3D;$(awk -F&#x3D; &#39;$1&#x3D;&#x3D;&quot;VERSION_ID&quot; &#123; print $2 ;&#125;&#39; &#x2F;etc&#x2F;os-release | cut -d . -f 1,2) &amp;&amp; \</span><br><span class="line">+    echo &quot;https:&#x2F;&#x2F;mirror.tuna.tsinghua.edu.cn&#x2F;alpine&#x2F;v$&#123;VER&#125;&#x2F;main&quot;         &gt;&gt; &#x2F;etc&#x2F;apk&#x2F;repositories &amp;&amp; \</span><br><span class="line">+    echo &quot;https:&#x2F;&#x2F;mirror.tuna.tsinghua.edu.cn&#x2F;alpine&#x2F;v$&#123;VER&#125;&#x2F;community&quot;    &gt;&gt; &#x2F;etc&#x2F;apk&#x2F;repositories &amp;&amp; \</span><br><span class="line">+    echo &quot;Asia&#x2F;Shanghai&quot; &gt; &#x2F;etc&#x2F;timezone</span><br><span class="line"> </span><br><span class="line"> RUN apk add --no-cache git openssh python make g++ util-linux</span><br><span class="line"> </span><br><span class="line">@@ -7,6 +17,7 @@ WORKDIR &#x2F;build</span><br><span class="line"> </span><br><span class="line"> # Install dependencies</span><br><span class="line"> COPY package*.json yarn.lock .&#x2F;</span><br><span class="line">+RUN yarn config set registry https:&#x2F;&#x2F;registry.npm.taobao.org&#x2F;</span><br><span class="line"> RUN yarn install --network-timeout 1000000</span><br><span class="line"> </span><br><span class="line"> # Copy config files</span><br><span class="line">@@ -20,6 +31,19 @@ RUN yarn build</span><br><span class="line"> </span><br><span class="line"> FROM golang:1.15-alpine AS builder</span><br><span class="line"> </span><br><span class="line">+ENV LANG en_US.UTF-8</span><br><span class="line">+ENV LANGUAGE en_US.UTF-8</span><br><span class="line">+ENV LC_ALL&#x3D;en_US.UTF-8</span><br><span class="line">+</span><br><span class="line">+RUN echo &#39;&#39; &gt; &#x2F;etc&#x2F;apk&#x2F;repositories &amp;&amp; \</span><br><span class="line">+    VER&#x3D;$(awk -F&#x3D; &#39;$1&#x3D;&#x3D;&quot;VERSION_ID&quot; &#123; print $2 ;&#125;&#39; &#x2F;etc&#x2F;os-release | cut -d . -f 1,2) &amp;&amp; \</span><br><span class="line">+    echo &quot;https:&#x2F;&#x2F;mirror.tuna.tsinghua.edu.cn&#x2F;alpine&#x2F;v$&#123;VER&#125;&#x2F;main&quot;         &gt;&gt; &#x2F;etc&#x2F;apk&#x2F;repositories &amp;&amp; \</span><br><span class="line">+    echo &quot;https:&#x2F;&#x2F;mirror.tuna.tsinghua.edu.cn&#x2F;alpine&#x2F;v$&#123;VER&#125;&#x2F;community&quot;    &gt;&gt; &#x2F;etc&#x2F;apk&#x2F;repositories &amp;&amp; \</span><br><span class="line">+    echo &quot;Asia&#x2F;Shanghai&quot; &gt; &#x2F;etc&#x2F;timezone</span><br><span class="line">+</span><br><span class="line">+RUN go env -w GO111MODULE&#x3D;on  &amp;&amp; \</span><br><span class="line">+    go env -w GOPROXY&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;goproxy&#x2F;,direct</span><br><span class="line">+</span><br><span class="line"> RUN apk add --no-cache git ca-certificates</span><br><span class="line"> RUN mkdir &#x2F;dozzle</span><br></pre></td></tr></table></figure><p>上面是我一般会做的镜像“加速”构建调整，执行下面的构建命令，稍等片刻，便能获得属于你的 Dozzle。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t dozzle:custom --build-arg TAG&#x3D;custom .</span><br></pre></td></tr></table></figure><p><img src="https://www.hi-linux.com/img/linux/dozzle4.png" alt></p><h2><span id="最后">最后</span></h2><p>希望本篇内容能解救不停登录服务器查看日志，或者耐着性子等云平台日志中心日志刷新的你。</p><blockquote><p>本文转载自：「苏洋博客」，原文：<a href="https://tinyurl.com/y5bsr5zq%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://tinyurl.com/y5bsr5zq，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;介绍一款使用了几个月的开源小工具，Dozzle。它是一款轻量、简单的容器日志查看工具。&lt;/p&gt;
&lt;p&gt;项目地址：&lt;a href=&quot;https://dozzle.dev/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://dozzle.dev/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本篇将简单介绍如何使用它，包括如何快速从源码构建它。&lt;/p&gt;
&lt;h2 id=&quot;写在前面&quot;&gt;写在前面&lt;/h2&gt;
&lt;p&gt;这款工具相比较一些重量级的工具，比如 ELK 系列而言，实在是太轻量了，容器版本不过 10MB 左右大小。&lt;/p&gt;
&lt;p&gt;主要原因是，它不存储和处理日志，仅仅提供实时查看功能，类似我们日常使用 ps、top 一样，使用它可以减少我们在服务器上低效执行 docker logs 或者 docker-compose logs 等类似命令的执行。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/dozzle1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;先来聊聊一般情况下，怎么用这个小工具。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Docker" scheme="https://www.hi-linux.com/categories/docker/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Dozzie" scheme="https://www.hi-linux.com/tags/Dozzie/"/>
    
  </entry>
  
  <entry>
    <title>如何在 Docker 中使用 Docker</title>
    <link href="https://www.hi-linux.com/posts/16745.html"/>
    <id>https://www.hi-linux.com/posts/16745.html</id>
    <published>2020-11-30T01:00:00.000Z</published>
    <updated>2020-11-30T07:19:06.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><h2><span id="1-典型适用场景">1. 典型适用场景</span></h2><p>在 CI 中，通常会有一个 CI Engine 负责解析流程，控制整个构建过程，而将真正的构建交给 Agent 去完成。例如，Jenkins 、GitLab 均是如此。</p><p>如下图, 连接 CI Engine 的 Agent, 种类很多。这是为了满足不同项目对构建环境的要求。</p><p><img src="https://www.hi-linux.com/img/linux/dind01.png" alt></p><p>同时 Agent 是动态的，构建时才需要，构建完成时即销毁。CI 非常适合实践容器、Serverless 等技术，因此在生产过程中 Agent 经常是容器化的。</p><p>那么问题就来了？如果 CI Engine 也是容器化的，在容器中如何使用 Agent 容器去构建呢？如果 Agent 已经是容器化的，那么在 Agent 上如何构建镜像呢？这就是本篇将给出的回答，如何在 Docker 中使用 Docker。</p><a id="more"></a><h2><span id="2-两种使用模式">2. 两种使用模式</span></h2><p>我们需要知道 Docker 以 C/S 模式工作，主要分为两个部分，Docker CLI 和 Docker Daemon 。Docker CLI ，也就是客户端，提供给用户命令行操作 Docker，例如 docker create/images/ps 等。Docker Damon ，也就是守护进程，负责接受用户指令，维护容器的生命周期。</p><h3><span id="21-docker-in-docker">2.1 Docker in Docker</span></h3><p>Docker in Docker ，以下简称 DinD 。</p><p><img src="https://www.hi-linux.com/img/linux/dind02.png" alt></p><p>如上图，可以在 Container 中直接运行一个 Docker Daemon ，然后使用 Container 中的 Docker CLI 工具操作容器。</p><p>这种方式下，容器中的 Docker Daemon 完全独立于外部，具有良好的隔离特性。看起来，Container 类似一个 VM ，但 DinD 的作者自己也不是很推荐。</p><p>主要原因还是安全问题。DinD 需要以特权模式启动，这种嵌套会带来潜在的安全风险。</p><p>这种方式下，响应命令的容器嵌套于使用 docker 命令的容器。</p><h3><span id="22-docker-outside-of-docker">2.2 Docker outside of Docker</span></h3><p>Docker outside of Docker ，以下简称 DooD 。</p><p><img src="https://www.hi-linux.com/img/linux/dind03.png" alt></p><p>如上图，Docker 以 C/S 模式工作，使用时用户关注的是 C 端，而生命周期的管理在 S 端。</p><p>因此，只需要将 Container 的外部 Docker Daemon 服务挂载到 Container 。让 Container 误以为本地运行了 Docker Daemon，使用 Docker CLI 命令操作时，外部的 Docker Daemon 会响应请求。</p><p>这种方式下，响应命令的容器与使用 docker 命令的容器处于同一层级。</p><h2><span id="3-docker-环境下的演示">3. Docker 环境下的演示</span></h2><h3><span id="31-dind">3.1 DinD</span></h3><ul><li>运行 DinD 容器</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --privileged -e DOCKER_TLS_CERTDIR&#x3D;&quot;&quot; -d --name dockerd  docker:dind</span><br><span class="line">d6414f2ff0076c42de19a8a1fe122481c1a72b3bd45fd490dbe1c427414b4139</span><br></pre></td></tr></table></figure><ul><li>运行带 CLI 的容器链接 DinD 容器</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm -it --link dockerd:docker docker:latest sh</span><br></pre></td></tr></table></figure><ul><li>在 DinD 容器中，拉取镜像</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 拉取镜像</span><br><span class="line">$ docker pull shaowenchen&#x2F;devops-java-sample</span><br><span class="line"></span><br><span class="line"># 查看镜像</span><br><span class="line">$ docker images</span><br><span class="line">REPOSITORY                       TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">shaowenchen&#x2F;devops-java-sample   latest              fa4651c24a18        6 weeks ago         122MB</span><br></pre></td></tr></table></figure><p>使用起来和一个独立的 Docker Daemon 环境一样。</p><ul><li>查看外部是否受影响</li></ul><p>键入 exit 退出容器，通过主机上的 Docker Daemon</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker images |grep fa4651c24a18</span><br></pre></td></tr></table></figure><p>符合预期。DinD 使用的是独立的 Docker Daemon，对外部的实例没有直接影响。</p><h3><span id="32-dood">3.2 DooD</span></h3><ul><li>运行一个容器</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm -it -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock alpine sh</span><br></pre></td></tr></table></figure><ul><li>安装 curl</li></ul><p>这里为了避免安装 Docker CLI ，直接使用 curl 调用 Docker Daemon 的 API。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ apk update &amp;&amp; apk add curl</span><br></pre></td></tr></table></figure><ul><li>拉取镜像</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ curl -XPOST --unix-socket &#x2F;var&#x2F;run&#x2F;docker.sock http:&#x2F;&#x2F;localhost&#x2F;images&#x2F;create?fromImage&#x3D;shaowenchen&#x2F;docker-robotframework&amp;tag&#x3D;latest</span><br><span class="line">...</span><br><span class="line">&#123;&quot;status&quot;:&quot;Status: Downloaded newer image for shaowenchen&#x2F;docker-robotframework&quot;&#125;</span><br></pre></td></tr></table></figure><ul><li>查看拉取的镜像</li></ul><p>键入 exit 退出容器，通过主机上的 Docker Daemon</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker images |grep robotframework</span><br><span class="line">shaowenchen&#x2F;docker-robotframework                              latest                         d99cfa7ee716        12 months ago       1.5GB</span><br></pre></td></tr></table></figure><p>符合预期。DooD 方式直接使用的外部 Docker Daemon。</p><h2><span id="4-kubernetes-环境下的演示">4. Kubernetes 环境下的演示</span></h2><h3><span id="41-dind">4.1 DinD</span></h3><ul><li>创建一个 dind.yaml 文件，内容如下:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: dind</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: dind</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: dind</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: dockerd</span><br><span class="line">          image: &#39;docker:dind&#39;</span><br><span class="line">          env:</span><br><span class="line">            - name: DOCKER_TLS_CERTDIR</span><br><span class="line">              value: &quot;&quot;</span><br><span class="line">          securityContext:</span><br><span class="line">            privileged: true</span><br><span class="line">        - name: docker-cli</span><br><span class="line">          image: &#39;docker:latest&#39;</span><br><span class="line">          env:</span><br><span class="line">          - name: DOCKER_HOST</span><br><span class="line">            value: 127.0.0.1</span><br><span class="line">          command: [&quot;&#x2F;bin&#x2F;sh&quot;]</span><br><span class="line">          args: [&quot;-c&quot;, &quot;sleep 86400;&quot;]</span><br></pre></td></tr></table></figure><ul><li>创建 Deployment</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f dind.yaml</span><br></pre></td></tr></table></figure><ul><li>查看创建的 Pod 名</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod |grep dind</span><br><span class="line">dind-5446ffbc8d-68q28   2&#x2F;2     Running       0          12s</span><br></pre></td></tr></table></figure><ul><li>进入 Pod</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl exec -it dind-5446ffbc8d-68q28  -c docker-cli sh</span><br></pre></td></tr></table></figure><ul><li>测试是否使用独立的 Docker Daemon</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker pull nginx</span><br><span class="line">$ docker images</span><br><span class="line">REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">nginx               latest              daee903b4e43        3 days ago          133MB</span><br></pre></td></tr></table></figure><p>符合预期，这里仅显示了刚拉取的 Nginx 的镜像，完全独立于主机的 Docker Daemon。</p><h3><span id="42-dood">4.2 DooD</span></h3><ul><li>创建一个 dood.yaml 文件，内容如下:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: dood</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: dood</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: dood</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - image: docker:latest</span><br><span class="line">          name: docker-cli</span><br><span class="line">          securityContext:</span><br><span class="line">            privileged: false</span><br><span class="line">          command: [&quot;&#x2F;bin&#x2F;sh&quot;]</span><br><span class="line">          args: [&quot;-c&quot;, &quot;sleep 86400;&quot;]</span><br><span class="line">          volumeMounts:</span><br><span class="line">          - mountPath: &#x2F;var&#x2F;run&#x2F;docker.sock</span><br><span class="line">            name: volume-docker</span><br><span class="line">      volumes:</span><br><span class="line">        - hostPath:</span><br><span class="line">            path: &#x2F;var&#x2F;run&#x2F;docker.sock</span><br><span class="line">            type: &quot;&quot;</span><br><span class="line">          name: volume-docker</span><br></pre></td></tr></table></figure><ul><li>创建 Deployment</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f dood.yaml</span><br></pre></td></tr></table></figure><ul><li>查看创建的 Pod 名</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod  |grep dood</span><br><span class="line">dood-667d8bcfc6-d5fzf   1&#x2F;1     Running   0          15s</span><br></pre></td></tr></table></figure><ul><li>进入 Pod</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl exec -it dood-667d8bcfc6-d5fzf  -c docker-cli sh</span><br></pre></td></tr></table></figure><ul><li>测试是否使用的是主机的 Docker Daemon</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker images |wc</span><br><span class="line">69       482      8509</span><br></pre></td></tr></table></figure><p>符合预期，这里 Docker 命令使用的就是外部的 Docker Daemon。</p><h2><span id="5-参考">5. 参考</span></h2><ol><li><a href="https://medium.com/better-programming/about-var-run-docker-sock-3bfd276e12fd" target="_blank" rel="noopener">https://medium.com/better-programming/about-var-run-docker-sock-3bfd276e12fd</a></li><li><a href="https://github.com/jpetazzo/dind" target="_blank" rel="noopener">https://github.com/jpetazzo/dind</a></li></ol><blockquote><p>本文转载自：「陈少文的博客」，原文：<a href="https://tinyurl.com/y2sbcrau%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://tinyurl.com/y2sbcrau，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-典型适用场景&quot;&gt;1. 典型适用场景&lt;/h2&gt;
&lt;p&gt;在 CI 中，通常会有一个 CI Engine 负责解析流程，控制整个构建过程，而将真正的构建交给 Agent 去完成。例如，Jenkins 、GitLab 均是如此。&lt;/p&gt;
&lt;p&gt;如下图, 连接 CI Engine 的 Agent, 种类很多。这是为了满足不同项目对构建环境的要求。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/dind01.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;同时 Agent 是动态的，构建时才需要，构建完成时即销毁。CI 非常适合实践容器、Serverless 等技术，因此在生产过程中 Agent 经常是容器化的。&lt;/p&gt;
&lt;p&gt;那么问题就来了？如果 CI Engine 也是容器化的，在容器中如何使用 Agent 容器去构建呢？如果 Agent 已经是容器化的，那么在 Agent 上如何构建镜像呢？这就是本篇将给出的回答，如何在 Docker 中使用 Docker。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Docker" scheme="https://www.hi-linux.com/categories/docker/"/>
    
    
      <category term="技巧" scheme="https://www.hi-linux.com/tags/%E6%8A%80%E5%B7%A7/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>一文搞懂 Redis 的三种集群方案</title>
    <link href="https://www.hi-linux.com/posts/34936.html"/>
    <id>https://www.hi-linux.com/posts/34936.html</id>
    <published>2020-11-23T01:00:00.000Z</published>
    <updated>2020-11-23T02:21:11.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>在开发测试环境中，我们一般搭建Redis的单实例来应对开发测试需求，但是在生产环境，如果对可用性、可靠性要求较高，则需要引入Redis的集群方案。虽然现在各大云平台有提供缓存服务可以直接使用，但了解一下其背后的实现与原理总还是有些必要（比如面试）， 本文就一起来学习一下Redis的几种集群方案。</p><p>Redis支持三种集群方案</p><ul><li>主从复制模式</li><li>Sentinel（哨兵）模式</li><li>Cluster模式</li></ul><h2><span id="主从复制模式">主从复制模式</span></h2><h3><span id="1-基本原理">1. 基本原理</span></h3><p>主从复制模式中包含一个主数据库实例（master）与一个或多个从数据库实例（slave），如下图</p><p><img src="https://www.hi-linux.com/img/linux/redis-cluster01.png" alt></p><p>客户端可对主数据库进行读写操作，对从数据库进行读操作，主数据库写入的数据会实时自动同步给从数据库。</p><p>具体工作机制为：</p><ol><li>slave启动后，向master发送SYNC命令，master接收到SYNC命令后通过bgsave保存快照（即上文所介绍的RDB持久化），并使用缓冲区记录保存快照这段时间内执行的写命令</li><li>master将保存的快照文件发送给slave，并继续记录执行的写命令</li><li>slave接收到快照文件后，加载快照文件，载入数据</li><li>master快照发送完后开始向slave发送缓冲区的写命令，slave接收命令并执行，完成复制初始化</li><li>此后master每次执行一个写命令都会同步发送给slave，保持master与slave之间数据的一致性</li></ol><a id="more"></a><h3><span id="2-部署示例">2. 部署示例</span></h3><p>本示例基于Redis 5.0.3版。</p><p>redis.conf的主要配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">###网络相关###</span><br><span class="line"># bind 127.0.0.1 # 绑定监听的网卡IP，注释掉或配置成0.0.0.0可使任意IP均可访问</span><br><span class="line">protected-mode no # 关闭保护模式，使用密码访问</span><br><span class="line">port 6379  # 设置监听端口，建议生产环境均使用自定义端口</span><br><span class="line">timeout 30 # 客户端连接空闲多久后断开连接，单位秒，0表示禁用</span><br><span class="line"></span><br><span class="line">###通用配置###</span><br><span class="line">daemonize yes # 在后台运行</span><br><span class="line">pidfile &#x2F;var&#x2F;run&#x2F;redis_6379.pid  # pid进程文件名</span><br><span class="line">logfile &#x2F;usr&#x2F;local&#x2F;redis&#x2F;logs&#x2F;redis.log # 日志文件的位置</span><br><span class="line"></span><br><span class="line">###RDB持久化配置###</span><br><span class="line">save 900 1 # 900s内至少一次写操作则执行bgsave进行RDB持久化</span><br><span class="line">save 300 10</span><br><span class="line">save 60 10000 </span><br><span class="line"># 如果禁用RDB持久化，可在这里添加 save &quot;&quot;</span><br><span class="line">rdbcompression yes #是否对RDB文件进行压缩，建议设置为no，以（磁盘）空间换（CPU）时间</span><br><span class="line">dbfilename dump.rdb # RDB文件名称</span><br><span class="line">dir &#x2F;usr&#x2F;local&#x2F;redis&#x2F;datas # RDB文件保存路径，AOF文件也保存在这里</span><br><span class="line"></span><br><span class="line">###AOF配置###</span><br><span class="line">appendonly yes # 默认值是no，表示不使用AOF增量持久化的方式，使用RDB全量持久化的方式</span><br><span class="line">appendfsync everysec # 可选值 always， everysec，no，建议设置为everysec</span><br><span class="line"></span><br><span class="line">###设置密码###</span><br><span class="line">requirepass 123456 # 设置复杂一点的密码</span><br></pre></td></tr></table></figure><p>部署主从复制模式只需稍微调整slave的配置，在redis.conf中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">replicaof 127.0.0.1 6379 # master的ip，port</span><br><span class="line">masterauth 123456 # master的密码</span><br><span class="line">replica-serve-stale-data no # 如果slave无法与master同步，设置成slave不可读，方便监控脚本发现问题</span><br></pre></td></tr></table></figure><p>本示例在单台服务器上配置master端口6379，两个slave端口分别为7001,7002，启动master，再启动两个slave</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-server-1 master-slave]# redis-server master.conf</span><br><span class="line">[root@dev-server-1 master-slave]# redis-server slave1.conf</span><br><span class="line">[root@dev-server-1 master-slave]# redis-server slave2.conf</span><br></pre></td></tr></table></figure><p>进入master数据库，写入一个数据，再进入一个slave数据库，立即便可访问刚才写入master数据库的数据。如下所示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-server-1 master-slave]# redis-cli </span><br><span class="line">127.0.0.1:6379&gt; auth 123456</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; set site blog.jboost.cn</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; get site</span><br><span class="line">&quot;blog.jboost.cn&quot;</span><br><span class="line">127.0.0.1:6379&gt; info replication</span><br><span class="line"># Replication</span><br><span class="line">role:master</span><br><span class="line">connected_slaves:2</span><br><span class="line">slave0:ip&#x3D;127.0.0.1,port&#x3D;7001,state&#x3D;online,offset&#x3D;13364738,lag&#x3D;1</span><br><span class="line">slave1:ip&#x3D;127.0.0.1,port&#x3D;7002,state&#x3D;online,offset&#x3D;13364738,lag&#x3D;0</span><br><span class="line">...</span><br><span class="line">127.0.0.1:6379&gt; exit</span><br><span class="line"></span><br><span class="line">[root@dev-server-1 master-slave]# redis-cli -p 7001</span><br><span class="line">127.0.0.1:7001&gt; auth 123456</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:7001&gt; get site</span><br><span class="line">&quot;blog.jboost.cn&quot;</span><br></pre></td></tr></table></figure><p>执行info replication命令可以查看连接该数据库的其它库的信息，如上可看到有两个slave连接到master</p><h3><span id="3-主从复制的优缺点">3. 主从复制的优缺点</span></h3><p>优点：</p><ol><li>master能自动将数据同步到slave，可以进行读写分离，分担master的读压力</li><li>master、slave之间的同步是以非阻塞的方式进行的，同步期间，客户端仍然可以提交查询或更新请求</li></ol><p>缺点：</p><ol><li>不具备自动容错与恢复功能，master或slave的宕机都可能导致客户端请求失败，需要等待机器重启或手动切换客户端IP才能恢复</li><li>master宕机，如果宕机前数据没有同步完，则切换IP后会存在数据不一致的问题</li><li>难以支持在线扩容，Redis的容量受限于单机配置</li></ol><h2><span id="sentinel哨兵模式">Sentinel（哨兵）模式</span></h2><h3><span id="1-基本原理">1. 基本原理</span></h3><p>哨兵模式基于主从复制模式，只是引入了哨兵来监控与自动处理故障。如图</p><p><img src="https://www.hi-linux.com/img/linux/redis-cluster02.png" alt></p><p>哨兵顾名思义，就是来为Redis集群站哨的，一旦发现问题能做出相应的应对处理。其功能包括</p><ol><li>监控master、slave是否正常运行</li><li>当master出现故障时，能自动将一个slave转换为master（大哥挂了，选一个小弟上位）</li><li>多个哨兵可以监控同一个Redis，哨兵之间也会自动监控</li></ol><p>哨兵模式的具体工作机制：</p><p>在配置文件中通过 sentinel monitor <master-name> <ip> <redis-port> <quorum> 来定位master的IP、端口，一个哨兵可以监控多个master数据库，只需要提供多个该配置项即可。哨兵启动后，会与要监控的master建立两条连接：</quorum></redis-port></ip></master-name></p><ol><li>一条连接用来订阅master的_sentinel_:hello频道与获取其他监控该master的哨兵节点信息</li><li>另一条连接定期向master发送INFO等命令获取master本身的信息</li></ol><p>与master建立连接后，哨兵会执行三个操作：</p><ol><li>定期（一般10s一次，当master被标记为主观下线时，改为1s一次）向master和slave发送INFO命令</li><li>定期向master和slave的_sentinel_:hello频道发送自己的信息</li><li>定期（1s一次）向master、slave和其他哨兵发送PING命令</li></ol><p>发送INFO命令可以获取当前数据库的相关信息从而实现新节点的自动发现。所以说哨兵只需要配置master数据库信息就可以自动发现其slave信息。获取到slave信息后，哨兵也会与slave建立两条连接执行监控。通过INFO命令，哨兵可以获取主从数据库的最新信息，并进行相应的操作，比如角色变更等。</p><p>接下来哨兵向主从数据库的sentinel:hello频道发送信息与同样监控这些数据库的哨兵共享自己的信息，发送内容为哨兵的ip端口、运行id、配置版本、master名字、master的ip端口还有master的配置版本。这些信息有以下用处：</p><ol><li>其他哨兵可以通过该信息判断发送者是否是新发现的哨兵，如果是的话会创建一个到该哨兵的连接用于发送PING命令。</li><li>其他哨兵通过该信息可以判断master的版本，如果该版本高于直接记录的版本，将会更新</li><li>当实现了自动发现slave和其他哨兵节点后，哨兵就可以通过定期发送PING命令定时监控这些数据库和节点有没有停止服务。</li></ol><p>如果被PING的数据库或者节点超时（通过 sentinel down-after-milliseconds master-name milliseconds 配置）未回复，哨兵认为其主观下线（sdown，s就是Subjectively —— 主观地）。如果下线的是master，哨兵会向其它哨兵发送命令询问它们是否也认为该master主观下线，如果达到一定数目（即配置文件中的quorum）投票，哨兵会认为该master已经客观下线（odown，o就是Objectively —— 客观地），并选举领头的哨兵节点对主从系统发起故障恢复。若没有足够的sentinel进程同意master下线，master的客观下线状态会被移除，若master重新向sentinel进程发送的PING命令返回有效回复，master的主观下线状态就会被移除</p><p>哨兵认为master客观下线后，故障恢复的操作需要由选举的领头哨兵来执行，选举采用Raft算法：</p><ol><li>发现master下线的哨兵节点（我们称他为A）向每个哨兵发送命令，要求对方选自己为领头哨兵</li><li>如果目标哨兵节点没有选过其他人，则会同意选举A为领头哨兵</li><li>如果有超过一半的哨兵同意选举A为领头，则A当选</li><li>如果有多个哨兵节点同时参选领头，此时有可能存在一轮投票无竞选者胜出，此时每个参选的节点等待一个随机时间后再次发起参选请求，进行下一轮投票竞选，直至选举出领头哨兵</li></ol><p>选出领头哨兵后，领头者开始对系统进行故障恢复，从出现故障的master的从数据库中挑选一个来当选新的master,选择规则如下：</p><ol><li>所有在线的slave中选择优先级最高的，优先级可以通过slave-priority配置</li><li>如果有多个最高优先级的slave，则选取复制偏移量最大（即复制越完整）的当选</li><li>如果以上条件都一样，选取id最小的slave</li></ol><p>挑选出需要继任的slave后，领头哨兵向该数据库发送命令使其升格为master，然后再向其他slave发送命令接受新的master，最后更新数据。将已经停止的旧的master更新为新的master的从数据库，使其恢复服务后以slave的身份继续运行。</p><h3><span id="2-部署演示">2. 部署演示</span></h3><p>本示例基于Redis 5.0.3版。</p><p>哨兵模式基于前文的主从复制模式。哨兵的配置文件为sentinel.conf，在文件中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sentinel monitor mymaster 127.0.0.1 6379 1 # mymaster定义一个master数据库的名称，后面是master的ip， port，1表示至少需要一个Sentinel进程同意才能将master判断为失效，如果不满足这个条件，则自动故障转移（failover）不会执行</span><br><span class="line">sentinel auth-pass mymaster 123456 # master的密码</span><br><span class="line">sentinel down-after-milliseconds mymaster 5000 # 5s未回复PING，则认为master主观下线，默认为30s</span><br><span class="line">sentinel parallel-syncs mymaster 2  # 指定在执行故障转移时，最多可以有多少个slave实例在同步新的master实例，在slave实例较多的情况下这个数字越小，同步的时间越长，完成故障转移所需的时间就越长</span><br><span class="line">sentinel failover-timeout mymaster 300000 # 如果在该时间（ms）内未能完成故障转移操作，则认为故障转移失败，生产环境需要根据数据量设置该值</span><br></pre></td></tr></table></figure><blockquote><p>一个哨兵可以监控多个master数据库，只需按上述配置添加多套</p></blockquote><p>分别以26379,36379,46379端口启动三个sentinel</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-server-1 sentinel]# redis-server sentinel1.conf --sentinel</span><br><span class="line">[root@dev-server-1 sentinel]# redis-server sentinel2.conf --sentinel</span><br><span class="line">[root@dev-server-1 sentinel]# redis-server sentinel3.conf --sentinel</span><br></pre></td></tr></table></figure><p>也可以使用redis-sentinel sentinel1.conf 命令启动。此时集群包含一个master、两个slave、三个sentinel，如图，</p><p><img src="https://www.hi-linux.com/img/linux/redis-cluster03.png" alt></p><p>我们来模拟master挂掉的场景，执行 kill -9 3017 将master进程干掉，进入slave中执行 info replication查看，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-server-1 sentinel]# redis-cli -p 7001</span><br><span class="line">127.0.0.1:7001&gt; auth 123456</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:7001&gt; info replication</span><br><span class="line"># Replication</span><br><span class="line">role:slave</span><br><span class="line">master_host:127.0.0.1</span><br><span class="line">master_port:7002</span><br><span class="line">master_link_status:up</span><br><span class="line">master_last_io_seconds_ago:1</span><br><span class="line">master_sync_in_progress:0</span><br><span class="line"># 省略</span><br><span class="line">127.0.0.1:7001&gt; exit</span><br><span class="line">[root@dev-server-1 sentinel]# redis-cli -p 7002</span><br><span class="line">127.0.0.1:7002&gt; auth 123456</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:7002&gt; info replication</span><br><span class="line"># Replication</span><br><span class="line">role:master</span><br><span class="line">connected_slaves:1</span><br><span class="line">slave0:ip&#x3D;127.0.0.1,port&#x3D;7001,state&#x3D;online,offset&#x3D;13642721,lag&#x3D;1</span><br><span class="line"># 省略</span><br></pre></td></tr></table></figure><p>可以看到slave 7002已经成功上位晋升为master（role：master），接收一个slave 7001的连接。此时查看slave2.conf配置文件，发现replicaof的配置已经被移除了，slave1.conf的配置文件里replicaof 127.0.0.1 6379 被改为 replicaof 127.0.0.1 7002。重新启动master，也可以看到master.conf配置文件中添加了replicaof 127.0.0.1 7002的配置项，可见大哥（master）下位后，再出来混就只能当当小弟（slave）了，三十年河东三十年河西。</p><h3><span id="3-哨兵模式的优缺点">3. 哨兵模式的优缺点</span></h3><p>优点：</p><ol><li>哨兵模式基于主从复制模式，所以主从复制模式有的优点，哨兵模式也有</li><li>哨兵模式下，master挂掉可以自动进行切换，系统可用性更高</li></ol><p>缺点：</p><ol><li>同样也继承了主从模式难以在线扩容的缺点，Redis的容量受限于单机配置</li><li>需要额外的资源来启动sentinel进程，实现相对复杂一点，同时slave节点作为备份节点不提供服务</li></ol><h2><span id="cluster模式">Cluster模式</span></h2><h3><span id="1-基本原理">1. 基本原理</span></h3><p>哨兵模式解决了主从复制不能自动故障转移，达不到高可用的问题，但还是存在难以在线扩容，Redis容量受限于单机配置的问题。Cluster模式实现了Redis的分布式存储，即每台节点存储不同的内容，来解决在线扩容的问题。如图</p><p><img src="https://www.hi-linux.com/img/linux/redis-cluster04.png" alt></p><p>Cluster采用无中心结构,它的特点如下：</p><ol><li>所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽</li><li>节点的fail是通过集群中超过半数的节点检测失效时才生效</li><li>客户端与redis节点直连,不需要中间代理层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可</li></ol><p>Cluster模式的具体工作机制：</p><ol><li>在Redis的每个节点上，都有一个插槽（slot），取值范围为0-16383</li><li>当我们存取key的时候，Redis会根据CRC16的算法得出一个结果，然后把结果对16384求余数，这样每个key都会对应一个编号在0-16383之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作</li><li>为了保证高可用，Cluster模式也引入主从复制模式，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点</li><li>当其它主节点ping一个主节点A时，如果半数以上的主节点与A通信超时，那么认为主节点A宕机了。如果主节点A和它的从节点都宕机了，那么该集群就无法再提供服务了</li></ol><p>Cluster模式集群节点最小配置6个节点(3主3从，因为需要半数以上)，其中主节点提供读写操作，从节点作为备用节点，不提供请求，只作为故障转移使用。</p><h3><span id="2-部署演示">2. 部署演示</span></h3><p>本示例基于Redis 5.0.3版。</p><p>Cluster模式的部署比较简单，首先在redis.conf中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">port 7100 # 本示例6个节点端口分别为7100,7200,7300,7400,7500,7600 </span><br><span class="line">daemonize yes # r后台运行 </span><br><span class="line">pidfile &#x2F;var&#x2F;run&#x2F;redis_7100.pid # pidfile文件对应7100,7200,7300,7400,7500,7600 </span><br><span class="line">cluster-enabled yes # 开启集群模式 </span><br><span class="line">masterauth passw0rd # 如果设置了密码，需要指定master密码</span><br><span class="line">cluster-config-file nodes_7100.conf # 集群的配置文件，同样对应7100,7200等六个节点</span><br><span class="line">cluster-node-timeout 15000 # 请求超时 默认15秒，可自行设置</span><br></pre></td></tr></table></figure><p>分别以端口7100,7200,7300,7400,7500,7600 启动六个实例(如果是每个服务器一个实例则配置可一样)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-server-1 cluster]# redis-server redis_7100.conf</span><br><span class="line">[root@dev-server-1 cluster]# redis-server redis_7200.conf</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>然后通过命令将这个6个实例组成一个3主节点3从节点的集群，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-cli --cluster create --cluster-replicas 1 127.0.0.1:7100 127.0.0.1:7200 127.0.0.1:7300 127.0.0.1:7400 127.0.0.1:7500 127.0.0.1:7600 -a passw0rd</span><br></pre></td></tr></table></figure><p>执行结果如图</p><p><img src="https://www.hi-linux.com/img/linux/redis-cluster05.png" alt></p><p>可以看到 7100， 7200， 7300 作为3个主节点，分配的slot分别为 0-5460， 5461-10922， 10923-16383， 7600作为7100的slave， 7500作为7300的slave，7400作为7200的slave。</p><p>我们连接7100设置一个值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@dev-server-1 cluster]# redis-cli -p 7100 -c -a passw0rd</span><br><span class="line">Warning: Using a password with &#39;-a&#39; or &#39;-u&#39; option on the command line interface may not be safe.</span><br><span class="line">127.0.0.1:7100&gt; set site blog.jboost.cn</span><br><span class="line">-&gt; Redirected to slot [9421] located at 127.0.0.1:7200</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:7200&gt; get site</span><br><span class="line">&quot;blog.jboost.cn&quot;</span><br><span class="line">127.0.0.1:7200&gt;</span><br></pre></td></tr></table></figure><p>注意添加 -c 参数表示以集群模式，否则报 (error) MOVED 9421 127.0.0.1:7200 错误， 以 -a 参数指定密码，否则报(error) NOAUTH Authentication required错误。</p><p>从上面命令看到key为site算出的slot为9421，落在7200节点上，所以有Redirected to slot [9421] located at 127.0.0.1:7200，集群会自动进行跳转。因此客户端可以连接任何一个节点来进行数据的存取。</p><p>通过cluster nodes可查看集群的节点信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">127.0.0.1:7200&gt; cluster nodes</span><br><span class="line">eb28aaf090ed1b6b05033335e3d90a202b422d6c 127.0.0.1:7500@17500 slave c1047de2a1b5d5fa4666d554376ca8960895a955 0 1584165266071 5 connected</span><br><span class="line">4cc0463878ae00e5dcf0b36c4345182e021932bc 127.0.0.1:7400@17400 slave 5544aa5ff20f14c4c3665476de6e537d76316b4a 0 1584165267074 4 connected</span><br><span class="line">dbbb6420d64db22f35a9b6fa460b0878c172a2fb 127.0.0.1:7100@17100 master - 0 1584165266000 1 connected 0-5460</span><br><span class="line">d4b434f5829e73e7e779147e905eea6247ffa5a2 127.0.0.1:7600@17600 slave dbbb6420d64db22f35a9b6fa460b0878c172a2fb 0 1584165265000 6 connected</span><br><span class="line">5544aa5ff20f14c4c3665476de6e537d76316b4a 127.0.0.1:7200@17200 myself,master - 0 1584165267000 2 connected 5461-10922</span><br><span class="line">c1047de2a1b5d5fa4666d554376ca8960895a955 127.0.0.1:7300@17300 master - 0 1584165268076 3 connected 10923-16383</span><br></pre></td></tr></table></figure><p>我们将7200通过 kill -9 pid杀死进程来验证集群的高可用，重新进入集群执行cluster nodes可以看到7200 fail了，但是7400成了master，重新启动7200，可以看到此时7200已经变成了slave。</p><h3><span id="3-cluster模式的优缺点">3. Cluster模式的优缺点</span></h3><p>优点：</p><ol><li>无中心架构，数据按照slot分布在多个节点。</li><li>集群中的每个节点都是平等的关系，每个节点都保存各自的数据和整个集群的状态。每个节点都和其他所有节点连接，而且这些连接保持活跃，这样就保证了我们只需要连接集群中的任意一个节点，就可以获取到其他节点的数据。</li><li>可线性扩展到1000多个节点，节点可动态添加或删除</li><li>能够实现自动故障转移，节点之间通过gossip协议交换状态信息，用投票机制完成slave到master的角色转换</li></ol><p>缺点：</p><ol><li>客户端实现复杂，驱动要求实现Smart Client，缓存slots mapping信息并及时更新，提高了开发难度。目前仅JedisCluster相对成熟，异常处理还不完善，比如常见的“max redirect exception”</li><li>节点会因为某些原因发生阻塞（阻塞时间大于 cluster-node-timeout）被判断下线，这种failover是没有必要的</li><li>数据通过异步复制，不保证数据的强一致性</li><li>slave充当“冷备”，不能缓解读压力</li><li>批量操作限制，目前只支持具有相同slot值的key执行批量操作，对mset、mget、sunion等操作支持不友好</li><li>key事务操作支持有线，只支持多key在同一节点的事务操作，多key分布不同节点时无法使用事务功能</li><li>不支持多数据库空间，单机redis可以支持16个db，集群模式下只能使用一个，即db 0</li></ol><p>Redis Cluster模式不建议使用pipeline和multi-keys操作，减少max redirect产生的场景。</p><h2><span id="总结">总结</span></h2><p>本文介绍了Redis集群方案的三种模式，其中主从复制模式能实现读写分离，但是不能自动故障转移；哨兵模式基于主从复制模式，能实现自动故障转移，达到高可用，但与主从复制模式一样，不能在线扩容，容量受限于单机的配置；Cluster模式通过无中心化架构，实现分布式存储，可进行线性扩展，也能高可用，但对于像批量操作、事务操作等的支持性不够好。三种模式各有优缺点，可根据实际场景进行选择。</p><h2><span id="参考">参考：</span></h2><ol><li><a href="https://blog.csdn.net/q649381130/article/details/79931791" target="_blank" rel="noopener">https://blog.csdn.net/q649381130/article/details/79931791</a></li><li><a href="https://www.cnblogs.com/51life/p/10233340.html" target="_blank" rel="noopener">https://www.cnblogs.com/51life/p/10233340.html</a></li><li><a href="https://www.cnblogs.com/chensuqian/p/10538365.html" target="_blank" rel="noopener">https://www.cnblogs.com/chensuqian/p/10538365.html</a></li><li><a href="https://stor.51cto.com/art/201910/604653.htm" target="_blank" rel="noopener">https://stor.51cto.com/art/201910/604653.htm</a></li></ol><blockquote><p>本文转载自：「半路雨歌」，原文：<a href="https://tinyurl.com/y5a6t3md%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://tinyurl.com/y5a6t3md，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在开发测试环境中，我们一般搭建Redis的单实例来应对开发测试需求，但是在生产环境，如果对可用性、可靠性要求较高，则需要引入Redis的集群方案。虽然现在各大云平台有提供缓存服务可以直接使用，但了解一下其背后的实现与原理总还是有些必要（比如面试）， 本文就一起来学习一下Redis的几种集群方案。&lt;/p&gt;
&lt;p&gt;Redis支持三种集群方案&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主从复制模式&lt;/li&gt;
&lt;li&gt;Sentinel（哨兵）模式&lt;/li&gt;
&lt;li&gt;Cluster模式&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;主从复制模式&quot;&gt;主从复制模式&lt;/h2&gt;
&lt;h3 id=&quot;1-基本原理&quot;&gt;1. 基本原理&lt;/h3&gt;
&lt;p&gt;主从复制模式中包含一个主数据库实例（master）与一个或多个从数据库实例（slave），如下图&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/redis-cluster01.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;客户端可对主数据库进行读写操作，对从数据库进行读操作，主数据库写入的数据会实时自动同步给从数据库。&lt;/p&gt;
&lt;p&gt;具体工作机制为：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;slave启动后，向master发送SYNC命令，master接收到SYNC命令后通过bgsave保存快照（即上文所介绍的RDB持久化），并使用缓冲区记录保存快照这段时间内执行的写命令&lt;/li&gt;
&lt;li&gt;master将保存的快照文件发送给slave，并继续记录执行的写命令&lt;/li&gt;
&lt;li&gt;slave接收到快照文件后，加载快照文件，载入数据&lt;/li&gt;
&lt;li&gt;master快照发送完后开始向slave发送缓冲区的写命令，slave接收命令并执行，完成复制初始化&lt;/li&gt;
&lt;li&gt;此后master每次执行一个写命令都会同步发送给slave，保持master与slave之间数据的一致性&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="Redis" scheme="https://www.hi-linux.com/categories/Redis/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Redis" scheme="https://www.hi-linux.com/tags/Redis/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>两款超好用的 Kubernetes 实时日志查看工具</title>
    <link href="https://www.hi-linux.com/posts/14566.html"/>
    <id>https://www.hi-linux.com/posts/14566.html</id>
    <published>2020-11-09T01:00:00.000Z</published>
    <updated>2020-11-09T05:23:07.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>通常情况下，<code>Kubernetes</code> 环境下的应用日志都需要通过日志系统来进行收集，比如：<code>Filebeat</code> + <code>ElasticSearch</code> + <code>Kibana</code> 的组合来实现。虽然这一组合的功能相当强大，但是在一些比较简陋的测试集群中，或者不具备浏览器条件的自动化/控制台环境下，自动合并多个 <code>Pod</code> 中的日志进行集中的查看，对处理问题和调试故障还是很有大帮助的。</p><p>今天，我们就给大家介绍两款超好用的多容器实时日志查看工具 <code>Stern</code> 和 <code>Kubetail</code>。</p><h2><span id="stern">Stern</span></h2><p><code>Kubectl</code> 本身的 <code>Log</code> 命令是不支持同时查看多个 <code>Pod</code> 容器中的日志，<code>Stern</code> 很好的解决了这个问题, 它除了可以同时 <code>tail</code> 多个容器的日志之外, 还支持以下一些强大的功能:</p><ul><li>允许使用正则表达式来选择需要 tail 的 PodName</li><li>自定义不同 Pod 的日志输出的颜色</li><li>自动添加符合规则的新创建 Pod 并进行 tail</li><li>…</li></ul><blockquote><p>项目地址： <a href="https://github.com/wercker/stern" target="_blank" rel="noopener">https://github.com/wercker/stern</a></p></blockquote><a id="more"></a><h3><span id="安装-stern">安装 Stern</span></h3><p><code>Stern</code> 使用 <code>Go</code> 语言开发，安装非常简单，开箱即用。你只需下载对应平台相关的二进制预编译安装包，就可以使用了。</p><p>以 <code>Linux</code> 平台为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ wget https:&#x2F;&#x2F;github.com&#x2F;wercker&#x2F;stern&#x2F;releases&#x2F;download&#x2F;1.11.0&#x2F;stern_linux_amd64</span><br><span class="line">$ chmod +x stern_linux_amd64</span><br><span class="line">$ mv stern_linux_amd64 &#x2F;usr&#x2F;local&#x2F;bin</span><br></pre></td></tr></table></figure><p>如果你使用的是 <code>macOS</code>，可以直接 <code>Homebrew</code> 进行安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install stern</span><br></pre></td></tr></table></figure><h3><span id="使用-stern">使用 Stern</span></h3><p><code>Stern</code> 支持的功能很多，用法也很丰富。下面我们来看几个比较常用的例子：</p><ol><li>实时查看当前 Namespace 中所有 Pod 中所有容器的日志</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stern  .</span><br></pre></td></tr></table></figure><ol start="2"><li>实时查看 Pod 中指定容器的日志</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stern envvars --container gateway</span><br></pre></td></tr></table></figure><ol start="3"><li>实时查看指定命名空间中除指定容器外的所有容器的日志</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stern -n staging --exclude-container istio-proxy .</span><br></pre></td></tr></table></figure><ol start="4"><li>实时查看指定时间范围内容器的日志，下面的例子表示是 15 分钟内</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stern auth -t --since 15m</span><br></pre></td></tr></table></figure><ol start="5"><li>实时查看指定命名空间中容器的日志</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stern kubernetes-dashboard --namespace kube-system</span><br></pre></td></tr></table></figure><ol start="6"><li>实时查看所有命名空间中符合指定标签容器的日志</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ stern --all-namespaces -l run&#x3D;nginx</span><br></pre></td></tr></table></figure><p>更多用法，可参考「<a href="https://github.com/wercker/stern" target="_blank" rel="noopener">Stern 官方文档</a>」。</p><h2><span id="kubetail">Kubetail</span></h2><p><code>Kubetail</code> 是一个 <code>Shell</code> 脚本，它可以将多个 <code>Pod</code> 的日志合并到一起，并支持彩色输出。</p><blockquote><p>项目地址：<a href="https://github.com/johanhaleby/kubetail" target="_blank" rel="noopener">https://github.com/johanhaleby/kubetail</a></p></blockquote><h3><span id="安装-kubetail">安装 Kubetail</span></h3><p>由于 <code>Kubetail</code> 只是一个 <code>Shell</code> 脚本，直接下载后便可使用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ wget https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;johanhaleby&#x2F;kubetail&#x2F;master&#x2F;kubetail</span><br><span class="line">$ chmod +x kubetail</span><br><span class="line">$ cp kubetail &#x2F;usr&#x2F;local&#x2F;bin</span><br></pre></td></tr></table></figure><p>如果你使用的是 <code>macOS</code>，也可以直接 <code>Homebrew</code> 进行安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew tap johanhaleby&#x2F;kubetail &amp;&amp; brew install kubetail</span><br></pre></td></tr></table></figure><p><code>Kubetail</code> 还支持各种 <code>SHELL</code> 管理框架，比如：<code>Oh-my-zsh</code>、<code>Antigen</code> 等，具体安装方法可参考「<a href="https://github.com/johanhaleby/kubetail" target="_blank" rel="noopener">官方安装文档</a>」。</p><h3><span id="使用-kubetail">使用 Kubetail</span></h3><p><code>Kubetail</code> 使用也是非常简单的，基本语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubetail &lt;search term&gt; [-h] [-c] [-n] [-t] [-l] [-d] [-p] [-s] [-b] [-k] [-v] [-r] [-i]</span><br></pre></td></tr></table></figure><p>一些常用参数的解释：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-c：指定多容器 Pod 中的容器名称</span><br><span class="line">-t：指定 Kubeconfig 文件中的 Context</span><br><span class="line">-l：标签过滤器，使用 -l 参数之后，会忽略 Pod 名称</span><br><span class="line">-n：指定命名空间</span><br><span class="line">-s：指定返回一个相对时间之后的日志，例如 5s，2m 或者 3h，缺省是 10s</span><br><span class="line">-b：是否使用 line-buffered，缺省为 false</span><br><span class="line">-k：指定输出内容的具体着色部分，pod：只给 pod 名称上色，line：整行上色（缺省），false：不上色</span><br></pre></td></tr></table></figure><p>一些使用实例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubetail my-pod-v1</span><br><span class="line">$ kubetail my-pod-v1 -c my-container</span><br><span class="line">$ kubetail my-pod-v1 -t int1-context -c my-container</span><br><span class="line">$ kubetail &#39;(service|consumer|thing)&#39; -e regex</span><br><span class="line">$ kubetail -l service&#x3D;my-service</span><br><span class="line">$ kubetail --selector service&#x3D;my-service --since 10m</span><br><span class="line">$ kubetail --tail 1</span><br></pre></td></tr></table></figure><p>至此，两种超实用的多容器实时日志查看工具就介绍完了。如果你还有更好的类似工具推荐，欢迎留言讨论哟！</p><h2><span id="参考文档">参考文档</span></h2><ol><li><a href="https://www.google.com" target="_blank" rel="noopener">https://www.google.com</a></li><li><a href="https://zhuanlan.zhihu.com/p/60987559" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/60987559</a></li><li><a href="https://blog.fleeto.us/post/introducing-kubetail/" target="_blank" rel="noopener">https://blog.fleeto.us/post/introducing-kubetail/</a></li></ol></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通常情况下，&lt;code&gt;Kubernetes&lt;/code&gt; 环境下的应用日志都需要通过日志系统来进行收集，比如：&lt;code&gt;Filebeat&lt;/code&gt; + &lt;code&gt;ElasticSearch&lt;/code&gt; + &lt;code&gt;Kibana&lt;/code&gt; 的组合来实现。虽然这一组合的功能相当强大，但是在一些比较简陋的测试集群中，或者不具备浏览器条件的自动化/控制台环境下，自动合并多个 &lt;code&gt;Pod&lt;/code&gt; 中的日志进行集中的查看，对处理问题和调试故障还是很有大帮助的。&lt;/p&gt;
&lt;p&gt;今天，我们就给大家介绍两款超好用的多容器实时日志查看工具 &lt;code&gt;Stern&lt;/code&gt; 和 &lt;code&gt;Kubetail&lt;/code&gt;。&lt;/p&gt;
&lt;h2 id=&quot;Stern&quot;&gt;Stern&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Kubectl&lt;/code&gt; 本身的 &lt;code&gt;Log&lt;/code&gt; 命令是不支持同时查看多个 &lt;code&gt;Pod&lt;/code&gt; 容器中的日志，&lt;code&gt;Stern&lt;/code&gt; 很好的解决了这个问题, 它除了可以同时 &lt;code&gt;tail&lt;/code&gt; 多个容器的日志之外, 还支持以下一些强大的功能:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;允许使用正则表达式来选择需要 tail 的 PodName&lt;/li&gt;
&lt;li&gt;自定义不同 Pod 的日志输出的颜色&lt;/li&gt;
&lt;li&gt;自动添加符合规则的新创建 Pod 并进行 tail&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;项目地址： &lt;a href=&quot;https://github.com/wercker/stern&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/wercker/stern&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>巧用 Docker 快速部署 GPU 的基础环境</title>
    <link href="https://www.hi-linux.com/posts/15874.html"/>
    <id>https://www.hi-linux.com/posts/15874.html</id>
    <published>2020-10-30T01:00:00.000Z</published>
    <updated>2020-11-04T09:40:30.000Z</updated>
    
    <content type="html"><![CDATA[<div id="vip-container"><p>在 Linux 服务器上使用 GPU 跑深度学习的模型很正常不过。如果我们想用 Docker 实现同样的需求，就需要做些额外的工作。本质上就是我们要在容器里能看到并且使用宿主机上的显卡。 在这篇文章里我们就介绍一下 Docker 使用 GPU 的环境搭建。</p><h2><span id="nvidia-驱动">Nvidia 驱动</span></h2><p>某些命令以 Ubuntu 作为示例。 首先宿主机上必现安装 Nvidia 驱动。</p><p>这里推荐从 Nvidia 官网下载脚本安装，安装和卸载都比较方便并且适用于任何 Linux 发行版，包括 CentOS，Ubuntu 等。 NVIDIA Telsa GPU 的 Linux 驱动在安装过程中需要编译 kernel module，系统需提前安装 gcc 和编译 Linux Kernel Module 所依赖的包，例如 <code>kernel-devel-$(uname -r)</code> 等。</p><h3><span id="安装-gcc-和-kernel-dev">安装 gcc 和 kernel-dev</span></h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt install gcc kernel-dev -y</span><br></pre></td></tr></table></figure><a id="more"></a><h3><span id="安装-nvidia-驱动">安装 Nvidia 驱动</span></h3><ol><li><p>访问 <a href="https://www.nvidia.com/Download/Find.aspx" target="_blank" rel="noopener">https://www.nvidia.com/Download/Find.aspx</a></p></li><li><p>选择对应操作系统和安装包，并单击 [SEARCH] 搜寻驱动，选择要下载的驱动版本</p></li></ol><p><img src="https://www.hi-linux.com/img/linux/nvdriver.png" alt></p><ol start="3"><li>在宿主机上下载并执行对应版本安装脚本</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ wget https:&#x2F;&#x2F;www.nvidia.com&#x2F;content&#x2F;DriverDownload-March2009&#x2F;confirmation.php?url&#x3D;&#x2F;tesla&#x2F;450.80.02&#x2F;NVIDIA-Linux-x86_64-450.80.02.run&amp;lang&#x3D;us&amp;type&#x3D;Tesla</span><br><span class="line">$ chmod +x NVIDIA-Linux-x86_64-450.80.02.run &amp;&amp; .&#x2F;NVIDIA-Linux-x86_64-450.80.02.run</span><br></pre></td></tr></table></figure><ol start="4"><li>验证</li></ol><p>使用 <code>nvidia-smi</code> 命令验证是否安装成功，如果输出类似下图则驱动安装成功。</p><p><img src="https://www.hi-linux.com/img/linux/nvsmi.png" alt></p><h2><span id="cuda-驱动">CUDA 驱动</span></h2><p>CUDA（Compute Unified Device Architecture）是显卡厂商 NVIDIA 推出的运算平台。<code>CUDA™</code>是一种由 NVIDIA 推出的通用并行计算架构，该架构使 GPU 能够解决复杂的计算问题。它包含了 CUDA 指令集架构（ISA）以及 GPU 内部的并行计算引擎。 这里安装的方式和显卡驱动安装类似。</p><ol><li>访问官网下载对应版本安装包，<a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">https://developer.nvidia.com/cuda-toolkit-archive</a></li></ol><p><img src="https://www.hi-linux.com/img/linux/cudadirver.png" alt></p><ol start="2"><li>配置环境变量</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo &#39;export PATH&#x3D;&#x2F;usr&#x2F;local&#x2F;cuda&#x2F;bin:$PATH&#39; | sudo tee &#x2F;etc&#x2F;profile.d&#x2F;cuda.sh </span><br><span class="line">$ source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure><h2><span id="nvidia-docker2">nvidia-docker2</span></h2><p>Docker 的安装这里就不展开了，具体查看官方文档非常详细。</p><p>这里我们就直接介绍安装 nvidia-docker2.</p><p>既然叫 nvidia-docker2 就有 nvidia-docker1 就是它的 1.0 版本目前已经废弃了，所以注意不要装错。</p><p>这里先简单说一下 nvidia-docker2 的原理，nvidia-docker2 的依赖由下几部分组成.</p><ul><li>libnvidia-container</li><li>nvidia-container-toolkit</li><li>nvidia-container-runtime</li></ul><p><img src="https://www.hi-linux.com/img/linux/nvidia-docker-arch.png" alt></p><p>nvidia-container-runtime 是在 runc 基础上多实现了 nvidia-container-runime-hook (现在叫 nvidia-container-toolkit)，该 hook 是在容器启动后（Namespace已创建完成），容器自定义命令(Entrypoint)启动前执行。当检测到 NVIDIA_VISIBLE_DEVICES 环境变量时，会调用 libnvidia-container 挂载 GPU Device 和 CUDA Driver。如果没有检测到 NVIDIA_VISIBLE_DEVICES 就会执行默认的 runc。</p><p>下面分两步安装</p><ol><li>设置 repository 和 GPG key</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ distribution&#x3D;$(. &#x2F;etc&#x2F;os-release;echo $ID$VERSION_ID)</span><br><span class="line">$ curl -s -L https:&#x2F;&#x2F;nvidia.github.io&#x2F;nvidia-docker&#x2F;gpgkey | sudo apt-key add -</span><br><span class="line">$ curl -s -L https:&#x2F;&#x2F;nvidia.github.io&#x2F;nvidia-docker&#x2F;$distribution&#x2F;nvidia-docker.list | sudo tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;nvidia-docker.list</span><br></pre></td></tr></table></figure><ol start="2"><li>安装</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install -y nvidia-docker2</span><br><span class="line">$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure><ol start="3"><li>验证</li></ol><p>执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm --gpus all nvidia&#x2F;cuda:10.2-base nvidia-smi</span><br></pre></td></tr></table></figure><p>如果输出跟直接在宿主机上执行 <code>nvidia-smi</code> 一致则说明安装成功。 如果跑的深度学习模型使用的是 tensorflow 可以在容器里执行:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">tf.contrib.eager.num_gpus()</span><br></pre></td></tr></table></figure><p>如果输出了宿主机上的 Nvidia 显卡数量，则模型能使用到显卡加速。 如果使用的是 pytorch 可以在容器里执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">torch.cuda.is_available()</span><br></pre></td></tr></table></figure><p>如果输出 True 证明环境也成功了，可以使用显卡。</p><ol start="4"><li>使用示例</li></ol><ul><li>使用所有显卡</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm --gpus all nvidia&#x2F;cuda nvidia-smi </span><br><span class="line">$ docker run --rm --runtime&#x3D;nvidia -e NVIDIA_VISIBLE_DEVICES&#x3D;all nvidia&#x2F;cuda nvidia-smi</span><br></pre></td></tr></table></figure><ul><li>指明使用哪几张卡</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --gpus &#39;&quot;device&#x3D;1,2&quot;&#39; nvidia&#x2F;cuda nvidia-smi </span><br><span class="line">$ docker run --rm --runtime&#x3D;nvidia -e NVIDIA_VISIBLE_DEVICES&#x3D;1,2 nvidia&#x2F;cuda nvidia-smi</span><br></pre></td></tr></table></figure><p>到这里在 Docker 下使用 Nvidia 显卡加速计算的基础环境搭建就介绍完了。后续我们可以继续研究一下 K8S 下调度 GPU 的实现。</p><blockquote><p>本文转载自：「lxkaka」，原文：<a href="https://lxkaka.wang/docker-nvidia/%EF%BC%8C%E7%89%88%E6%9D%83%E5%BD%92%E5%8E%9F%E4%BD%9C%E8%80%85%E6%89%80%E6%9C%89%E3%80%82%E6%AC%A2%E8%BF%8E%E6%8A%95%E7%A8%BF%EF%BC%8C%E6%8A%95%E7%A8%BF%E9%82%AE%E7%AE%B1:" target="_blank" rel="noopener">https://lxkaka.wang/docker-nvidia/，版权归原作者所有。欢迎投稿，投稿邮箱:</a> <a href="mailto:editor@hi-linux.com">editor@hi-linux.com</a>。</p></blockquote></div><script src="https://my.openwrite.cn/js/readmore.js" type="text/javascript"></script><script>var isMobile = navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i);if (!isMobile) {    var btw = new BTWPlugin();    btw.init({        "id": "vip-container",        "blogId": "10135-1588830050631-449",        "name": "「奇妙的 Linux 世界」",        "qrcode": "https://www.hi-linux.com/img/wechat/mp_qrcode_12.jpg",        "keyword": "VIP"    });}</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在 Linux 服务器上使用 GPU 跑深度学习的模型很正常不过。如果我们想用 Docker 实现同样的需求，就需要做些额外的工作。本质上就是我们要在容器里能看到并且使用宿主机上的显卡。 在这篇文章里我们就介绍一下 Docker 使用 GPU 的环境搭建。&lt;/p&gt;
&lt;h2 id=&quot;Nvidia-驱动&quot;&gt;Nvidia 驱动&lt;/h2&gt;
&lt;p&gt;某些命令以 Ubuntu 作为示例。 首先宿主机上必现安装 Nvidia 驱动。&lt;/p&gt;
&lt;p&gt;这里推荐从 Nvidia 官网下载脚本安装，安装和卸载都比较方便并且适用于任何 Linux 发行版，包括 CentOS，Ubuntu 等。 NVIDIA Telsa GPU 的 Linux 驱动在安装过程中需要编译 kernel module，系统需提前安装 gcc 和编译 Linux Kernel Module 所依赖的包，例如 &lt;code&gt;kernel-devel-$(uname -r)&lt;/code&gt; 等。&lt;/p&gt;
&lt;h3 id=&quot;安装-gcc-和-kernel-dev&quot;&gt;安装 gcc 和 kernel-dev&lt;/h3&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo apt install gcc kernel-dev -y&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="Docker" scheme="https://www.hi-linux.com/categories/docker/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
      <category term="GPU" scheme="https://www.hi-linux.com/tags/GPU/"/>
    
  </entry>
  
</feed>
