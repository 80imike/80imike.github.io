<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>运维之美</title>
  
  <subtitle>种一棵树最好的时间是十年前，其次是现在。</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.hi-linux.com/"/>
  <updated>2018-08-28T08:59:51.527Z</updated>
  <id>https://www.hi-linux.com/</id>
  
  <author>
    <name>Mike</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>初识 Istio</title>
    <link href="https://www.hi-linux.com/posts/28535.html"/>
    <id>https://www.hi-linux.com/posts/28535.html</id>
    <published>2018-08-28T01:00:00.000Z</published>
    <updated>2018-08-28T08:59:51.527Z</updated>
    
    <content type="html"><![CDATA[<p>如果你比较关注新兴技术的话，那么很可能在不同的地方听说过 Istio，并且知道它和 Service Mesh 有着牵扯。这篇文章可以作为了解 Istio 的入门介绍。通过本文你可以了解什么是 Istio，Istio 为什么最近这么火，以及 Istio 能够我们带来什么好处。</p><h3 id="什么是-istio">什么是 Istio？</h3><p>官方对 Istio 的介绍浓缩成了一句话：</p><blockquote><p>An open platform to connect, secure, control and observe services.</p></blockquote><p>翻译过来，就是&quot;连接、安全加固、控制和观察服务的开放平台&quot;。开放平台就是指它本身是开源的，服务对应的是微服务，也可以粗略地理解为单个应用。 中间的四个动词就是 Istio 的主要功能，官方也各有一句话的说明。这里再阐释一下：</p><ul><li>连接（Connect）：智能控制服务之间的调用流量，能够实现灰度升级、AB 测试和红黑部署等功能。</li><li>安全加固（Secure）：自动为服务之间的调用提供认证、授权和加密。</li><li>控制（Control）：应用用户定义的 Policy，保证资源在消费者中公平分配。</li><li>观察（Observe）：查看服务运行期间的各种数据，比如日志、监控和 Tracing，了解服务的运行情况。</li></ul><p><img src="https://www.hi-linux.com/img/linux/istio1.jpg" alt=""></p><a id="more"></a><p>虽然听起来非常高级，功能非常强大，但是一股脑出现这么多名词，还都是非常虚的概念，说了跟没说一样。要想理解上面这几句话的含义，我们还是从头说起，先聊聊 Service Mesh。</p><blockquote><p>NOTE：其实 Istio 的源头是微服务，但这又是一个比较大的话题，目前可以参考网络上各种文章。如果有机会，我们再来聊聊微服务。</p></blockquote><h3 id="什么是-service-mesh">什么是 Service Mesh</h3><p>一般介绍 Service Mesh 的文章都会从网络层的又一个抽象说起，把 Service Mesh 看做建立在 TCP 层之上的微服务层。我这次换个思路，从 Service Mesh 的技术根基——网络代理来分析。</p><p>说起网络代理，如果对软件架构比较熟悉的会想到 Nginx 等反向代理软件。其实网络代理的范围比较广，可以肯定的说，有网络访问的地方就会有代理的存在。</p><p>Wikipedia 对代理的定义如下：</p><blockquote><p>In computer networks, a proxy server is a server (a computer system or an application) that acts as an intermediary for requests from clients seeking resources from other servers.</p></blockquote><p>NOTE：代理可以是嵌套的，也就是说通信双方 A、B 中间可以多层代理，而这些代理的存在有可能对 A、B 是透明的。</p><p>简单来说，网络代理可以简单类比成现实生活中的中介，本来需要通信的双方因为各种原因在中间再加上一道关卡。本来双方就能完成的通信，为何非要多此一举呢？那是因为代理可以为整个通信带来更多的功能，比如：</p><ul><li>拦截：代理可以选择性拦截传输的网络流量，比如：一些公司限制员工在上班的时候不能访问某些游戏或者电商网站，数据中心中拒绝恶意访问网关等。</li><li>统计：既然所有的流量都经过代理，那么代理也可以用来统计网络中的数据信息，比如了解哪些人在访问哪些网站，通信的应答延迟等。</li><li>缓存：如果通信双方比较远，访问比较慢，那么代理可以把最近访问的数据缓存在本地，后面的访问不用访问后端来做到加速。CDN 就是这个功能的典型场景。</li><li>分发：如果某个通信方有多个服务器后端，代理可以根据某些规则来选择如何把流量发送给多个服务器，也就是我们常说的负载均衡功能。比如著名的 Nginx 软件。</li><li>跳板：如果 A、B 双方因为某些原因不能直接访问，而代理可以和双方通信，那么通过代理，双方可以绕过原来的限制进行通信。这应该是国内网民比较熟悉的场景。</li><li>注入：既然代理可以看到流量，那么它也可以修改网络流量，可以自动在收到的流量中添加一些数据，比如有些宽带提供商的弹窗广告。</li><li>……</li></ul><p><img src="https://www.hi-linux.com/img/linux/istio2.jpg" alt=""></p><p>不是要讲 Service Mesh 吗？为什么扯了一堆代理的事情？因为 Service Mesh 可以看做是传统代理的升级版，用来解决现在微服务框架中出现的问题，可以把 Service Mesh 看做是分布式的微服务代理。</p><p>在传统模式下，代理一般是集中式的单独的服务器，所有的请求都要先通过代理，然后再流入转发到实际的后端。而在 Service Mesh 中，代理变成了分布式的，它常驻在了应用的身边（最常见的就是 Kubernetes Sidecar 模式，每一个应用的 Pod 中都运行着一个代理，负责流量相关的事情）。这样的话，应用所有的流量都被代理接管，那么这个代理就能做到上面提到的所有可能的事情，从而带来无限的想象力。</p><p><img src="https://www.hi-linux.com/img/linux/istio3.jpg" alt=""></p><p>此外，原来的代理都是基于网络流量的，一般都是工作在 IP 或者 TCP 层，很少关心具体的应用逻辑。但是 Service Mesh 中，代理会知道整个集群的所有应用信息，并且额外添加了热更新、注入服务发现、降级熔断、认证授权、超时重试、日志监控等功能，让这些通用的功能不必每个应用都自己实现，放在代理中即可。换句话说，Service Mesh 中的代理对微服务中的应用做了定制化的改进！</p><p><img src="https://www.hi-linux.com/img/linux/istio4.jpg" alt=""></p><p>就这样，借着微服务和容器化的东风，传统的代理摇身一变，成了如今炙手可热的 Service Mesh。应用微服务之后，每个单独的微服务都会有很多副本，而且可能会有多个版本，这么多微服务之间的相互调用和管理非常复杂，但是有了 Service Mesh，我们可以把这块内容统一在代理层。</p><p><img src="https://www.hi-linux.com/img/linux/istio5.gif" alt=""></p><p>有了看起来四通八达的分布式代理，我们还需要对这些代理进行统一的管理。手动更新每个代理的配置，对代理进行升级或者维护是个不可持续的事情，在前面的基础上，在加上一个控制中心，一个完整的 Service Mesh 就成了。管理员只需要根据控制中心的 API 来配置整个集群的应用流量、安全规则即可，代理会自动和控制中心打交道根据用户的期望改变自己的行为。</p><p><img src="https://www.hi-linux.com/img/linux/istio6.jpg" alt=""></p><blockquote><p>NOTE：所以你也可以理解 Service Mesh 中的代理会抢了 Nginx 的生意，这也是为了 Nginx 也要开始做 NginMesh 的原因。</p></blockquote><h3 id="再来看-istio">再来看 Istio</h3><p>了解了 Service Mesh 的概念，我们再来看 Istio ，也许就会清楚很多。首先来看 Istio 官方给出的架构图：</p><p><img src="https://www.hi-linux.com/img/linux/istio7.jpg" alt=""></p><p>可以看到，Istio 就是我们上述提到的 Service Mesh 架构的一种实现，服务之间的通信（比如这里的 Service A 访问 Service B）会通过代理（默认是 Envoy）来进行，而且中间的网络协议支持 HTTP/1.1、HTTP/2、gRPC 或者 TCP，可以说覆盖了主流的通信协议。控制中心做了进一步的细分，分成了 Pilot、Mixer、和 Citadel，它们的各自功能如下：</p><ul><li>Pilot：为 Envoy 提供了服务发现，流量管理和智能路由（AB测试、金丝雀发布等），以及错误处理（超时、重试、熔断）功能。用户通过 Pilot 的 API 管理网络相关的资源对象，Pilot 会根据用户的配置和服务的信息把网络流量管理变成 Envoy 能识别的格式分发到各个 Sidecar 代理中。</li><li>Mixer：为整个集群执行访问控制（哪些用户可以访问哪些服务）和 Policy 管理（Rate Limit，Quota 等），并且收集代理观察到的服务之间的流量统计数据。</li><li>Citadel：为服务之间提供认证和证书管理，可以让服务自动升级成 TLS 协议。</li></ul><p>代理会和控制中心通信，一方面可以获取需要的服务之间的信息，另一方面也可以汇报服务调用的 Metrics 数据。知道 Istio 的核心架构，再来看看它的功能描述就非常容易理解了。</p><ul><li>连接：控制中心可以从集群中获取所有服务的信息，并分发给代理，这样代理就能根据用户的期望来完成服务之间的通信（自动地服务发现、负载均衡、流量控制等）。</li><li>安全加固：因为所有的流量都是通过代理的，那么代理接收到不加密的网络流量之后，可以自动做一次封装，把它升级成安全的加密流量。</li><li>控制：用户可以配置各种规则（比如 RBAC 授权、白名单、Rate Limit 或者 Quota 等），当代理发现服务之间的访问不符合这些规则，就直接拒绝掉。</li><li>观察：所有的流量都经过代理，因此代理对整个集群的访问情况知道得一清二楚，它把这些数据上报到控制中心，那么管理员就能观察到整个集群的流量情况了。</li></ul><h3 id="istio-解决什么问题">Istio 解决什么问题</h3><p>虽然看起来非常炫酷，功能也很强大，但是一个架构和产品出来都是要解决具体的问题。所以这部分我们来看看微服务架构中的难题以及 Istio 给出的答案。</p><p>首先，原来的单个应用拆分成了许多分散的微服务，它们之间相互调用才能完成一个任务，而一旦某个过程出错（组件越多，出错的概率也就越大），就非常难以排查。</p><p>用户请求出现问题无外乎两个问题：错误和响应慢。如果请求错误，那么我们需要知道那个步骤出错了，这么多的微服务之间的调用怎么确定哪个有调用成功？哪个没有调用成功呢？如果是请求响应太慢，我们就需要知道到底哪些地方比较慢？整个链路的调用各阶段耗时是多少？哪些调用是并发执行的，哪些是串行的？这些问题需要我们能非常清楚整个集群的调用以及流量情况。</p><p><img src="https://www.hi-linux.com/img/linux/istio8.jpg" alt=""></p><p>此外，微服务拆分成这么多组件，如果单个组件出错的概率不变，那么整体有地方出错的概率就会增大。服务调用的时候如果没有错误处理机制，那么会导致非常多的问题。比如如果应用没有配置超时参数，或者配置的超时参数不对，则会导致请求的调用链超时叠加，对于用户来说就是请求卡住了；如果没有重试机制，那么因为各种原因导致的偶发故障也会导致直接返回错误给用户，造成不好的用户体验；此外，如果某些节点异常（比如网络中断，或者负载很高），也会导致应用整体的响应时间变成，集群服务应该能自动避开这些节点上的应用；最后，应用也是会出现 Bug 的，各种 Bug 会导致某些应用不可访问。这些问题需要每个应用能及时发现问题，并做好对应的处理措施。</p><p><img src="https://www.hi-linux.com/img/linux/istio9.jpg" alt=""></p><p>应用数量的增多，对于日常的应用发布来说也是个难题。应用的发布需要非常谨慎，如果应用都是一次性升级的，出现错误会导致整个线上应用不可用，影响范围太大；而且，很多情况我们需要同时存在不同的版本，使用 AB 测试验证哪个版本更好；如果版本升级改动了 API，并且互相有依赖，那么我们还希望能自动地控制发布期间不同版本访问不同的地址。这些问题都需要智能的流量控制机制。</p><p><img src="https://www.hi-linux.com/img/linux/istio10.jpg" alt=""></p><p>为了保证整个系统的安全性，每个应用都需要实现一套相似的认证、授权、HTTPS、限流等功能。一方面大多数的程序员都安全相关的功能并不擅长或者感兴趣，另外这些完全相似的内容每次都要实现一遍是非常冗余的。这个问题需要一个能自动管理安全相关内容的系统。</p><p><img src="https://www.hi-linux.com/img/linux/istio11.jpg" alt=""></p><p>上面提到的这些问题是不是非常熟悉？它们就是 istio 尝试解决的问题，如果把上面的问题和 istio 提供的功能做个映射，你会发现它们是非常匹配，毕竟 istio 就是为了解决微服务的这些问题才出现的。</p><p><img src="https://www.hi-linux.com/img/linux/istio12.gif" alt=""></p><h3 id="用什么姿势接入-istio">用什么姿势接入 Istio？</h3><p>虽然 Istio 能解决那么多的问题，但是引入 Istio 并不是没有代价的。最大的问题是 Istio 的复杂性，强大的功能也意味着 Istio 的概念和组件非常多，要想理解和掌握 Istio ，并成功在生产环境中部署需要非常详细的规划。一般情况下，集群管理团队需要对 Kubernetes 非常熟悉，了解常用的使用模式，然后采用逐步演进的方式把 Istio 的功能分批掌控下来。</p><p>第一步，自然是在测试环境搭建一套 Istio 的集群，理解所有的核心概念和组件。了解 Istio 提供的接口和资源，知道它们的用处，思考如何应用到自己的场景中，然后是熟悉 Istio 的源代码，跟进社区的 Issues，了解目前还存在的 Issues 和 Bug，思考如何规避或者修复。这一步是基础，需要积累到 Istio 安装部署、核心概念、功能和缺陷相关的知识，为后面做好准备。</p><p>第二步，可以考虑接入 Istio 的观察性功能，包括 Logging、Tracing、Metrics 数据。应用部署到集群中，选择性地（一般是流量比较小，影响范围不大的应用）为一些应用开启 Istio 自动注入功能，接管应用的流量，并安装 Prometheus 和 Zipkin 等监控组件，收集系统所有的监控数据。这一步可以试探性地了解 Istio 对应用的性能影响，同时建立服务的性能测试基准，发现服务的性能瓶颈，帮助快速定位应用可能出现的问题。此时，这些功能可以是对应用开发者透明的，只需要集群管理员感知，这样可以减少可能带来的风险。</p><p>第三步，为应用配置 Timeout 超时参数、自动重试、熔断和降级等功能，增加服务的容错性。这样可以避免某些应用错误进行这些配置导致问题的出现，这一步完成后需要通知所有的应用开发者删除掉在应用代码中对应的处理逻辑。这一步需要开发者和集群管理员同时参与。</p><p>第四步，和 Ingress、Helm、应用上架等相关组件和流程对接，使用 Istio 接管应用的升级发布流程。让开发者可以配置应用灰度发布升级的策略，支持应用的蓝绿发布、金丝雀发布以及 AB 测试。</p><p>第五步，接入安全功能。配置应用的 TLS 互信，添加 RBAC 授权，设置应用的流量限制，提升整个集群的安全性。因为安全的问题配置比较繁琐，而且优先级一般会比功能性相关的特性要低，所以这里放在了最后。</p><p>当然这个步骤只是一个参考，每个公司需要根据自己的情况、人力、时间和节奏来调整，找到适合自己的方案。</p><h3 id="总结">总结</h3><p>Istio 的架构在数据中心和集群管理中非常常见，每个 Agent 分布在各个节点上（可以是服务器、虚拟机、Pod、容器）负责接收指令并执行，以及汇报信息；控制中心负责汇聚整个集群的信息，并提供 API 让用户对集群进行管理。Kubernetes 也是类似的架构，SDN（Software Defined Network） 也是如此。相信以后会有更多类似架构的出现，这是因为数据中心要管理的节点越来越多，我们需要把任务执行分布到各节点（Agent 负责的功能），同时也需要对整个集群进行管理和控制（Control Plane 的功能），完全去中心化的架构是无法满足后面这个要求的。</p><p>Istio 的出现为负责的微服务架构减轻了很多的负担，开发者不用关心服务调用的超时、重试、Rate Limit 的实现，服务之间的安全、授权也自动得到了保证；集群管理员也能够很方便地发布应用（AB 测试和灰度发布），并且能清楚看到整个集群的运行情况。</p><p>但是这并不表明有了 Istio 就可以高枕无忧了，Istio 只是把原来分散在应用内部的复杂性统一抽象出来放到了统一的地方，并没有让原来的复杂消失不见。因此我们需要维护 Istio 整个集群，而 Istio 的架构比较复杂，尤其是它一般还需要架在 Kubernetes 之上，这两个系统都比较复杂，而且它们的稳定性和性能会影响到整个集群。因此再采用 Isito 之前，必须做好清楚的规划，权衡它带来的好处是否远大于额外维护它的花费，需要有相关的人才对整个网络、Kubernetes 和 Istio 都比较了解才行。</p><h3 id="参考资料">参考资料</h3><p><a href="https://istio.io/docs/concepts/what-is-istio/" target="_blank" rel="noopener">Istio / What is Istio?</a>：Istio 官网上对 Istio 进行介绍的文档<br><a href="http://philcalcado.com/2017/08/03/pattern_service_mesh.html" target="_blank" rel="noopener">Pattern: Service Mesh</a>：Service Mesh Pattern 详解的文章</p><blockquote><p>来源：Cizixs Writes Here<br>原文：<a href="http://t.cn/RkksFOW" target="_blank" rel="noopener">http://t.cn/RkksFOW</a><br>题图：来自谷歌图片搜索<br>版权：本文版权归原作者所有</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果你比较关注新兴技术的话，那么很可能在不同的地方听说过 Istio，并且知道它和 Service Mesh 有着牵扯。这篇文章可以作为了解 Istio 的入门介绍。通过本文你可以了解什么是 Istio，Istio 为什么最近这么火，以及 Istio 能够我们带来什么好处。&lt;/p&gt;
&lt;h3 id=&quot;什么是-istio？&quot;&gt;什么是 Istio？&lt;/h3&gt;
&lt;p&gt;官方对 Istio 的介绍浓缩成了一句话：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;An open platform to connect, secure, control and observe services.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;翻译过来，就是&amp;quot;连接、安全加固、控制和观察服务的开放平台&amp;quot;。开放平台就是指它本身是开源的，服务对应的是微服务，也可以粗略地理解为单个应用。 中间的四个动词就是 Istio 的主要功能，官方也各有一句话的说明。这里再阐释一下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;连接（Connect）：智能控制服务之间的调用流量，能够实现灰度升级、AB 测试和红黑部署等功能。&lt;/li&gt;
&lt;li&gt;安全加固（Secure）：自动为服务之间的调用提供认证、授权和加密。&lt;/li&gt;
&lt;li&gt;控制（Control）：应用用户定义的 Policy，保证资源在消费者中公平分配。&lt;/li&gt;
&lt;li&gt;观察（Observe）：查看服务运行期间的各种数据，比如日志、监控和 Tracing，了解服务的运行情况。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/istio1.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>使用 IPVS 实现 Kubernetes 入口流量负载均衡</title>
    <link href="https://www.hi-linux.com/posts/29792.html"/>
    <id>https://www.hi-linux.com/posts/29792.html</id>
    <published>2018-08-27T01:00:00.000Z</published>
    <updated>2018-08-27T06:17:59.479Z</updated>
    
    <content type="html"><![CDATA[<p>新搭建的 Kubernetes 集群如何承接外部访问的流量，是刚上手 Kubernetes 时常常会遇到的问题。 在公有云上，官方给出了比较直接的答案，使用 LoadBalancer 类型的 Service，利用公有云提供的负载均衡服务来承接流量，同时在多台服务器之间进行负载均衡。</p><p>而在私有环境中，如何正确的将外部流量引入到集群内部，却暂时没有标准的做法。 本文将介绍一种基于 IPVS 来承接流量并实现负载均衡的方法，供大家参考。在阅读本文前建议先了解文中相关基础知识点，推荐阅读下「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486130&amp;idx=1&amp;sn=41ee30f02113dac86398653f542a3c70&amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3&amp;token=1694324409&amp;lang=zh_CN#rd" target="_blank" rel="noopener">浅析从外部访问 Kubernetes 集群中应用的几种方式</a>」一文。</p><h3 id="ipvs">IPVS</h3><p>IPVS 是 LVS 项目的一部分，是一款运行在 Linux kernel 当中的 4 层负载均衡器，性能异常优秀。 根据<a href="https://www.lvtao.net/server/taobao-linux-kernel.html" target="_blank" rel="noopener">这篇文章</a>的介绍，使用调优后的内核，可以轻松处理每秒 10 万次以上的转发请求。目前在中大型互联网项目中，IPVS 被广泛的使用，用于承接网站入口处的流量。</p><h3 id="kubernetes-service">Kubernetes Service</h3><p><a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486130&amp;idx=1&amp;sn=41ee30f02113dac86398653f542a3c70&amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3&amp;token=1694324409&amp;lang=zh_CN#rd" target="_blank" rel="noopener">Service</a> 是 Kubernetes 的基础概念之一，它将一组 Pod 抽象成为一项服务，统一的对外提供服务，在各个 Pod 之间实现负载均衡。 Service 有多种类型，最基本的 ClusterIP 类型解决了集群内部访问服务的需求，NodePort 类型通过 Node 节点的端口暴露服务， 再配合上 LoadBalancer 类型所定义的负载均衡器，实现了流量经过前端负载均衡器分发到各个 Node 节点暴露出的端口， 再通过 IPtables进行一次负载均衡，最终分发到实际的 Pod 上这个过程。</p><p>在 Service 的 Spec 中，externalIPs 字段平常鲜有人提到，当把 IP 地址填入这个字段后，Kube-Proxy 会增加对应的 IPtables 规则，当有以对应 IP 为目标的流量发送到 Node节点时，IPtables将进行 NAT，将流量转发到对应的服务上。一般情况下，很少会遇到服务器接受非自身绑定 IP 流量的情况，所以 externalIPs 不常被使用，但配合网络层的其他工具，它可以实现给 Service 绑定外部 IP 的效果。</p><p>今天我们将使用 externalIPs 配合 IPVS 的 DR(Direct Routing )模式实现将外部流量引入到集群内部，同时实现负载均衡。</p><a id="more"></a><h3 id="环境搭建">环境搭建</h3><p>为了演示，我们搭建了 4 台服务器组成的集群。一台服务器运行 IPVS，扮演负载均衡器的作用。一台服务器运行 Kubernetes Master 组件，其他两台服务器作为 Node 加入到 Kubernetes 集群当中。搭建过程这里不详细介绍，大家可以参考相关文档，比如：「<a href="https://k8s-install.opsnull.com/" target="_blank" rel="noopener">和我一步步部署 kubernetes 集群</a>」。</p><p>所有服务器在 172.17.8.0/24 这个网段中，服务的 VIP 我们设定为 172.17.8.201。整体架构如下图所示：</p><p><img src="https://www.hi-linux.com/img/linux/ipvs-kubernetes.png" alt=""></p><p>接下来让我们来配置 IPVS 和 Kubernetes。</p><h4 id="使用-externalips-暴露-kubernetes-service">使用 externalIPs 暴露 Kubernetes Service</h4><p>首先在集群内部运行 2 个 nginx Pod 用作演示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run nginx --image=nginx --replicas=2</span><br></pre></td></tr></table></figure><p>再将它暴露为 Service，同时设定 externalIPs 字段</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl expose deployment nginx --port 80 --external-ip 172.17.8.201</span><br></pre></td></tr></table></figure><p>查看 IPtables 配置，确认对应的 IPtables 规则已经被加入。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo iptables -t nat -L KUBE-SERVICES -n</span><br><span class="line">Chain KUBE-SERVICES (2 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">KUBE-SVC-4N57TFCL4MD7ZTDA  tcp  --  0.0.0.0/0            10.3.0.156           /* default/nginx: cluster IP */ tcp dpt:80</span><br><span class="line">KUBE-MARK-MASQ  tcp  --  0.0.0.0/0            172.17.8.201         /* default/nginx: external IP */ tcp dpt:80</span><br><span class="line">KUBE-SVC-4N57TFCL4MD7ZTDA  tcp  --  0.0.0.0/0            172.17.8.201         /* default/nginx: external IP */ tcp dpt:80 PHYSDEV match ! --physdev-is-in ADDRTYPE match src-type !LOCAL</span><br><span class="line">KUBE-SVC-4N57TFCL4MD7ZTDA  tcp  --  0.0.0.0/0            172.17.8.201         /* default/nginx: external IP */ tcp dpt:80 ADDRTYPE match dst-type LOCAL</span><br><span class="line">KUBE-SVC-NPX46M4PTMTKRN6Y  tcp  --  0.0.0.0/0            10.3.0.1             /* default/kubernetes:https cluster IP */ tcp dpt:443</span><br><span class="line">KUBE-NODEPORTS  all  --  0.0.0.0/0            0.0.0.0/0            /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL</span><br></pre></td></tr></table></figure><h4 id="配置-ipvs-实现流量转发">配置 IPVS 实现流量转发</h4><p>首先在 IPVS 服务器上，打开 ipv4_forward。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sysctl -w net.ipv4.ip_forward=1</span><br></pre></td></tr></table></figure><p>接下来加载 IPVS 内核模块。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo modprobe ip_vs</span><br></pre></td></tr></table></figure><p>将 VIP 绑定在网卡上。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ifconfig eth0:0 172.17.8.201 netmask 255.255.255.0 broadcast 172.17.8.255</span><br></pre></td></tr></table></figure><p>再使用 ipvsadm 来配置 IPVS，这里我们直接使用 Docker 镜像，避免和特定发行版绑定。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --privileged -it --rm --net host luizbafilho/ipvsadm</span><br><span class="line">/ # ipvsadm</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">/ # ipvsadm -A -t 172.17.8.201:80</span><br><span class="line">/ # ipvsadm -a -t 172.17.8.201:80 -r 172.17.8.11:80 -g</span><br><span class="line">/ # ipvsadm -a -t 172.17.8.201:80 -r 172.17.8.12:80 -g</span><br><span class="line">/ # ipvsadm</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  172.17.8.201:http wlc</span><br><span class="line">  -&gt; 172.17.8.11:http             Route   1      0          0</span><br><span class="line">  -&gt; 172.17.8.12:http             Route   1      0          0</span><br></pre></td></tr></table></figure><p>可以看到，我们成功建立了从 VIP 到后端服务器的转发。</p><h4 id="验证转发效果">验证转发效果</h4><p>首先使用 curl 来测试是否能够正常访问 Nginx 服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://172.17.8.201</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;style&gt;</span><br><span class="line">    body &#123;</span><br><span class="line">        width: 35em;</span><br><span class="line">        margin: 0 auto;</span><br><span class="line">        font-family: Tahoma, Verdana, Arial, sans-serif;</span><br><span class="line">    &#125;</span><br><span class="line">&lt;/style&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;For online documentation and support please refer to</span><br><span class="line">&lt;a href=&quot;http://nginx.org/&quot;&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;</span><br><span class="line">Commercial support is available at</span><br><span class="line">&lt;a href=&quot;http://nginx.com/&quot;&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;</span><br><span class="line"></span><br><span class="line">&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>接下来在 172.17.8.11 上抓包来确认 IPVS 的工作情况。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tcpdump -i any port 80</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv for full protocol decode</span><br><span class="line">listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes</span><br><span class="line">04:09:07.503858 IP 172.17.8.1.51921 &gt; 172.17.8.201.http: Flags [S], seq 2747628840, win 65535, options [mss 1460,nop,wscale 5,nop,nop,TS val 1332071005 ecr 0,sackOK,eol], length 0</span><br><span class="line">04:09:07.504241 IP 10.2.0.1.51921 &gt; 10.2.0.3.http: Flags [S], seq 2747628840, win 65535, options [mss 1460,nop,wscale 5,nop,nop,TS val 1332071005 ecr 0,sackOK,eol], length 0</span><br><span class="line">04:09:07.504498 IP 10.2.0.1.51921 &gt; 10.2.0.3.http: Flags [S], seq 2747628840, win 65535, options [mss 1460,nop,wscale 5,nop,nop,TS val 1332071005 ecr 0,sackOK,eol], length 0</span><br><span class="line">04:09:07.504827 IP 10.2.0.3.http &gt; 10.2.0.1.51921: Flags [S.], seq 3762638044, ack 2747628841, win 28960, options [mss 1460,sackOK,TS val 153786592 ecr 1332071005,nop,wscale 7], length 0</span><br><span class="line">04:09:07.504827 IP 10.2.0.3.http &gt; 172.17.8.1.51921: Flags [S.], seq 3762638044, ack 2747628841, win 28960, options [mss 1460,sackOK,TS val 153786592 ecr 1332071005,nop,wscale 7], length 0</span><br><span class="line">04:09:07.504888 IP 172.17.8.201.http &gt; 172.17.8.1.51921: Flags [S.], seq 3762638044, ack 2747628841, win 28960, options [mss 1460,sackOK,TS val 153786592 ecr 1332071005,nop,wscale 7], length 0</span><br><span class="line">04:09:07.505599 IP 172.17.8.1.51921 &gt; 172.17.8.201.http: Flags [.], ack 1, win 4117, options [nop,nop,TS val 1332071007 ecr 153786592], length 0</span><br></pre></td></tr></table></figure><p>可以看到，由客户端 172.17.8.1 发送给 172.17.8.201 的封包，经过 IPVS 的中转发送给了 172.17.8.11 这台服务器，并经过 NAT 后发送给了 10.2.0.3 这个 Pod。返回的封包不经过 IPVS 服务器直接从 172.17.8.11 发送给了 172.17.8.1。 说明 IPVS 的 DR 模式工作正常。重复多次测试可以看到流量分别从 172.17.8.11 和 172.17.8.12 进入，再分发给不同的 Pod，说明负载均衡工作正常。</p><p>与传统的 IPVS DR 模式配置不同的是，我们并未在承接流量的服务器上执行绑定 VIP，再关闭 ARP 的操作。 那是因为对 VIP 的处理直接发生在 IPtables上，我们无需在服务器上运行程序来承接流量，IPtables 会将流量转发到对应的 Pod 上。</p><p>使用这种方法来承接流量，仅需要配置 externalIPs 为 VIP 即可，无需对服务器做任何特殊的设置，使用起来相当方便。</p><h3 id="总结">总结</h3><p>在本文中演示了使用 IPVS 配合 externalIPs 实现将外部流量导入到 Kubernetes 集群中，并实现负载均衡的方法。 希望可以帮助大家理解 IPVS 和 externalIPs 的工作原理，以便在恰当的场景下合理使用这两项技术解决问题。 实际部署时，还需要考虑后台服务器可用性检查，IPVS 节点主从备份，水平扩展等问题。在这里就不详细介绍了。</p><p>在 Kubernetes 中还有许多与 externalIPs 类似的非常用功能，有些甚至是使用 Annotation 来进行配置，将来有机会再进一步分享。</p><blockquote><p>来源：极术<br>原文：<a href="http://t.cn/RX8vzvC" target="_blank" rel="noopener">http://t.cn/RX8vzvC</a><br>题图：来自谷歌图片搜索<br>版权：本文版权归原作者所有</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;新搭建的 Kubernetes 集群如何承接外部访问的流量，是刚上手 Kubernetes 时常常会遇到的问题。 在公有云上，官方给出了比较直接的答案，使用 LoadBalancer 类型的 Service，利用公有云提供的负载均衡服务来承接流量，同时在多台服务器之间进行负载均衡。&lt;/p&gt;
&lt;p&gt;而在私有环境中，如何正确的将外部流量引入到集群内部，却暂时没有标准的做法。 本文将介绍一种基于 IPVS 来承接流量并实现负载均衡的方法，供大家参考。在阅读本文前建议先了解文中相关基础知识点，推荐阅读下「&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247486130&amp;amp;idx=1&amp;amp;sn=41ee30f02113dac86398653f542a3c70&amp;amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3&amp;amp;token=1694324409&amp;amp;lang=zh_CN#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;浅析从外部访问 Kubernetes 集群中应用的几种方式&lt;/a&gt;」一文。&lt;/p&gt;
&lt;h3 id=&quot;ipvs&quot;&gt;IPVS&lt;/h3&gt;
&lt;p&gt;IPVS 是 LVS 项目的一部分，是一款运行在 Linux kernel 当中的 4 层负载均衡器，性能异常优秀。 根据&lt;a href=&quot;https://www.lvtao.net/server/taobao-linux-kernel.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这篇文章&lt;/a&gt;的介绍，使用调优后的内核，可以轻松处理每秒 10 万次以上的转发请求。目前在中大型互联网项目中，IPVS 被广泛的使用，用于承接网站入口处的流量。&lt;/p&gt;
&lt;h3 id=&quot;kubernetes-service&quot;&gt;Kubernetes Service&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247486130&amp;amp;idx=1&amp;amp;sn=41ee30f02113dac86398653f542a3c70&amp;amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3&amp;amp;token=1694324409&amp;amp;lang=zh_CN#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Service&lt;/a&gt; 是 Kubernetes 的基础概念之一，它将一组 Pod 抽象成为一项服务，统一的对外提供服务，在各个 Pod 之间实现负载均衡。 Service 有多种类型，最基本的 ClusterIP 类型解决了集群内部访问服务的需求，NodePort 类型通过 Node 节点的端口暴露服务， 再配合上 LoadBalancer 类型所定义的负载均衡器，实现了流量经过前端负载均衡器分发到各个 Node 节点暴露出的端口， 再通过 IPtables进行一次负载均衡，最终分发到实际的 Pod 上这个过程。&lt;/p&gt;
&lt;p&gt;在 Service 的 Spec 中，externalIPs 字段平常鲜有人提到，当把 IP 地址填入这个字段后，Kube-Proxy 会增加对应的 IPtables 规则，当有以对应 IP 为目标的流量发送到 Node节点时，IPtables将进行 NAT，将流量转发到对应的服务上。一般情况下，很少会遇到服务器接受非自身绑定 IP 流量的情况，所以 externalIPs 不常被使用，但配合网络层的其他工具，它可以实现给 Service 绑定外部 IP 的效果。&lt;/p&gt;
&lt;p&gt;今天我们将使用 externalIPs 配合 IPVS 的 DR(Direct Routing )模式实现将外部流量引入到集群内部，同时实现负载均衡。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>浅谈 Kubernetes 数据持久化方案</title>
    <link href="https://www.hi-linux.com/posts/14136.html"/>
    <id>https://www.hi-linux.com/posts/14136.html</id>
    <published>2018-08-25T01:00:00.000Z</published>
    <updated>2018-08-24T06:10:17.544Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kubernetes-volume-相关概念">Kubernetes Volume 相关概念</h3><p>缺省情况下，一个运行中的容器对文件系统的写入都是发生在其分层文件系统的可写层。一旦容器运行结束，所有写入都会被丢弃。如果数据需要长期存储，那就需要对容器数据做持久化支持。</p><p>Kubernetes 和 Docker 类似，也是通过 Volume 的方式提供对存储的支持。Volume 被定义在 Pod 上，可以被 Pod 里的多个容器挂载到相同或不同的路径下。Kubernetes 中 Volume 的 概念与Docker 中的 Volume 类似，但不完全相同。具体区别如下：</p><ul><li>Kubernetes 中的 Volume 与 Pod 的生命周期相同，但与容器的生命周期不相关。当容器终止或重启时，Volume 中的数据也不会丢失。</li><li>当 Pod 被删除时，Volume 才会被清理。并且数据是否丢失取决于 Volume 的具体类型，比如：emptyDir 类型的 Volume 数据会丢失，而 PV 类型的数据则不会丢失。</li></ul><p>Volume 的核心是目录，可以通过 Pod 中的容器来访问。该目录是如何形成的、支持该目录的介质以及其内容取决于所使用的特定卷类型。要使用 Volume，需要为 Pod 指定为 Volume (<code>spec.volumes</code> 字段) 以及将它挂载到容器的位置 (<code>spec.containers.volumeMounts</code> 字段)。Kubernetes 支持多种类型的卷，一个 Pod 可以同时使用多种类型的 Volume。</p><p>容器中的进程看到的是由其 Docker 镜像和 Volume 组成的文件系统视图。 Docker 镜像位于文件系统层次结构的根目录，任何 Volume 都被挂载在镜像的指定路径中。Volume 无法挂载到其他 Volume 上或与其他 Volume 的硬连接。Pod 中的每个容器都必须独立指定每个 Volume 的挂载位置。</p><p>Kubernetes 目前支持多种 Volume 类型，大致如下：</p><ul><li>awsElasticBlockStore</li><li>azureDisk</li><li>azureFile</li><li>cephfs</li><li>csi</li><li>downwardAPI</li><li>emptyDir</li><li>fc (fibre channel)</li><li>flocker</li><li>gcePersistentDisk</li><li>gitRepo</li><li>glusterfs</li><li>hostPath</li><li>iscsi</li><li>local</li><li>nfs</li><li>persistentVolumeClaim</li><li>projected</li><li>portworxVolume</li><li>quobyte</li><li>rbd</li><li>scaleIO</li><li>secret</li><li>storageos</li><li>vsphereVolume</li></ul><blockquote><p>注：这些 Volume 并非全部都是持久化的，比如: emptyDir、secret、gitRepo 等，就会随着 Pod 的消亡而消失。</p></blockquote><a id="more"></a><h3 id="kubernetes-非持久化存储方式">Kubernetes 非持久化存储方式</h3><p>下面我们对一些常见的 Volume 做一个基本的介绍。</p><h4 id="emptrydir">emptryDir</h4><p>emptryDir，顾名思义是一个空目录，它的生命周期和所属的 Pod 是完全一致的。emptyDir 类型的 Volume 在 Pod 分配到 Node 上时会被创建，Kubernetes 会在 Node 上自动分配一个目录，因此无需指定 Node 宿主机上对应的目录文件。这个目录的初始内容为空，当 Pod 从 Node 上移除（Pod 被删除或者 Pod 发生迁移）时，emptyDir 中的数据会被永久删除。</p><p>emptyDir Volume 主要用于某些应用程序无需永久保存的临时目录，在多个容器之间共享数据等。缺省情况下，emptryDir 是使用主机磁盘进行存储的。你也可以使用其它介质作为存储，比如：网络存储、内存等。设置 <code>emptyDir.medium</code> 字段的值为 Memory 就可以使用内存进行存储，使用内存做为存储可以提高整体速度，但是要注意一旦机器重启，内容就会被清空，并且也会受到容器内存的限制。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">test-pd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - image:</span> <span class="string">gcr.io/google_containers/test-webserver</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">test-container</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - mountPath:</span> <span class="string">/cache</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">cache-volume</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">cache-volume</span></span><br><span class="line"><span class="attr">    emptyDir:</span> <span class="string">&#123;&#125;</span></span><br></pre></td></tr></table></figure><h4 id="hostpath">hostPath</h4><p>hostPath 类型的 Volume 允许用户挂载 Node 宿主机上的文件或目录到 Pod 中。大多数 Pod 都用不到这种 Volume，其缺点比较明显，比如：</p><ul><li>由于每个节点上的文件都不同，具有相同配置（例如：从 podTemplate 创建的）的 Pod 在不同节点上的行为可能会有所不同。</li><li>在底层主机上创建的文件或目录只能由 root 写入。您需要在特权容器中以 root 身份运行进程，或修改主机上的文件权限才可以写入 hostPath 卷。</li></ul><p>当然，存在即合理。这种类型的 Volume 主要用在以下场景中：</p><ul><li>运行中的容器需要访问 Docker 内部的容器，使用 /var/lib/docker 来做为 hostPath 让容器内应用可以直接访问 Docker 的文件系统。</li><li>在容器中运行 cAdvisor，使用 /dev/cgroups 来做为 hostPath。</li><li>和 DaemonSet 搭配使用，用来操作主机文件。例如：日志采集方案 FLK 中的 FluentD 就采用这种方式来加载主机的容器日志目录，达到收集本主机所有日志的目的。</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">test-pd</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">  - image:</span> <span class="string">k8s.gcr.io/test-webserver</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">test-container</span></span><br><span class="line"><span class="attr">    volumeMounts:</span></span><br><span class="line"><span class="attr">    - mountPath:</span> <span class="string">/test-pd</span></span><br><span class="line"><span class="attr">      name:</span> <span class="string">test-volume</span></span><br><span class="line"><span class="attr">  volumes:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">test-volume</span></span><br><span class="line"><span class="attr">    hostPath:</span></span><br><span class="line">      <span class="comment"># directory location on host</span></span><br><span class="line"><span class="attr">      path:</span> <span class="string">/data</span></span><br><span class="line">      <span class="comment"># this field is optional</span></span><br><span class="line"><span class="attr">      type:</span> <span class="string">Directory</span></span><br></pre></td></tr></table></figure><h3 id="kubernetes-持久化存储方式">Kubernetes 持久化存储方式</h3><p>Kubernetes 目前可以使用 PersistentVolume、PersistentVolumeClaim、StorageClass 三种 API 资源来进行持久化存储，下面分别介绍下各种资源的概念。</p><h4 id="pv">PV</h4><p>PV 的全称是：PersistentVolume（持久化卷）。PersistentVolume 是 Volume 的一种类型，是对底层的共享存储的一种抽象。PV 由集群管理员进行创建和配置，就像节点 (Node) 是集群中的资源一样，PV 也是集群资源的一种。PV 包含存储类型、存储大小和访问模式。PV 的生命周期独立于 Pod，例如：当使用它的 Pod 销毁时对 PV 没有影响。</p><p>PersistentVolume 通过插件机制实现与共享存储的对接。Kubernetes 目前支持以下插件类型：</p><ul><li>GCEPersistentDisk</li><li>AWSElasticBlockStore</li><li>AzureFile</li><li>AzureDisk</li><li>FC (Fibre Channel)</li><li>FlexVolume</li><li>Flocker</li><li>NFS</li><li>iSCSI</li><li>RBD (Ceph Block Device)</li><li>CephFS</li><li>Cinder (OpenStack block storage)</li><li>Glusterfs</li><li>VsphereVolume</li><li>Quobyte Volumes</li><li>HostPath</li><li>VMware Photon</li><li>Portworx Volumes</li><li>ScaleIO Volumes</li><li>StorageOS</li></ul><h4 id="pvc">PVC</h4><p>PVC 的全称是：PersistentVolumeClaim（持久化卷声明），PVC 是用户对存储资源的一种请求。PVC 和 Pod 比较类似，Pod 消耗的是节点资源，PVC 消耗的是 PV 资源。Pod 可以请求 CPU 和内存，而 PVC 可以请求特定的存储空间和访问模式。对于真正使用存储的用户不需要关心底层的存储实现细节，只需要直接使用 PVC 即可。</p><h4 id="storageclass">StorageClass</h4><p>由于不同的应用程序对于存储性能的要求也不尽相同，比如：读写速度、并发性能、存储大小等。如果只能通过 PVC 对 PV 进行静态申请，显然这并不能满足任何应用对于存储的各种需求。为了解决这一问题，Kubernetes 引入了一个新的资源对象：StorageClass，通过 StorageClass 的定义，集群管理员可以先将存储资源定义为不同类型的资源，比如快速存储、慢速存储等。</p><p>当用户通过 PVC 对存储资源进行申请时，StorageClass 会使用 Provisioner（不同 Volume 对应不同的 Provisioner）来自动创建用户所需 PV。这样应用就可以随时申请到合适的存储资源，而不用担心集群管理员没有事先分配好需要的 PV。</p><ul><li>自动创建的 PV 以 <code>${namespace}-${pvcName}-${pvName}</code> 这样的命名格式创建在后端存储服务器上的共享数据目录中。</li><li>自动创建的 PV 被回收后会以 <code>archieved-${namespace}-${pvcName}-${pvName}</code> 这样的命名格式存在后端存储服务器上。</li></ul><h3 id="kubernetes-访问存储资源的方式">Kubernetes 访问存储资源的方式</h3><p>Kubernetes 目前可以使用三种方式来访问存储资源。</p><ul><li>直接访问</li></ul><p>该种方式移植性比较差，可扩展能力差。把 Volume 的基本信息完全暴露给用户，有安全隐患。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-pv01.png" alt=""></p><ul><li>静态 PV</li></ul><p>集群管理员提前手动创建一些 PV。它们带有可供集群用户使用的实际存储的细节，之后便可用于 PVC 消费。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-pv02.png" alt=""></p><blockquote><p>注：这种方式请求的 PVC 必须要与管理员创建的 PV 保持一致，如：存储大小和访问模式，否则不能将 PVC 绑定到 PV 上。</p></blockquote><ul><li>动态 PV</li></ul><p>当集群管理员创建的静态 PV 都不匹配用户的 PVC 时，PVC 请求存储类 StorageClass，StorageClass 动态的为 PVC 创建所需的 PV。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-pv03.png" alt=""></p><blockquote><p>注：此功能需要基于 StorageClass。集群管理员必须先创建并配置好请求的 StorageClass，只有请求的 StorageClass 存在的情况下才能进行动态的创建。</p></blockquote><h3 id="使用-pv-进行持久化存储实例">使用 PV 进行持久化存储实例</h3><p>这里我们将介绍如何使用 PV 资源进行数据持久化，这也是本文的重点内容。我们将以 NFS 做为后端存储结合 PV 为例，讲解 Kubernetes 如何实现数据持久化。</p><h4 id="部署-nfs-服务器">部署 NFS 服务器</h4><h5 id="安装-nfs-服务端">安装 NFS 服务端</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Ubuntu / Debian</span><br><span class="line">$ sudo apt install nfs-kernel-server</span><br></pre></td></tr></table></figure><h5 id="新建数据目录和设置目录权限">新建数据目录和设置目录权限</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir -p /data/kubernetes/</span><br><span class="line">$ sudo chmod 755 /data/kubernetes/</span><br></pre></td></tr></table></figure><h5 id="配置-nfs-服务端">配置 NFS 服务端</h5><p>NFS 的默认配置文件是 <code>/etc/exports</code> ，在该配置文件中添加下面的配置信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/exports</span><br><span class="line">/data/kubernetes  *(rw,sync,no_root_squash)</span><br></pre></td></tr></table></figure><p>配置文件说明：</p><ul><li>/data/kubernetes  设置共享的数据的目录。</li><li><code>*</code> 表示任何人都有权限连接，当然也可以设置成是一个网段、一个 IP、或者是域名。</li><li>rw 设置共享目录的读写权限。</li><li>sync 表示文件同时写入硬盘和内存。</li><li>no_root_squash 当登录 NFS 主机使用共享目录的使用者是 root 时，其权限将被转换成为匿名使用者，通常它的 UID 与 GID，都会变成 nobody 身份。</li></ul><h5 id="启动-nfs-服务端">启动 NFS 服务端</h5><ul><li>启动 NFS 服务端</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl restart nfs-kernel-server</span><br></pre></td></tr></table></figure><ul><li>验证 NFS 服务端是否正常启动</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo rpcinfo -p|grep nfs</span><br><span class="line">100003    3   tcp   2049  nfs</span><br><span class="line">100003    4   tcp   2049  nfs</span><br><span class="line">100003    3   udp   2049  nfs</span><br></pre></td></tr></table></figure><ul><li>查看具体目录挂载权限</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cat /var/lib/nfs/etab</span><br><span class="line">/data/kubernetes*(rw,sync,wdelay,hide,nocrossmnt,secure,no_root_squash,no_all_squash,no_subtree_check,secure_locks,acl,no_pnfs,anonuid=65534,anongid=65534,sec=sys,rw,secure,no_root_squash,no_all_squash)</span><br></pre></td></tr></table></figure><p>如果以上步骤都正常的话，到这里 NFS 服务端就已经正常安装完成。</p><h5 id="安装-nfs-客户端">安装 NFS 客户端</h5><ul><li>安装 NFS 客户端</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install nfs-common</span><br></pre></td></tr></table></figure><blockquote><p>注：所有 Node 宿主机都需要安装 NFS 客户端。</p></blockquote><ul><li>验证 RPC 服务状态</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo systemctl status rpcbind.service</span><br><span class="line">● rpcbind.service - RPC bind portmap service</span><br><span class="line">   Loaded: loaded (/lib/systemd/system/rpcbind.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Tue 2018-08-07 09:54:29 CST; 49s ago</span><br><span class="line">     Docs: man:rpcbind(8)</span><br><span class="line"> Main PID: 17501 (rpcbind)</span><br><span class="line">    Tasks: 1 (limit: 2313)</span><br><span class="line">   CGroup: /system.slice/rpcbind.service</span><br><span class="line">           └─17501 /sbin/rpcbind -f -w</span><br></pre></td></tr></table></figure><ul><li>检查 NFS 服务端可用的共享目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo showmount -e 192.168.100.213</span><br><span class="line">Export list for 192.168.100.213:</span><br><span class="line">/data/kubernetes *</span><br></pre></td></tr></table></figure><ul><li>挂载 NFS 共享目录到本地</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo mkdir -p /data/kubernetes/</span><br><span class="line">$ sudo mount -t nfs 192.168.100.213:/data/kubernetes/ /data/kubernetes/</span><br></pre></td></tr></table></figure><ul><li>验证 NFS 客户端</li></ul><p>挂载成功后，在客户端上面的目录中新建一个文件，然后检查在 NFS 服务端的共享目录下是否也会出现该文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 在 NFS 客户端新建</span><br><span class="line">$ sudo touch /data/kubernetes/test.txt</span><br><span class="line"></span><br><span class="line"># 在 NFS 服务端查看</span><br><span class="line">$ sudo ls -ls /data/kubernetes/</span><br><span class="line">total 0</span><br><span class="line">0 -rw-r--r-- 1 root root 0 Aug  7 09:59 test.txt</span><br></pre></td></tr></table></figure><h4 id="实现静态-pv">实现静态 PV</h4><h5 id="新建-pv-资源">新建 PV 资源</h5><p>完成上面的共享存储后，我们就可以来使用 PV 和 PVC 来管理和使用这些共享存储。PV 作为存储资源主要包括存储能力、访问模式、存储类型、回收策略等关键信息。</p><p>下面我们来新建一个 PV 对象并使用 NFS 做为后端存储类型，该 PV 包括 1G 的存储空间、访问模式为 ReadWriteOnce、回收策略为 Recyle。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">pv1-nfs.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span>  <span class="string">pv1-nfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"><span class="attr">  nfs:</span></span><br><span class="line"><span class="attr">    path:</span> <span class="string">/data/kubernetes</span></span><br><span class="line"><span class="attr">    server:</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.213</span></span><br></pre></td></tr></table></figure><blockquote><p>注：Kubernetes 支持的 PV 类型有很多，比如常见的 Ceph、GlusterFs、NFS 等。更多的支持类型可以查看<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/" target="_blank" rel="noopener">官方文档</a>。</p></blockquote><p>我们先使用 Kubectl 创建该 PV 资源。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f pv1-nfs.yaml</span><br><span class="line">persistentvolume &quot;pv1-nfs&quot; created</span><br></pre></td></tr></table></figure><p>从下面的结果，我们可以看到 pv1-nfs 已经创建成功。状态是 Available，这表示 pv1-nfs 准备就绪，可以被 PVC 申请。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Available                                      46s</span><br></pre></td></tr></table></figure><p>我们对上面的 PV 属性来做一个简单的解读。</p><p><strong>Capacity（存储能力）</strong></p><p>一般来说，一个 PV 对象都要指定一个存储能力，通过 PV 的 Capacity 属性来设置。这里的 storage=1Gi 表示设置存储空间的大小。</p><p><strong>AccessModes（访问模式）</strong></p><p>AccessModes 是用来对 PV 进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：</p><ul><li>ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载。</li><li>ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载。</li><li>ReadWriteMany（RWX）：读写权限，可以被多个节点挂载。</li></ul><blockquote><p>注：一些 PV 可能支持多种访问模式，但是在挂载的时候只能使用一种访问模式，多种访问模式是不会生效的。</p></blockquote><p>下图是一些常用的 Volume 插件支持的访问模式：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-pv04.png" alt=""></p><p><strong>PersistentVolumeReclaimPolicy（回收策略）</strong></p><p>当前 PV 设置的回收策略，我们这里指定的 PV 的回收策略为 Recycle。目前 PV 支持的策略有三种：</p><ul><li>Retain（保留）- 保留数据，需要管理员手工清理数据。</li><li>Recycle（回收）- 清除 PV 中的数据，效果相当于执行 <code>rm -rf /thevoluem/*</code> 。</li><li>Delete（删除）- 与 PV 相连的后端存储完成 Volume 的删除操作，这种方式常见于云服务商的存储服务，比如 ASW EBS。</li></ul><blockquote><p>注：目前只有 NFS 和 HostPath 两种类型支持回收策略。设置为 Retain 这种策略会更加保险一些。</p></blockquote><p><strong>状态</strong></p><p>一个 PV 的生命周期中，可能会处于 4 种不同的阶段。</p><ul><li>Available（可用）：表示可用状态，还未被任何 PVC 绑定。</li><li>Bound（已绑定）：表示 PV 已经被 PVC 绑定。</li><li>Released（已释放）：PVC 被删除，但是资源还未被集群重新声明。</li><li>Failed（失败）： 表示该 PV 的自动回收失败。</li></ul><h5 id="新建-pvc-资源">新建 PVC 资源</h5><p>我们平时真正使用的资源其实是 PVC，就类似于我们的服务是通过 Pod 来运行的，而不是 Node，只是 Pod 跑在 Node 上而已。</p><p>首先，我们新建一个数据卷声明，向 PV 请求 1Gi 的存储容量。其访问模式设置为 ReadWriteOnce。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">pvc1-nfs.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pvc1-nfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">1</span><span class="string">Gi</span></span><br></pre></td></tr></table></figure><p>在新建 PVC 之前，我们可以看下之前创建的 PV 的状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Available                                      28m</span><br></pre></td></tr></table></figure><p>我们可以看到当前 pv1-nfs 是在 Available 的一个状态，所以这个时候我们的 PVC 可以和这个 PV 进行绑定。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f pvc1-nfs.yaml</span><br><span class="line">persistentvolumeclaim &quot;pvc1-nfs&quot; created</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1-nfs   Bound     pv1-nfs   1Gi        RWO                           34s</span><br></pre></td></tr></table></figure><p>从上面的结果可以看到 pvc1-nfs 创建成功了，并且状态是 Bound 状态。这个时候我们再看下 PV 的状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM              STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Bound     default/pvc1-nfs                            31m</span><br></pre></td></tr></table></figure><p>同样我们可以看到 PV 也是 Bound 状态，对应的声明是 default/pvc1-nfs，表示 default 命名空间下面的 pvc1-nfs，表示我们刚刚新建的 pvc1-nfs 和 pv1-nfs 绑定成功。</p><p>PV 和 PVC 的绑定是系统自动完成的，不需要显示指定要绑定的 PV。系统会根据 PVC 中定义的要求去查找处于 Available 状态的 PV。</p><ul><li>如果找到合适的 PV 就完成绑定。</li><li>如果没有找到合适的 PV 那么 PVC 就会一直处于 Pending 状态，直到找到合适的 PV 完成绑定为止。</li></ul><p>下面我们来看一个例子，这里声明一个 PVC 的对象，它要求 PV 的访问模式是 ReadWriteOnce、存储容量 2Gi 和标签值为 app=nfs。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">pvc2-nfs.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pvc2-nfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  resources:</span></span><br><span class="line"><span class="attr">    requests:</span></span><br><span class="line"><span class="attr">      storage:</span> <span class="number">2</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">nfs</span></span><br></pre></td></tr></table></figure><p>我们先查看下当前系统的所有 PV 资源。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM              STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Bound     default/pvc1-nfs                            52m</span><br></pre></td></tr></table></figure><p>从结果可以看到，目前所有 PV 都是 Bound 状态，并没有 Available 状态的 PV。所以我们现在用上面新建的 PVC 是无法匹配到合适的 PV 的。我们来创建 PVC 看看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f pvc2-nfs.yaml</span><br><span class="line">persistentvolumeclaim &quot;pvc2-nfs&quot; created</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1-nfs   Bound     pv1-nfs   1Gi        RWO                           28m</span><br><span class="line">pvc2-nfs   Pending                                                      16s</span><br></pre></td></tr></table></figure><p>从结果我们可以看到 pvc2-nfs 当前就是 Pending 状态，因为并没有合适的 PV 给这个 PVC 使用。现在我们来新建一个合适该 PVC 使用的 PV。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">pv2-nfs.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">pv2-nfs</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">nfs</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    storage:</span> <span class="number">2</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">  accessModes:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">ReadWriteOnce</span></span><br><span class="line"><span class="attr">  persistentVolumeReclaimPolicy:</span> <span class="string">Recycle</span></span><br><span class="line"><span class="attr">  nfs:</span></span><br><span class="line"><span class="attr">    server:</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.213</span></span><br><span class="line"><span class="attr">    path:</span> <span class="string">/data/kubernetes</span></span><br></pre></td></tr></table></figure><p>使用 Kubectl 创建该 PV。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f pv2-nfs.yaml</span><br><span class="line">persistentvolume &quot;pv2-nfs&quot; created</span><br><span class="line"></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM              STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Bound     default/pvc1-nfs                            1h</span><br><span class="line">pv2-nfs   2Gi        RWO            Recycle          Bound     default/pvc2-nfs                            18s</span><br></pre></td></tr></table></figure><p>创建完 pv2-nfs 后，从上面的结果你会发现该 PV 已经是 Bound 状态了。其对应的 PVC 是 default/pvc2-nfs，这就证明 pvc2-nfs 终于找到合适的 PV 且完成了绑定。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1-nfs   Bound     pv1-nfs   1Gi        RWO                           36m</span><br><span class="line">pvc2-nfs   Bound     pv2-nfs   2Gi        RWO                           9m</span><br></pre></td></tr></table></figure><blockquote><p>注：如果 PVC 申请的容量大小小于 PV 提供的大小，PV 同样会分配该 PV 所有容量给 PVC，如果 PVC 申请的容量大小大于 PV 提供的大小，此次申请就会绑定失败。</p></blockquote><h5 id="使用-pvc-资源">使用 PVC 资源</h5><p>这里我们已经完成了 PV 和 PVC 创建，现在我们就可以使用这个 PVC 了。这里我们使用 Nginx 的镜像来创建一个 Deployment，将容器的 <code>/usr/share/nginx/html</code> 目录通过 Volume 挂载到名为 pvc2-nfs 的 PVC 上，并通过 NodePort 类型的 Service 来暴露服务。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">nfs-pvc-deploy.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nfs-pvc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">3</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">nfs-pvc</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">        image:</span> <span class="attr">nginx:1.7.9</span></span><br><span class="line"><span class="attr">        imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line"><span class="attr">        ports:</span></span><br><span class="line"><span class="attr">        - containerPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">          name:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">        volumeMounts:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">www</span></span><br><span class="line"><span class="attr">          mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">www</span></span><br><span class="line"><span class="attr">        persistentVolumeClaim:</span></span><br><span class="line"><span class="attr">          claimName:</span> <span class="string">pvc2-nfs</span></span><br><span class="line"></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nfs-pvc</span></span><br><span class="line"><span class="attr">  labels:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">nfs-pvc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">NodePort</span></span><br><span class="line"><span class="attr">  ports:</span></span><br><span class="line"><span class="attr">  - port:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    targetPort:</span> <span class="string">web</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    app:</span> <span class="string">nfs-pvc</span></span><br></pre></td></tr></table></figure><p>使用 Kubectl 创建这个 Deployment。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nfs-pvc-deploy.yaml</span><br><span class="line">deployment.extensions &quot;nfs-pvc&quot; created</span><br><span class="line">service &quot;nfs-pvc&quot; created</span><br><span class="line"></span><br><span class="line">$ kubectl get pods -o wide|grep nfs-pvc</span><br><span class="line">nfs-pvc-789788587b-ctp58                        1/1       Running     0          4m        172.30.24.6   dev-node-02</span><br><span class="line">nfs-pvc-789788587b-q294p                        1/1       Running     0          4m        172.30.92.6   dev-node-03</span><br><span class="line">nfs-pvc-789788587b-rtl5s                        1/1       Running     0          4m        172.30.87.9   dev-node-01</span><br><span class="line"></span><br><span class="line">$ kubectl get svc|grep nfs-pvc</span><br><span class="line">nfs-pvc                         NodePort       10.254.5.24      &lt;none&gt;                                            80:8682/TCP                5m</span><br></pre></td></tr></table></figure><p>通过 NodePort 访问该服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ curl -I http://192.168.100.211:8682</span><br><span class="line">HTTP/1.1 403 Forbidden</span><br><span class="line">Server: nginx/1.7.9</span><br><span class="line">Date: Tue, 07 Aug 2018 03:42:30 GMT</span><br><span class="line">Content-Type: text/html</span><br><span class="line">Content-Length: 168</span><br><span class="line">Connection: keep-alive</span><br></pre></td></tr></table></figure><p>我们可以看到 Nginx 返回了 403，这是因为我们用 NFS 中的共享目录做为 Nginx 的默认站点目录，目前这个 NFS 共享目录中没有可用的 index.html 文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls  /data/kubernetes/</span><br><span class="line">test.txt</span><br></pre></td></tr></table></figure><p>在 NFS 服务端共享目录下新建一个 index.html 的文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sh -c &quot;echo &apos;&lt;h1&gt;Hello Kubernetes~&lt;/h1&gt;&apos; &gt; /data/kubernetes/index.html&quot;</span><br><span class="line">$ ls /data/kubernetes/</span><br><span class="line">index.html  test.txt</span><br></pre></td></tr></table></figure><p>再次通过 NodePort 访问该服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl  http://192.168.100.211:8682</span><br><span class="line">&lt;h1&gt;Hello Kubernetes~&lt;/h1&gt;</span><br></pre></td></tr></table></figure><h6 id="使用-subpath-对同一个-pv-进行隔离">使用 subPath 对同一个 PV 进行隔离</h6><p>从上面的例子中，我们可以看到容器中的数据是直接放到共享数据目录根目录下的。如果有多个容器都使用一个 PVC 的话，这样就很容易造成文件冲突。Pod 中 <code>volumeMounts.subPath</code> 属性可用于指定引用卷内的路径，只需设置该属性就可以解决该问题。</p><p>修改刚才创建 Deployment 的 YAML 文件，增加 subPath 行。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">nfs-pvc-deploy.yaml</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">volumeMounts:</span></span><br><span class="line"><span class="attr">- name:</span> <span class="string">www</span></span><br><span class="line"><span class="attr">  subPath:</span> <span class="string">nginx-pvc-test</span></span><br><span class="line"><span class="attr">  mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure><p>更改完 YAML 文件后，重新更新下 Deployment 即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl apply -f nfs-pvc-deploy.yaml</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">deployment.extensions &quot;nfs-pvc&quot; configured</span><br><span class="line">Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl apply</span><br><span class="line">service &quot;nfs-pvc&quot; configured</span><br></pre></td></tr></table></figure><p>更新完后，NFS 的数据共享目录下就会自动新增一个同 subPath 名字一样的目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls  /data/kubernetes/</span><br><span class="line">index.html      nginx-pvc-test/ test.txt</span><br></pre></td></tr></table></figure><p>同样 nginx-pvc-test 目录下默认是空的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ls /data/kubernetes/nginx-pvc-test/</span><br></pre></td></tr></table></figure><p>新增一个 index 文件后访问该服务，一切安好。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sh -c &quot;echo &apos;&lt;h1&gt;Hello Kubernetes~&lt;/h1&gt;&apos; &gt; /data/kubernetes/nginx-pvc-test/index.html&quot;</span><br><span class="line">$ curl  http://192.168.100.211:8682</span><br><span class="line">&lt;h1&gt;Hello Kubernetes~&lt;/h1&gt;</span><br></pre></td></tr></table></figure><h5 id="验证-pvc-中的数据持久化">验证 PVC 中的数据持久化</h5><p>上面我们已经成功的在 Pod 中使用了 PVC 来做为存储，现在我们来验证下数据是否会丢失。我们分两种情况来验证：一种是直接删除 Deployment 和 Service，另一种是先删除 PVC 后再删除 Deployment 和 Service。</p><h6 id="直接删除-deployment-和-service">直接删除 Deployment 和 Service</h6><p>在这种情况下数据会永久保存下来，删除 Deployment 和 Service 不会对数据造成任何影响。</p><ul><li>删除 Deployment 和 Service</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete -f nfs-pvc-deploy.yaml</span><br><span class="line">service &quot;nfs-pvc&quot; deleted</span><br><span class="line">deployment.extensions &quot;nfs-pvc&quot; deleted</span><br></pre></td></tr></table></figure><ul><li>查看数据共享目录下面的数据</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls /data/kubernetes/nginx-pvc-test/</span><br><span class="line">index.html</span><br></pre></td></tr></table></figure><h6 id="先删除-pvc-后再删除-deployment-和-service">先删除 PVC 后再删除 Deployment 和 Service</h6><ul><li>删除 PVC</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete pvc pvc2-nfs</span><br><span class="line">persistentvolumeclaim &quot;pvc2-nfs&quot; deleted</span><br></pre></td></tr></table></figure><p>我们可以看到 PVC 状态已经变成了 Terminating，但是现在数据共享目录中的文件和服务都是可以正常访问的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS        VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1-nfs   Bound         pv1-nfs   1Gi        RWO                           3h</span><br><span class="line">pvc2-nfs   Terminating   pv2-nfs   2Gi        RWO                           6m</span><br><span class="line"></span><br><span class="line">$ ls /data/kubernetes/nginx-pvc-test/</span><br><span class="line">index.html</span><br><span class="line"></span><br><span class="line">$ curl  http://192.168.100.211:8928</span><br><span class="line">&lt;h1&gt;Hello Kubernetes~&lt;/h1&gt;</span><br></pre></td></tr></table></figure><p>这是因为还有 Pod 正在使用 pvc2-nfs 这个 PVC，那么对应的资源依然可用。如果无 Pod 继续使用 pvc2-nfs 这个 PVC，则相应 PVC 对应的资源就会被收回。</p><ul><li>删除 Deployment 和 Service</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl delete -f nfs-pvc-deploy.yaml</span><br><span class="line">deployment.extensions &quot;nfs-pvc&quot; deleted</span><br><span class="line">service &quot;nfs-pvc&quot; deleted</span><br><span class="line"></span><br><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">pvc1-nfs   Bound     pv1-nfs   1Gi        RWO                           3h</span><br><span class="line"></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM              STORAGECLASS   REASON    AGE</span><br><span class="line">pv1-nfs   1Gi        RWO            Recycle          Bound       default/pvc1-nfs                            4h</span><br><span class="line">pv2-nfs   2Gi        RWO            Recycle          Available                                               25m</span><br><span class="line"></span><br><span class="line">$ ls /data/kubernetes/</span><br></pre></td></tr></table></figure><p>从上面的结果我们可以看到 pvc2-nfs 这个 PVC 已经不存在了，pv2-nfs 这个 PV 的状态也变成 Available 了。由于我们设置的 PV 的回收策略是 Recycle，我们可以发现 NFS 的共享数据目录下面的数据也没了，这是因为我们把 PVC 给删除掉后回收了数据。</p><h4 id="使用-storageclass-实现动态-pv">使用 StorageClass 实现动态 PV</h4><p>上面的例子中我们学习了静态 PV 和 PVC 的使用方法，所谓静态 PV 就是我要使用的一个 PVC 的话就必须手动去创建一个 PV。</p><p>这种方式在很多使用场景下使用起来都不灵活，需要依赖集群管理员事先完成 PV 的建立。特别是对于 StatefulSet 类型的应用，简单的使用静态的 PV 就不是很合适了。这种情况下我们就需要用到动态 PV，动态 PV 的实现需要用到 StorageClass。</p><h5 id="创建-provisioner">创建 Provisioner</h5><p>要使用 StorageClass，我们就得安装对应的自动配置程序。比如：我们这里存储后端使用的是 NFS，那么我们就需要使用到一个对应的自动配置程序。支持 NFS 的自动配置程序就是 <a href="https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client" target="_blank" rel="noopener">nfs-client</a>，我们把它称作 Provisioner。这个程序可以使用我们已经配置好的 NFS 服务器，来自动创建持久卷，也就是自动帮我们创建 PV。</p><ul><li>以 Deployment 方式部署一个 Provisioner</li></ul><p>根据实际情况将下面的环境变量 <code>NFS_SERVER</code>、<code>NFS_PATH</code> 和 NFS 相关配置替换成你的对应的值。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">$</span> <span class="string">vim</span> <span class="string">nfs-client.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">extensions/v1beta1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">1</span></span><br><span class="line"><span class="attr">  strategy:</span></span><br><span class="line"><span class="attr">    type:</span> <span class="string">Recreate</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      serviceAccountName:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">nfs-client-provisioner</span></span><br><span class="line"><span class="attr">          image:</span> <span class="string">quay.io/external_storage/nfs-client-provisioner:latest</span></span><br><span class="line"><span class="attr">          volumeMounts:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">nfs-client-root</span></span><br><span class="line"><span class="attr">              mountPath:</span> <span class="string">/persistentvolumes</span></span><br><span class="line"><span class="attr">          env:</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">PROVISIONER_NAME</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">fuseim.pri/ifs</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">NFS_SERVER</span></span><br><span class="line"><span class="attr">              value:</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.213</span></span><br><span class="line"><span class="attr">            - name:</span> <span class="string">NFS_PATH</span></span><br><span class="line"><span class="attr">              value:</span> <span class="string">/data/kubernetes</span></span><br><span class="line"><span class="attr">      volumes:</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">nfs-client-root</span></span><br><span class="line"><span class="attr">          nfs:</span></span><br><span class="line"><span class="attr">            server:</span> <span class="number">192.168</span><span class="number">.100</span><span class="number">.213</span></span><br><span class="line"><span class="attr">            path:</span> <span class="string">/data/kubernetes</span></span><br></pre></td></tr></table></figure><p>使用 Kubectl 命令建立这个 Deployment</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nfs-client.yaml</span><br><span class="line">deployment.extensions &quot;nfs-client-provisioner&quot; created</span><br></pre></td></tr></table></figure><ul><li>给 nfs-client-provisioner 创建 ServiceAccount</li></ul><p>从 Kubernetes 1.6 版本开始，API Server 启用了 RBAC 授权。Provisioner 要想在 Kubernetes 中创建对应的 PV 资源，就得有对应的权限。</p><p>这里我们新建一个名为 nfs-client-provisioner 的 ServiceAccount 并绑定在一个名为 nfs-client-provisioner-runner 的 ClusterRole 上。该 ClusterRole 包含对 PersistentVolumes 的增、删、改、查等权限。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">$ vim nfs-client-sa.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumes&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumeclaims&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;storage.k8s.io&quot;]</span><br><span class="line">    resources: [&quot;storageclasses&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;events&quot;]</span><br><span class="line">    verbs: [&quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    namespace: default</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure><p>使用 Kubectl 命令建立这个 ServiceAccount。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nfs-client-sa.yaml</span><br><span class="line">serviceaccount &quot;nfs-client-provisioner&quot; created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io &quot;nfs-client-provisioner-runner&quot; created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io &quot;run-nfs-client-provisioner&quot; created</span><br></pre></td></tr></table></figure><ul><li>创建 StorageClass 对象</li></ul><p>这里我们创建了一个名为 course-nfs-storage 的 StorageClass 对象，注意下面的 Provisioner 对应的值一定要和上面的 Deployment下面 PROVISIONER_NAME 这个环境变量的值一样。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ vim nfs-client-class.yaml</span><br><span class="line"></span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: course-nfs-storage</span><br><span class="line">provisioner: fuseim.pri/ifs # or choose another name, must match deployment&apos;s env PROVISIONER_NAME&apos;</span><br></pre></td></tr></table></figure><p>使用 Kubectl 命令建立这个 StorageClass。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f nfs-client-class.yaml</span><br><span class="line">storageclass.storage.k8s.io &quot;course-nfs-storage&quot; created</span><br></pre></td></tr></table></figure><p>以上都创建完成后查看下相关资源的状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods|grep nfs-client</span><br><span class="line">NAME                                            READY     STATUS      RESTARTS   AGE</span><br><span class="line">nfs-client-provisioner-9d94b899c-nn4c7          1/1       Running     0          1m</span><br><span class="line"></span><br><span class="line">$ kubectl get storageclass</span><br><span class="line">NAME                 PROVISIONER      AGE</span><br><span class="line">course-nfs-storage   fuseim.pri/ifs   1m</span><br></pre></td></tr></table></figure><h5 id="手动创建的一个-pvc-对象">手动创建的一个 PVC 对象</h5><ul><li>新建一个 PVC 对象</li></ul><p>我们这里就来建立一个能使用 StorageClass 资源对象来动态建立 PV 的 PVC，要创建使用 StorageClass 资源对象的 PVC 有以下两种方法。</p><p>方法一：在这个 PVC 对象中添加一个 Annotations 属性来声明 StorageClass 对象的标识。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 这里我们声明了一个 PVC 对象，采用 ReadWriteMany 的访问模式并向 PV 请求 100Mi 的空间。</span><br><span class="line">$ vim test-pvc.yaml</span><br><span class="line"></span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pvc</span><br><span class="line">  annotations:</span><br><span class="line">    volume.beta.kubernetes.io/storage-class: &quot;course-nfs-storage&quot;</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 100Mi</span><br></pre></td></tr></table></figure><p>方法二：把名为 course-nfs-storage 的 StorageClass 设置为 Kubernetes 的默认后端存储。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl patch storageclass course-nfs-storage -p &apos;&#123;&quot;metadata&quot;: &#123;&quot;annotations&quot;:&#123;&quot;storageclass.kubernetes.io/is-default-class&quot;:&quot;true&quot;&#125;&#125;&#125;&apos;</span><br><span class="line">storageclass.storage.k8s.io &quot;course-nfs-storage&quot; patched</span><br></pre></td></tr></table></figure><p>上面这两种方法都是可以的，为了不影响系统的默认行为，这里我们采用第一种方法，直接使用 YAML 文件创建即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f test-pvc.yaml</span><br><span class="line">persistentvolumeclaim &quot;test-pvc&quot; created</span><br></pre></td></tr></table></figure><p>创建完成后，我们来看看对应的资源是否创建成功。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pvc</span><br><span class="line">NAME       STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE</span><br><span class="line">pvc1-nfs   Bound     pv1-nfs                                    1Gi        RWO                                 4h</span><br><span class="line">test-pvc   Bound     pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79   100Mi      RWX            course-nfs-storage   41s</span><br><span class="line"></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM              STORAGECLASS         REASON    AGE</span><br><span class="line">pv1-nfs                                    1Gi        RWO            Recycle          Bound       default/pvc1-nfs                                  5h</span><br><span class="line">pv2-nfs                                    2Gi        RWO            Recycle          Available                                                     1h</span><br><span class="line">pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79   100Mi      RWX            Delete           Bound       default/test-pvc   course-nfs-storage             2m</span><br></pre></td></tr></table></figure><p>从上面的结果我们可以看到一个名为 test-pvc 的 PVC 对象创建成功并且状态已经是 Bound 了。对应也自动创建了一个名为 pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79 的 PV 对象，其访问模式是 RWX，回收策略是 Delete。STORAGECLASS 栏中的值也正是我们创建的 StorageClass 对象 course-nfs-storage。</p><ul><li>测试</li></ul><p>我们用一个简单的示例来测试下用 StorageClass 方式声明的 PVC 对象是否能正常存储。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ vim test-pod.yaml</span><br><span class="line"></span><br><span class="line">kind: Pod</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: test-pod</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: test-pod</span><br><span class="line">    image: busybox</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    command:</span><br><span class="line">    - &quot;/bin/sh&quot;</span><br><span class="line">    args:</span><br><span class="line">    - &quot;-c&quot;</span><br><span class="line">    - &quot;touch /mnt/SUCCESS &amp;&amp; exit 0 || exit 1&quot;</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: nfs-pvc</span><br><span class="line">      mountPath: &quot;/mnt&quot;</span><br><span class="line">  restartPolicy: &quot;Never&quot;</span><br><span class="line">  volumes:</span><br><span class="line">  - name: nfs-pvc</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: test-pvc</span><br></pre></td></tr></table></figure><p>上面这个 Pod 的作用非常简单，就是在一个 busybox 容器里的 /mnt 目录下面新建一个 SUCCESS 的文件，而 /mnt 目录是挂载到 test-pvc 这个资源对象上的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f test-pod.yaml</span><br><span class="line">pod &quot;test-pod&quot; created</span><br></pre></td></tr></table></figure><p>完成 Pod 创建后，我们可以在 NFS 服务器的共享数据目录下面查看数据是否存在。我们可以看到下面有一个名字很长的文件夹，这个文件夹的命名方式是：<code>${namespace}-${pvcName}-${pvName}</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls /data/kubernetes/</span><br><span class="line">default-test-pvc-pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79</span><br></pre></td></tr></table></figure><p>再看下这个文件夹下面的文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls /data/kubernetes/default-test-pvc-pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79/</span><br><span class="line">SUCCESS</span><br></pre></td></tr></table></figure><p>我们看到下面有一个 SUCCESS 的文件，说明 PV 对应的存储里可以成功写入文件。</p><h5 id="自动创建的一个-pvc-对象">自动创建的一个 PVC 对象</h5><p>在上面的演示过程中，我们可以看到是手动创建的一个 PVC 对象，而在实际使用中更多使用 StorageClass 的是 StatefulSet 类型的服务。</p><p>StatefulSet 类型的服务是可以通过一个 volumeClaimTemplates 属性来直接使用 StorageClass。volumeClaimTemplates 其实就是一个 PVC 对象的模板，类似于 StatefulSet 下面的 template，而这种模板可以动态的去创建相应的 PVC 对象。</p><ul><li>创建一个 StatefulSet 对象</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$ vim test-statefulset-nfs.yaml</span><br><span class="line">apiVersion: apps/v1beta1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-web</span><br><span class="line">spec:</span><br><span class="line">  serviceName: &quot;nginx&quot;</span><br><span class="line">  replicas: 3</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-web</span><br><span class="line">    spec:</span><br><span class="line">      terminationGracePeriodSeconds: 10</span><br><span class="line">      containers:</span><br><span class="line">      - name: nginx</span><br><span class="line">        image: nginx:1.7.9</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          name: web</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: www</span><br><span class="line">          mountPath: /usr/share/nginx/html</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      name: www</span><br><span class="line">      annotations:</span><br><span class="line">        volume.beta.kubernetes.io/storage-class: course-nfs-storage</span><br><span class="line">    spec:</span><br><span class="line">      accessModes: [ &quot;ReadWriteOnce&quot; ]</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 1Gi</span><br></pre></td></tr></table></figure><p>使用 Kubectl 命令建立这个 StatefulSet 对象。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f test-statefulset-nfs.yaml</span><br><span class="line">statefulset.apps &quot;nfs-web&quot; created</span><br></pre></td></tr></table></figure><ul><li>检查相应资源对像是否已完成创建</li></ul><p>创建完成后可以看到上面 StatefulSet 对象中定义的 3 个 Pod 已经运行成功。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods</span><br><span class="line">NAME                                            READY     STATUS      RESTARTS   AGE</span><br><span class="line">nfs-web-0                                       1/1       Running     0          19s</span><br><span class="line">nfs-web-1                                       1/1       Running     0          16s</span><br><span class="line">nfs-web-2                                       1/1       Running     0          6s</span><br></pre></td></tr></table></figure><p>再查看下 PVC 和 PV 对象。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pvc</span><br><span class="line">NAME            STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS         AGE</span><br><span class="line">www-nfs-web-0   Bound     pvc-16ba792f-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            course-nfs-storage   1m</span><br><span class="line">www-nfs-web-1   Bound     pvc-18c631d4-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            course-nfs-storage   1m</span><br><span class="line">www-nfs-web-2   Bound     pvc-1ed50c38-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            course-nfs-storage   1m</span><br><span class="line"></span><br><span class="line">$ kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                   STORAGECLASS         REASON    AGE</span><br><span class="line">pvc-16ba792f-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            Delete           Bound       default/www-nfs-web-0   course-nfs-storage             3m</span><br><span class="line">pvc-18c631d4-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            Delete           Bound       default/www-nfs-web-1   course-nfs-storage             3m</span><br><span class="line">pvc-1ed50c38-9a15-11e8-9a96-001c42c61a79   1Gi        RWO            Delete           Bound       default/www-nfs-web-2   course-nfs-storage             3m</span><br></pre></td></tr></table></figure><p>我们可以看到生成了 3 个 PVC 对象，名称由模板名称加上 Pod 的名称组合而成，而这 3 个 PVC 对象也都是绑定状态。</p><ul><li>检查 NFS 服务器上面的是否生成相应的数据目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ls  /data/kubernetes/ -l</span><br><span class="line">total 16</span><br><span class="line">drwxrwxrwx 2 root root 4096 Aug  7 15:32 default-test-pvc-pvc-3d8d6ecf-9a13-11e8-9a96-001c42c61a79</span><br><span class="line">drwxrwxrwx 2 root root 4096 Aug  7 15:40 default-www-nfs-web-0-pvc-16ba792f-9a15-11e8-9a96-001c42c61a79</span><br><span class="line">drwxrwxrwx 2 root root 4096 Aug  7 15:40 default-www-nfs-web-1-pvc-18c631d4-9a15-11e8-9a96-001c42c61a79</span><br><span class="line">drwxrwxrwx 2 root root 4096 Aug  7 15:40 default-www-nfs-web-2-pvc-1ed50c38-9a15-11e8-9a96-001c42c61a79</span><br></pre></td></tr></table></figure><h4 id="部署一个使用-storageclass-的应用">部署一个使用 StorageClass 的应用</h4><p>上面的例子中都是简单的运行了一个 Nginx 来演示功能，接下来我们用 Helm 来部署一个具体的应用看看效果。如果你对 Helm 还不够了解，可以先读读 「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486154&amp;idx=1&amp;sn=becd5dd0fadfe0b6072f5dfdc6fdf786&amp;chksm=eac52be3ddb2a2f555b8b1028db97aa3e92d0a4880b56f361e4b11cd252771147c44c08c8913&amp;mpshare=1&amp;scene=23&amp;srcid=0809XT1uzvaUqkaWiouHAUv4%23rd" target="_blank" rel="noopener">Helm 入门指南</a>」一文。</p><p>这里我们同样以部署 DokuWiki 的为例。在「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486201&amp;idx=1&amp;sn=8ba9e1ce4ce1cd3e62472db8b946daf9&amp;chksm=eac52bd0ddb2a2c6c579d0e083eb180d1a57c6ad968f1c6dd0f5b74f7b0702e77d85325a5128&amp;mpshare=1&amp;scene=23&amp;srcid=08211PurH1JwOAE6ueZYuZVt%23rd" target="_blank" rel="noopener">利用 Helm 快速部署 Ingress</a>」一文中我们在部署时关闭了 PersistentVolume。现在我们就演示加上 PersistentVolume 的效果。</p><p>DokuWiki 默认是启用 Persistence 特性的，这里主要通过 <code>persistence.apache.storageClass</code>、<code>persistence.apache.size</code> 和 <code>persistence.dokuwiki.storageClass</code>、<code>persistence.dokuwiki.size</code> 几个参数来设置 Apache 和 DokuWiki 两个应用对应的 storageClass 名称和存储大小 。</p><ul><li>使用 helm install 进行一键部署</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">$ cd /home/k8s/charts/stable</span><br><span class="line">$ helm install --name dokuwiki --set &quot;ingress.enabled=true,ingress.hosts[0].name=wiki.hi-linux.com,persistence.apache.storageClass=course-nfs-storage,persistence.apache.size=500Mi,persistence.dokuwiki.storageClass=course-nfs-storage,persistence.dokuwiki.size=500Mi&quot;  dokuwiki</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Secret</span><br><span class="line">NAME               TYPE    DATA  AGE</span><br><span class="line">dokuwiki-dokuwiki  Opaque  1     6m</span><br><span class="line"></span><br><span class="line">==&gt; v1/PersistentVolumeClaim</span><br><span class="line">NAME                        STATUS  VOLUME                                    CAPACITY  ACCESS MODES  STORAGECLASS        AGE</span><br><span class="line">dokuwiki-dokuwiki-apache    Bound   pvc-1bfd0981-9af0-11e8-9a96-001c42c61a79  500Mi     RWO           course-nfs-storage  6m</span><br><span class="line">dokuwiki-dokuwiki-dokuwiki  Bound   pvc-1bffad3d-9af0-11e8-9a96-001c42c61a79  500Mi     RWO           course-nfs-storage  6m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME               TYPE          CLUSTER-IP     EXTERNAL-IP  PORT(S)                   AGE</span><br><span class="line">dokuwiki-dokuwiki  LoadBalancer  10.254.95.241  &lt;pending&gt;    80:8592/TCP,443:8883/TCP  6m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/Deployment</span><br><span class="line">NAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">dokuwiki-dokuwiki  1        1        1           1          6m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/Ingress</span><br><span class="line">NAME               HOSTS              ADDRESS  PORTS  AGE</span><br><span class="line">dokuwiki-dokuwiki  wiki.hi-linux.com  80       6m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                               READY  STATUS   RESTARTS  AGE</span><br><span class="line">dokuwiki-dokuwiki-bf9fb965c-d9x2w  1/1    Running  1         6m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line"></span><br><span class="line">** Please be patient while the chart is being deployed **</span><br><span class="line"></span><br><span class="line">1. Get the DokuWiki URL indicated on the Ingress Rule and associate it to your cluster external IP:</span><br><span class="line"></span><br><span class="line">   export CLUSTER_IP=$(minikube ip) # On Minikube. Use: `kubectl cluster-info` on others K8s clusters</span><br><span class="line">   export HOSTNAME=$(kubectl get ingress --namespace default dokuwiki-dokuwiki -o jsonpath=&apos;&#123;.spec.rules[0].host&#125;&apos;)</span><br><span class="line">   echo &quot;Dokuwiki URL: http://$HOSTNAME/&quot;</span><br><span class="line">   echo &quot;$CLUSTER_IP  $HOSTNAME&quot; | sudo tee -a /etc/hosts</span><br><span class="line"></span><br><span class="line">2. Login with the following credentials</span><br><span class="line"></span><br><span class="line">  echo Username: user</span><br><span class="line">  echo Password: $(kubectl get secret --namespace default dokuwiki-dokuwiki -o jsonpath=&quot;&#123;.data.dokuwiki-password&#125;&quot; | base64 --decode)</span><br></pre></td></tr></table></figure><ul><li>查看部署完成后状态</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm list</span><br><span class="line">NAME         REVISIONUPDATED                 STATUS  CHART              NAMESPACE</span><br><span class="line">dokuwiki     1       Wed Aug  8 17:47:48 2018DEPLOYEDdokuwiki-2.0.3     default</span><br></pre></td></tr></table></figure><ul><li>在后端存储上查看对应的数据目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 应用对应的数据目录已经自动创建</span><br><span class="line">$ ls /data/kubernetes/*dokuwiki* -ld</span><br><span class="line">drwxrwxrwx 3 root   root   4096 Aug  8 17:52 /data/kubernetes/default-dokuwiki-dokuwiki-apache-pvc-1bfd0981-9af0-11e8-9a96-001c42c61a79</span><br><span class="line">drwxrwxrwx 5 daemon daemon 4096 Aug  8 17:53 /data/kubernetes/default-dokuwiki-dokuwiki-dokuwiki-pvc-1bffad3d-9af0-11e8-9a96-001c42c61a79</span><br><span class="line"></span><br><span class="line"># 查看应用对应的数据目录下文件</span><br><span class="line">$ ls  /data/kubernetes/default-dokuwiki-dokuwiki-dokuwiki-pvc-1bffad3d-9af0-11e8-9a96-001c42c61a79/ -l</span><br><span class="line">total 12</span><br><span class="line">drwxr-xr-x  2 daemon daemon 4096 Aug  8 17:49 conf</span><br><span class="line">drwxr-xr-x 12 daemon daemon 4096 Aug  8 17:48 data</span><br><span class="line">drwxr-xr-x  5 daemon daemon 4096 Aug  8 17:48 lib</span><br><span class="line"></span><br><span class="line">$ ls  /data/kubernetes/default-dokuwiki-dokuwiki-apache-pvc-1bfd0981-9af0-11e8-9a96-001c42c61a79/conf/</span><br><span class="line">bitnami  deflate.conf  extra  httpd.conf  magic  mime.types  original  vhosts</span><br></pre></td></tr></table></figure><ul><li>访问 Web</li></ul><p>根据提示生成相应的登陆用户名和密码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ echo Username: user</span><br><span class="line">Username: user</span><br><span class="line"></span><br><span class="line">$ echo Password: $(kubectl get secret --namespace default dokuwiki-dokuwiki -o jsonpath=&quot;&#123;.data.dokuwiki-password&#125;&quot; | base64 --decode)</span><br><span class="line">Password: e2GrABBkwF</span><br></pre></td></tr></table></figure><p>通过浏览器访问该应用。效果图如下：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-ingress01.png" alt=""></p><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RmDscuQ" target="_blank" rel="noopener">http://t.cn/RmDscuQ</a><br><a href="http://t.cn/RDqXk2U" target="_blank" rel="noopener">http://t.cn/RDqXk2U</a><br><a href="http://t.cn/RDqX1qi" target="_blank" rel="noopener">http://t.cn/RDqX1qi</a><br><a href="http://t.cn/RDqT4Xw" target="_blank" rel="noopener">http://t.cn/RDqT4Xw</a><br><a href="http://t.cn/RmDscuQ" target="_blank" rel="noopener">http://t.cn/RmDscuQ</a><br><a href="http://t.cn/RDqg4D0" target="_blank" rel="noopener">http://t.cn/RDqg4D0</a><br><a href="http://t.cn/RDqkyoC" target="_blank" rel="noopener">http://t.cn/RDqkyoC</a><br><a href="http://t.cn/RDVE0bW" target="_blank" rel="noopener">http://t.cn/RDVE0bW</a><br><a href="http://t.cn/R6GaBUK" target="_blank" rel="noopener">http://t.cn/R6GaBUK</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;kubernetes-volume-相关-概念&quot;&gt;Kubernetes Volume 相关概念&lt;/h3&gt;
&lt;p&gt;缺省情况下，一个运行中的容器对文件系统的写入都是发生在其分层文件系统的可写层。一旦容器运行结束，所有写入都会被丢弃。如果数据需要长期存储，那就需要对容器数据做持久化支持。&lt;/p&gt;
&lt;p&gt;Kubernetes 和 Docker 类似，也是通过 Volume 的方式提供对存储的支持。Volume 被定义在 Pod 上，可以被 Pod 里的多个容器挂载到相同或不同的路径下。Kubernetes 中 Volume 的 概念与Docker 中的 Volume 类似，但不完全相同。具体区别如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Kubernetes 中的 Volume 与 Pod 的生命周期相同，但与容器的生命周期不相关。当容器终止或重启时，Volume 中的数据也不会丢失。&lt;/li&gt;
&lt;li&gt;当 Pod 被删除时，Volume 才会被清理。并且数据是否丢失取决于 Volume 的具体类型，比如：emptyDir 类型的 Volume 数据会丢失，而 PV 类型的数据则不会丢失。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Volume 的核心是目录，可以通过 Pod 中的容器来访问。该目录是如何形成的、支持该目录的介质以及其内容取决于所使用的特定卷类型。要使用 Volume，需要为 Pod 指定为 Volume (&lt;code&gt;spec.volumes&lt;/code&gt; 字段) 以及将它挂载到容器的位置 (&lt;code&gt;spec.containers.volumeMounts&lt;/code&gt; 字段)。Kubernetes 支持多种类型的卷，一个 Pod 可以同时使用多种类型的 Volume。&lt;/p&gt;
&lt;p&gt;容器中的进程看到的是由其 Docker 镜像和 Volume 组成的文件系统视图。 Docker 镜像位于文件系统层次结构的根目录，任何 Volume 都被挂载在镜像的指定路径中。Volume 无法挂载到其他 Volume 上或与其他 Volume 的硬连接。Pod 中的每个容器都必须独立指定每个 Volume 的挂载位置。&lt;/p&gt;
&lt;p&gt;Kubernetes 目前支持多种 Volume 类型，大致如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;awsElasticBlockStore&lt;/li&gt;
&lt;li&gt;azureDisk&lt;/li&gt;
&lt;li&gt;azureFile&lt;/li&gt;
&lt;li&gt;cephfs&lt;/li&gt;
&lt;li&gt;csi&lt;/li&gt;
&lt;li&gt;downwardAPI&lt;/li&gt;
&lt;li&gt;emptyDir&lt;/li&gt;
&lt;li&gt;fc (fibre channel)&lt;/li&gt;
&lt;li&gt;flocker&lt;/li&gt;
&lt;li&gt;gcePersistentDisk&lt;/li&gt;
&lt;li&gt;gitRepo&lt;/li&gt;
&lt;li&gt;glusterfs&lt;/li&gt;
&lt;li&gt;hostPath&lt;/li&gt;
&lt;li&gt;iscsi&lt;/li&gt;
&lt;li&gt;local&lt;/li&gt;
&lt;li&gt;nfs&lt;/li&gt;
&lt;li&gt;persistentVolumeClaim&lt;/li&gt;
&lt;li&gt;projected&lt;/li&gt;
&lt;li&gt;portworxVolume&lt;/li&gt;
&lt;li&gt;quobyte&lt;/li&gt;
&lt;li&gt;rbd&lt;/li&gt;
&lt;li&gt;scaleIO&lt;/li&gt;
&lt;li&gt;secret&lt;/li&gt;
&lt;li&gt;storageos&lt;/li&gt;
&lt;li&gt;vsphereVolume&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;注：这些 Volume 并非全部都是持久化的，比如: emptyDir、secret、gitRepo 等，就会随着 Pod 的消亡而消失。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>浅谈 DDoS 攻击与防御</title>
    <link href="https://www.hi-linux.com/posts/50873.html"/>
    <id>https://www.hi-linux.com/posts/50873.html</id>
    <published>2018-08-20T01:00:00.000Z</published>
    <updated>2018-08-24T06:09:58.162Z</updated>
    
    <content type="html"><![CDATA[<h3 id="什么是-ddos">什么是 DDoS</h3><p>DDoS 是英文 Distributed Denial of Service 的缩写，中文译作分布式拒绝服务。那什么又是拒绝服务（Denial of Service）呢？凡是能导致合法用户不能够正常访问网络服务的行为都算是拒绝服务攻击。也就是说拒绝服务攻击的目的非常明确，就是要阻止合法用户对正常网络资源的访问，从而达成攻击者不可告人的目的。</p><p>分布式拒绝服务攻击一旦被实施，攻击网络包就会从很多 DoS 攻击源犹如洪水般涌向受害主机。从而把合法用户的网络请求淹没，导致合法用户无法正常访问服务器的网络资源。</p><h3 id="ddos-攻击方式分类">DDoS 攻击方式分类</h3><ul><li>反射型</li></ul><p>一般而言，我们会根据针对的协议类型和攻击方式的不同，把 DDoS 分成 SYN Flood、ACK Flood、Connection Flood、UDP Flood、NTP Flood、SSDP Flood、DNS Flood、HTTP Flood、ICMP Flood、CC 等各类攻击类型。</p><p>每一种攻击类型都有其特点，而反射型的 DDoS 攻击是一种新的变种。攻击者并不直接攻击目标服务的 IP，而是利用互联网的某些特殊服务开放的服务器，通过伪造被攻击者的 IP 地址向有开放服务的服务器发送构造的请求报文，该服务器会将数倍于请求报文的回复数据发送到被攻击 IP，从而对后者间接形成 DDoS 攻击。</p><p>如下图所示，这里的攻击者（Attacker，实际情况中更多的会利用傀儡机进行攻击）不直接把攻击包发给受害者，而是冒充受害者给放大器（Amplifiers）发包，然后通过放大器再反射给受害者。</p><a id="more"></a><p><img src="https://www.hi-linux.com/img/linux/ddos1.png" alt=""></p><p>在反射型攻击中，攻击者利用了网络协议的缺陷或者漏洞进行 IP 欺骗，主要是因为很多协议（例如 ICMP、UDP 等）对源 IP 不进行认证。同时，要达到更好的攻击效果，黑客一般会选择具有放大效果的协议服务进行攻击。</p><p>总结一下就是利用 IP 欺骗进行反射和放大，从而达到四两拨千斤的效果。目前常见的反射攻击有：DNS 反射攻击、NTP 反射攻击、SSDP 反射攻击等。</p><blockquote><p>注：将源地址设为假的无法回应，即为 SYN Flood 攻击。制造流量和攻击目标收到的流量为 1:1，回报率低。</p></blockquote><ul><li>流量放大型</li></ul><p>通过递归等手法将攻击流量放大的攻击类型，比如：以反射型中常见的 SSDP 协议为例，攻击者将 Search type 设置为 ALL。搜索所有可用的设备和服务，这种递归效果产生的放大倍数是非常大的，攻击者只需要以较小的伪造源地址的查询流量就可以制造出几十甚至上百倍的应答流量发送至目标。</p><ul><li>混合型</li></ul><p>在实际情况中，攻击者只求达到打垮对方的目的。发展到现在，高级攻击者已经不倾向使用单一的攻击手段。而是根据目标系统的具体环境灵动组合，发动多种攻击手段。</p><p>比如：TCP 和 UDP、网络层和应用层攻击同时进行，这样的攻击既具备了海量的流量，又利用了协议、系统的缺陷，尽其所能地展开攻势。对于被攻击目标来说，需要面对不同协议、不同资源的分布式的攻击，分析、响应和处理的成本就会大大增加。</p><p><img src="https://www.hi-linux.com/img/linux/ddos2.png" alt=""></p><ul><li>脉冲波型</li></ul><p>这是一种新型的 DDoS 攻击方法，给某些 DDoS 攻击解决方案带来了问题，因为它允许攻击者攻击以前认为是安全的服务器。之所以将这种新技术命名为脉冲波，是由于其攻击流量展现出来的图形看起来很像不连贯的重复的脉冲状。这类攻击通常呈现一个有上有下的斜三角形的形状，这个过程体现了攻击者正在慢慢地组装机器人并将目标对准待攻击的目标。</p><p><img src="https://www.hi-linux.com/img/linux/ddos3.png" alt=""></p><p>一次新的脉冲波攻击从零开始，在很短的时间跨度内达到最大值，然后归零，再回到最大值，如此循环重复，中间的时间间隔很短。脉冲波型 DDoS 相对难以防御，因为其攻击方式避开了触发自动化的防御机制。</p><p><img src="https://www.hi-linux.com/img/linux/ddos4.png" alt=""></p><ul><li>链路泛洪</li></ul><p>随着 DDoS 攻击技术的发展，又出现了一种新型的攻击方式 Link Flooding Attack，这种方式不直接攻击目标而是以堵塞目标网络的上一级链路为目的。对于使用了 IP Anycast 的企业网络来说，常规的 DDoS 攻击流量会被分摊到不同地址的基础设施，这样能有效缓解大流量攻击。所以攻击者发明了一种新方法，攻击至目标网络 traceroute 的倒数第二跳，即上联路由，致使链路拥塞。</p><p><img src="https://www.hi-linux.com/img/linux/ddos5.png" alt=""></p><h3 id="常见-ddos-攻击方法">常见 DDoS 攻击方法</h3><p>DDoS 攻击从层次上可分为网络层攻击与应用层攻击，从攻击手法上可分为快型流量攻击与慢型流量攻击，但其原理都是造成资源过载，导致服务不可用。</p><h4 id="网络层-ddos-攻击">网络层 DDoS 攻击</h4><p>网络层 DDoS 攻击常见手段有：SYN Flood、ACK Flood、Connection Flood、UDP Flood、ICMP Flood、TCP Flood、Proxy Flood 等。</p><ul><li>SYN Flood 攻击</li></ul><p>SYN Flood 攻击是一种利用 TCP 协议缺陷，发送大量伪造的 TCP 连接请求，从而使得被攻击方资源耗尽（CPU 满负载或内存不足）的攻击方式。建立 TCP连接，需要三次握手（客户端发送 SYN 报文、服务端收到请求并返回报文表示接受、客户端也返回确认，完成连接）。</p><p>SYN Flood 就是用户向服务器发送报文后突然死机或掉线，那么服务器在发出应答报文后就无法收到客户端的确认报文（第三次握手无法完成），这时服务器端一般会重试并等待一段时间（至少 30s）后再丢弃这个未完成的连接。</p><p>一个用户出现异常导致服务器的一个线程等待一会儿并不是大问题，但恶意攻击者大量模拟（构造源 IP 去发送 SYN 包）这种情况，服务器端为了维护数以万计的半连接而消耗非常多的资源，结果往往是无暇理睬正常客户的请求，甚至崩溃。从正常客户的角度看来，网站失去了响应，无法访问。</p><p><img src="https://www.hi-linux.com/img/linux/ddos6.png" alt=""></p><ul><li>ACK Flood</li></ul><p>ACK Flood 攻击是在 TCP 连接建立之后进行的。所有数据传输的 TCP 报文都是带有 ACK 标志位的，主机在接收到一个带有 ACK 标志位的数据包的时候，需要检查该数据包所表示的连接四元组是否存在。如果存在则检查该数据包所表示的状态是否合法，然后再向应用层传递该数据包。如果在检查中发现该数据包不合法（例如：该数据包所指向的目的端口在本机并未开放），则主机操作系统协议栈会回应 RST 包告诉对方此端口不存在。</p><p>这里，服务器要做两个动作：查表、回应 ACK/RST。对比主机以及防火墙在接收到 ACK 报文和 SYN 报文时所做动作的复杂程度，显然 ACK 报文带来的负载要小得多。这种攻击方式显然没有 SYN Flood 给服务器带来的冲击大，因此攻击者一定要用大流量 ACK 小包冲击才会对服务器造成影响。所以在实际环境中，只有当攻击程序每秒钟发送 ACK 报文的速率达到一定的程度，才能使主机和防火墙的负载有大的变化。</p><p>当发包速率很大的时候，主机操作系统将耗费大量的精力接收报文、判断状态，同时要主动回应 RST 报文，正常的数据包就可能无法得到及时的处理。这时候客户端的表现就是访问页面反应很慢，丢包率较高。但是状态检测的防火墙通过判断 ACK 报文的状态是否合法，借助其强大的硬件能力可以较为有效的过滤攻击报文。当然如果攻击流量非常大，由于需要维护很大的连接状态表同时要检查数量巨大的 ACK 报文的状态，防火墙也会不堪重负导致网络瘫痪。</p><p>目前 ACK Flood 并没有成为攻击的主流，而通常是与其他攻击方式组合在一起使用。</p><p><img src="https://www.hi-linux.com/img/linux/ddos7.png" alt=""></p><ul><li>Connection Flood</li></ul><p>Connection Flood 是典型的并且非常有效的利用小流量冲击大带宽网络服务的攻击方式。这种攻击的原理是利用真实的 IP 地址向服务器发起大量的连接，并且建立连接之后很长时间不释放。长期占用服务器的资源，造成服务器上残余连接 (WAIT 状态) 过多，效率降低，甚至资源耗尽，无法响应其它客户所发起的连接。</p><p>其中一种攻击方法是每秒钟向服务器发起大量的连接请求，这类似于固定源 IP 的 SYN Flood 攻击，不同的是采用了真实的源 IP 地址。通常这可以在防火墙上限制每个源 IP 地址每秒钟的连接数来达到防护目的。</p><p>但现在已有工具采用慢速连接的方式，也即几秒钟才和服务器建立一个连接，连接建立成功之后并不释放并定时发送垃圾数据包给服务器使连接得以长时间保持。这样一个 IP 地址就可以和服务器建立成百上千的连接，而服务器可以承受的连接数是有限的，这就达到了拒绝服务的效果。</p><p><img src="https://www.hi-linux.com/img/linux/ddos8.png" alt=""></p><ul><li>UDP Flood 攻击</li></ul><p>由于 UDP 是一种无连接的协议，因此攻击者可以伪造大量的源 IP 地址去发送 UDP 包，此种攻击属于大流量攻击。正常应用情况下，UDP 包双向流量会基本相等，因此在消耗对方资源的时候也在消耗自己的资源。</p><ul><li>ICMP Flood 攻击</li></ul><p>此攻击属于大流量攻击，其原理就是不断发送不正常的 ICMP 包（所谓不正常就是 ICMP 包内容很大），导致目标带宽被占用。但其本身资源也会被消耗，并且目前很多服务器都是禁 ping 的（在防火墙里可以屏蔽 ICMP 包），因此这种方式已经落伍。</p><ul><li>Smurf 攻击</li></ul><p>这种攻击类似于 ICMP Flood 攻击，但它能巧妙地修改进程。Smurf 攻击通过使用将回复地址设置成受害网络的广播地址的 ICMP 应答请求数据包，来淹没受害主机。最终导致该网络的所有主机都对此 ICMP 应答请求做出答复，导致网络阻塞。更加复杂的 Smurf 将源地址改为第三方的受害者，最终导致第三方崩溃。</p><h4 id="应用层-ddos-攻击">应用层 DDoS 攻击</h4><p>应用层 DDoS 攻击不是发生在网络层，是发生在 TCP 建立握手成功之后，应用程序处理请求的时候。常见的有：CC 攻击、DNS Flood、慢速连接攻击等。</p><ul><li>CC 攻击</li></ul><p>CC 攻击（Challenge Collapsar）是 DDoS 攻击的一种，其前身名为 Fatboy 攻击，也是一种常见的网站攻击方法。CC 攻击还有一段比较有趣的历史，Collapsar 是绿盟科技的一款防御 DDoS 攻击的产品品牌，Collapasar 在对抗拒绝服务攻击的领域内具有比较高的影响力和口碑。然而黑客为了挑衅，研发了一款 Challenge Collapasar 工具简称 CC，表示要向 Collapasar 发起挑战。</p><p>CC 攻击的原理就是借助代理服务器针对目标系统的消耗资源比较大的页面不断发起正常的请求，造成对方服务器资源耗尽，一直到宕机崩溃。因此在发送 CC 攻击前，我们需要寻找加载比较慢，消耗资源比较多的网页。比如：需要查询数据库的页面、读写硬盘的文件等。相比其它的 DDoS 攻击 CC 更有技术含量一些，这种攻击你见不到真实源 IP。见不到特别大的异常流量，但造成服务器无法进行正常连接。</p><p><img src="https://www.hi-linux.com/img/linux/ddos9.png" alt=""></p><ul><li>Slowloris 攻击</li></ul><p>Slowloris 是一种慢速连接攻击，Slowloris 是利用 Web Server 的漏洞或设计缺陷，直接造成拒绝服务。其原理是：以极低的速度往服务器发送 HTTP 请求，Apache 等中间件默认会设置最大并发链接数，而这种攻击就是会持续保持连接，导致服务器链接饱和不可用。Slowloris 有点类似于 SYN Flood 攻击，只不过 Slowloris 是基于 HTTP 协议。</p><p>Slowloris  PoC</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 构造以下畸形 HTTP 请求包</span><br><span class="line">GET / HTTP/1.1\r\n</span><br><span class="line">Host: Victim host\r\n</span><br><span class="line">User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; .NET CLR 1.1.4322; .NET CLR 2.0.503l3; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; MSOffice 12)\r\n</span><br><span class="line">Content-Length: 42\r\n</span><br></pre></td></tr></table></figure><p>完整的 HTTP 请求头结尾应该是两次的 <code>\r\n\r\n</code>，这里少了一次，因此服务器将会一直等待。</p><ul><li>Slow Attack</li></ul><p>Slow Attack 也是一种慢速 DoS 攻击，它通过消耗服务器的系统资源和连接数，导致 Web 服务器无法正常工作。常见的攻击方式包括 Slow Header、Slow Body 和 Slow Read。</p><ol><li>Slow Header：正常的 HTTP Header 以两个 CLRF 结尾，通过发送只包含一个 CLRF 的畸形 Header 请求来占用 Web 服务器连接，从而达到消耗掉服务器所有可用的连接数。最终造成 Web 服务器资源饱和并拒绝新的服务。</li><li>Slow Read：向服务器请求很大的文件，然后通过设置 TCP 滑动窗口较小值，导致服务器以极慢的速度传输文件。这样，就会占用服务器大量的内存，从而造成拒绝服务。</li><li>Slow Body：在向服务器发送 HTTP Post 包时，指定一个非常大的 Content-Length 值，然后以极低的速度发包并保持连接不断，最终导致服务器连接饱和不可用。</li></ol><p>Kali Linux 提供的专用测试工具 SlowHTTPTest 能够实现以上三种 Slow Attack 方式。</p><ul><li>JavaScript DDoS</li></ul><p>基于 JavaScript 的 DDoS 攻击利用的工具是普通网民的上网终端，这也意味着只要装有浏览器的电脑，都能被用作为 DDoS 攻击者的工具。当被操纵的浏览器数量达到一定程度时，这种 DDoS 攻击方式将会带来巨大的破坏性。</p><p>攻击者会在海量访问的网页中嵌入指向攻击目标网站的恶意 JavaScript 代码，当互联网用户访问该网页时，则流量被指向攻击目标网站。比较典型攻击事件：GitHub DDoS 攻击。</p><ul><li>ReDoS 攻击</li></ul><p>ReDoS (Regular expression Denial of Service)， 中文译作正则表达式拒绝服务攻击。开发人员使用了正则表达式来对用户输入的数据进行有效性校验，当编写校验的正则表达式存在缺陷或者不严谨时，攻击者可以构造特殊的字符串来大量消耗服务器的系统资源，从而造成服务器的服务中断或停止。 更详细介绍可参考：「<a href="https://cloud.tencent.com/developer/article/1041326" target="_blank" rel="noopener">浅析 ReDoS 的原理与实践</a>」一文。</p><ul><li>DNS Query Flood</li></ul><p>DNS 作为互联网的核心服务之一，自然也是 DDoS 攻击的一大主要目标。DNS Query Flood 采用的方法是操纵大量傀儡机器，向目标服务器发送大量的域名解析请求。服务器在接收到域名解析请求时，首先会在服务器上查找是否有对应的缓存，若查找不到且该域名无法直接解析时，便向其上层 DNS 服务器递归查询域名信息。</p><p>通常，攻击者请求解析的域名是随机生成或者是网络上根本不存在的域名。由于在本地域名服务器无法查到对应的结果，本地域名服务器必须使用递归查询向上层域名服务器提交解析请求，引起连锁反应。解析过程给本地域名服务器带来一定的负载，每秒钟域名解析请求超过一定的数量就会造成域名服务器解析域名超时。</p><p><img src="https://www.hi-linux.com/img/linux/ddos10.png" alt=""></p><p>根据微软的统计数据，一台 DNS 服务器所能承受的动态域名查询的上限是每秒钟 9000 个请求。而一台 P3 的 PC 机上可以轻易地构造出每秒钟几万个域名解析请求，足以使一台硬件配置极高的 DNS 服务器瘫痪，由此可见 DNS 服务器的脆弱性。</p><h4 id="无线-ddos-攻击">无线 DDoS 攻击</h4><ul><li>Auth Flood 攻击</li></ul><p>Auth Flood 攻击，即身份验证洪水攻击。该攻击目标主要针对那些处于通过验证和 AP 建立关联的关联客户端，攻击者将向 AP 发送大量伪造的身份验证请求帧（伪造的身份验证服务和状态代码），当收到大量伪造的身份验证请求超过所能承受的能力时，AP将断开其他无线服务连接。</p><ul><li>Deauth Flood 攻击</li></ul><p>Deauth Flood 攻击即为取消验证洪水攻击，它旨在通过欺骗从 AP 到客户端单播地址的取消身份验证帧来将客户端转为未关联 / 未认证的状态。对于目前的工具来说，这种形式的攻击在打断客户无线服务方面非常有效和快捷。一般来说，在攻击者发送另一个取消身份验证帧之前，客户端会重新关联和认证以再次获取服务。攻击者反复欺骗取消身份验证帧就能使所有客户端持续拒绝服务。</p><ul><li>Association Flood 攻击</li></ul><p>Association Flood 攻击即为关联洪水攻击。在无线路由器或者接入点内置一个列表即为连接状态表，里面可显示出所有与该 AP 建立连接的无线客户端状态。它试图通过利用大量模仿和伪造的无线客户端关联来填充 AP 的客户端关联表，从而达到淹没 AP 的目的。</p><p>由于开放身份验证（空身份验证）允许任何客户端通过身份验证后关联。利用这种漏洞的攻击者可以通过创建多个到达已连接或已关联的客户端来模仿很多客户端，从而淹没目标 AP 的客户端关联表。</p><ul><li>Disassociation Flood 攻击</li></ul><p>Disassociation Flood 攻击即为取消关联洪水攻击，和 Deauth Flood 攻击表现方式很相似。它通过欺骗从 AP 到客户端的取消关联帧来强制客户端成为未关联 / 未认证的状态。一般来说，在攻击者发送另一个取消关联帧之前，客户端会重新关联以再次获取服务。攻击者反复欺骗取消关联帧就能使客户端持续拒绝服务。</p><p>Disassociation Broadcast 攻击和 Disassociation Flood 攻击原理基本一致，只是在发送程度及使用工具上有所区别。前者很多时候用于配合进行无线中间人攻击，而后者常用于目标确定的点对点无线 DoS，比如：破坏或干扰指定机构或部门的无线接入点等。</p><ul><li>RF Jamming 攻击</li></ul><p>RF Jamming 攻击即为 RF 干扰攻击。该攻击是通过发出干扰射频达到破坏正常无线通信的目的。而前面几种攻击主要是基于无线通信过程及协议的。RF 为射频，主要包括无线信号发射机及收信机等。</p><h3 id="ddos-攻击现象判定方法">DDoS 攻击现象判定方法</h3><ul><li>SYN 类攻击判断</li></ul><ol><li>服务器 CPU 占用率很高。</li><li>出现大量的 SYN_RECEIVED 的网络连接状态。</li><li>网络恢复后，服务器负载瞬时变高。网络断开后瞬时负载下将。</li></ol><ul><li>UDP 类攻击判断</li></ul><ol><li>服务器 CPU 占用率很高。</li><li>网卡每秒接受大量的数据包。</li><li>网络 TCP 状态信息正常。</li></ol><ul><li>CC 类攻击判断</li></ul><ol><li>服务器 CPU 占用率很高。</li><li>Web 服务器出现类似 Service Unavailable 提示。</li><li>出现大量的 ESTABLISHED 的网络连接状态且单个 IP 高达几十个甚至上百个连接。</li><li>用户无法正常访问网站页面或打开过程非常缓慢，软重启后短期内恢复正常，几分钟后又无法访问。</li></ol><h3 id="ddos-攻击防御方法">DDoS 攻击防御方法</h3><ul><li>网络层 DDoS 防御</li></ul><ol><li>限制单 IP 请求频率。</li><li>网络架构上做好优化，采用负载均衡分流。</li><li>防火墙等安全设备上设置禁止 ICMP 包等。</li><li>通过 DDoS 硬件防火墙的数据包规则过滤、数据流指纹检测过滤、及数据包内容定制过滤等技术对异常流量进行清洗过滤。</li><li>采用 ISP 近源清洗，使用电信运营商提供的近源清洗和流量压制，避免全站服务对所有用户彻底无法访问。这是对超过自身带宽储备和自身 DDoS 防御能力之外超大流量的补充性缓解措施。</li></ol><ul><li>应用层 DDoS 防御</li></ul><ol><li>优化操作系统的 TCP/IP 栈。</li><li>应用服务器严格限制单个 IP 允许的连接数和 CPU 使用时间。</li><li>编写代码时，尽量实现优化并合理使用缓存技术。尽量让网站静态化，减少不必要的动态查询。网站静态化不仅能大大提高抗攻击能力，而且还给骇客入侵带来不少麻烦，至少到现在为止关于 HTML 的溢出还没出现。</li><li>增加 WAF（Web Application Firewall）设备，WAF 的中文名称叫做 Web 应用防火墙。Web 应用防火墙是通过执行一系列针对 HTTP / HTTPS 的安全策略来专门为 Web 应用提供保护的一款产品。</li><li>使用 CDN / 云清洗，在攻击发生时，进行云清洗。通常云清洗厂商策略有以下几步：预先设置好网站的 CNAME，将域名指向云清洗厂商的 DNS 服务器；在一般情况下，云清洗厂商的 DNS 仍将穿透 CDN 的回源的请求指向源站，在检测到攻击发生时，域名指向自己的清洗集群，然后再将清洗后的流量回源。</li><li>CDN 仅对 Web 类服务有效，针对游戏类 TCP 直连的服务无效。这时可以使用 DNS 引流 + ADS (Anti-DDoS System) 设备来清洗，还有在客户端和服务端通信协议做处理（如：封包加标签，依赖信息对称等）。</li></ol><p>DDoS 攻击究其本质其实是无法彻底防御的，我们能做得就是不断优化自身的网络和服务架构，来提高对 DDoS 的防御能力。</p><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RrSkw6a" target="_blank" rel="noopener">http://t.cn/RrSkw6a</a><br><a href="http://t.cn/RrSkNKe" target="_blank" rel="noopener">http://t.cn/RrSkNKe</a><br><a href="http://t.cn/RrSFJ1B" target="_blank" rel="noopener">http://t.cn/RrSFJ1B</a><br><a href="http://t.cn/RrovtI3" target="_blank" rel="noopener">http://t.cn/RrovtI3</a><br><a href="http://t.cn/RrKGEIb" target="_blank" rel="noopener">http://t.cn/RrKGEIb</a><br><a href="http://t.cn/RCwYkYf" target="_blank" rel="noopener">http://t.cn/RCwYkYf</a><br><a href="http://t.cn/RrKIAlN" target="_blank" rel="noopener">http://t.cn/RrKIAlN</a><br><a href="http://t.cn/RrKQ8j8" target="_blank" rel="noopener">http://t.cn/RrKQ8j8</a><br><a href="http://t.cn/RcCzPCO" target="_blank" rel="noopener">http://t.cn/RcCzPCO</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;什么是-ddos&quot;&gt;什么是 DDoS&lt;/h3&gt;
&lt;p&gt;DDoS 是英文 Distributed Denial of Service 的缩写，中文译作分布式拒绝服务。那什么又是拒绝服务（Denial of Service）呢？凡是能导致合法用户不能够正常访问网络服务的行为都算是拒绝服务攻击。也就是说拒绝服务攻击的目的非常明确，就是要阻止合法用户对正常网络资源的访问，从而达成攻击者不可告人的目的。&lt;/p&gt;
&lt;p&gt;分布式拒绝服务攻击一旦被实施，攻击网络包就会从很多 DoS 攻击源犹如洪水般涌向受害主机。从而把合法用户的网络请求淹没，导致合法用户无法正常访问服务器的网络资源。&lt;/p&gt;
&lt;h3 id=&quot;ddos-攻击方式分类&quot;&gt;DDoS 攻击方式分类&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;反射型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一般而言，我们会根据针对的协议类型和攻击方式的不同，把 DDoS 分成 SYN Flood、ACK Flood、Connection Flood、UDP Flood、NTP Flood、SSDP Flood、DNS Flood、HTTP Flood、ICMP Flood、CC 等各类攻击类型。&lt;/p&gt;
&lt;p&gt;每一种攻击类型都有其特点，而反射型的 DDoS 攻击是一种新的变种。攻击者并不直接攻击目标服务的 IP，而是利用互联网的某些特殊服务开放的服务器，通过伪造被攻击者的 IP 地址向有开放服务的服务器发送构造的请求报文，该服务器会将数倍于请求报文的回复数据发送到被攻击 IP，从而对后者间接形成 DDoS 攻击。&lt;/p&gt;
&lt;p&gt;如下图所示，这里的攻击者（Attacker，实际情况中更多的会利用傀儡机进行攻击）不直接把攻击包发给受害者，而是冒充受害者给放大器（Amplifiers）发包，然后通过放大器再反射给受害者。&lt;/p&gt;
    
    </summary>
    
      <category term="DDoS" scheme="https://www.hi-linux.com/categories/DDoS/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="DDoS" scheme="https://www.hi-linux.com/tags/DDoS/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes 资源管理概述</title>
    <link href="https://www.hi-linux.com/posts/65337.html"/>
    <id>https://www.hi-linux.com/posts/65337.html</id>
    <published>2018-08-16T01:00:00.000Z</published>
    <updated>2018-08-24T05:54:08.501Z</updated>
    
    <content type="html"><![CDATA[<p>Kubernetes 从创建之初的核心模块之一就是资源调度。想要在生产环境使用好 Kubernetes，必须对它的资源模型以及资源管理非常了解。这篇文章算是对散布在网络上的 Kubernetes 资源管理内容的一个总结。干货文章，强列推荐一读。</p><h3 id="kubernetes-资源简介">Kubernetes 资源简介</h3><h4 id="什么是资源">什么是资源？</h4><p>在 Kubernetes 中，有两个基础但是非常重要的概念：Node 和 Pod。Node 翻译成节点，是对集群资源的抽象；Pod 是对容器的封装，是应用运行的实体。Node 提供资源，而 Pod 使用资源，这里的资源分为计算（CPU、Memory、GPU）、存储（Disk、SSD）、网络（Network Bandwidth、IP、Ports）。这些资源提供了应用运行的基础，正确理解这些资源以及集群调度如何使用这些资源，对于大规模的 Kubernetes 集群来说至关重要，不仅能保证应用的稳定性，也可以提高资源的利用率。</p><p>在这篇文章，我们主要介绍 CPU 和内存这两个重要的资源，它们虽然都属于计算资源，但也有所差距。CPU 可分配的是使用时间，也就是操作系统管理的时间片，每个进程在一定的时间片里运行自己的任务（另外一种方式是绑核，也就是把 CPU 完全分配给某个 Pod 使用，但这种方式不够灵活会造成严重的资源浪费，Kubernetes 中并没有提供）；而对于内存，系统提供的是内存大小。</p><p>CPU 的使用时间是可压缩的，换句话说它本身无状态，申请资源很快，也能快速正常回收；而内存大小是不可压缩的，因为它是有状态的（内存里面保存的数据），申请资源很慢（需要计算和分配内存块的空间），并且回收可能失败（被占用的内存一般不可回收）。</p><p>把资源分成<strong>可压缩</strong>和<strong>不可压缩</strong>，是因为在资源不足的时候，它们的表现很不一样。对于不可压缩资源，如果资源不足，也就无法继续申请资源（内存用完就是用完了），并且会导致 Pod 的运行产生无法预测的错误（应用申请内存失败会导致一系列问题）；而对于可压缩资源，比如 CPU 时间片，即使 Pod 使用的 CPU 资源很多，CPU 使用也可以按照权重分配给所有 Pod 使用，虽然每个人使用的时间片减少，但不会影响程序的逻辑。</p><a id="more"></a><p>在 Kubernetes 集群管理中，有一个非常核心的功能：就是为 Pod 选择一个主机运行。调度必须满足一定的条件，其中最基本的是主机上要有足够的资源给 Pod 使用。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-res1.png" alt="Kubernetes scheduler architecture"></p><p>资源除了和调度相关之外，还和很多事情紧密相连，这正是这篇文章要解释的。</p><h4 id="kubernetes-资源的表示">Kubernetes 资源的表示</h4><p>用户在 Pod 中可以配置要使用的资源总量，Kubernetes 根据配置的资源数进行调度和运行。目前主要可以配置的资源是 CPU 和 Memory，对应的配置字段是 <code>spec.containers[].resource.limits/request.cpu/memory</code>。</p><p>需要注意的是，用户是对每个容器配置 Request 值，所有容器的资源请求之和就是 Pod 的资源请求总量，而我们一般会说 Pod 的资源请求和 Limits。</p><p><code>Limits</code> 和 <code>Requests</code> 的区别我们下面会提到，这里先说说比较容易理解的 CPU 和 Memory。</p><p><code>CPU</code> 一般用核数来标识，一核 CPU 相对于物理服务器的一个超线程核，也就是操作系统 <code>/proc/cpuinfo</code> 中列出来的核数。因为对资源进行了池化和虚拟化，因此 Kubernetes 允许配置非整数个的核数，比如 <code>0.5</code> 是合法的，它标识应用可以使用半个 CPU 核的计算量。CPU 的请求有两种方式，一种是刚提到的 <code>0.5</code>，<code>1</code> 这种直接用数字标识 CPU 核心数；另外一种表示是 <code>500m</code>，它等价于 <code>0.5</code>，也就是说 <code>1 Core = 1000m</code>。</p><p>内存比较容易理解，是通过字节大小指定的。如果直接一个数字，后面没有任何单位，表示这么多字节的内存；数字后面还可以跟着单位， 支持的单位有 <code>E</code>、<code>P</code>、<code>T</code>、<code>G</code>、<code>M</code>、<code>K</code>，前者分别是后者的 <code>1000</code> 倍大小的关系，此外还支持  <code>Ei</code>、<code>Pi</code>、<code>Ti</code>、<code>Gi</code>、<code>Mi</code>、<code>Ki</code>，其对应的倍数关系是 <code>2^10 = 1024</code>。比如要使用 100M 内存的话，直接写成 <code>100Mi</code>即可。</p><h4 id="节点可用资源">节点可用资源</h4><p>理想情况下，我们希望节点上所有的资源都可以分配给 Pod 使用，但实际上节点上除了运行 Pods 之外，还会运行其他的很多进程：系统相关的进程（比如 SSHD、Udev等），以及 Kubernetes 集群的组件（Kubelet、Docker等）。我们在分配资源的时候，需要给这些进程预留一些资源，剩下的才能给 Pod 使用。预留的资源可以通过下面的参数控制：</p><ul><li><code>--kube-reserved=[cpu=100m][,][memory=100Mi][,][ephemeral-storage=1Gi]</code>：控制预留给 Kubernetes 集群组件的 CPU、Memory 和存储资源。</li><li><code>--system-reserved=[cpu=100mi][,][memory=100Mi][,][ephemeral-storage=1Gi]</code>：预留给系统的 CPU、Memory 和存储资源。</li></ul><p>这两块预留之后的资源才是 Pod 真正能使用的，不过考虑到 Eviction 机制（下面的章节会提到），Kubelet 会保证节点上的资源使用率不会真正到 100%，因此 Pod 的实际可使用资源会稍微再少一点。主机上的资源逻辑分配图如下所示：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-res2.png" alt="Kubernetes reserved resource"></p><blockquote><p>NOTE：需要注意的是，Allocatable 不是指当前机器上可以分配的资源，而是指能分配给 Pod 使用的资源总量，一旦 Kubelet 启动这个值是不会变化的。</p></blockquote><p>Allocatable 的值可以在 Node 对象的 <code>status</code> 字段中读取，比如下面这样：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">status:</span></span><br><span class="line"><span class="attr">  allocatable:</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="string">"2"</span></span><br><span class="line"><span class="attr">    ephemeral-storage:</span> <span class="string">"35730597829"</span></span><br><span class="line"><span class="attr">    hugepages-2Mi:</span> <span class="string">"0"</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="number">3779348</span><span class="string">Ki</span></span><br><span class="line"><span class="attr">    Pods:</span> <span class="string">"110"</span></span><br><span class="line"><span class="attr">  capacity:</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="string">"2"</span></span><br><span class="line"><span class="attr">    ephemeral-storage:</span> <span class="number">38770180</span><span class="string">Ki</span></span><br><span class="line"><span class="attr">    hugepages-2Mi:</span> <span class="string">"0"</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="number">3881748</span><span class="string">Ki</span></span><br><span class="line"><span class="attr">    Pods:</span> <span class="string">"110"</span></span><br></pre></td></tr></table></figure><h3 id="kubernetes-资源对象">Kubernetes 资源对象</h3><p>在这部分，我们来介绍 Kubernetes 中提供的让我们管理 Pod 资源的原生对象。</p><h4 id="请求requests和上限limits">请求（Requests）和上限（Limits）</h4><p>前面说过用户在创建 Pod 的时候，可以指定每个容器的 Requests 和 Limits 两个字段，下面是一个实例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line"><span class="attr">  requests:</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="string">"64Mi"</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="string">"250m"</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="string">"128Mi"</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="string">"500m"</span></span><br></pre></td></tr></table></figure><p><code>Requests</code> 是容器请求要使用的资源，Kubernetes 会保证 Pod 能使用到这么多的资源。请求的资源是调度的依据，只有当节点上的可用资源大于 Pod 请求的各种资源时，调度器才会把 Pod 调度到该节点上（如果 CPU 资源足够，内存资源不足，调度器也不会选择该节点）。</p><p>需要注意的是，调度器只关心节点上可分配的资源，以及节点上所有 Pods 请求的资源，而<strong>不关心</strong>节点资源的实际使用情况，换句话说，如果节点上的 Pods 申请的资源已经把节点上的资源用满，即使它们的使用率非常低，比如说 CPU 和内存使用率都低于 10%，调度器也不会继续调度 Pod 上去。</p><p><code>Limits</code> 是 Pod 能使用的资源上限，是实际配置到内核 cgroups 里面的配置数据。对于内存来说，会直接转换成 <code>docker run</code> 命令行的 <code>--memory</code> 大小，最终会配置到 cgroups 对应任务的 <code>/sys/fs/cgroup/memory/……/memory.limit_in_bytes</code> 文件中。</p><blockquote><p>NOTE：如果 Limit 没有配置，则表明没有资源的上限，只要节点上有对应的资源，Pod 就可以使用。</p></blockquote><p>使用 Requests 和 Limits 概念，我们能分配更多的 Pod，提升整体的资源使用率。但是这个体系有个非常重要的问题需要考虑，那就是<strong>怎么去准确地评估 Pod 的资源 Requests</strong>？如果评估地过低，会导致应用不稳定；如果过高，则会导致使用率降低。这个问题需要开发者和系统管理员共同讨论和定义。</p><h4 id="limit-range默认资源配置">Limit Range（默认资源配置)</h4><p>为每个 Pod 都手动配置这些参数是挺麻烦的事情，Kubernetes 提供了 <code>LimitRange</code> 资源，可以让我们配置某个 Namespace 默认的 Request 和 Limit 值，比如下面的实例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">"v1"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">"LimitRange"</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">you-shall-have-limits</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  limits:</span></span><br><span class="line"><span class="attr">    - type:</span> <span class="string">"Container"</span></span><br><span class="line"><span class="attr">      max:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="string">"2"</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="string">"1Gi"</span></span><br><span class="line"><span class="attr">      min:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="string">"100m"</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="string">"4Mi"</span></span><br><span class="line"><span class="attr">      default:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="string">"500m"</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="string">"200Mi"</span></span><br><span class="line"><span class="attr">      defaultRequest:</span></span><br><span class="line"><span class="attr">        cpu:</span> <span class="string">"200m"</span></span><br><span class="line"><span class="attr">        memory:</span> <span class="string">"100Mi"</span></span><br></pre></td></tr></table></figure><p>如果对应 Namespace 创建的 Pod 没有写资源的 Requests 和 Limits 字段，那么它会自动拥有下面的配置信息：</p><ul><li>内存请求是 100Mi，上限是 200Mi</li><li>CPU 请求是 200m，上限是 500m</li></ul><p>当然，如果 Pod 自己配置了对应的参数，Kubernetes 会使用 Pod 中的配置。使用 LimitRange 能够让 Namespace 中的 Pod 资源规范化，便于统一的资源管理。</p><h4 id="资源配额resource-quota">资源配额（Resource Quota）</h4><p>前面讲到的资源管理和调度可以认为 Kubernetes 把这个集群的资源整合起来，组成一个资源池，每个应用（Pod）会自动从整个池中分配资源来使用。默认情况下只要集群还有可用的资源，应用就能使用，并没有限制。Kubernetes 本身考虑到了多用户和多租户的场景，提出了 Namespace 的概念来对集群做一个简单的隔离。</p><p>基于 Namespace，Kubernetes 还能够对资源进行隔离和限制，这就是 Resource Quota 的概念，翻译成资源配额，它限制了某个 Namespace 可以使用的资源总额度。这里的资源包括 CPU、Memory 的总量，也包括 Kubernetes 自身对象（比如 Pod、Services 等）的数量。通过 Resource Quota，Kubernetes 可以防止某个 Namespace 下的用户不加限制地使用超过期望的资源，比如说不对资源进行评估就大量申请 16核 CPU 32G 内存的 Pod。</p><p>下面是一个资源配额的实例，它限制了 Namespace 只能使用 20 核 CPU 和 1G 内存，并且能创建 10 个 Pod、20 个 RC、5 个 Service，可能适用于某个测试场景。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ResourceQuota</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">quota</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  hard:</span></span><br><span class="line"><span class="attr">    cpu:</span> <span class="string">"20"</span></span><br><span class="line"><span class="attr">    memory:</span> <span class="number">1</span><span class="string">Gi</span></span><br><span class="line"><span class="attr">    Pods:</span> <span class="string">"10"</span></span><br><span class="line"><span class="attr">    replicationcontrollers:</span> <span class="string">"20"</span></span><br><span class="line"><span class="attr">    resourcequotas:</span> <span class="string">"1"</span></span><br><span class="line"><span class="attr">    services:</span> <span class="string">"5"</span></span><br></pre></td></tr></table></figure><p>Resource Quota 能够配置的选项还很多，比如 GPU、存储、Configmaps、PersistentVolumeClaims 等等，更多信息可以参考官方文档。</p><p>Resource Quota 要解决的问题和使用都相对独立和简单，但是它也有一个限制：那就是它不能根据集群资源动态伸缩。一旦配置之后，Resource Quota 就不会改变，即使集群增加了节点，整体资源增多也没有用。Kubernetes 现在没有解决这个问题，但是用户可以通过编写一个 Controller 的方式来自己实现。</p><h3 id="应用优先级">应用优先级</h3><h4 id="qos服务质量">QoS（服务质量）</h4><p>Requests 和 Limits 的配置除了表明资源情况和限制资源使用之外，还有一个隐藏的作用：它决定了 Pod 的 QoS 等级。</p><p>上一节我们提到了一个细节：如果 Pod 没有配置 Limits ，那么它可以使用节点上任意多的可用资源。这类 Pod 能灵活使用资源，但这也导致它不稳定且危险，对于这类 Pod 我们一定要在它占用过多资源导致节点资源紧张时处理掉。优先处理这类 Pod，而不是处理资源使用处于自己请求范围内的 Pod 是非常合理的想法，而这就是 Pod QoS 的含义：根据 Pod 的资源请求把 Pod 分成不同的重要性等级。</p><p>Kubernetes 把 Pod 分成了三个 QoS 等级：</p><ul><li><strong>Guaranteed</strong>：优先级最高，可以考虑数据库应用或者一些重要的业务应用。除非 Pods 使用超过了它们的 Limits，或者节点的内存压力很大而且没有 QoS 更低的 Pod，否则不会被杀死。</li><li><strong>Burstable</strong>：这种类型的 Pod 可以多于自己请求的资源（上限由 Limit 指定，如果 Limit 没有配置，则可以使用主机的任意可用资源），但是重要性认为比较低，可以是一般性的应用或者批处理任务。</li><li><strong>Best Effort</strong>：优先级最低，集群不知道 Pod 的资源请求情况，调度不考虑资源，可以运行到任意节点上（从资源角度来说），可以是一些临时性的不重要应用。Pod 可以使用节点上任何可用资源，但在资源不足时也会被优先杀死。</li></ul><p>Pod 的 Requests 和 Limits 是如何对应到这三个 QoS 等级上的，可以用下面一张表格概括：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-res3.png" alt="Pod QuS mapping"></p><p>看到这里，你也许看出来一个问题了：<strong>如果不配置 Requests 和 Limits，Pod 的 QoS 竟然是最低的</strong>。没错，所以推荐大家理解 QoS 的概念，并且按照需求<strong>一定要给 Pod 配置 Requests 和 Limits 参数</strong>，不仅可以让调度更准确，也能让系统更加稳定。</p><blockquote><p>NOTE：按照现在的方法根据 Pod 请求的资源进行配置不够灵活和直观，更理想的情况是用户可以直接配置 Pod 的 QoS，而不用关心具体的资源申请和上限值。但 Kubernetes 目前还没有这方面的打算。</p></blockquote><p>Pod 的 QoS 还决定了容器的 OOM（Out-Of-Memory）值，它们对应的关系如下：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-res4.png" alt="Pod QoS oom score"></p><p>可以看到，QoS 越高的 Pod OOM 值越低，也就越不容易被系统杀死。对于 Bustable Pod，它的值是根据 Request 和节点内存总量共同决定的:</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oomScoreAdjust := <span class="number">1000</span> - (<span class="number">1000</span>*memoryRequest)/memoryCapacity</span><br></pre></td></tr></table></figure><p>其中 <code>memoryRequest</code> 是 Pod 申请的资源，<code>memoryCapacity</code> 是节点的内存总量。可以看到，申请的内存越多，OOM 值越低，也就越不容易被杀死。</p><p>QoS 的作用会在后面介绍 Eviction 的时候详细讲解。</p><h4 id="pod-优先级priority">Pod 优先级（Priority）</h4><p>除了 QoS，Kubernetes 还允许我们自定义 Pod 的优先级，比如：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PriorityClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">high-priority</span></span><br><span class="line"><span class="attr">value:</span> <span class="number">1000000</span></span><br><span class="line"><span class="attr">globalDefault:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">"This priority class should be used for XYZ service Pods only."</span></span><br></pre></td></tr></table></figure><p>优先级的使用也比较简单，只需要在 <code>Pod.spec.PriorityClassName</code> 指定要使用的优先级名字，即可以设置当前 Pod 的优先级为对应的值。</p><p>Pod 的优先级在调度的时候会使用到。首先，待调度的 Pod 都在同一个队列中，启用了 Pod priority 之后，调度器会根据优先级的大小，把优先级高的 Pod 放在前面，提前调度。</p><p>另外，如果在调度的时候，发现某个 Pod 因为资源不足无法找到合适的节点，调度器会尝试 Preempt 的逻辑。简单来说，调度器会试图找到这样一个节点：找到它上面优先级低于当前要调度 Pod 的所有 Pod，如果杀死它们，能腾足够的资源，调度器会执行删除操作，把 Pod 调度到节点上。更多内容可以参考：<a href="https://Kubernetes.io/docs/concepts/configuration/Pod-priority-preemption/" target="_blank" rel="noopener">Pod Priority and Preemption - Kubernetes</a></p><h3 id="驱逐eviction">驱逐（Eviction）</h3><p>至此，我们讲述的都是理想情况下 Kubernetes 的工作状况，我们假设资源完全够用，而且应用也都是在使用规定范围内的资源。</p><p>但现实不会如此简单，在管理集群的时候我们常常会遇到资源不足的情况，在这种情况下我们要<strong>保证整个集群可用</strong>，并且尽可能<strong>减少应用的损失</strong>。保证集群可用比较容易理解，首先要保证系统层面的核心进程正常，其次要保证 Kubernetes 本身组件进程不出问题；但是如何量化应用的损失呢？首先能想到的是如果要杀死 Pod，要尽量减少总数。另外一个就和 Pod 的优先级相关了，那就是尽量杀死不那么重要的应用，让重要的应用不受影响。</p><p>Pod 的驱逐是在 Kubelet 中实现的，因为 Kubelet 能动态地感知到节点上资源使用率实时的变化情况。其核心的逻辑是：Kubelet 实时监控节点上各种资源的使用情况，一旦发现某个不可压缩资源出现要耗尽的情况，就会主动终止节点上的 Pod，让节点能够正常运行。被终止的 Pod 所有容器会停止，状态会被设置为 Failed。</p><h4 id="驱逐触发条件">驱逐触发条件</h4><p>那么哪些资源不足会导致 Kubelet 执行驱逐程序呢？目前主要有三种情况：实际内存不足、节点文件系统的可用空间（文件系统剩余大小和 Inode 数量）不足、以及镜像文件系统的可用空间（包括文件系统剩余大小和 Inode 数量）不足。</p><p>下面这图是具体的触发条件：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-res5.png" alt="eviction conddition"></p><p>有了数据的来源，另外一个问题是触发的时机，也就是到什么程度需要触发驱逐程序？Kubernetes 运行用户自己配置，并且支持两种模式：按照百分比和按照绝对数量。比如对于一个 32G 内存的节点当可用内存少于 10% 时启动驱逐程序，可以配置 <code>memory.available&lt;10%</code>或者 <code>memory.available&lt;3.2Gi</code>。</p><blockquote><p>NOTE：默认情况下，Kubelet 的驱逐规则是 <code>memory.available&lt;100Mi</code>，对于生产环境这个配置是不可接受的，所以一定要根据实际情况进行修改。</p></blockquote><h4 id="软驱逐soft-eviction和硬驱逐hard-eviction">软驱逐（Soft Eviction）和硬驱逐（Hard Eviction）</h4><p>因为驱逐 Pod 是具有毁坏性的行为，因此必须要谨慎。有时候内存使用率增高只是暂时性的，有可能 20s 内就能恢复，这时候启动驱逐程序意义不大，而且可能会导致应用的不稳定，我们要考虑到这种情况应该如何处理；另外需要注意的是，如果内存使用率过高，比如高于 95%（或者 90%，取决于主机内存大小和应用对稳定性的要求），那么我们不应该再多做评估和考虑，而是赶紧启动驱逐程序，因为这种情况再花费时间去判断可能会导致内存继续增长，系统完全崩溃。</p><p>为了解决这个问题，Kubernetes 引入了 Soft Eviction 和 Hard Eviction 的概念。</p><p><strong>软驱逐</strong>可以在资源紧缺情况并没有哪些严重的时候触发，比如内存使用率为 85%，软驱逐还需要配置一个时间指定软驱逐条件持续多久才触发，也就是说 Kubelet 在发现资源使用率达到设定的阈值之后，并不会立即触发驱逐程序，而是继续观察一段时间，如果资源使用率高于阈值的情况持续一定时间，才开始驱逐。并且驱逐 Pod 的时候，会遵循 Grace Period ，等待 Pod 处理完清理逻辑。和软驱逐相关的启动参数是：</p><ul><li><code>--eviction-soft</code>：软驱逐触发条件，比如 <code>memory.available&lt;1Gi</code>。</li><li><code>--eviction-sfot-grace-period</code>：触发条件持续多久才开始驱逐，比如 <code>memory.available=2m30s</code>。</li><li><code>--eviction-max-Pod-grace-period</code>：Kill Pod 时等待 Grace Period 的时间让 Pod 做一些清理工作，如果到时间还没有结束就做 Kill。</li></ul><p>前面两个参数必须同时配置，软驱逐才能正常工作；后一个参数会和 Pod 本身配置的 Grace Period 比较，选择较小的一个生效。</p><p><strong>硬驱逐</strong>更加直接干脆，Kubelet 发现节点达到配置的硬驱逐阈值后，立即开始驱逐程序，并且不会遵循 Grace Period，也就是说立即强制杀死 Pod。对应的配置参数只有一个 <code>--evictio-hard</code>，可以选择上面表格中的任意条件搭配。</p><p>设置这两种驱逐程序是为了平衡节点稳定性和对 Pod 的影响，软驱逐照顾到了 Pod 的优雅退出，减少驱逐对 Pod 的影响；而硬驱逐则照顾到节点的稳定性，防止资源的快速消耗导致节点不可用。</p><p>软驱逐和硬驱逐可以单独配置，不过还是推荐两者都进行配置，一起使用。</p><h4 id="驱逐哪些-pods">驱逐哪些 Pods？</h4><p>上面我们已经整体介绍了 Kubelet 驱逐 Pod 的逻辑和过程，那这里就牵涉到一个具体的问题：<strong>要驱逐哪些 Pod</strong>？驱逐的重要原则是尽量减少对应用程序的影响。</p><p>如果是存储资源不足，Kubelet 会根据情况清理状态为 Dead 的 Pod 和它的所有容器，以及清理所有没有使用的镜像。如果上述清理并没有让节点回归正常，Kubelet 就开始清理 Pod。</p><p>一个节点上会运行多个 Pod，驱逐所有的 Pods 显然是不必要的，因此要做出一个抉择：在节点上运行的所有 Pod 中选择一部分来驱逐。虽然这些 Pod 乍看起来没有区别，但是它们的地位是不一样的，正如乔治·奥威尔在《动物庄园》的那句话：</p><blockquote><p>所有动物生而平等，但有些动物比其他动物更平等。</p></blockquote><p>Pod 也是不平等的，有些 Pod 要比其他 Pod 更重要。只管来说，系统组件的 Pod 要比普通的 Pod 更重要，另外运行数据库的 Pod 自然要比运行一个无状态应用的 Pod 更重要。Kubernetes 又是怎么决定 Pod 的优先级的呢？这个问题的答案就藏在我们之前已经介绍过的内容里：Pod Requests 和 Limits、优先级（Priority），以及 Pod 实际的资源使用。</p><p>简单来说，Kubelet 会根据以下内容对 Pod 进行排序：Pod 是否使用了超过请求的紧张资源、Pod 的优先级、然后是使用的紧缺资源和请求的紧张资源之间的比例。具体来说，Kubelet 会按照如下的顺序驱逐 Pod：</p><ul><li>使用的紧张资源超过请求数量的 <code>BestEffort</code> 和 <code>Burstable</code> Pod，这些 Pod 内部又会按照优先级和使用比例进行排序。</li><li>紧张资源使用量低于 Requests 的 <code>Burstable</code> 和 <code>Guaranteed</code> 的 Pod 后面才会驱逐，只有当系统组件（Kubelet、Docker、Journald 等）内存不够，并且没有上面 QoS 比较低的 Pod 时才会做。执行的时候还会根据 Priority 排序，优先选择优先级低的 Pod。</li></ul><h4 id="防止波动">防止波动</h4><p>这里的波动有两种情况，我们先说说第一种。驱逐条件出发后，如果 Kubelet 驱逐一部分 Pod，让资源使用率低于阈值就停止，那么很可能过一段时间资源使用率又会达到阈值，从而再次出发驱逐，如此循环往复……为了处理这种问题，我们可以使用 <code>--eviction-minimum-reclaim</code>解决，这个参数配置每次驱逐至少清理出来多少资源才会停止。</p><p>另外一个波动情况是这样的：Pod 被驱逐之后并不会从此消失不见，常见的情况是 Kubernetes 会自动生成一个新的 Pod 来取代，并经过调度选择一个节点继续运行。如果不做额外处理，有理由相信 Pod 选择原来节点的可能性比较大（因为调度逻辑没变，而它上次调度选择的就是该节点），之所以说可能而不是绝对会再次选择该节点，是因为集群 Pod 的运行和分布和上次调度时极有可能发生了变化。</p><p>无论如何，如果被驱逐的 Pod 再次调度到原来的节点，很可能会再次触发驱逐程序，然后 Pod 再次被调度到当前节点，循环往复…… 这种事情当然是我们不愿意看到的，虽然看似复杂，但这个问题解决起来非常简单：驱逐发生后，Kubelet 更新节点状态，调度器感知到这一情况，暂时不往该节点调度 Pod 即可。<code>--eviction-pressure-transition-period</code> 参数可以指定 Kubelet 多久才上报节点的状态，因为默认的上报状态周期比较短，频繁更改节点状态会导致驱逐波动。</p><p>做一个总结，下面是一个使用了上面多种参数的驱逐配置实例（你应该能看懂它们是什么意思了）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">–eviction-soft=memory.available&lt;80%,nodefs.available&lt;2Gi \</span><br><span class="line">–eviction-soft-grace-period=memory.available=1m30s,nodefs.available=1m30s \</span><br><span class="line">–eviction-max-Pod-grace-period=120 \</span><br><span class="line">–eviction-hard=memory.available&lt;500Mi,nodefs.available&lt;1Gi \</span><br><span class="line">–eviction-pressure-transition-period=30s \</span><br><span class="line">--eviction-minimum-reclaim=<span class="string">"memory.available=0Mi,nodefs.available=500Mi,imagefs.available=2Gi"</span></span><br></pre></td></tr></table></figure><h3 id="碎片整理和重调度">碎片整理和重调度</h3><p>Kubernetes 的调度器在为 Pod 选择运行节点的时候，只会考虑到调度那个时间点集群的状态，经过一系列的算法选择一个<strong>当时最合适</strong>的节点。但是集群的状态是不断变化的，用户创建的 Pod 也是动态的，随着时间变化，原来调度到某个节点上的 Pod 现在看来可能有更好的节点可以选择。比如考虑到下面这些情况：</p><ul><li>调度 Pod 的条件已经不再满足，比如节点的 Taints 和 Labels 发生了变化。</li><li>新节点加入了集群。如果默认配置了把 Pod 打散，那么应该有一些 Pod 最好运行在新节点上。</li><li>节点的使用率不均匀。调度后，有些节点的分配率和使用率比较高，另外一些比较低。</li><li>节点上有资源碎片。有些节点调度之后还剩余部分资源，但是又低于任何 Pod 的请求资源；或者 Memory 资源已经用完，但是 CPU 还有挺多没有使用。</li></ul><p>想要解决上述的这些问题，都需要把 Pod 重新进行调度（把 Pod 从当前节点移动到另外一个节点）。但是默认情况下，一旦 Pod 被调度到节点上，除非给杀死否则不会移动到另外一个节点的。</p><p>为此 Kubernetes 社区孵化了一个称为  <a href="https://github.com/Kubernetes-incubator/descheduler" target="_blank" rel="noopener"><code>Descheduler</code></a> 的项目，专门用来做重调度。重调度的逻辑很简单：找到上面几种情况中已经不是最优的 Pod，把它们驱逐掉（Eviction）。</p><p>目前，Descheduler 不会决定驱逐的 Pod 应该调度到哪台机器，而是<strong>假定默认的调度器会做出正确的调度抉择</strong>。也就是说，之所以 Pod 目前不合适，不是因为调度器的算法有问题，而是因为集群的情况发生了变化。如果让调度器重新选择，调度器现在会把 Pod 放到合适的节点上。这种做法让 Descheduler 逻辑比较简单，而且避免了调度逻辑出现在两个组件中。</p><p>Descheduler 执行的逻辑是可以配置的，目前有几种场景：</p><ul><li><code>RemoveDuplicates</code>：RS、Deployment 中的 Pod 不能同时出现在一台机器上。</li><li><code>LowNodeUtilization</code>：找到资源使用率比较低的 Node，然后驱逐其他资源使用率比较高节点上的 Pod，期望调度器能够重新调度让资源更均衡。</li><li><code>RemovePodsViolatingInterPodAntiAffinity</code>：找到已经违反 Pod Anti Affinity 规则的 Pods 进行驱逐，可能是因为反亲和是后面加上去的。</li><li><code>RemovePodsViolatingNodeAffinity</code>：找到违反 Node Affinity 规则的 Pods 进行驱逐，可能是因为 Node 后面修改了 Label。</li></ul><p>当然，为了保证应用的稳定性，Descheduler 并不会随意地驱逐 Pod，还是会尊重 Pod 运行的规则，包括 Pod 的优先级（不会驱逐 Critical Pod，并且按照优先级顺序进行驱逐）和 PDB（如果违反了 PDB，则不会进行驱逐），并且不会驱逐没有 Deployment、RS、Jobs 的 Pod 不会驱逐，Daemonset Pod 不会驱逐，有 Local storage 的 Pod 也不会驱逐。</p><p>Descheduler 不是一个常驻的任务，每次执行完之后会退出，因此推荐使用 CronJob 来运行。</p><p>总的来说，Descheduler 是对原生调度器的补充，用来解决原生调度器的调度决策随着时间会变得失效，或者不够优化的缺陷。</p><h3 id="资源动态调整">资源动态调整</h3><p>动态调整的思路：应用的实际流量会不断变化，因此使用率也是不断变化的，为了应对应用流量的变化，我们应用能够自动调整应用的资源。比如在线商品应用在促销的时候访问量会增加，我们应该自动增加 Pod 运算能力来应对；当促销结束后，有需要自动降低 Pod 的运算能力防止浪费。</p><p>运算能力的增减有两种方式：改变单个 Pod 的资源，以及增减 Pod 的数量。这两种方式对应了 Kubernetes 的 HPA 和 VPA。</p><h4 id="horizontal-pod-autoscaling横向-pod-自动扩展">Horizontal Pod AutoScaling（横向 Pod 自动扩展）</h4><p><img src="https://www.hi-linux.com/img/linux/k8s-res6.png" alt="kubernetes HPA"></p><p>横向 Pod 自动扩展的思路是这样的：Kubernetes 会运行一个 Controller，周期性地监听 Pod 的资源使用情况，当高于设定的阈值时，会自动增加 Pod 的数量；当低于某个阈值时，会自动减少 Pod 的数量。自然，这里的阈值以及 Pod 的上限和下限的数量都是需要用户配置的。</p><p>上面这句话隐藏了一个重要的信息：HPA 只能和 RC、Deployment、RS 这些可以动态修改 Replicas 的对象一起使用，而无法用于单个 Pod、Daemonset（因为它控制的 Pod 数量不能随便修改）等对象。</p><p>目前官方的监控数据来源是 Metrics Server 项目，可以配置的资源只有 CPU，但是用户可以使用自定义的监控数据（比如：Prometheus）。其他资源（比如：Memory）的 HPA 支持也已经在路上了。</p><h4 id="vertical-pod-autoscaling">Vertical Pod AutoScaling</h4><p>和 HPA 的思路相似，只不过 VPA 调整的是单个 Pod 的 Request 值（包括 CPU 和 Memory）。VPA 包括三个组件：</p><ul><li>Recommander：消费 Metrics Server 或者其他监控组件的数据，然后计算 Pod 的资源推荐值。</li><li>Updater：找到被 VPA 接管的 Pod 中和计算出来的推荐值差距过大的，对其做 Update 操作（目前是 Evict，新建的 Pod 在下面 Admission Controller 中会使用推荐的资源值作为 Request）。</li><li>Admission Controller：新建的 Pod 会经过该 Admission  Controller，如果 Pod 是被 VPA 接管的，会使用 Recommander 计算出来的推荐值。</li></ul><p>可以看到，这三个组件的功能是互相补充的，共同实现了动态修改 Pod 请求资源的功能。相对于 HPA，目前 VPA 还处于 Alpha，并且还没有合并到官方的 Kubernetes Release 中，后续的接口和功能很可能会发生变化。</p><h4 id="cluster-auto-scaler">Cluster Auto Scaler</h4><p>随着业务的发展，应用会逐渐增多，每个应用使用的资源也会增加，总会出现集群资源不足的情况。为了动态地应对这一状况，我们还需要 CLuster Auto Scaler，能够根据整个集群的资源使用情况来增减节点。</p><p>对于公有云来说，Cluster Auto Scaler 就是监控这个集群因为资源不足而 Pending 的 Pod，根据用户配置的阈值调用公有云的接口来申请创建机器或者销毁机器。对于私有云，则需要对接内部的管理平台。</p><p>目前 HPA 和 VPA 不兼容，只能选择一个使用，否则两者会相互干扰。而且 VPA 的调整需要重启 Pod，这是因为 Pod 资源的修改是比较大的变化，需要重新走一下 Apiserver、调度的流程，保证整个系统没有问题。目前社区也有计划在做原地升级，也就是说不通过杀死 Pod 再调度新 Pod 的方式，而是直接修改原有 Pod 来更新。</p><p>理论上 HPA 和 VPA 是可以共同工作的，HPA 负责瓶颈资源，VPA 负责其他资源。比如对于 CPU 密集型的应用，使用 HPA  监听 CPU 使用率来调整 Pods 个数，然后用 VPA 监听其他资源（Memory、IO）来动态扩展这些资源的 Request 大小即可。当然这只是理想情况，</p><h3 id="总结">总结</h3><p>从前面介绍的各种 Kubernetes 调度和资源管理方案可以看出来，提高应用的资源使用率、保证应用的正常运行、维护调度和集群的公平性是件非常复杂的事情，Kubernetes 并没有<em>完美</em>的方法，而是对各种可能的问题不断提出一些针对性的方案。</p><p>集群的资源使用并不是静态的，而是随着时间不断变化的，目前 Kubernetes 的调度决策都是基于调度时集群的一个静态资源切片进行的，动态地资源调整是通过 Kubelet 的驱逐程序进行的，HPA 和 VPA 等方案也不断提出，相信后面会不断完善这方面的功能，让 Kubernetes 更加智能。</p><p>资源管理和调度、应用优先级、监控、镜像中心等很多东西相关，是个非常复杂的领域。在具体的实施和操作的过程中，常常要考虑到企业内部的具体情况和需求，做出针对性的调整，并且需要开发者、系统管理员、SRE、监控团队等不同小组一起合作。但是这种付出从整体来看是值得的，提升资源的利用率能有效地节约企业的成本，也能让应用更好地发挥出作用。</p><h3 id="参考文档">参考文档</h3><p>Kubernetes 官方文档：</p><ul><li><a href="https://Kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/" target="_blank" rel="noopener">Managing Compute Resources for Containers</a>：如何为 Pod 配置 cpu 和 memory 资源</li><li><a href="https://Kubernetes.io/docs/tasks/configure-Pod-container/quality-service-Pod/" target="_blank" rel="noopener">Configure Quality of Service for Pods - Kubernetes</a>：Pod QoS 的定义和配置规则</li><li><a href="https://Kubernetes.io/docs/tasks/administer-cluster/out-of-resource/" target="_blank" rel="noopener">Configure Out Of Resource Handling - Kubernetes</a>：配置资源不足时 Kubernetes 的 处理方式，也就是 eviction</li><li><a href="https://Kubernetes.io/docs/concepts/policy/resource-quotas/" target="_blank" rel="noopener">Kubernetes 官方文档：Resource Quota</a>：为 namespace 配置 quota</li><li><a href="https://github.com/Kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md" target="_blank" rel="noopener">community/resource-qos.md at master · Kubernetes/community · GitHub</a>：QoS 设计文档</li><li><a href="https://Kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/" target="_blank" rel="noopener">Reserve Compute Resources for System Daemons</a>：如何在节点上预留资源</li><li><a href="https://github.com/Kubernetes-incubator/descheduler" target="_blank" rel="noopener">GitHub - Kubernetes-incubator/descheduler: Descheduler for Kubernetes</a>：descheduler 重调度官方 repo</li><li><a href="https://Kubernetes.io/docs/tasks/run-application/horizontal-Pod-autoscale/" target="_blank" rel="noopener">Horizontal Pod Autoscaler - Kubernetes</a>：Kubernetes HPA 介绍文档</li><li><a href="https://github.com/Kubernetes/community/blob/master/contributors/design-proposals/autoscaling/vertical-Pod-autoscaler.md" target="_blank" rel="noopener">community/vertical-Pod-autoscaler.md at master · Kubernetes/community · GitHub</a>: Kubernetes VPA 设计文档</li></ul><p>其他文档：</p><ul><li><a href="https://speakerdeck.com/thockin/everything-you-ever-wanted-to-know-about-resource-scheduling-dot-dot-dot-almost" target="_blank" rel="noopener">Everything You Ever Wanted To Know About Resource Scheduling… Almost - Speaker Deck</a>: Tim Hockin 在 kubecon 上介绍的 Kubernetes 资源管理理念，强烈推荐</li><li><a href="https://mp.weixin.qq.com/s/hyPNOcR3Nhy9bAiDhXUP7A" target="_blank" rel="noopener">聊聊Kubernetes计算资源模型（上）——资源抽象、计量与调度</a></li><li><a href="https://www.atatech.org/articles/99071" target="_blank" rel="noopener">【Sigma敏捷版系列文章】Kubernetes应用驱逐分析</a></li><li><a href="https://www.atatech.org/articles/99071" target="_blank" rel="noopener">Kubernetes 应用驱逐分析</a></li><li><a href="http://www.noqcks.io/notes/2018/02/03/understanding-Kubernetes-resources/" target="_blank" rel="noopener">Understanding Kubernetes Resources | Benji Visser</a>：介绍了 Kubernetes 的资源模型</li><li><a href="https://cloud.tencent.com/developer/article/1004976" target="_blank" rel="noopener">Kubernetes 资源分配之 Request 和 Limit 解析 - 云+社区 - 腾讯云</a>：用图表的方式解释了 requests 和 limits 的含义，以及在提高资源使用率方面的作用</li><li><a href="https://my.oschina.net/HardySimpson/blog/1359276" target="_blank" rel="noopener">Kubernetes中容器资源控制的那些事儿</a>：这篇文章介绍了 Kubernetes Pod 中 cpu 和 memory 的 request 和 limits 是如何最终变成 cgroups 配置的</li><li><a href="https://medium.com/@Rancher_Labs/the-three-pillars-of-Kubernetes-container-orchestration-247f42115a4a" target="_blank" rel="noopener">The Three Pillars of Kubernetes Container Orchestration</a>：Kubernetes 调度、资源管理和服务介绍</li><li><a href="https://www.slideshare.net/AmazonWebServices/dem19-advanced-auto-scaling-and-deployment-tools-for-Kubernetes-and-ecs" target="_blank" rel="noopener">DEM19 Advanced Auto Scaling and Deployment Tools for Kubernetes and E…</a></li><li><a href="https://my.oschina.net/jxcdwangtao/blog/837875" target="_blank" rel="noopener">Kubernetes Resource QoS机制解读</a></li><li><a href="https://www.jianshu.com/p/a5a7b3fb6806" target="_blank" rel="noopener">深入解析 Kubernetes 资源管理，容器云牛人有话说 - 简书</a></li></ul><blockquote><p>来源：Cizixs Writes Here<br>原文：<a href="http://t.cn/ReTAKl3" target="_blank" rel="noopener">http://t.cn/ReTAKl3</a><br>题图：来自谷歌图片搜索<br>版权：本文版权归原作者所有</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Kubernetes 从创建之初的核心模块之一就是资源调度。想要在生产环境使用好 Kubernetes，必须对它的资源模型以及资源管理非常了解。这篇文章算是对散布在网络上的 Kubernetes 资源管理内容的一个总结。干货文章，强列推荐一读。&lt;/p&gt;
&lt;h3 id=&quot;kubernetes-资源简介&quot;&gt;Kubernetes 资源简介&lt;/h3&gt;
&lt;h4 id=&quot;什么是资源？&quot;&gt;什么是资源？&lt;/h4&gt;
&lt;p&gt;在 Kubernetes 中，有两个基础但是非常重要的概念：Node 和 Pod。Node 翻译成节点，是对集群资源的抽象；Pod 是对容器的封装，是应用运行的实体。Node 提供资源，而 Pod 使用资源，这里的资源分为计算（CPU、Memory、GPU）、存储（Disk、SSD）、网络（Network Bandwidth、IP、Ports）。这些资源提供了应用运行的基础，正确理解这些资源以及集群调度如何使用这些资源，对于大规模的 Kubernetes 集群来说至关重要，不仅能保证应用的稳定性，也可以提高资源的利用率。&lt;/p&gt;
&lt;p&gt;在这篇文章，我们主要介绍 CPU 和内存这两个重要的资源，它们虽然都属于计算资源，但也有所差距。CPU 可分配的是使用时间，也就是操作系统管理的时间片，每个进程在一定的时间片里运行自己的任务（另外一种方式是绑核，也就是把 CPU 完全分配给某个 Pod 使用，但这种方式不够灵活会造成严重的资源浪费，Kubernetes 中并没有提供）；而对于内存，系统提供的是内存大小。&lt;/p&gt;
&lt;p&gt;CPU 的使用时间是可压缩的，换句话说它本身无状态，申请资源很快，也能快速正常回收；而内存大小是不可压缩的，因为它是有状态的（内存里面保存的数据），申请资源很慢（需要计算和分配内存块的空间），并且回收可能失败（被占用的内存一般不可回收）。&lt;/p&gt;
&lt;p&gt;把资源分成&lt;strong&gt;可压缩&lt;/strong&gt;和&lt;strong&gt;不可压缩&lt;/strong&gt;，是因为在资源不足的时候，它们的表现很不一样。对于不可压缩资源，如果资源不足，也就无法继续申请资源（内存用完就是用完了），并且会导致 Pod 的运行产生无法预测的错误（应用申请内存失败会导致一系列问题）；而对于可压缩资源，比如 CPU 时间片，即使 Pod 使用的 CPU 资源很多，CPU 使用也可以按照权重分配给所有 Pod 使用，虽然每个人使用的时间片减少，但不会影响程序的逻辑。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>利用 Helm 快速部署 Ingress</title>
    <link href="https://www.hi-linux.com/posts/35116.html"/>
    <id>https://www.hi-linux.com/posts/35116.html</id>
    <published>2018-08-16T01:00:00.000Z</published>
    <updated>2018-08-24T06:03:49.438Z</updated>
    
    <content type="html"><![CDATA[<p>Ingress 是一种 Kubernetes 资源，也是将 Kubernetes 集群内服务暴露到外部的一种方式。本文将讲一讲如何用 Helm 在 Kubernetes 集群中部署 Ingress，并部署两个应用来演示 Ingress 的具体使用。</p><p>阅读本文前你需要先掌握 Helm 和一些 Kubernetes 服务暴露的相关知识点，如果你还不了解可以先读一读我之前写的 「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486154&amp;idx=1&amp;sn=becd5dd0fadfe0b6072f5dfdc6fdf786&amp;chksm=eac52be3ddb2a2f555b8b1028db97aa3e92d0a4880b56f361e4b11cd252771147c44c08c8913#rd" target="_blank" rel="noopener">Helm 入门指南</a>」和「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486130&amp;idx=1&amp;sn=41ee30f02113dac86398653f542a3c70&amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3#rd" target="_blank" rel="noopener">浅析从外部访问 Kubernetes 集群中应用的几种方式</a>」这两篇文章。</p><h3 id="部署-ingress-controller">部署 Ingress Controller</h3><p><code>Ingress</code> 只是一个统称，其由 <code>Ingress</code> 和 <code>Ingress Controller</code> 两部分组成。<code>Ingress</code> 用作将原来需要手动配置的规则抽象成一个 Ingress 对象，使用 YAML 格式的文件来创建和管理。<code>Ingress Controller</code> 用作通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化。</p><p>目前可用的 Ingress Controller 类型有很多，比如：Nginx、HAProxy、Traefik 等，我们将演示如何部署一个基于 Nginx 的 Ingress Controller。</p><p>这里我们使用 Helm 来部署，在开始部署前，请确认您已经安装和配置好 Helm 相关环境。</p><h4 id="查找软件仓库中是否有-nginx-ingress-包">查找软件仓库中是否有 Nginx Ingress 包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm search nginx-ingress</span><br><span class="line">NAME                CHART VERSIONAPP VERSIONDESCRIPTION</span><br><span class="line">stable/nginx-ingress0.9.5        0.10.2     An nginx Ingress controller that uses ConfigMap...</span><br></pre></td></tr></table></figure><blockquote><p>注：这里我们使用的是在阿里云 Helm 镜像仓库。如果你还不知道如何增加三方仓库，可先阅读 「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247486154&amp;idx=1&amp;sn=becd5dd0fadfe0b6072f5dfdc6fdf786&amp;chksm=eac52be3ddb2a2f555b8b1028db97aa3e92d0a4880b56f361e4b11cd252771147c44c08c8913#rd" target="_blank" rel="noopener">Helm 入门指南</a>」一文。</p></blockquote><p>阿里云 Helm 镜像仓库里的 nginx-ingress 软件包已经将要用到的相关容器镜像地址改成了国内可访问的地址。安装时需要用到的容器镜像有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">repository: registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.10.2</span><br><span class="line">repository: registry.cn-hangzhou.aliyuncs.com/google_containers/defaultbackend:1.3</span><br><span class="line">repository: sophos/nginx-vts-exporter:0.6</span><br></pre></td></tr></table></figure><a id="more"></a><h4 id="使用-helm-部署-nginx-ingress-controller">使用 Helm 部署 Nginx Ingress Controller</h4><p>Ingress Controller 本身对外暴露的方式有几种，比如：hostNetwork、externalIP 等。这里我们采用 externalIP 的方式进行，如果你要使用 hostNetwork 方式，可以使用 <code>controller.hostNetwork=true</code> 参数进行设置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"># 启用 RBAC 支持</span><br><span class="line">$ helm install --name nginx-ingress --set &quot;rbac.create=true,controller.service.externalIPs[0]=192.168.100.211,controller.service.externalIPs[1]=192.168.100.212,controller.service.externalIPs[2]=192.168.100.213&quot; stable/nginx-ingress</span><br><span class="line"></span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/ConfigMap</span><br><span class="line">NAME                      DATA  AGE</span><br><span class="line">nginx-ingress-controller  1     2m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/ClusterRole</span><br><span class="line">NAME           AGE</span><br><span class="line">nginx-ingress  2m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/ClusterRoleBinding</span><br><span class="line">NAME           AGE</span><br><span class="line">nginx-ingress  2m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/RoleBinding</span><br><span class="line">NAME           AGE</span><br><span class="line">nginx-ingress  2m</span><br><span class="line"></span><br><span class="line">==&gt; v1/ServiceAccount</span><br><span class="line">NAME           SECRETS  AGE</span><br><span class="line">nginx-ingress  1        2m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/Role</span><br><span class="line">NAME           AGE</span><br><span class="line">nginx-ingress  2m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                           TYPE          CLUSTER-IP      EXTERNAL-IP                                      PORT(S)                   AGE</span><br><span class="line">nginx-ingress-controller       LoadBalancer  10.254.84.72    192.168.100.211,192.168.100.212,192.168.100.213  80:8410/TCP,443:8948/TCP  2m</span><br><span class="line">nginx-ingress-default-backend  ClusterIP     10.254.206.175  &lt;none&gt;                                           80/TCP                    2m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/Deployment</span><br><span class="line">NAME                           DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">nginx-ingress-controller       1        1        1           1          2m</span><br><span class="line">nginx-ingress-default-backend  1        1        1           1          2m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/PodDisruptionBudget</span><br><span class="line">NAME                           MIN AVAILABLE  MAX UNAVAILABLE  ALLOWED DISRUPTIONS  AGE</span><br><span class="line">nginx-ingress-controller       1              N/A              0                    2m</span><br><span class="line">nginx-ingress-default-backend  1              N/A              0                    2m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                           READY  STATUS   RESTARTS  AGE</span><br><span class="line">nginx-ingress-controller-665cd897fc-n5rq4      1/1    Running  0         2m</span><br><span class="line">nginx-ingress-default-backend-df594cfc6-pbcxk  1/1    Running  0         2m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">The nginx-ingress controller has been installed.</span><br><span class="line">It may take a few minutes for the LoadBalancer IP to be available.</span><br><span class="line">You can watch the status by running &apos;kubectl --namespace default get services -o wide -w nginx-ingress-controller&apos;</span><br><span class="line"></span><br><span class="line">An example Ingress that makes use of the controller:</span><br><span class="line"></span><br><span class="line">  apiVersion: extensions/v1beta1</span><br><span class="line">  kind: Ingress</span><br><span class="line">  metadata:</span><br><span class="line">    annotations:</span><br><span class="line">      kubernetes.io/ingress.class: nginx</span><br><span class="line">    name: example</span><br><span class="line">    namespace: foo</span><br><span class="line">  spec:</span><br><span class="line">    rules:</span><br><span class="line">      - host: www.example.com</span><br><span class="line">        http:</span><br><span class="line">          paths:</span><br><span class="line">            - backend:</span><br><span class="line">                serviceName: exampleService</span><br><span class="line">                servicePort: 80</span><br><span class="line">              path: /</span><br><span class="line">    # This section is only required if TLS is to be enabled for the Ingress</span><br><span class="line">    tls:</span><br><span class="line">        - hosts:</span><br><span class="line">            - www.example.com</span><br><span class="line">          secretName: example-tls</span><br><span class="line"></span><br><span class="line">If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided:</span><br><span class="line"></span><br><span class="line">  apiVersion: v1</span><br><span class="line">  kind: Secret</span><br><span class="line">  metadata:</span><br><span class="line">    name: example-tls</span><br><span class="line">    namespace: foo</span><br><span class="line">  data:</span><br><span class="line">    tls.crt: &lt;base64 encoded cert&gt;</span><br><span class="line">    tls.key: &lt;base64 encoded key&gt;</span><br><span class="line">  type: kubernetes.io/tls</span><br></pre></td></tr></table></figure><p>部署完成后我们可以看到 Kubernetes 服务中增加了 <code>nginx-ingress-controller</code> 和 <code>nginx-ingress-default-backend</code> 两个服务。<code>nginx-ingress-controller</code> 为 <code>Ingress Controller</code>，主要做为一个七层的负载均衡器来提供 HTTP 路由、粘性会话、SSL 终止、SSL直通、TCP 和 UDP 负载平衡等功能。<code>nginx-ingress-default-backend</code> 为默认的后端，当集群外部的请求通过 Ingress 进入到集群内部时，如果无法负载到相应后端的 Service 上时，这种未知的请求将会被负载到这个默认的后端上。</p><p>由于我们采用了 externalIP 方式对外暴露服务， 所以 <code>nginx-ingress-controller</code> 会在 192.168.100.211、192.168.100.212、192.168.100.213 三台节点宿主机上的 暴露 80/443 端口。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc</span><br><span class="line">NAME                            TYPE           CLUSTER-IP       EXTERNAL-IP                                       PORT(S)                    AGE</span><br><span class="line">kubernetes                      ClusterIP      10.254.0.1       &lt;none&gt;                                            443/TCP                    18d</span><br><span class="line">nginx-ingress-controller        LoadBalancer   10.254.84.72     192.168.100.211,192.168.100.212,192.168.100.213   80:8410/TCP,443:8948/TCP   46s</span><br><span class="line">nginx-ingress-default-backend   ClusterIP      10.254.206.175   &lt;none&gt;                                            80/TCP                     46s</span><br></pre></td></tr></table></figure><h4 id="访问-nginx-ingress-controller">访问 Nginx Ingress Controller</h4><p>我们可以使用以下命令来获取 Nginx 的 HTTP 和 HTTPS 地址。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl --namespace default get services -o wide -w nginx-ingress-controller</span><br><span class="line">NAME                       TYPE           CLUSTER-IP     EXTERNAL-IP                                       PORT(S)                    AGE       SELECTOR</span><br><span class="line">nginx-ingress-controller   LoadBalancer   10.254.84.72   192.168.100.211,192.168.100.212,192.168.100.213   80:8410/TCP,443:8948/TCP   4h        app=nginx-ingress,component=controller,release=nginx-ingress</span><br></pre></td></tr></table></figure><p>因为我们还没有在 Kubernetes 集群中创建 Ingress资源，所以直接对 ExternalIP 的请求被负载到了 nginx-ingress-default-backend 上。nginx-ingress-default-backend 默认提供了两个 URL 进行访问，其中的 /healthz 用作健康检查返回 200，而 / 返回 404 错误。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># 返回 200</span><br><span class="line">$ curl -I  http://192.168.100.212/healthz/</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Server: nginx/1.13.8</span><br><span class="line">Date: Tue, 24 Jul 2018 06:25:58 GMT</span><br><span class="line">Content-Type: text/html</span><br><span class="line">Content-Length: 0</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Strict-Transport-Security: max-age=15724800; includeSubDomains;</span><br><span class="line"></span><br><span class="line"># 返回 404</span><br><span class="line">$ curl -I  http://192.168.100.212/</span><br><span class="line">HTTP/1.1 404 Not Found</span><br><span class="line">Server: nginx/1.13.8</span><br><span class="line">Date: Tue, 24 Jul 2018 06:26:39 GMT</span><br><span class="line">Content-Type: text/plain; charset=utf-8</span><br><span class="line">Content-Length: 21</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Strict-Transport-Security: max-age=15724800; includeSubDomains;</span><br><span class="line"></span><br><span class="line"># 返回 200</span><br><span class="line">$ curl -I --insecure https://192.168.100.212/healthz/</span><br><span class="line">HTTP/2 200</span><br><span class="line">server: nginx/1.13.8</span><br><span class="line">date: Tue, 24 Jul 2018 06:27:41 GMT</span><br><span class="line">content-type: text/html</span><br><span class="line">content-length: 0</span><br><span class="line">strict-transport-security: max-age=15724800; includeSubDomains;</span><br><span class="line"></span><br><span class="line"># 返回 404</span><br><span class="line">$ curl -I --insecure https://192.168.100.212/</span><br><span class="line">HTTP/2 404</span><br><span class="line">server: nginx/1.13.8</span><br><span class="line">date: Tue, 24 Jul 2018 06:28:25 GMT</span><br><span class="line">content-type: text/plain; charset=utf-8</span><br><span class="line">content-length: 21</span><br><span class="line">strict-transport-security: max-age=15724800; includeSubDomains;</span><br></pre></td></tr></table></figure><p>在几台节点宿主机上查看，我们可以看到 ExternalIP 的 Service 是通过 Kube-Proxy对外暴露的，这里的 192.168.100.211、192.168.100.212、192.168.100.213 是三个内网 IP。 实际生产应用中是需要通过边缘路由器或全局统一接入层的负载均衡器将到达公网 IP 的外网流量转发到这几个内网 IP 上，外部用户再通过域名访问集群中以 Ingress 暴露的所有服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ sudo netstat -tlunp|grep kube-proxy|grep -E &apos;80|443&apos;</span><br><span class="line">tcp        0      0 192.168.100.211:80      0.0.0.0:*               LISTEN      714/kube-proxy</span><br><span class="line">tcp        0      0 192.168.100.211:443     0.0.0.0:*               LISTEN      714/kube-proxy</span><br><span class="line"></span><br><span class="line">$ sudo netstat -tlunp|grep kube-proxy|grep -E &apos;80|443&apos;</span><br><span class="line">tcp        0      0 192.168.100.212:80      0.0.0.0:*               LISTEN      690/kube-proxy</span><br><span class="line">tcp        0      0 192.168.100.212:443     0.0.0.0:*               LISTEN      690/kube-proxy</span><br><span class="line"></span><br><span class="line">$ sudo netstat -tlunp|grep kube-proxy|grep -E &apos;80|443&apos;</span><br><span class="line">tcp        0      0 192.168.100.213:80      0.0.0.0:*               LISTEN      748/kube-proxy</span><br><span class="line">tcp        0      0 192.168.100.213:443     0.0.0.0:*               LISTEN      748/kube-proxy</span><br></pre></td></tr></table></figure><h4 id="卸载-nginx-ingress-controller">卸载 Nginx Ingress Controller</h4><p>使用 Helm 卸载 Nginx Ingress Controller 非常的简单，只需下面一条指令就搞定了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm delete --purge nginx-ingress</span><br></pre></td></tr></table></figure><p>使用 <code>--purge</code> 参数可以彻底删除 Release 不留下任何记录，否则下一次部署的时候不能使用重名的 Release。</p><h3 id="部署-ingress">部署 Ingress</h3><p>接下来，我们通过 Helm 以 Ingress 方式在 Kubernetes 集群中部署两个应用。由于测试环境没有使用 PersistentVolume（持久卷，简称 PV），下面两个例子中都暂时将其关闭。有关于 PersistentVolume 的知识点，我将在后面的文章来讲一讲，敬请期待。</p><h4 id="部署-dokuwiki">部署 DokuWiki</h4><p>DokuWiki 是一个针对小公司文件需求而开发的 Wiki 引擎，用 PHP 语言开发。DokuWiki 基于文本存储，不需要数据库。因为 DokuWiki 非常的轻量，所以我选择它来做演示。</p><p>这里我们使用 Helm 官方仓库里的 Chart 包来进行，因为阿里云镜像仓库中的很多 Chart 都不是最新的版本并且不支持以 Ingress 方式部署。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 从 Helm 官方 Chart 仓库迁出所有软件包</span><br><span class="line">$ git clone https://github.com/helm/charts.git</span><br></pre></td></tr></table></figure><p>使用 <code>helm install</code> 进行一键部署，并通过 <code>ingress.enabled=true</code> 和 <code>ingress.hosts[0].name=wiki.hi-linux.com</code> 参数启用 Ingress 特性和设置对应的主机名。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">$ cd /home/k8s/charts/stable</span><br><span class="line">$ helm install --name dokuwiki --set &quot;ingress.enabled=true,ingress.hosts[0].name=wiki.hi-linux.com,persistence.enabled=false&quot; dokuwiki</span><br><span class="line"></span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1beta1/Ingress</span><br><span class="line">NAME               HOSTS              ADDRESS  PORTS  AGE</span><br><span class="line">dokuwiki-dokuwiki  wiki.hi-linux.com  80       53s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                READY  STATUS   RESTARTS  AGE</span><br><span class="line">dokuwiki-dokuwiki-747b45cddb-qt8l2  1/1    Running  0         53s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Secret</span><br><span class="line">NAME               TYPE    DATA  AGE</span><br><span class="line">dokuwiki-dokuwiki  Opaque  1     54s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME               TYPE          CLUSTER-IP      EXTERNAL-IP  PORT(S)                   AGE</span><br><span class="line">dokuwiki-dokuwiki  LoadBalancer  10.254.235.137  &lt;pending&gt;    80:8430/TCP,443:8848/TCP  54s</span><br><span class="line"></span><br><span class="line">==&gt; v1beta1/Deployment</span><br><span class="line">NAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">dokuwiki-dokuwiki  1        1        1           1          54s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line"></span><br><span class="line">** Please be patient while the chart is being deployed **</span><br><span class="line"></span><br><span class="line">1. Get the DokuWiki URL indicated on the Ingress Rule and associate it to your cluster external IP:</span><br><span class="line"></span><br><span class="line">   export CLUSTER_IP=$(minikube ip) # On Minikube. Use: `kubectl cluster-info` on others K8s clusters</span><br><span class="line">   export HOSTNAME=$(kubectl get ingress --namespace default dokuwiki-dokuwiki -o jsonpath=&apos;&#123;.spec.rules[0].host&#125;&apos;)</span><br><span class="line">   echo &quot;Dokuwiki URL: http://$HOSTNAME/&quot;</span><br><span class="line">   echo &quot;$CLUSTER_IP  $HOSTNAME&quot; | sudo tee -a /etc/hosts</span><br><span class="line"></span><br><span class="line">2. Login with the following credentials</span><br><span class="line"></span><br><span class="line">  echo Username: user</span><br><span class="line">  echo Password: $(kubectl get secret --namespace default dokuwiki-dokuwiki -o jsonpath=&quot;&#123;.data.dokuwiki-password&#125;&quot; | base64 --decode)</span><br></pre></td></tr></table></figure><p>部署完成后，根据提示生成相应的登陆用户名和密码。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ echo Username: user</span><br><span class="line">Username: user</span><br><span class="line"></span><br><span class="line">$ echo Password: $(kubectl get secret --namespace default dokuwiki-dokuwiki -o jsonpath=&quot;&#123;.data.dokuwiki-password&#125;&quot; | base64 --decode)</span><br><span class="line">Password: e2GrABBkwF</span><br></pre></td></tr></table></figure><p>测试从各节点的宿主机 IP 访问应用，这里我们直接使用 Curl 命令进行访问。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">$ curl -I  http://wiki.hi-linux.com/doku.php -x 192.168.100.211:80</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Server: nginx/1.13.8</span><br><span class="line">Date: Wed, 25 Jul 2018 05:16:02 GMT</span><br><span class="line">Content-Type: text/html; charset=utf-8</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Vary: Accept-Encoding</span><br><span class="line">X-Powered-By: PHP/7.0.31</span><br><span class="line">Vary: Cookie,Accept-Encoding</span><br><span class="line">Set-Cookie: DokuWiki=k2clt6f2qe472ehsq6tcmh6v20; path=/; HttpOnly</span><br><span class="line">Expires: Thu, 19 Nov 1981 08:52:00 GMT</span><br><span class="line">Cache-Control: no-store, no-cache, must-revalidate</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Set-Cookie: DW68700bfd16c2027de7de74a5a8202a6f=deleted; expires=Thu, 01-Jan-1970 00:00:01 GMT; Max-Age=0; path=/; HttpOnly</span><br><span class="line">X-UA-Compatible: IE=edge,chrome=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ curl -I  http://wiki.hi-linux.com/doku.php -x 192.168.100.212:80</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Server: nginx/1.13.8</span><br><span class="line">Date: Wed, 25 Jul 2018 05:18:13 GMT</span><br><span class="line">Content-Type: text/html; charset=utf-8</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Vary: Accept-Encoding</span><br><span class="line">X-Powered-By: PHP/7.0.31</span><br><span class="line">Vary: Cookie,Accept-Encoding</span><br><span class="line">Set-Cookie: DokuWiki=ork8sv8qpurteblasuq3eb3nt2; path=/; HttpOnly</span><br><span class="line">Expires: Thu, 19 Nov 1981 08:52:00 GMT</span><br><span class="line">Cache-Control: no-store, no-cache, must-revalidate</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Set-Cookie: DW68700bfd16c2027de7de74a5a8202a6f=deleted; expires=Thu, 01-Jan-1970 00:00:01 GMT; Max-Age=0; path=/; HttpOnly</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ curl -I  http://wiki.hi-linux.com/doku.php -x 192.168.100.213:80</span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Server: nginx/1.13.8</span><br><span class="line">Date: Wed, 25 Jul 2018 05:18:30 GMT</span><br><span class="line">Content-Type: text/html; charset=utf-8</span><br><span class="line">Connection: keep-alive</span><br><span class="line">Vary: Accept-Encoding</span><br><span class="line">X-Powered-By: PHP/7.0.31</span><br><span class="line">Vary: Cookie,Accept-Encoding</span><br><span class="line">Set-Cookie: DokuWiki=6ulgtsddqq3rlo0mriavj64jc4; path=/; HttpOnly</span><br><span class="line">Expires: Thu, 19 Nov 1981 08:52:00 GMT</span><br><span class="line">Cache-Control: no-store, no-cache, must-revalidate</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Set-Cookie: DW68700bfd16c2027de7de74a5a8202a6f=deleted; expires=Thu, 01-Jan-1970 00:00:01 GMT; Max-Age=0; path=/; HttpOnly</span><br><span class="line">X-UA-Compatible: IE=edge,chrome=1</span><br></pre></td></tr></table></figure><p>Curl 用法很多，你也可以使用下面方式来达到相同的效果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -H &quot;Host:wiki.hi-linux.com&quot;  &quot;http://192.168.100.211/doku.php&quot;</span><br></pre></td></tr></table></figure><p>当然你也可以在本地 hosts 文件中对 IP 和域名进行绑定后，通过浏览器访问该应用。效果图如下：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-ingress01.png" alt=""></p><h4 id="部署-minio">部署 Minio</h4><p>Minio 是一个基于 Apache License v2.0 开源协议的对象存储服务，Minio 使用 Go 语言开发，具有良好的跨平台性，同样是一个非常轻量的服务。它兼容亚马逊 S3 云存储服务接口，非常适合于存储大容量非结构化的数据，例如：图片、视频、日志文件、备份数据和容器/虚拟机镜像等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 从 Helm 官方 Chart 仓库迁出所有软件包</span><br><span class="line">$ git clone https://github.com/helm/charts.git</span><br></pre></td></tr></table></figure><p>如果你需要修改主机名，请修改 values.yaml 文件中的 hosts 的值。我想你一定觉得很奇怪，为什么在这个例子我没用使用传递参数的方式来动态修改模板中对应的值？真相只有一个，哪就是我没有找到能成功修改模板中对应的变量，惊不惊喜，意不意外呢？哈哈哈。如果你知道可以留言告诉我哟！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cd /home/k8s/charts/stable</span><br><span class="line">$ cat minio/values.yaml</span><br><span class="line">  hosts:</span><br><span class="line">    - minio.hi-linux.com</span><br><span class="line">    #- chart-example.local</span><br></pre></td></tr></table></figure><p>使用 <code>helm install</code> 进行一键部署，并通过 <code>ingress.enabled=true</code> 参数启用 Ingress 特性。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --name minio  --set &quot;ingress.enabled=true,persistence.enabled=false&quot; minio</span><br><span class="line"></span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1beta1/Ingress</span><br><span class="line">NAME   HOSTS               ADDRESS  PORTS  AGE</span><br><span class="line">minio  minio.hi-linux.com  80       1m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                   READY  STATUS   RESTARTS  AGE</span><br><span class="line">minio-7c7cf49d4-gqf8p  1/1    Running  0         1m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Secret</span><br><span class="line">NAME   TYPE    DATA  AGE</span><br><span class="line">minio  Opaque  2     1m</span><br><span class="line"></span><br><span class="line">==&gt; v1/ConfigMap</span><br><span class="line">NAME   DATA  AGE</span><br><span class="line">minio  2     1m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME   TYPE       CLUSTER-IP  EXTERNAL-IP  PORT(S)   AGE</span><br><span class="line">minio  ClusterIP  None        &lt;none&gt;       9000/TCP  1m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta2/Deployment</span><br><span class="line">NAME   DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">minio  1        1        1           1          1m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line"></span><br><span class="line">Minio can be accessed via port 9000 on the following DNS name from within your cluster:</span><br><span class="line">minio-svc.default.svc.cluster.local</span><br><span class="line"></span><br><span class="line">To access Minio from localhost, run the below commands:</span><br><span class="line"></span><br><span class="line">  1. export POD_NAME=$(kubectl get pods --namespace default -l &quot;release=minio&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line"></span><br><span class="line">  2. kubectl port-forward $POD_NAME 9000 --namespace default</span><br><span class="line"></span><br><span class="line">Read more about port forwarding here: http://kubernetes.io/docs/user-guide/kubectl/kubectl_port-forward/</span><br><span class="line"></span><br><span class="line">You can now access Minio server on http://localhost:9000. Follow the below steps to connect to Minio server with mc client:</span><br><span class="line"></span><br><span class="line">  1. Download the Minio mc client - https://docs.minio.io/docs/minio-client-quickstart-guide</span><br><span class="line"></span><br><span class="line">  2. mc config host add minio-local http://localhost:9000 AKIAIOSFODNN7EXAMPLE wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY S3v4</span><br><span class="line"></span><br><span class="line">  3. mc ls minio-local</span><br><span class="line"></span><br><span class="line">Alternately, you can use your browser or the Minio SDK to access the server - https://docs.minio.io/categories/17</span><br></pre></td></tr></table></figure><p>部署完成后，我们在本地 hosts 文件中对 IP 和域名进行绑定，并通过浏览器访问该应用。效果图如下：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-ingress03.png" alt=""></p><blockquote><p>登陆用户名和密码在部署完成后的提示信息中。</p></blockquote><p>最后我们在 Kubernetes 上来查看下部署成功后的 Ingress 信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get ingress</span><br><span class="line">NAME                HOSTS                ADDRESS   PORTS     AGE</span><br><span class="line">dokuwiki-dokuwiki   wiki.hi-linux.com              80        44m</span><br><span class="line">minio               minio.hi-linux.com             80        50s</span><br><span class="line">`</span><br></pre></td></tr></table></figure><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/Revoeu7" target="_blank" rel="noopener">http://t.cn/Revoeu7</a><br><a href="http://t.cn/ReZkSqf" target="_blank" rel="noopener">http://t.cn/ReZkSqf</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Ingress 是一种 Kubernetes 资源，也是将 Kubernetes 集群内服务暴露到外部的一种方式。本文将讲一讲如何用 Helm 在 Kubernetes 集群中部署 Ingress，并部署两个应用来演示 Ingress 的具体使用。&lt;/p&gt;
&lt;p&gt;阅读本文前你需要先掌握 Helm 和一些 Kubernetes 服务暴露的相关知识点，如果你还不了解可以先读一读我之前写的 「&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247486154&amp;amp;idx=1&amp;amp;sn=becd5dd0fadfe0b6072f5dfdc6fdf786&amp;amp;chksm=eac52be3ddb2a2f555b8b1028db97aa3e92d0a4880b56f361e4b11cd252771147c44c08c8913#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Helm 入门指南&lt;/a&gt;」和「&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247486130&amp;amp;idx=1&amp;amp;sn=41ee30f02113dac86398653f542a3c70&amp;amp;chksm=eac52b9bddb2a28d6472d23eb764b6af0c8c290782e11c792dffde1a8f63c60ed7430445e0a3#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;浅析从外部访问 Kubernetes 集群中应用的几种方式&lt;/a&gt;」这两篇文章。&lt;/p&gt;
&lt;h3 id=&quot;部署-ingress-controller&quot;&gt;部署 Ingress Controller&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Ingress&lt;/code&gt; 只是一个统称，其由 &lt;code&gt;Ingress&lt;/code&gt; 和 &lt;code&gt;Ingress Controller&lt;/code&gt; 两部分组成。&lt;code&gt;Ingress&lt;/code&gt; 用作将原来需要手动配置的规则抽象成一个 Ingress 对象，使用 YAML 格式的文件来创建和管理。&lt;code&gt;Ingress Controller&lt;/code&gt; 用作通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化。&lt;/p&gt;
&lt;p&gt;目前可用的 Ingress Controller 类型有很多，比如：Nginx、HAProxy、Traefik 等，我们将演示如何部署一个基于 Nginx 的 Ingress Controller。&lt;/p&gt;
&lt;p&gt;这里我们使用 Helm 来部署，在开始部署前，请确认您已经安装和配置好 Helm 相关环境。&lt;/p&gt;
&lt;h4 id=&quot;查找软件仓库中是否有-nginx-ingress-包&quot;&gt;查找软件仓库中是否有 Nginx Ingress 包&lt;/h4&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ helm search nginx-ingress&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;NAME                	CHART VERSION	APP VERSION	DESCRIPTION&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;stable/nginx-ingress	0.9.5        	0.10.2     	An nginx Ingress controller that uses ConfigMap...&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;blockquote&gt;
&lt;p&gt;注：这里我们使用的是在阿里云 Helm 镜像仓库。如果你还不知道如何增加三方仓库，可先阅读 「&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247486154&amp;amp;idx=1&amp;amp;sn=becd5dd0fadfe0b6072f5dfdc6fdf786&amp;amp;chksm=eac52be3ddb2a2f555b8b1028db97aa3e92d0a4880b56f361e4b11cd252771147c44c08c8913#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Helm 入门指南&lt;/a&gt;」一文。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;阿里云 Helm 镜像仓库里的 nginx-ingress 软件包已经将要用到的相关容器镜像地址改成了国内可访问的地址。安装时需要用到的容器镜像有：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;repository: registry.cn-hangzhou.aliyuncs.com/google_containers/nginx-ingress-controller:0.10.2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;repository: registry.cn-hangzhou.aliyuncs.com/google_containers/defaultbackend:1.3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;repository: sophos/nginx-vts-exporter:0.6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>推荐一款比 Find 快 10 倍的搜索工具 FD</title>
    <link href="https://www.hi-linux.com/posts/15017.html"/>
    <id>https://www.hi-linux.com/posts/15017.html</id>
    <published>2018-08-13T01:00:00.000Z</published>
    <updated>2018-08-24T06:07:47.758Z</updated>
    
    <content type="html"><![CDATA[<p>fd 是基于 Rust 开发的一个速度超快的命令行搜索工具，fd 旨在成为 Linux / Unix 下 find 命令的替代品。</p><p>fd 虽然不能提供现在 find 命令所有的强大功能，但它也提供了足够强大的功能来满足你日常需要。比如：简洁的语法、彩色的终端输出、超快的查询速度、智能大小写、支持正则表达式以及可并行执行命令等特性。</p><p>项目地址：<a href="https://github.com/sharkdp/fd" target="_blank" rel="noopener">https://github.com/sharkdp/fd</a></p><h3 id="安装-fd">安装 fd</h3><p>fd 具有良好跨平台特性，支持在 Linux、macOS、Windows 等多种平台下安装。下面我们介绍下几个比较常用平台的安装方法：</p><ul><li>Ubuntu / Debain</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://github.com/sharkdp/fd/releases/download/v7.0.0/fd_7.0.0_amd64.deb</span><br><span class="line">$ sudo dpkg -i fd_7.0.0_amd64.deb</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>Fedora</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ dnf install fd-find</span><br></pre></td></tr></table></figure><ul><li>macOS</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew install fd</span><br></pre></td></tr></table></figure><ul><li>Windows</li></ul><p><a href="http://scoop.sh/" target="_blank" rel="noopener">Scoop</a> 和 <a href="https://chocolatey.org/" target="_blank" rel="noopener">Chocolatey</a> 都是 Windows 下的包管理系统，其具体使用方法都可参考其官网。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 通过 Scoop 安装</span><br><span class="line">$ scoop install fd</span><br><span class="line"></span><br><span class="line"># 通过 Chocolatey 安装</span><br><span class="line">$ choco install fd</span><br></pre></td></tr></table></figure><p>更多系统的安装方法可参考<a href="https://github.com/sharkdp/fd" target="_blank" rel="noopener">官方文档</a>。</p><h3 id="fd-命令行选项">fd 命令行选项</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">USAGE:</span><br><span class="line">    fd [FLAGS/OPTIONS] [&lt;pattern&gt;] [&lt;path&gt;...]</span><br><span class="line"></span><br><span class="line">FLAGS:</span><br><span class="line">    -H, --hidden            Search hidden files and directories</span><br><span class="line">    -I, --no-ignore         Do not respect .(git|fd)ignore files</span><br><span class="line">        --no-ignore-vcs     Do not respect .gitignore files</span><br><span class="line">    -s, --case-sensitive    Case-sensitive search (default: smart case)</span><br><span class="line">    -i, --ignore-case       Case-insensitive search (default: smart case)</span><br><span class="line">    -F, --fixed-strings     Treat the pattern as a literal string</span><br><span class="line">    -a, --absolute-path     Show absolute instead of relative paths</span><br><span class="line">    -L, --follow            Follow symbolic links</span><br><span class="line">    -p, --full-path         Search full path (default: file-/dirname only)</span><br><span class="line">    -0, --print0            Separate results by the null character</span><br><span class="line">    -h, --help              Prints help information</span><br><span class="line">    -V, --version           Prints version information</span><br><span class="line"></span><br><span class="line">OPTIONS:</span><br><span class="line">    -d, --max-depth &lt;depth&gt;        Set maximum search depth (default: none)</span><br><span class="line">    -t, --type &lt;filetype&gt;...       Filter by type: file (f), directory (d), symlink (l),</span><br><span class="line">                                   executable (x)</span><br><span class="line">    -e, --extension &lt;ext&gt;...       Filter by file extension</span><br><span class="line">    -x, --exec &lt;cmd&gt;               Execute a command for each search result</span><br><span class="line">    -E, --exclude &lt;pattern&gt;...     Exclude entries that match the given glob pattern</span><br><span class="line">        --ignore-file &lt;path&gt;...    Add a custom ignore-file in .gitignore format</span><br><span class="line">    -c, --color &lt;when&gt;             When to use colors: never, *auto*, always</span><br><span class="line">    -j, --threads &lt;num&gt;            Set number of threads to use for searching &amp; executing</span><br><span class="line"></span><br><span class="line">ARGS:</span><br><span class="line">    &lt;pattern&gt;    the search pattern, a regular expression (optional)</span><br><span class="line">    &lt;path&gt;...    the root directory for the filesystem search (optional)</span><br></pre></td></tr></table></figure><h3 id="fd-使用实例">fd 使用实例</h3><h4 id="简单搜索">简单搜索</h4><p>fd 只需带上一个需要查找的参数就可以执行最简单的搜索，该参数就是你要搜索的任何东西。例如：你想要找一个包含 pace 关键字的文件名或目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ fd pace</span><br><span class="line">source/lib/Han/dist/font/han-space.otf</span><br><span class="line">source/lib/Han/dist/font/han-space.woff</span><br><span class="line">source/lib/pace</span><br><span class="line">source/lib/pace/pace-theme-barber-shop.min.css</span><br><span class="line">source/lib/pace/pace-theme-big-counter.min.css</span><br><span class="line">source/lib/pace/pace-theme-bounce.min.css</span><br><span class="line">source/lib/pace/pace-theme-center-atom.min.css</span><br><span class="line">source/lib/pace/pace-theme-center-circle.min.css</span><br><span class="line">source/lib/pace/pace-theme-center-radar.min.css</span><br></pre></td></tr></table></figure><blockquote><p>注：fd 默认是不区分大小写和支持模糊查询的。</p></blockquote><h4 id="按指定类型进行搜索">按指定类型进行搜索</h4><p>默认情况下，fd 会搜索所有符合条件的结果。如果你想指定搜索的类型可以使用 <code>-t</code> 参数，fd 目前支持四种类型：<code>f</code>、<code>d</code>、<code>l</code>、<code>x</code>，分别表示：文件、目录、符号链接、可执行文件。下面我们来看几个实际的例子：</p><ul><li>只搜索包含 pace 关键字的文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ fd -tf pace</span><br><span class="line">source/lib/Han/dist/font/han-space.otf</span><br><span class="line">source/lib/Han/dist/font/han-space.woff</span><br><span class="line">source/lib/pace/pace-theme-barber-shop.min.css</span><br><span class="line">source/lib/pace/pace-theme-big-counter.min.css</span><br><span class="line">source/lib/pace/pace-theme-bounce.min.css</span><br><span class="line">source/lib/pace/pace-theme-center-atom.min.css</span><br><span class="line">source/lib/pace/pace-theme-center-circle.min.css</span><br><span class="line">source/lib/pace/pace-theme-center-radar.min.css</span><br></pre></td></tr></table></figure><ul><li>只搜索包含 pace 关键字的目录</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ fd -td pace</span><br><span class="line">source/lib/pace</span><br></pre></td></tr></table></figure><h4 id="搜索指定目录">搜索指定目录</h4><p>fd 默认会在当前目录和其下所有子目录中搜索，如果你想搜索指定的目录就需要在第二个参数中指定。例如：要在指定的 <code>/etc</code> 目录中搜索包含 passwd 关键字的文件或目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ fd passwd /etc</span><br><span class="line">/etc/master.passwd</span><br><span class="line">/etc/pam.d/chkpasswd</span><br><span class="line">/etc/pam.d/passwd</span><br><span class="line">/etc/passwd</span><br></pre></td></tr></table></figure><h4 id="通过正则表达式搜索">通过正则表达式搜索</h4><ul><li>搜索当前目录下以 head 开头并以 swig 结尾的文件。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ fd &apos;^head.*swig$&apos;</span><br><span class="line">layout/_custom/header.swig</span><br><span class="line">layout/_partials/head.swig</span><br><span class="line">layout/_partials/header.swig</span><br></pre></td></tr></table></figure><ul><li>搜索当前目录下文件名包含字母且文件名后缀为 PNG 的文件。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ fd &apos;[a-z]\.png$&apos;</span><br><span class="line">source/images/apple-touch-icon-next.png</span><br><span class="line">source/images/searchicon.png</span><br><span class="line">source/lib/fancybox/source/fancybox_overlay.png</span><br><span class="line">source/lib/fancybox/source/fancybox_sprite.png</span><br><span class="line">source/lib/fancybox/source/helpers/fancybox_buttons.png</span><br></pre></td></tr></table></figure><h4 id="其它技巧">其它技巧</h4><ul><li>搜索隐藏文件</li></ul><p>fd 支持隐藏文件搜索，如果你需要搜索隐藏文件可以加上 <code>-H</code> 参数。例如：在当前目录下搜索关键字为 zshrc 的隐藏文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ fd -H zshrc</span><br><span class="line">.zshrc</span><br></pre></td></tr></table></figure><ul><li>搜索指定扩展名的文件</li></ul><p>在当前目录下搜索文件扩展名为 md 的文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ fd -e md</span><br><span class="line">README.cn.md</span><br><span class="line">README.md</span><br><span class="line">source/lib/fastclick/README.md</span><br><span class="line">source/lib/jquery_lazyload/CONTRIBUTING.md</span><br><span class="line">source/lib/jquery_lazyload/README.md</span><br></pre></td></tr></table></figure><p>在当前目录下搜索文件名包含 reademe 且扩展名为 md 的文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ fd -e md readme</span><br><span class="line">README.cn.md</span><br><span class="line">README.md</span><br><span class="line">source/lib/fastclick/README.md</span><br><span class="line">source/lib/jquery_lazyload/README.md</span><br></pre></td></tr></table></figure><ul><li>排除特定的目录或文件</li></ul><p>搜索当前目录下除 lib 目录外的所有包含关键字 readme 的文件或目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ fd -E lib readme</span><br><span class="line">README.cn.md</span><br><span class="line">README.md</span><br></pre></td></tr></table></figure><p>搜索指定目录下除文件名后缀为 js 的所有文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ fd  -E &apos;*.js&apos; -tf  . source/lib/fastclick</span><br><span class="line">source/lib/fastclick/LICENSE</span><br><span class="line">source/lib/fastclick/README.md</span><br><span class="line">source/lib/fastclick/bower.json</span><br></pre></td></tr></table></figure><ul><li>结合外部命令对结果进行批量处理</li></ul><p>实现的方式有两种：一是和 find 命令的类似的处理方法，通过 <code>xargs</code> 命令来关联相关命令处理。二是通过 fd 自己的 <code>-x</code> 参数来实现。</p><p>我们来看一个具体的例子，统计当前目录下所有文件名后缀为 js 的文件的行数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 通过 -x 参数实现</span><br><span class="line">$ fd -e js -x  wc  -l</span><br><span class="line"></span><br><span class="line">      30 scripts/merge-configs.js</span><br><span class="line">    2225 scripts/merge.js</span><br><span class="line">      31 scripts/tags/button.js</span><br><span class="line">      12 scripts/tags/center-quote.js</span><br><span class="line">      59 scripts/tags/exturl.js</span><br><span class="line">      26 scripts/tags/full-image.js</span><br><span class="line">     833 scripts/tags/group-pictures.js</span><br><span class="line"></span><br><span class="line"># 通过 xargs 参数实现</span><br><span class="line">$ fd -0 -e js | xargs -0 wc  -l</span><br><span class="line">      30 scripts/merge-configs.js</span><br><span class="line">    2225 scripts/merge.js</span><br><span class="line">      31 scripts/tags/button.js</span><br><span class="line">      12 scripts/tags/center-quote.js</span><br><span class="line">      59 scripts/tags/exturl.js</span><br><span class="line">      26 scripts/tags/full-image.js</span><br><span class="line">     833 scripts/tags/group-pictures.js</span><br></pre></td></tr></table></figure><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RD03Aom" target="_blank" rel="noopener">http://t.cn/RD03Aom</a><br><a href="http://t.cn/ROV0Xos" target="_blank" rel="noopener">http://t.cn/ROV0Xos</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;fd 是基于 Rust 开发的一个速度超快的命令行搜索工具，fd 旨在成为 Linux / Unix 下 find 命令的替代品。&lt;/p&gt;
&lt;p&gt;fd 虽然不能提供现在 find 命令所有的强大功能，但它也提供了足够强大的功能来满足你日常需要。比如：简洁的语法、彩色的终端输出、超快的查询速度、智能大小写、支持正则表达式以及可并行执行命令等特性。&lt;/p&gt;
&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/sharkdp/fd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/sharkdp/fd&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装-fd&quot;&gt;安装 fd&lt;/h3&gt;
&lt;p&gt;fd 具有良好跨平台特性，支持在 Linux、macOS、Windows 等多种平台下安装。下面我们介绍下几个比较常用平台的安装方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ubuntu / Debain&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ wget https://github.com/sharkdp/fd/releases/download/v7.0.0/fd_7.0.0_amd64.deb&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo dpkg -i fd_7.0.0_amd64.deb&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="工具" scheme="https://www.hi-linux.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="工具" scheme="https://www.hi-linux.com/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>Helm 入门指南</title>
    <link href="https://www.hi-linux.com/posts/21466.html"/>
    <id>https://www.hi-linux.com/posts/21466.html</id>
    <published>2018-08-10T01:00:00.000Z</published>
    <updated>2018-08-24T05:29:01.283Z</updated>
    
    <content type="html"><![CDATA[<p>Helm 是 Kubernetes 生态系统中的一个软件包管理工具。本文将介绍 Helm 中的相关概念和基本工作原理，并通过一个具体的示例学习如何使用 Helm 打包、分发、安装、升级及回退 Kubernetes 应用。</p><h3 id="kubernetes-应用部署的挑战">Kubernetes 应用部署的挑战</h3><p>Kubernetes 是一个提供了基于容器的应用集群管理解决方案，Kubernetes 为容器化应用提供了部署运行、资源调度、服务发现和动态伸缩等一系列完整功能。</p><p>Kubernetes 的核心设计理念是: 用户定义要部署的应用程序的规则，而 Kubernetes 则负责按照定义的规则部署并运行应用程序。如果应用程序出现问题导致偏离了定义的规格，Kubernetes 负责对其进行自动修正。例如：定义的应用规则要求部署两个实例（Pod），其中一个实例异常终止了，Kubernetes 会检查到并重新启动一个新的实例。</p><p>用户通过使用 Kubernetes API 对象来描述应用程序规则，包括 Pod、Service、Volume、Namespace、ReplicaSet、Deployment、Job等等。一般这些资源对象的定义需要写入一系列的 YAML 文件中，然后通过 Kubernetes 命令行工具 Kubectl 调 Kubernetes API 进行部署。</p><p>以一个典型的三层应用 Wordpress 为例，该应用程序就涉及到多个 Kubernetes API 对象，而要描述这些 Kubernetes API 对象就可能要同时维护多个 YAML 文件。</p><p><img src="https://www.hi-linux.com/img/linux/helm01.png" alt=""></p><p>从上图可以看到，在进行 Kubernetes 软件部署时，我们面临下述几个问题：</p><ul><li>如何管理、编辑和更新这些这些分散的 Kubernetes 应用配置文件。</li><li>如何把一套相关的配置文件作为一个应用进行管理。</li><li>如何分发和重用 Kubernetes 的应用配置。</li></ul><p>Helm 的出现就是为了很好地解决上面这些问题。</p><a id="more"></a><h3 id="helm-是什么">Helm 是什么？</h3><p>Helm 是 <a href="https://deis.com/" target="_blank" rel="noopener">Deis</a> 开发的一个用于 Kubernetes 应用的包管理工具，主要用来管理 Charts。有点类似于 Ubuntu 中的 APT 或 CentOS 中的 YUM。</p><p>Helm Chart 是用来封装 Kubernetes 原生应用程序的一系列 YAML 文件。可以在你部署应用的时候自定义应用程序的一些 Metadata，以便于应用程序的分发。</p><p>对于应用发布者而言，可以通过 Helm 打包应用、管理应用依赖关系、管理应用版本并发布应用到软件仓库。</p><p>对于使用者而言，使用 Helm 后不用需要编写复杂的应用部署文件，可以以简单的方式在 Kubernetes 上查找、安装、升级、回滚、卸载应用程序。</p><h3 id="helm-组件及相关术语">Helm 组件及相关术语</h3><ul><li>Helm</li></ul><p>Helm 是一个命令行下的客户端工具。主要用于 Kubernetes 应用程序 Chart 的创建、打包、发布以及创建和管理本地和远程的 Chart 仓库。</p><ul><li>Tiller</li></ul><p>Tiller 是 Helm 的服务端，部署在 Kubernetes 集群中。Tiller 用于接收 Helm 的请求，并根据 Chart 生成 Kubernetes 的部署文件（ Helm 称为 Release ），然后提交给 Kubernetes 创建应用。Tiller 还提供了 Release 的升级、删除、回滚等一系列功能。</p><ul><li>Chart</li></ul><p>Helm 的软件包，采用 TAR 格式。类似于 APT 的 DEB 包或者 YUM 的 RPM 包，其包含了一组定义 Kubernetes 资源相关的 YAML 文件。</p><ul><li>Repoistory</li></ul><p>Helm 的软件仓库，Repository 本质上是一个 Web 服务器，该服务器保存了一系列的 Chart 软件包以供用户下载，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。</p><ul><li>Release</li></ul><p>使用 <code>helm install</code> 命令在 Kubernetes 集群中部署的 Chart 称为 Release。</p><blockquote><p>注：需要注意的是：Helm 中提到的 Release 和我们通常概念中的版本有所不同，这里的 Release 可以理解为 Helm 使用 Chart 包部署的一个应用实例。</p></blockquote><h3 id="helm-工作原理">Helm 工作原理</h3><p>这张图描述了 Helm 的几个关键组件 Helm（客户端）、Tiller（服务器）、Repository（Chart 软件仓库）、Chart（软件包）之间的关系。</p><p><img src="https://www.hi-linux.com/img/linux/helm02.png" alt=""></p><p><strong>Chart Install 过程</strong></p><ul><li>Helm 从指定的目录或者 TAR 文件中解析出 Chart 结构信息。</li><li>Helm 将指定的 Chart 结构和 Values 信息通过 gRPC 传递给 Tiller。</li><li>Tiller 根据 Chart 和 Values 生成一个 Release。</li><li>Tiller 将 Release 发送给 Kubernetes 用于生成 Release。</li></ul><p><strong>Chart Update 过程</strong></p><ul><li>Helm 从指定的目录或者 TAR 文件中解析出 Chart 结构信息。</li><li>Helm 将需要更新的 Release 的名称、Chart 结构和 Values 信息传递给 Tiller。</li><li>Tiller 生成 Release 并更新指定名称的 Release 的 History。</li><li>Tiller 将 Release 发送给 Kubernetes 用于更新 Release。</li></ul><p><strong>Chart Rollback 过程</strong></p><ul><li>Helm 将要回滚的 Release 的名称传递给 Tiller。</li><li>Tiller 根据 Release 的名称查找 History。</li><li>Tiller 从 History 中获取上一个 Release。</li><li>Tiller 将上一个 Release 发送给 Kubernetes 用于替换当前 Release。</li></ul><p><strong>Chart 处理依赖说明</strong></p><p>Tiller 在处理 Chart 时，直接将 Chart 以及其依赖的所有 Charts 合并为一个 Release，同时传递给 Kubernetes。因此 Tiller 并不负责管理依赖之间的启动顺序。Chart 中的应用需要能够自行处理依赖关系。</p><h3 id="部署-helm">部署 Helm</h3><h4 id="安装-helm-客户端">安装 Helm 客户端</h4><p>Helm 的安装方式很多，这里采用二进制的方式安装。更多安装方法可以参考 Helm 的<a href="https://docs.helm.sh/using_helm/#installing-helm" target="_blank" rel="noopener">官方帮助文档</a>。</p><ul><li>使用官方提供的脚本一键安装</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get &gt; get_helm.sh</span><br><span class="line">$ chmod 700 get_helm.sh</span><br><span class="line">$ ./get_helm.sh</span><br></pre></td></tr></table></figure><ul><li>手动下载安装</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 下载 Helm </span><br><span class="line">$ wget https://storage.googleapis.com/kubernetes-helm/helm-v2.9.1-linux-amd64.tar.gz</span><br><span class="line"># 解压 Helm</span><br><span class="line">$ tar -zxvf helm-v2.9.1-linux-amd64.tar.gz</span><br><span class="line"># 复制客户端执行文件到 bin 目录下</span><br><span class="line">$ cp linux-amd64/helm /usr/local/bin/</span><br></pre></td></tr></table></figure><blockquote><p>注：<a href="http://storage.googleapis.com" target="_blank" rel="noopener">storage.googleapis.com</a> 默认是不能访问的，该问题请自行解决。</p></blockquote><h4 id="安装-helm-服务器端-tiller">安装 Helm 服务器端 Tiller</h4><p>Tiller 是以 Deployment 方式部署在 Kubernetes 集群中的，只需使用以下指令便可简单的完成安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm init</span><br></pre></td></tr></table></figure><p>由于 Helm 默认会去 <a href="http://storage.googleapis.com" target="_blank" rel="noopener">storage.googleapis.com</a> 拉取镜像，如果你当前执行的机器不能访问该域名的话可以使用以下命令来安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 使用阿里云镜像安装并把默认仓库设置为阿里云上的镜像仓库</span><br><span class="line">$ helm init --upgrade --tiller-image registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.9.1 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br></pre></td></tr></table></figure><h5 id="给-tiller-授权">给 Tiller 授权</h5><p>因为 Helm 的服务端 Tiller 是一个部署在 Kubernetes 中 Kube-System Namespace 下 的 Deployment，它会去连接 Kube-Api 在 Kubernetes 里创建和删除应用。</p><p>而从 Kubernetes 1.6 版本开始，API Server 启用了 RBAC 授权。目前的 Tiller 部署时默认没有定义授权的 ServiceAccount，这会导致访问 API Server 时被拒绝。所以我们需要明确为 Tiller 部署添加授权。</p><ul><li>创建 Kubernetes 的服务帐号和绑定角色</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deployment --all-namespaces</span><br><span class="line">NAMESPACE     NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">kube-system   tiller-deploy          1         1         1            1           1h</span><br><span class="line">$ kubectl create serviceaccount --namespace kube-system tiller</span><br><span class="line">$ kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller</span><br></pre></td></tr></table></figure><ul><li>为 Tiller 设置帐号</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 使用 kubectl patch 更新 API 对象</span><br><span class="line">$ kubectl patch deploy --namespace kube-system tiller-deploy -p &apos;&#123;&quot;spec&quot;:&#123;&quot;template&quot;:&#123;&quot;spec&quot;:&#123;&quot;serviceAccount&quot;:&quot;tiller&quot;&#125;&#125;&#125;&#125;&apos;</span><br><span class="line">deployment.extensions &quot;tiller-deploy&quot; patched</span><br></pre></td></tr></table></figure><ul><li>查看是否授权成功</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deploy --namespace kube-system   tiller-deploy  --output yaml|grep  serviceAccount</span><br><span class="line">serviceAccount: tiller</span><br><span class="line">serviceAccountName: tiller</span><br></pre></td></tr></table></figure><h5 id="验证-tiller-是否安装成功">验证 Tiller 是否安装成功</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kube-system get pods|grep tiller</span><br><span class="line">tiller-deploy-6d68f5c78f-nql2z          1/1       Running   0          5m</span><br><span class="line"></span><br><span class="line">$ helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.9.1&quot;, GitCommit:&quot;20adb27c7c5868466912eebdf6664e7390ebe710&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure><h4 id="卸载-helm-服务器端-tiller">卸载 Helm 服务器端 Tiller</h4><p>如果你需要在 Kubernetes 中卸载已部署的 Tiller，可使用以下命令完成卸载。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm reset</span><br></pre></td></tr></table></figure><h3 id="构建一个-helm-chart">构建一个 Helm Chart</h3><p>下面我们通过一个完整的示例来学习如何使用 Helm 创建、打包、分发、安装、升级及回退Kubernetes应用。</p><h4 id="创建一个名为-mychart-的-chart">创建一个名为 mychart 的 Chart</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm create mychart</span><br></pre></td></tr></table></figure><p>该命令创建了一个 mychart 目录，该目录结构如下所示。这里我们主要关注目录中的 Chart.yaml、values.yaml、NOTES.txt 和 Templates 目录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ tree mychart/</span><br><span class="line">mychart/</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   └── service.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">2 directories, 7 files</span><br></pre></td></tr></table></figure><ul><li>Chart.yaml 用于描述这个 Chart的相关信息，包括名字、描述信息以及版本等。</li><li>values.yaml 用于存储 templates 目录中模板文件中用到变量的值。</li><li>NOTES.txt 用于介绍 Chart 部署后的一些信息，例如：如何使用这个 Chart、列出缺省的设置等。</li><li>Templates 目录下是 YAML 文件的模板，该模板文件遵循 Go template 语法。</li></ul><p>Templates 目录下 YAML 文件模板的值默认都是在 values.yaml 里定义的，比如在 deployment.yaml 中定义的容器镜像。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">image: &quot;&#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag &#125;&#125;&quot;</span><br></pre></td></tr></table></figure><p>其中的 <code>.Values.image.repository</code> 的值就是在  values.yaml 里定义的 nginx，<code>.Values.image.tag</code> 的值就是 stable。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cat mychart/values.yaml|grep repository</span><br><span class="line">repository: nginx</span><br><span class="line"></span><br><span class="line">$ cat mychart/values.yaml|grep tag</span><br><span class="line">tag: stable</span><br></pre></td></tr></table></figure><p>以上两个变量值是在 <code>create chart</code> 的时候就自动生成的默认值，你可以根据实际情况进行修改。</p><blockquote><p>如果你需要了解更多关于 Go 模板的相关信息，可以查看 <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a> 的一个关于 <a href="https://gohugo.io/templates/go-templates/" target="_blank" rel="noopener">Go 模板</a> 的介绍。</p></blockquote><h4 id="编写应用的介绍信息">编写应用的介绍信息</h4><p>打开 Chart.yaml, 填写你部署的应用的详细信息，以 mychart 为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ cat mychart/Chart.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: &quot;1.0&quot;</span><br><span class="line">description: A Helm chart for Kubernetes</span><br><span class="line">name: mychart</span><br><span class="line">version: 0.1.0</span><br></pre></td></tr></table></figure><h4 id="编写应用具体部署信息">编写应用具体部署信息</h4><p>编辑 values.yaml，它默认会在 Kubernetes 部署一个 Nginx。下面是 mychart 应用的 values.yaml 文件的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">$ cat mychart/values.yaml</span><br><span class="line"># Default values for mychart.</span><br><span class="line"># This is a YAML-formatted file.</span><br><span class="line"># Declare variables to be passed into your templates.</span><br><span class="line"></span><br><span class="line">replicaCount: 1</span><br><span class="line"></span><br><span class="line">image:</span><br><span class="line">  repository: nginx</span><br><span class="line">  tag: stable</span><br><span class="line">  pullPolicy: IfNotPresent</span><br><span class="line"></span><br><span class="line">service:</span><br><span class="line">  type: ClusterIP</span><br><span class="line">  port: 80</span><br><span class="line"></span><br><span class="line">ingress:</span><br><span class="line">  enabled: false</span><br><span class="line">  annotations: &#123;&#125;</span><br><span class="line">    # kubernetes.io/ingress.class: nginx</span><br><span class="line">    # kubernetes.io/tls-acme: &quot;true&quot;</span><br><span class="line">  path: /</span><br><span class="line">  hosts:</span><br><span class="line">    - chart-example.local</span><br><span class="line">  tls: []</span><br><span class="line">  #  - secretName: chart-example-tls</span><br><span class="line">  #    hosts:</span><br><span class="line">  #      - chart-example.local</span><br><span class="line"></span><br><span class="line">resources: &#123;&#125;</span><br><span class="line">  # We usually recommend not to specify default resources and to leave this as a conscious</span><br><span class="line">  # choice for the user. This also increases chances charts run on environments with little</span><br><span class="line">  # resources, such as Minikube. If you do want to specify resources, uncomment the following</span><br><span class="line">  # lines, adjust them as necessary, and remove the curly braces after &apos;resources:&apos;.</span><br><span class="line">  # limits:</span><br><span class="line">  #  cpu: 100m</span><br><span class="line">  #  memory: 128Mi</span><br><span class="line">  # requests:</span><br><span class="line">  #  cpu: 100m</span><br><span class="line">  #  memory: 128Mi</span><br><span class="line"></span><br><span class="line">nodeSelector: &#123;&#125;</span><br><span class="line"></span><br><span class="line">tolerations: []</span><br><span class="line"></span><br><span class="line">affinity: &#123;&#125;</span><br></pre></td></tr></table></figure><h4 id="检查依赖和模板配置是否正确">检查依赖和模板配置是否正确</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ helm lint mychart/</span><br><span class="line">==&gt; Linting .</span><br><span class="line">[INFO] Chart.yaml: icon is recommended</span><br><span class="line"></span><br><span class="line">1 chart(s) linted, no failures</span><br></pre></td></tr></table></figure><p>如果文件格式错误，可以根据提示进行修改。</p><h4 id="将应用打包">将应用打包</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm package mychart</span><br><span class="line">Successfully packaged chart and saved it to: /home/k8s/mychart-0.1.0.tgz</span><br></pre></td></tr></table></figure><p>mychart 目录会被打包为一个 mychart-0.1.0.tgz 格式的压缩包，该压缩包会被放到当前目录下，并同时被保存到了 Helm 的本地缺省仓库目录中。</p><p>如果你想看到更详细的输出，可以加上 <code>--debug</code> 参数来查看打包的输出，输出内容应该类似如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm package mychart --debug</span><br><span class="line">Successfully packaged chart and saved it to: /home/k8s/mychart-0.1.0.tgz</span><br><span class="line">[debug] Successfully saved /home/k8s/mychart-0.1.0.tgz to /home/k8s/.helm/repository/local</span><br></pre></td></tr></table></figure><h4 id="将应用发布到-repository">将应用发布到 Repository</h4><p>虽然我们已经打包了 Chart 并发布到了 Helm 的本地目录中，但通过 <code>helm search</code> 命令查找，并不能找不到刚才生成的 mychart包。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm search mychart</span><br><span class="line">No results found</span><br></pre></td></tr></table></figure><p>这是因为 Repository 目录中的 Chart 包还没有被 Helm 管理。通过 <code>helm repo list</code> 命令可以看到目前 Helm 中已配置的 Repository 的信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm repo list</span><br><span class="line">NAME    URL</span><br><span class="line">stable  https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts</span><br></pre></td></tr></table></figure><blockquote><p>注：新版本中执行 helm init 命令后默认会配置一个名为 local 的本地仓库。</p></blockquote><p>我们可以在本地启动一个 Repository Server，并将其加入到 Helm Repo 列表中。Helm Repository 必须以 Web 服务的方式提供，这里我们就使用 <code>helm serve</code> 命令启动一个 Repository Server，该 Server 缺省使用 <code>$HOME/.helm/repository/local</code> 目录作为 Chart 存储，并在 8879 端口上提供服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm serve &amp;</span><br><span class="line">Now serving you on 127.0.0.1:8879</span><br></pre></td></tr></table></figure><p>默认情况下该服务只监听 127.0.0.1，如果你要绑定到其它网络接口，可使用以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm serve --address 192.168.100.211:8879 &amp;</span><br></pre></td></tr></table></figure><p>如果你想使用指定目录来做为 Helm Repository 的存储目录，可以加上 <code>--repo-path</code> 参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm serve --address 192.168.100.211:8879 --repo-path /data/helm/repository/ --url http://192.168.100.211:8879/charts/</span><br></pre></td></tr></table></figure><p>通过 <code>helm repo index</code> 命令将 Chart 的 Metadata 记录更新在 index.yaml 文件中:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># 更新 Helm Repository 的索引文件</span><br><span class="line">$ cd /home/k8s/.helm/repository/local</span><br><span class="line">$ helm repo index --url=http://192.168.100.211:8879 .</span><br></pre></td></tr></table></figure><p>完成启动本地 Helm Repository Server 后，就可以将本地 Repository 加入 Helm 的 Repo 列表。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm repo add local http://127.0.0.1:8879</span><br><span class="line">&quot;local&quot; has been added to your repositories</span><br></pre></td></tr></table></figure><p>现在再次查找 mychart 包，就可以搜索到了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ helm repo update</span><br><span class="line">$ helm search mychart</span><br><span class="line">NAME         CHART VERSIONAPP VERSIONDESCRIPTION</span><br><span class="line">local/mychart0.1.0        1.0        A Helm chart for Kubernetes</span><br></pre></td></tr></table></figure><h4 id="在-kubernetes-中部署应用">在 Kubernetes 中部署应用</h4><h5 id="部署一个应用">部署一个应用</h5><p>Chart 被发布到仓储后，就可以通过 <code>helm install</code> 命令部署该 Chart。</p><ul><li>检查配置和模板是否有效</li></ul><p>当使用 <code>helm install</code> 命令部署应用时，实际上就是将 templates 目录下的模板文件渲染成 Kubernetes 能够识别的 YAML 格式。</p><p>在部署前我们可以使用 <code>helm install --dry-run --debug &lt;chart_dir&gt; --name &lt;release_name&gt;</code>命令来验证 Chart 的配置。该输出中包含了模板的变量配置与最终渲染的 YAML 文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --dry-run --debug local/mychart --name mike-test</span><br><span class="line">[debug] Created tunnel using local port: &apos;46649&apos;</span><br><span class="line"></span><br><span class="line">[debug] SERVER: &quot;127.0.0.1:46649&quot;</span><br><span class="line"></span><br><span class="line">[debug] Original chart version: &quot;&quot;</span><br><span class="line">[debug] Fetched local/mychart to /home/k8s/.helm/cache/archive/mychart-0.1.0.tgz</span><br><span class="line"></span><br><span class="line">[debug] CHART PATH: /home/k8s/.helm/cache/archive/mychart-0.1.0.tgz</span><br><span class="line"></span><br><span class="line">NAME:   mike-test</span><br><span class="line">REVISION: 1</span><br><span class="line">RELEASED: Mon Jul 23 10:39:49 2018</span><br><span class="line">CHART: mychart-0.1.0</span><br><span class="line">USER-SUPPLIED VALUES:</span><br><span class="line">&#123;&#125;</span><br><span class="line"></span><br><span class="line">COMPUTED VALUES:</span><br><span class="line">affinity: &#123;&#125;</span><br><span class="line">image:</span><br><span class="line">  pullPolicy: IfNotPresent</span><br><span class="line">  repository: nginx</span><br><span class="line">  tag: stable</span><br><span class="line">ingress:</span><br><span class="line">  annotations: &#123;&#125;</span><br><span class="line">  enabled: false</span><br><span class="line">  hosts:</span><br><span class="line">  - chart-example.local</span><br><span class="line">  path: /</span><br><span class="line">  tls: []</span><br><span class="line">nodeSelector: &#123;&#125;</span><br><span class="line">replicaCount: 1</span><br><span class="line">resources: &#123;&#125;</span><br><span class="line">service:</span><br><span class="line">  port: 80</span><br><span class="line">  type: ClusterIP</span><br><span class="line">tolerations: []</span><br><span class="line"></span><br><span class="line">HOOKS:</span><br><span class="line">MANIFEST:</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"># Source: mychart/templates/service.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: mike-test-mychart</span><br><span class="line">  labels:</span><br><span class="line">    app: mychart</span><br><span class="line">    chart: mychart-0.1.0</span><br><span class="line">    release: mike-test</span><br><span class="line">    heritage: Tiller</span><br><span class="line">spec:</span><br><span class="line">  type: ClusterIP</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">      targetPort: http</span><br><span class="line">      protocol: TCP</span><br><span class="line">      name: http</span><br><span class="line">  selector:</span><br><span class="line">    app: mychart</span><br><span class="line">    release: mike-test</span><br><span class="line">---</span><br><span class="line"># Source: mychart/templates/deployment.yaml</span><br><span class="line">apiVersion: apps/v1beta2</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: mike-test-mychart</span><br><span class="line">  labels:</span><br><span class="line">    app: mychart</span><br><span class="line">    chart: mychart-0.1.0</span><br><span class="line">    release: mike-test</span><br><span class="line">    heritage: Tiller</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: mychart</span><br><span class="line">      release: mike-test</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: mychart</span><br><span class="line">        release: mike-test</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">        - name: mychart</span><br><span class="line">          image: &quot;nginx:stable&quot;</span><br><span class="line">          imagePullPolicy: IfNotPresent</span><br><span class="line">          ports:</span><br><span class="line">            - name: http</span><br><span class="line">              containerPort: 80</span><br><span class="line">              protocol: TCP</span><br><span class="line">          livenessProbe:</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /</span><br><span class="line">              port: http</span><br><span class="line">          readinessProbe:</span><br><span class="line">            httpGet:</span><br><span class="line">              path: /</span><br><span class="line">              port: http</span><br><span class="line">          resources:</span><br><span class="line">            &#123;&#125;</span><br></pre></td></tr></table></figure><p>验证完成没有问题后，我们就可以使用以下命令将其部署到 Kubernetes 上了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># 部署时需指定 Chart 名及 Release（部署的实例）名。</span><br><span class="line">$ helm install local/mychart --name mike-test</span><br><span class="line">NAME:   mike-test</span><br><span class="line">LAST DEPLOYED: Mon Jul 23 10:41:20 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">mike-test-mychart  ClusterIP  10.254.120.177  &lt;none&gt;       80/TCP   1s</span><br><span class="line"></span><br><span class="line">==&gt; v1beta2/Deployment</span><br><span class="line">NAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">mike-test-mychart  1        0        0           0          0s</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                READY  STATUS   RESTARTS  AGE</span><br><span class="line">mike-test-mychart-6d56f8c8c9-d685v  0/1    Pending  0         0s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=mychart,release=mike-test&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">  echo &quot;Visit http://127.0.0.1:8080 to use your application&quot;</span><br><span class="line">  kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><blockquote><p>注：helm install 默认会用到 socat，需要在所有节点上安装 socat 软件包。</p></blockquote><p>完成部署后，现在 Nginx 就已经部署到 Kubernetes 集群上。在本地主机上执行提示中的命令后，就可在本机访问到该 Nginx 实例。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=mychart,release=mike-test&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">$ echo &quot;Visit http://127.0.0.1:8080 to use your application&quot;</span><br><span class="line">$ kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><p>在本地访问 Nginx</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://127.0.0.1:8080</span><br><span class="line">.....</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>使用下面的命令列出的所有已部署的 Release 以及其对应的 Chart。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm list</span><br><span class="line">NAME     REVISIONUPDATED                 STATUS  CHART        NAMESPACE</span><br><span class="line">mike-test1       Mon Jul 23 10:41:20 2018DEPLOYEDmychart-0.1.0default</span><br></pre></td></tr></table></figure><p>你还可以使用 <code>helm status</code> 查询一个特定的 Release 的状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ helm status mike-test</span><br><span class="line">LAST DEPLOYED: Mon Jul 23 10:41:20 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                READY  STATUS   RESTARTS  AGE</span><br><span class="line">mike-test-mychart-6d56f8c8c9-d685v  1/1    Running  0         1m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">mike-test-mychart  ClusterIP  10.254.120.177  &lt;none&gt;       80/TCP   1m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta2/Deployment</span><br><span class="line">NAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">mike-test-mychart  1        1        1           1          1m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=mychart,release=mike-test&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">  echo &quot;Visit http://127.0.0.1:8080 to use your application&quot;</span><br><span class="line">  kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><h5 id="升级和回退一个应用">升级和回退一个应用</h5><p>从上面 <code>helm list</code> 输出的结果中我们可以看到有一个 Revision（更改历史）字段，该字段用于表示某一个 Release 被更新的次数，我们可以用该特性对已部署的 Release 进行回滚。</p><ul><li>修改 Chart.yaml 文件</li></ul><p>将版本号从 0.1.0 修改为 0.2.0, 然后使用 <code>helm package</code> 命令打包并发布到本地仓库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ cat mychart/Chart.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">appVersion: &quot;1.0&quot;</span><br><span class="line">description: A Helm chart for Kubernetes</span><br><span class="line">name: mychart</span><br><span class="line">version: 0.2.0</span><br><span class="line"></span><br><span class="line">$ helm package mychart</span><br><span class="line">Successfully packaged chart and saved it to: /home/k8s/mychart-0.2.0.tgz</span><br></pre></td></tr></table></figure><ul><li>查询本地仓库中的 Chart 信息</li></ul><p>我们可以看到在本地仓库中 mychart 有两个版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ helm search mychart -l</span><br><span class="line">NAME         CHART VERSIONAPP VERSIONDESCRIPTION</span><br><span class="line">local/mychart0.2.0        1.0        A Helm chart for Kubernetes</span><br><span class="line">local/mychart0.1.0        1.0        A Helm chart for Kubernetes</span><br></pre></td></tr></table></figure><ul><li>升级一个应用</li></ul><p>现在用 <code>helm upgrade</code> 命令将已部署的 mike-test 升级到新版本。你可以通过 <code>--version</code> 参数指定需要升级的版本号，如果没有指定版本号，则缺省使用最新版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ helm upgrade mike-test local/mychart</span><br><span class="line">Release &quot;mike-test&quot; has been upgraded. Happy Helming!</span><br><span class="line">LAST DEPLOYED: Mon Jul 23 10:50:25 2018</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                READY  STATUS   RESTARTS  AGE</span><br><span class="line">mike-test-mychart-6d56f8c8c9-d685v  1/1    Running  0         9m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)  AGE</span><br><span class="line">mike-test-mychart  ClusterIP  10.254.120.177  &lt;none&gt;       80/TCP   9m</span><br><span class="line"></span><br><span class="line">==&gt; v1beta2/Deployment</span><br><span class="line">NAME               DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">mike-test-mychart  1        1        1           1          9m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the application URL by running these commands:</span><br><span class="line">  export POD_NAME=$(kubectl get pods --namespace default -l &quot;app=mychart,release=mike-test&quot; -o jsonpath=&quot;&#123;.items[0].metadata.name&#125;&quot;)</span><br><span class="line">  echo &quot;Visit http://127.0.0.1:8080 to use your application&quot;</span><br><span class="line">  kubectl port-forward $POD_NAME 8080:80</span><br></pre></td></tr></table></figure><p>完成后，可以看到已部署的 mike-test 被升级到 0.2.0 版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm list</span><br><span class="line">NAME     REVISIONUPDATED                 STATUS  CHART        NAMESPACE</span><br><span class="line">mike-test2       Mon Jul 23 10:50:25 2018DEPLOYEDmychart-0.2.0default</span><br></pre></td></tr></table></figure><ul><li>回退一个应用</li></ul><p>如果更新后的程序由于某些原因运行有问题，需要回退到旧版本的应用。首先我们可以使用 <code>helm history</code> 命令查看一个 Release 的所有变更记录。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ helm history mike-test</span><br><span class="line">REVISIONUPDATED                 STATUS    CHART        DESCRIPTION</span><br><span class="line">1       Mon Jul 23 10:41:20 2018SUPERSEDEDmychart-0.1.0Install complete</span><br><span class="line">2       Mon Jul 23 10:50:25 2018DEPLOYED  mychart-0.2.0Upgrade complete</span><br></pre></td></tr></table></figure><p>其次，我们可以使用下面的命令对指定的应用进行回退。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm rollback mike-test 1</span><br><span class="line">Rollback was a success! Happy Helming!</span><br></pre></td></tr></table></figure><blockquote><p>注：其中的参数 1 是 helm history 查看到 Release 的历史记录中 REVISION 对应的值。</p></blockquote><p>最后，我们使用 <code>helm list</code> 和 <code>helm history</code> 命令都可以看到 mychart 的版本已经回退到 0.1.0 版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ helm list</span><br><span class="line">NAME     REVISIONUPDATED                 STATUS  CHART        NAMESPACE</span><br><span class="line">mike-test3       Mon Jul 23 10:53:42 2018DEPLOYEDmychart-0.1.0default</span><br><span class="line"></span><br><span class="line">$ helm history mike-test</span><br><span class="line">REVISIONUPDATED                 STATUS    CHART        DESCRIPTION</span><br><span class="line">1       Mon Jul 23 10:41:20 2018SUPERSEDEDmychart-0.1.0Install complete</span><br><span class="line">2       Mon Jul 23 10:50:25 2018SUPERSEDEDmychart-0.2.0Upgrade complete</span><br><span class="line">3       Mon Jul 23 10:53:42 2018DEPLOYED  mychart-0.1.0Rollback to 1</span><br></pre></td></tr></table></figure><h5 id="删除一个应用">删除一个应用</h5><p>如果需要删除一个已部署的 Release，可以利用 <code>helm delete</code> 命令来完成删除。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm delete mike-test</span><br><span class="line">release &quot;mike-test&quot; deleted</span><br></pre></td></tr></table></figure><p>确认应用是否删除，该应用已被标记为 DELETED 状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm ls -a mike-test</span><br><span class="line">NAME     REVISIONUPDATED                 STATUS CHART        NAMESPACE</span><br><span class="line">mike-test3       Mon Jul 23 10:53:42 2018DELETEDmychart-0.1.0default</span><br></pre></td></tr></table></figure><p>也可以使用 <code>--deleted</code> 参数来列出已经删除的 Release</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ helm ls --deleted</span><br><span class="line">NAME     REVISIONUPDATED                 STATUS CHART        NAMESPACE</span><br><span class="line">mike-test3       Mon Jul 23 10:53:42 2018DELETEDmychart-0.1.0default</span><br></pre></td></tr></table></figure><p>从上面的结果也可以看出，默认情况下已经删除的 Release 只是将状态标识为 DELETED 了 ，但该 Release 的历史信息还是继续被保存的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ helm hist mike-test</span><br><span class="line">REVISIONUPDATED                 STATUS    CHART        DESCRIPTION</span><br><span class="line">1       Mon Jul 23 10:41:20 2018SUPERSEDEDmychart-0.1.0Install complete</span><br><span class="line">2       Mon Jul 23 10:50:25 2018SUPERSEDEDmychart-0.2.0Upgrade complete</span><br><span class="line">3       Mon Jul 23 10:53:42 2018DELETED   mychart-0.1.0Deletion complete</span><br></pre></td></tr></table></figure><p>如果要移除指定 Release 所有相关的 Kubernetes 资源和 Release 的历史记录，可以用如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm delete --purge mike-test</span><br><span class="line">release &quot;mike-test&quot; deleted</span><br></pre></td></tr></table></figure><p>再次查看已删除的 Release，已经无法找到相关信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ helm hist mike-test</span><br><span class="line">Error: release: &quot;mike-test&quot; not found</span><br><span class="line"></span><br><span class="line"># helm ls 命令也已均无查询记录。</span><br><span class="line">$ helm ls --deleted</span><br><span class="line">$ helm ls -a mike-test</span><br></pre></td></tr></table></figure><h3 id="helm-部署应用实例">Helm 部署应用实例</h3><h4 id="部署-wordpress">部署 Wordpress</h4><p>这里以一个典型的三层应用 Wordpress 为例，包括 MySQL、PHP 和 Apache。</p><p>由于测试环境没有可用的 PersistentVolume（持久卷，简称 PV），这里暂时将其关闭。关于 Persistent Volumes 的相关信息我们会在后续的相关文章进行讲解。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">$ helm install --name wordpress-test --set &quot;persistence.enabled=false,mariadb.persistence.enabled=false,serviceType=NodePort&quot;  stable/wordpress</span><br><span class="line"></span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line"></span><br><span class="line">RESOURCES:</span><br><span class="line">==&gt; v1beta1/Deployment</span><br><span class="line">NAME                      DESIRED  CURRENT  UP-TO-DATE  AVAILABLE  AGE</span><br><span class="line">wordpress-test-mariadb    1        1        1           1          26m</span><br><span class="line">wordpress-test-wordpress  1        1        1           1          26m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Pod(related)</span><br><span class="line">NAME                                       READY  STATUS   RESTARTS  AGE</span><br><span class="line">wordpress-test-mariadb-84b866bf95-n26ff    1/1    Running  1         26m</span><br><span class="line">wordpress-test-wordpress-5ff8c64b6c-sgtvv  1/1    Running  6         26m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Secret</span><br><span class="line">NAME                      TYPE    DATA  AGE</span><br><span class="line">wordpress-test-mariadb    Opaque  2     26m</span><br><span class="line">wordpress-test-wordpress  Opaque  2     26m</span><br><span class="line"></span><br><span class="line">==&gt; v1/ConfigMap</span><br><span class="line">NAME                          DATA  AGE</span><br><span class="line">wordpress-test-mariadb        1     26m</span><br><span class="line">wordpress-test-mariadb-tests  1     26m</span><br><span class="line"></span><br><span class="line">==&gt; v1/Service</span><br><span class="line">NAME                      TYPE       CLUSTER-IP     EXTERNAL-IP  PORT(S)                   AGE</span><br><span class="line">wordpress-test-mariadb    ClusterIP  10.254.99.67   &lt;none&gt;       3306/TCP                  26m</span><br><span class="line">wordpress-test-wordpress  NodePort   10.254.175.16  &lt;none&gt;       80:8563/TCP,443:8839/TCP  26m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NOTES:</span><br><span class="line">1. Get the WordPress URL:</span><br><span class="line"></span><br><span class="line">  Or running:</span><br><span class="line"></span><br><span class="line">  export NODE_PORT=$(kubectl get --namespace default -o jsonpath=&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services wordpress-test-wordpress)</span><br><span class="line">  export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">  echo http://$NODE_IP:$NODE_PORT/admin</span><br><span class="line"></span><br><span class="line">2. Login with the following credentials to see your blog</span><br><span class="line"></span><br><span class="line">  echo Username: user</span><br><span class="line">  echo Password: $(kubectl get secret --namespace default wordpress-test-wordpress -o jsonpath=&quot;&#123;.data.wordpress-password&#125;&quot; | base64 --decode)</span><br></pre></td></tr></table></figure><h4 id="访问-wordpress">访问 Wordpress</h4><p>部署完成后，我们可以通过上面的提示信息生成相应的访问地址和用户名、密码等相关信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 生成 Wordpress 管理后台地址</span><br><span class="line">$ export NODE_PORT=$(kubectl get --namespace default -o jsonpath=&quot;&#123;.spec.ports[0].nodePort&#125;&quot; services wordpress-test-wordpress)</span><br><span class="line">$ export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=&quot;&#123;.items[0].status.addresses[0].address&#125;&quot;)</span><br><span class="line">$ echo http://$NODE_IP:$NODE_PORT/admin</span><br><span class="line">http://192.168.100.211:8433/admin</span><br><span class="line"></span><br><span class="line"># 生成 Wordpress 管理帐号和密码</span><br><span class="line">$ echo Username: user</span><br><span class="line">Username: user</span><br><span class="line">$ echo Password: $(kubectl get secret --namespace default wordpress-test-wordpress -o jsonpath=&quot;&#123;.data.wordpress-password&#125;&quot; | base64 --decode)</span><br><span class="line">Password: 9jEXJgnVAY</span><br></pre></td></tr></table></figure><p>给一张访问效果图吧：</p><p><img src="https://www.hi-linux.com/img/linux/helm03.png" alt=""></p><h3 id="helm-其它使用技巧">Helm 其它使用技巧</h3><ul><li>如何设置 helm 命令自动补全？</li></ul><p>为了方便 <code>helm</code> 命令的使用，Helm 提供了自动补全功能，如果使用 ZSH 请执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ source &lt;(helm completion zsh)</span><br></pre></td></tr></table></figure><p>如果使用 BASH 请执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ source &lt;(helm completion bash)</span><br></pre></td></tr></table></figure><ul><li>如何使用第三方的 Chart 存储库？</li></ul><p>随着 Helm 越来越普及，除了使用预置官方存储库，三方仓库也越来越多了（前提是网络是可达的）。你可以使用如下命令格式添加三方 Chart 存储库。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm repo add 存储库名 存储库URL</span><br><span class="line">$ helm repo update</span><br></pre></td></tr></table></figure><p>一些三方存储库资源:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># Prometheus Operator</span><br><span class="line">https://github.com/coreos/prometheus-operator/tree/master/helm</span><br><span class="line"></span><br><span class="line"># Bitnami Library for Kubernetes</span><br><span class="line">https://github.com/bitnami/charts</span><br><span class="line"></span><br><span class="line"># Openstack-Helm</span><br><span class="line">https://github.com/att-comdev/openstack-helm</span><br><span class="line">https://github.com/sapcc/openstack-helm</span><br><span class="line"></span><br><span class="line"># Tick-Charts</span><br><span class="line">https://github.com/jackzampolin/tick-charts</span><br></pre></td></tr></table></figure><ul><li>Helm 如何结合 CI/CD ？</li></ul><p>采用 Helm 可以把零散的 Kubernetes 应用配置文件作为一个 Chart 管理，Chart 源码可以和源代码一起放到 Git 库中管理。通过把 Chart 参数化，可以在测试环境和生产环境采用不同的 Chart 参数配置。</p><p>下图是采用了 Helm 的一个 CI/CD 流程</p><p><img src="https://www.hi-linux.com/img/linux/helm04.png" alt=""></p><ul><li>Helm 如何管理多环境下 (Test、Staging、Production) 的业务配置？</li></ul><p>Chart 是支持参数替换的，可以把业务配置相关的参数设置为模板变量。使用 <code>helm install</code> 命令部署的时候指定一个参数值文件，这样就可以把业务参数从 Chart 中剥离了。例如： <code>helm install --values=values-production.yaml wordpress</code>。</p><ul><li>Helm 如何解决服务依赖？</li></ul><p>在 Chart 里可以通过 requirements.yaml 声明对其它 Chart 的依赖关系。如下面声明表明 Chart 依赖 Apache 和 MySQL 这两个第三方 Chart。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dependencies:</span><br><span class="line">- name: mariadb</span><br><span class="line">  version: 2.1.1</span><br><span class="line">  repository: https://kubernetes-charts.storage.googleapis.com/</span><br><span class="line">  condition: mariadb.enabled</span><br><span class="line">  tags:</span><br><span class="line">    - wordpress-database</span><br><span class="line">- name: apache</span><br><span class="line">    version: 1.4.0</span><br><span class="line">    repository: https://kubernetes-charts.storage.googleapis.com/</span><br></pre></td></tr></table></figure><ul><li>如何让 Helm 连接到指定 Kubernetes 集群？</li></ul><p>Helm 默认使用和 kubectl 命令相同的配置访问 Kubernetes 集群，其配置默认在 <code>~/.kube/config</code> 中。</p><ul><li>如何在部署时指定命名空间？</li></ul><p><code>helm install</code> 默认情况下是部署在 default 这个命名空间的。如果想部署到指定的命令空间，可以加上 <code>--namespace</code> 参数，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm install local/mychart --name mike-test --namespace mynamespace</span><br></pre></td></tr></table></figure><ul><li>如何查看已部署应用的详细信息？</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm get wordpress-test</span><br></pre></td></tr></table></figure><p>默认情况下会显示最新的版本的相关信息，如果想要查看指定发布版本的信息可加上 <code>--revision</code> 参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ helm get  --revision 1  wordpress-test</span><br></pre></td></tr></table></figure><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RgEE0dm" target="_blank" rel="noopener">http://t.cn/RgEE0dm</a><br><a href="http://t.cn/RgE3MyP" target="_blank" rel="noopener">http://t.cn/RgE3MyP</a><br><a href="http://t.cn/RgpiUAz" target="_blank" rel="noopener">http://t.cn/RgpiUAz</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Helm 是 Kubernetes 生态系统中的一个软件包管理工具。本文将介绍 Helm 中的相关概念和基本工作原理，并通过一个具体的示例学习如何使用 Helm 打包、分发、安装、升级及回退 Kubernetes 应用。&lt;/p&gt;
&lt;h3 id=&quot;kubernetes-应用部署的挑战&quot;&gt;Kubernetes 应用部署的挑战&lt;/h3&gt;
&lt;p&gt;Kubernetes 是一个提供了基于容器的应用集群管理解决方案，Kubernetes 为容器化应用提供了部署运行、资源调度、服务发现和动态伸缩等一系列完整功能。&lt;/p&gt;
&lt;p&gt;Kubernetes 的核心设计理念是: 用户定义要部署的应用程序的规则，而 Kubernetes 则负责按照定义的规则部署并运行应用程序。如果应用程序出现问题导致偏离了定义的规格，Kubernetes 负责对其进行自动修正。例如：定义的应用规则要求部署两个实例（Pod），其中一个实例异常终止了，Kubernetes 会检查到并重新启动一个新的实例。&lt;/p&gt;
&lt;p&gt;用户通过使用 Kubernetes API 对象来描述应用程序规则，包括 Pod、Service、Volume、Namespace、ReplicaSet、Deployment、Job等等。一般这些资源对象的定义需要写入一系列的 YAML 文件中，然后通过 Kubernetes 命令行工具 Kubectl 调 Kubernetes API 进行部署。&lt;/p&gt;
&lt;p&gt;以一个典型的三层应用 Wordpress 为例，该应用程序就涉及到多个 Kubernetes API 对象，而要描述这些 Kubernetes API 对象就可能要同时维护多个 YAML 文件。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/helm01.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;从上图可以看到，在进行 Kubernetes 软件部署时，我们面临下述几个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何管理、编辑和更新这些这些分散的 Kubernetes 应用配置文件。&lt;/li&gt;
&lt;li&gt;如何把一套相关的配置文件作为一个应用进行管理。&lt;/li&gt;
&lt;li&gt;如何分发和重用 Kubernetes 的应用配置。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Helm 的出现就是为了很好地解决上面这些问题。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>从 Kubectl run 开始揭开 Kubernetes 的神迷面纱</title>
    <link href="https://www.hi-linux.com/posts/30305.html"/>
    <id>https://www.hi-linux.com/posts/30305.html</id>
    <published>2018-08-08T01:00:00.000Z</published>
    <updated>2018-08-24T06:01:16.435Z</updated>
    
    <content type="html"><![CDATA[<p>想象一下，如果我想将 <code>Nginx</code> 部署到 <code>Kubernetes</code> 集群，我可能会在终端中输入类似这样的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl run --image=nginx --replicas=3</span><br></pre></td></tr></table></figure><p>然后回车。几秒钟后，你就会看到三个 <code>Nginx Pod</code> 分布在所有的工作节点上。这一切就像变魔术一样，但你并不知道这一切的背后究竟发生了什么事情。</p><p><code>Kubernetes</code> 的神奇之处在于：它可以通过用户友好的 <code>API</code> 来处理跨基础架构的 <code>Deployments</code>，而背后的复杂性被隐藏在简单的抽象中。但为了充分理解它为我们提供的价值，我们需要理解它的内部原理。</p><p>本指南将引导您理解从 <code>Client</code> 到 <code>Kubelet</code> 请求的完整生命周期，必要时会通过源代码来说明背后发生了什么。</p><p>这是一份可以在线修改的文档，如果你发现有什么可以改进或重写的，欢迎提供帮助！</p><h3 id="1-kubectl">1. Kubectl</h3><p><strong>验证和生成器</strong></p><p>当敲下回车键以后，<code>Kubectl</code> 首先会执行一些客户端验证操作，以确保不合法的请求（例如：创建不支持的资源或使用格式错误的镜像名称）将会快速失败，也不会发送给 <code>Kube-Apiserver</code>。通过减少不必要的负载来提高系统性能。</p><p>验证通过之后，<code>Kubectl</code> 开始将发送给 <code>Kube-Apiserver</code> 的 <code>HTTP</code> 请求进行封装。<code>Kube-Apiserver</code> 与 <code>Etcd</code> 进行通信，所有尝试访问或更改 <code>Kubernetes</code> 系统状态的请求都会通过 <code>Kube-Apiserver</code> 进行，<code>Kubectl</code> 也不例外。<code>Kubectl</code> 使用生成器（<a href="https://kubernetes.io/docs/user-guide/kubectl-conventions/#generators" target="_blank" rel="noopener">Generators</a>）来构造 <code>HTTP</code> 请求。生成器是一个用来处理序列化的抽象概念。</p><p>通过 <code>kubectl run</code> 不仅可以运行 <code>Deployment</code>，还可以通过指定参数 <code>--generator</code> 来部署其他多种资源类型。如果没有指定 <code>--generator</code> 参数的值，<code>Kubectl</code> 将会自动判断资源的类型。</p><p>例如：带有参数 <code>--restart-policy=Always</code> 的资源将被部署为 <code>Deployment</code>，而带有参数 <code>--restart-policy=Never</code> 的资源将被部署为 <code>Pod</code>。同时 <code>Kubectl</code> 也会检查是否需要触发其他操作，例如：记录命令（用来进行回滚或审计）。</p><p>在 <code>Kubectl</code> 判断出要创建一个 <code>Deployment</code> 后，它将使用 <code>DeploymentV1Beta1</code> 生成器从我们提供的参数中生成一个运行时对象。</p><a id="more"></a><p><strong>API 版本协商与 API 组</strong></p><p>为了更容易地消除字段或者重新组织资源结构，<code>Kubernetes</code> 支持多个 <code>API</code> 版本，每个版本都在不同的 <code>API</code> 路径下，例如 <code>/api/v1</code> 或者 <code>/apis/extensions/v1beta1</code>。不同的 <code>API</code> 版本表明不同的稳定性和支持级别，更详细的描述可以参考 <a href="https://k8smeetup.github.io/docs/reference/api-overview/" target="_blank" rel="noopener">Kubernetes API 概述</a>。</p><p><code>API</code> 组主要作用是对类似资源进行分类，以便使得 <code>Kubernetes API</code> 更容易扩展。<code>API</code> 的组别在 <code>REST</code> 路径或者序列化对象的 <code>apiVersion</code> 字段中指定。例如：<code>Deployment</code> 的 <code>API</code> 组名是 <code>apps</code>，最新的 <code>API</code> 版本是 <code>v1beta2</code>，这就是为什么你要在 <code>Deployment manifests</code> 顶部输入 <code>apiVersion: apps/v1beta2</code>。</p><p><code>Kubectl</code> 在生成运行时对象后，开始为它找到适当的 <code>API</code> 组和 <code>API</code> 版本，然后组装成一个版本化客户端，该客户端知道资源的各种 <code>REST</code> 语义。该阶段被称为版本协商，<code>Kubectl</code> 会扫描 <code>Remote API</code> 上的 <code>/apis</code> 路径来检索所有可能的 <code>API</code> 组。由于 <code>Kube-Apiserver</code> 在 <code>/apis</code> 路径上公开了 <code>OpenAPI</code> 格式的模式文档， 因此客户端很容易找到合适的 <code>API</code>。</p><p>为了提高性能，<code>Kubectl</code> 将 <code>OpenAPI</code> 模式缓存到了 <code>~/.kube/cache</code> 目录。如果你想了解 <code>API</code> 发现的过程，请尝试删除该目录并在运行 <code>kubectl</code> 命令时将 <code>-v</code> 参数的值设为最大值，然后你将会看到所有试图找到这些 <code>API</code> 版本的 <code>HTTP</code> 请求。参考 <a href="https://k8smeetup.github.io/docs/reference/kubectl/cheatsheet/" target="_blank" rel="noopener">Kubectl 备忘单</a>。</p><p>最后一步才是真正地发送 <code>HTTP</code> 请求。一旦请求发送之后获得成功的响应，<code>Kubectl</code> 将会根据所需的输出格式打印 Success Message。</p><p><strong>客户端身份认证</strong></p><p>在发送 <code>HTTP</code> 请求之前还要进行客户端认证，这是之前没有提到的，现在可以来看一下。</p><p>为了能够成功发送请求，<code>Kubectl</code> 需要先进行身份认证。用户凭证保存在 <code>Kubeconfig</code> 文件中，<code>Kubectl</code> 通过以下顺序来找到 <code>Kubeconfig</code> 文件：</p><ul><li>如果提供了 <code>--kubeconfig</code> 参数， <code>Kubectl</code> 就使用 <code>--kubeconfig</code> 参数提供的 <code>Kubeconfig</code> 文件。</li><li>如果没有提供 <code>--kubeconfig</code> 参数，但设置了环境变量 <code>$KUBECONFIG</code>，则使用该环境变量提供的 <code>Kubeconfig</code> 文件。</li><li>如果 <code>--kubeconfig</code> 参数和环境变量 <code>$KUBECONFIG</code> 都没有提供，<code>Kubectl</code> 就使用默认的 <code>Kubeconfig</code> 文件 <code>$HOME/.kube/config</code>。</li></ul><p>解析完 <code>Kubeconfig</code> 文件后，<code>Kubectl</code> 会确定当前要使用的上下文、当前指向的群集以及与当前用户关联的任何认证信息。如果用户提供了额外的参数（例如: <code>--username</code>），则优先使用这些参数覆盖 <code>Kubeconfig</code> 中指定的值。一旦拿到这些信息之后， <code>Kubectl</code> 就会把这些信息填充到将要发送的 <code>HTTP</code> 请求头中：</p><ul><li><code>x509</code> 证书使用 <code>tls.TLSConfig</code> 发送（包括 <code>CA</code> 证书）。</li><li><code>bearer tokens</code> 在 HTTP 请求头 <code>Authorization</code> 中发送。</li><li>用户名和密码通过 <code>HTTP</code> 基本认证发送。</li><li><code>OpenID</code> 认证过程是由用户事先手动处理的，产生一个像 <code>Bearer Token</code> 一样被发送的 <code>Token</code>。</li></ul><h3 id="2-kube-apiserver">2. Kube-Apiserver</h3><p><strong>认证</strong></p><p>现在我们的请求已经成功发送了，接下来将会发生什么？这时候就该 <code>Kube-Apiserver</code> 闪亮登场了！<code>Kube-Apiserver</code> 是客户端和系统组件用来保存和检索集群状态的主要接口。为了执行相应的功能，<code>Kube-Apiserver</code> 需要能够验证请求者是合法的，这个过程被称为认证。</p><p>那么 <code>Apiserver</code> 如何对请求进行认证呢？当 <code>Kube-Apiserver</code> 第一次启动时，它会查看用户提供的所有 <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/" target="_blank" rel="noopener">CLI</a> 参数，并组合成一个合适的令牌列表。</p><p>举个例子：如果提供了 <code>--client-ca-file</code> 参数，则会将 <code>x509</code> 客户端证书认证添加到令牌列表中；如果提供了 <code>--token-auth-file</code> 参数，则会将 <code>breaer token</code> 添加到令牌列表中。</p><p>每次收到请求时，<code>Apiserver</code> 都会通过令牌链进行认证，直到某一个认证成功为止：</p><ul><li><code>x509</code> 处理程序将验证 <code>HTTP</code> 请求是否是由 <code>CA</code> 根证书签名的 <code>TLS</code> 密钥进行编码的。</li><li><code>bearer token</code> 处理程序将验证 <code>--token-auth-file</code> 参数提供的 <code>Token</code> 文件是否存在。</li><li>基本认证处理程序确保 <code>HTTP</code> 请求的基本认证凭证与本地的状态匹配。</li></ul><p>如果认证失败，则请求失败并返回相应的错误信息；如果验证成功，则将请求中的 <code>Authorization</code> 请求头删除，并将用户信息添加到其上下文中。这给后续的授权和准入控制器提供了访问之前建立的用户身份的能力。</p><p><strong>授权</strong></p><p>OK，现在请求已经发送，并且 <code>Kube-Apiserver</code> 已经成功验证我们是谁，终于解脱了！</p><p>然而事情并没有结束，虽然我们已经证明了我们是合法的，但我们有权执行此操作吗？毕竟身份和权限不是一回事。为了进行后续的操作，<code>Kube-Apiserver</code> 还要对用户进行授权。</p><p><code>Kube-Apiserver</code> 处理授权的方式与处理身份验证的方式相似：通过 <code>Kube-Apiserver</code> 的启动参数 <code>--authorization_mode</code> 参数设置。它将组合一系列授权者，这些授权者将针对每个传入的请求进行授权。如果所有授权者都拒绝该请求，则该请求会被禁止响应并且不会再继续响应。如果某个授权者批准了该请求，则请求继续。</p><p><code>Kube-Apiserver</code> 目前支持以下几种授权方法：</p><ul><li><a href="https://k8smeetup.github.io/docs/admin/authorization/webhook/" target="_blank" rel="noopener">webhook</a>: 它与集群外的 <code>HTTP(S)</code> 服务交互。</li><li><a href="https://k8smeetup.github.io/docs/admin/authorization/abac/" target="_blank" rel="noopener">ABAC</a>: 它执行静态文件中定义的策略。</li><li><a href="https://k8smeetup.github.io/docs/admin/authorization/rbac/" target="_blank" rel="noopener">RBAC</a>: 它使用 <code>rbac.authorization.k8s.io</code> API Group实现授权决策，允许管理员通过 <code>Kubernetes API</code> 动态配置策略。</li><li><a href="https://k8smeetup.github.io/docs/admin/authorization/node/" target="_blank" rel="noopener">Node</a>: 它确保 <code>Kubelet</code> 只能访问自己节点上的资源。</li></ul><p><strong>准入控制</strong></p><p>突破了之前所说的认证和授权两道关口之后，客户端的调用请求就能够得到 <code>API Server</code> 的真正响应了吗？答案是：不能！</p><p>从 <code>Kube-Apiserver</code> 的角度来看，它已经验证了我们的身份并且赋予了相应的权限允许我们继续，但对于 <code>Kubernetes</code> 而言，其他组件对于应不应该允许发生的事情还是很有意见的。所以这个请求还需要通过 <code>Admission Controller</code> 所控制的一个准入控制链的层层考验，官方标准的关卡有近十个之多，而且还能自定义扩展！</p><p>虽然授权的重点是回答用户是否有权限，但准入控制器会拦截请求以确保它符合集群的更广泛的期望和规则。它们是资源对象保存到 <code>Etcd</code> 之前的最后一个堡垒，封装了一系列额外的检查以确保操作不会产生意外或负面结果。不同于授权和认证只关心请求的用户和操作，准入控制还处理请求的内容，并且仅对创建、更新、删除或连接（如：代理）等有效，而对读操作无效。</p><blockquote><p>注：准入控制器的工作方式与授权者和验证者的工作方式类似，但有一点区别：与验证链和授权链不同，如果某个准入控制器检查不通过，则整个链会中断，整个请求将立即被拒绝并且返回一个错误给终端用户。</p></blockquote><p>准入控制器设计的重点在于提高可扩展性，每个控制器都作为一个插件存储在 <code>plugin/pkg/admission</code> 目录中，并且与每一个接口相匹配，最后被编译到 <code>Kube-Apiserver</code> 二进制文件中。</p><p>大部分准入控制器都比较容易理解，接下来着重介绍 <code>SecurityContextDeny</code>、<code>ResourceQuota</code> 及 <code>LimitRanger</code> 这三个准入控制器。</p><ul><li><p><code>SecurityContextDeny</code> 该插件将禁止创建设置了 <code>Security Context</code> 的 <code>Pod</code>。</p></li><li><p><code>ResourceQuota</code> 不仅能限制某个 <code>Namespace</code> 中创建资源的数量，而且能限制某个 <code>Namespace</code> 中被 <code>Pod</code> 所请求的资源总量。该准入控制器和资源对象 <code>ResourceQuota</code> 一起实现了资源配额管理。</p></li><li><p><code>LimitRanger</code> 作用类似于上面的 <code>ResourceQuota</code> 控制器，针对 <code>Namespace</code> 资源的每个个体（<code>Pod</code> 与 <code>Container</code> 等）的资源配额。该插件和资源对象 <code>LimitRange</code> 一起实现资源配额管理。</p></li></ul><h3 id="3-etcd">3. Etcd</h3><p>到现在为止，<code>Kubernetes</code> 已经对该客户端的调用请求进行了全面彻底地审查，并且已经验证通过，运行它进入下一个环节。下一步 <code>Kube-Apiserver</code> 将对 <code>HTTP</code> 请求进行反序列化，然后利用得到的结果构建运行时对象（有点像 <code>Kubectl</code> 生成器的逆过程），并保存到 <code>Etcd</code> 中。下面我们将这个过程分解一下。</p><p>当收到请求时，<code>Kube-Apiserver</code> 是如何知道它该怎么做的呢？事实上，在客户端发送调用请求之前就已经产生了一系列非常复杂的流程。我们就从 <code>Kube-Apiserver</code> 二进制文件首次运行开始分析吧：</p><ol><li>当运行 <code>Kube-Apiserver</code> 二进制文件时，它会创建一个允许 <code>Apiserver</code> 聚合的服务链。这是一种对 <code>Kubernetes API</code> 进行扩展的方式。</li><li>同时会创建一个 <code>Generic Apiserver</code> 作为默认的 <code>Apiserver</code>。</li><li>然后生成 <code>OpenAPI</code> 规范的配置</li><li>然后 <code>Kube-Apiserver</code> 遍历数据结构中指定的所有 <code>API</code> 组，并将每一个 <code>API</code> 组作为通用的存储抽象保存到 <code>Etcd</code> 中。当你访问或变更资源状态时，<code>Kube-Apiserver</code> 就会调用这些 <code>API</code> 组。</li><li>每个 <code>API</code> 组都会遍历它的所有组版本，并且将每个 <code>HTTP</code> 路由映射到 <code>REST</code> 路径中。</li><li>当请求的 <code>METHOD</code> 是 <code>POST</code> 时，<code>Kube-Apiserver</code> 就会将请求转交给资源创建处理器。</li></ol><p>现在 <code>Kube-Apiserver</code> 已经知道了所有的路由及其对应的 <code>REST</code> 路径，以便在请求匹配时知道调用哪些处理器和键值存储。多么机智的设计！现在假设客户端的 <code>HTTP</code> 请求已经被 <code>Kube-Apiserver</code> 收到了：</p><ol><li>如果处理链可以将请求与已经注册的路由进行匹配，就会将该请求交给注册到该路由的专用处理器来处理；如果没有任何一个路由可以匹配该请求，就会将请求转交给基于路径的处理器（比如：当调用 /apis 时）；如果没有任何一个基于路径的处理器注册到该路径，请求就会被转交给 <code>not found</code> 处理器，最后返回 404。</li><li>幸运的是，我们有一个名为 <code>CreateHandler</code> 的注册路由！它有什么作用呢？首先它会解码 <code>HTTP</code> 请求并进行基本的验证，例如：确保请求提供的 <code>JSON</code> 与 <code>API</code> 资源的版本相匹配。</li><li>接下来进入审计和准入控制阶段。</li><li>然后资源将会通过 <code>Storage Provider</code> 保存到 <code>Etcd</code> 中。默认情况下保存到 <code>Etcd</code> 中的键的格式为 <code>&lt;namespace&gt;/&lt;name&gt;</code>，你也可以自定义。</li><li>资源创建过程中出现的任何错误都会被捕获，最后 <code>Storage Provider</code> 会执行 <code>Get</code> 调用来确认该资源是否被成功创建。如果需要额外的清理工作，就会调用后期创建的处理器和装饰器。</li><li>最后构造 <code>HTTP</code> 响应并返回给客户端。</li></ol><p>原来 <code>Apiserver</code> 做了这么多的工作，以前竟然没有发现呢！到目前为止，我们创建的 <code>Deployment</code> 资源已经保存到了 <code>Etcd</code> 中，但 <code>Apiserver</code> 仍然看不到它。</p><h3 id="4-初始化">4. 初始化</h3><p>在一个资源对象被持久化到数据存储之后，<code>Apiserver</code> 还无法完全看到或调度它，在此之前还要执行一系列<a href="https://kubernetes.io/docs/admin/extensible-admission-controllers/#initializers" target="_blank" rel="noopener">初始化器</a>。初始化器是一种与资源类型相关联的控制器，它会在资源对外可用之前执行某些逻辑。如果某个资源类型没有初始化器，就会跳过此初始化步骤立即使资源对外可见。</p><p>正如<a href="https://ahmet.im/blog/initializers/" target="_blank" rel="noopener">大佬的博客</a>指出的那样，初始化器是一个强大的功能，因为它允许我们执行通用引导操作。例如：</p><ul><li>将代理 <code>Sidecar</code> 容器注入到暴露 80 端口的 <code>Pod</code> 中，或者加上特定的 <code>Annotation</code>。</li><li>将保存着测试证书的<code>Volume</code> 注入到特定命名空间的所有 <code>Pod</code> 中。</li><li>如果 <code>Secret</code> 中的密码小于 20 个字符，就组织其创建。</li></ul><p><code>initializerConfiguration</code> 资源对象允许你声明某些资源类型应该运行哪些初始化器。如果你想每创建一个 <code>Pod</code> 时就运行一个自定义初始化器，你可以这样做：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">admissionregistration.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">InitializerConfiguration</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">custom-pod-initializer</span></span><br><span class="line"><span class="attr">initializers:</span></span><br><span class="line"><span class="attr">  - name:</span> <span class="string">podimage.example.com</span></span><br><span class="line"><span class="attr">    rules:</span></span><br><span class="line"><span class="attr">      - apiGroups:</span></span><br><span class="line"><span class="bullet">          -</span> <span class="string">""</span></span><br><span class="line"><span class="attr">        apiVersions:</span></span><br><span class="line"><span class="bullet">          -</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">        resources:</span></span><br><span class="line"><span class="bullet">          -</span> <span class="string">pods</span></span><br></pre></td></tr></table></figure><p>通过该配置创建资源对象 <code>InitializerConfiguration</code> 之后，就会在每个 <code>Pod</code> 的 <code>metadata.initializers.pending</code> 字段中添加 <code>custom-pod-initializer</code> 字段。该初始化控制器会定期扫描新的 <code>Pod</code>，一旦在 <code>Pod</code> 的 <code>pending</code> 字段中检测到自己的名称，就会执行其逻辑，执行完逻辑之后就会将 <code>pending</code> 字段下的自己的名称删除。</p><p>只有在 <code>pending</code> 字段下的列表中的第一个初始化器可以对资源进行操作，当所有的初始化器执行完成，并且 <code>pending</code> 字段为空时，该对象就会被认为初始化成功。</p><p>你可能会注意到一个问题：如果 <code>Kube-Apiserver</code> 不能显示这些资源，那么用户级控制器是如何处理资源的呢？</p><p>为了解决这个问题，<code>Kube-Apiserver</code> 暴露了一个 <code>?includeUninitialized</code> 查询参数，它会返回所有的资源对象（包括未初始化的）。</p><h3 id="5-控制循环">5. 控制循环</h3><p><strong>Deployments controller</strong></p><p>到了这个阶段，我们的 <code>Deployment</code> 记录已经保存在 <code>Etcd</code> 中，并且所有的初始化逻辑都执行完成，接下来的阶段将会涉及到该资源所依赖的拓扑结构。在 <code>Kubernetes</code> 中，<code>Deployment</code> 实际上只是一系列 <code>Replicaset</code> 的集合，而 <code>Replicaset</code> 是一系列 <code>Pod</code> 的集合。那么 <code>Kubernetes</code> 是如何从一个 <code>HTTP</code> 请求按照层级结构依次创建这些资源的呢？其实这些工作都是由 <code>Kubernetes</code> 内置的 <code>Controller</code>(控制器) 来完成的。</p><p><code>Kubernetes</code> 在整个系统中使用了大量的 <code>Controller</code>，<code>Controller</code> 是一个用于将系统状态从当前状态修正到期望状态的异步脚本。所有 <code>Controller</code> 都通过 <code>Kube-Controller-Manager</code> 组件并行运行，每种 <code>Controller</code> 都负责一种具体的控制流程。首先介绍一下 <code>Deployment Controller</code>：</p><p>将 <code>Deployment</code> 记录存储到 <code>Etcd</code> 并初始化后，就可以通过 <code>Kube-Apiserver</code> 使其可见，然后 <code>Deployment Controller</code> 就会检测到它（它的工作就是负责监听 <code>Deployment</code> 记录的更改）。在我们的例子中，控制器通过一个 <code>Informer</code> 注册一个创建事件的特定回调函数（更多信息参加下文）。</p><p>当 <code>Deployment</code> 第一次对外可见时，该 <code>Controller</code> 就会将该资源对象添加到内部工作队列，然后开始处理这个资源对象：</p><blockquote><p>通过使用标签选择器查询 <code>Kube-Apiserver</code> 来检查该 <code>Deployment</code> 是否有与其关联的 <code>ReplicaSet</code> 或 <code>Pod</code> 记录。</p></blockquote><p>有趣的是，这个同步过程是状态不可知的，它核对新记录与核对已经存在的记录采用的是相同的方式。</p><p>在意识到没有与其关联的 <code>ReplicaSet</code> 或 <code>Pod</code> 记录后，<code>Deployment Controller</code> 就会开始执行弹性伸缩流程：</p><blockquote><p>创建 <code>ReplicaSet</code> 资源，为其分配一个标签选择器并将其版本号设置为 1。</p></blockquote><p><code>ReplicaSet</code> 的 <code>PodSpec</code> 字段从 <code>Deployment</code> 的 <code>manifest</code> 以及其他相关元数据中复制而来。有时 <code>Deployment</code> 记录在此之后也需要更新（例如：如果设置了 <code>Process Deadline</code>）。</p><p>当完成以上步骤之后，该 <code>Deployment</code> 的 <code>Status</code> 就会被更新，然后重新进入与之前相同的循环，等待 <code>Deployment</code> 与期望的状态相匹配。由于 <code>Deployment Controller</code> 只关心 <code>ReplicaSet</code>，因此需要通过 <code>ReplicaSet Controller</code> 来继续协调。</p><p><strong>ReplicaSets controller</strong></p><p>在前面的步骤中，<code>Deployment Controller</code> 创建了第一个 <code>ReplicaSet</code>，但仍然还是没有 <code>Pod</code>，这时候就该 <code>ReplicaSet Controller</code> 登场了！<code>ReplicaSet Controller</code> 的工作是监视 <code>ReplicaSets</code> 及其相关资源（<code>Pod</code>）的生命周期。和大多数其他 <code>Controller</code> 一样，它通过触发某些事件的处理器来实现此目的。</p><p>当创建 <code>ReplicaSet</code> 时（由 <code>Deployment Controller</code> 创建），<code>RS Controller</code> 检查新 <code>ReplicaSet</code> 的状态，并检查当前状态与期望状态之间存在的偏差，然后通过调整 <code>Pod</code> 的副本数来达到期望的状态。</p><p><code>Pod</code> 的创建也是批量进行的，从 <code>SlowStartInitialBatchSize</code> 开始，然后在每次成功的迭代中以一种 <code>Slow Start</code> 操作加倍。这样做的目的是在大量 <code>Pod</code> 启动失败时（例如：由于资源配额），可以减轻 <code>Kube-Apiserver</code> 被大量不必要的 <code>HTTP</code> 请求吞没的风险。如果创建失败，最好能够优雅地失败，并且对其他的系统组件造成的影响最小！</p><p><code>Kubernetes</code> 通过 <code>Owner References</code>（在子级资源的某个字段中引用其父级资源的 ID） 来构造严格的资源对象层级结构。这确保了一旦 <code>Controller</code> 管理的资源被删除（级联删除），子资源就会被垃圾收集器删除，同时还为父级资源提供了一种有效的方式来避免他们竞争同一个子级资源（想象两对父母都认为他们拥有同一个孩子的场景）。</p><p><code>Owner References</code> 的另一个好处是：它是有状态的。如果有任何 <code>Controller</code> 重启了，那么由于资源对象的拓扑关系与 <code>Controller</code> 无关，该操作不会影响到系统的稳定运行。这种对资源隔离的重视也体现在 <code>Controller</code> 本身的设计中：<code>Controller</code> 不能对自己没有明确拥有的资源进行操作，它们应该选择对资源的所有权，互不干涉，互不共享。</p><p>有时系统中也会出现孤儿（<code>Orphaned</code>）资源，通常由以下两种途径产生：</p><ul><li>父级资源被删除，但子级资源没有被删除</li><li>垃圾收集策略禁止删除子级资源</li></ul><p>当发生这种情况时，<code>Controller</code> 将会确保孤儿资源拥有新的 <code>Owner</code>。多个父级资源可以相互竞争同一个孤儿资源，但只有一个会成功（其他父级资源会收到验证错误）。</p><p><strong>Informers</strong></p><p>你可能已经注意到，某些 <code>Controller</code>（例如：<code>RBAC</code> 授权器或 <code>Deployment Controller</code>）需要先检索集群状态然后才能正常运行。拿 <code>RBAC</code> 授权器举例，当请求进入时，授权器会将用户的初始状态缓存下来，然后用它来检索与 <code>Etcd</code> 中的用户关联的所有角色（<code>Role</code>）和 角色绑定（<code>RoleBinding</code>）。那么问题来了，<code>Controller</code> 是如何访问和修改这些资源对象的呢？事实上 <code>Kubernetes</code> 是通过 <code>Informer</code> 机制来解决这个问题的。</p><p><code>Infomer</code> 是一种模式，它允许 <code>Controller</code> 查找缓存在本地内存中的数据(这份数据由 <code>Informer</code> 自己维护)并列出它们感兴趣的资源。</p><p>虽然 <code>Informer</code> 的设计很抽象，但它在内部实现了大量的对细节的处理逻辑（例如：缓存），缓存很重要，因为它不但可以减少对 <code>Kubenetes API</code> 的直接调用，同时也能减少 <code>Server</code> 和 <code>Controller</code> 的大量重复性工作。通过使用 <code>Informer</code>，不同的 <code>Controller</code> 之间以线程安全（<code>Thread safety</code>）的方式进行交互，而不必担心多个线程访问相同的资源时会产生冲突。</p><p>有关 <code>Informer</code> 的更多详细解析，请参考这篇文章：<a href="https://borismattijssen.github.io/articles/kubernetes-informers-controllers-reflectors-stores" target="_blank" rel="noopener">Kubernetes: Controllers, Informers, Reflectors and Stores</a></p><p><strong>Scheduler</strong></p><p>当所有的 <code>Controller</code> 正常运行后，<code>Etcd</code> 中就会保存一个 <code>Deployment</code>、一个 <code>ReplicaSet</code> 和三个 <code>Pod</code> 资源记录，并且可以通过 <code>Kube-Apiserver</code> 查看。然而，这些 <code>Pod</code> 资源现在还处于 <code>Pending</code> 状态，因为它们还没有被调度到集群中合适的 <code>Node</code> 上运行。这个问题最终要靠调度器 <code>Scheduler</code> 来解决。</p><p><code>Scheduler</code> 作为一个独立的组件运行在集群控制平面上，工作方式与其他 <code>Controller</code> 相同：监听实际并将系统状态调整到期望的状态。具体来说，<code>Scheduler</code> 的作用是将待调度的 <code>Pod</code> 按照特定的算法和调度策略绑定到集群中某个合适的  <code>Node</code> 上，并将绑定信息写入 <code>Etcd</code> 中（它会过滤其 <code>PodSpec</code> 中 <code>NodeName</code> 字段为空的 <code>Pod</code>），默认的调度算法的工作方式如下：</p><ol><li><p>当 <code>Scheduler</code> 启动时，会注册一个默认的预选策略链，这些预选策略会对备选节点进行评估，判断备选节点是否满足备选 <code>Pod</code> 的需求。例如：如果 <code>PodSpec</code> 字段限制了 <code>CPU</code> 和内存资源，那么当备选节点的资源容量不满足备选 <code>Pod</code> 的需求时，备选 <code>Pod</code> 就不会被调度到该节点上（资源容量=备选节点资源总量-节点中已存在 <code>Pod</code> 的所有容器的需求资源（<code>CPU</code> 和内存的总和）</p></li><li><p>一旦筛选出符合要求的候选节点，就会采用优选策略计算出每个候选节点的积分，然后对这些候选节点进行排序，积分最高者胜出。例如：为了在整个系统中分摊工作负载，这些优选策略会从备选节点列表中选出资源消耗最小的节点。每个节点通过优选策略时都会算出一个得分，计算各项得分，最终选出分值大的节点作为优选的结果。</p></li></ol><p>一旦找到了合适的节点，<code>Scheduler</code> 就会创建一个 <code>Binding</code> 对象，该对象的 <code>Name</code> 和 <code>UID</code> 与 <code>Pod</code> 相匹配，并且其 <code>ObjectReference</code> 字段包含所选节点的名称，然后通过 <code>POST</code> 请求发送给 <code>Apiserver</code>。</p><p>当 <code>Kube-Apiserver</code> 接收到此 <code>Binding</code> 对象时，注册表会将该对象反序列化并更新 <code>Pod</code> 资源中的以下字段：</p><ul><li>将 <code>NodeName</code> 的值设置为 <code>ObjectReference</code> 中的 <code>NodeName</code>。</li><li>添加相关的注释。</li><li>将 <code>PodScheduled</code> 的 <code>Status</code> 值设置为 <code>True</code>。可以通过 <code>Kubectl</code> 来查看：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get &lt;PODNAME&gt; -o go-template=<span class="string">'&#123;&#123;range .status.conditions&#125;&#125;&#123;&#123;if eq .type "PodScheduled"&#125;&#125;&#123;&#123;.status&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;end&#125;&#125;'</span></span><br></pre></td></tr></table></figure><p>一旦 <code>Scheduler</code> 将 <code>Pod</code> 调度到某个节点上，该节点的 <code>Kubelet</code> 就会接管该 <code>Pod</code> 并开始部署。</p><blockquote><p>注：预选策略和优选策略都可以通过 <code>--policy-config-file</code> 参数来扩展，如果默认的调度器不满足要求，还可以部署自定义的调度器。如果 <code>podSpec.schedulerName</code> 的值设置为其他的调度器，则 <code>Kubernetes</code> 会将该 <code>Pod</code> 的调度转交给那个调度器。</p></blockquote><h3 id="6-kubelet">6. Kubelet</h3><p><strong>Pod 同步</strong></p><p>现在，所有的 <code>Controller</code> 都完成了工作，我们来总结一下：</p><ul><li><code>HTTP</code> 请求通过了认证、授权和准入控制阶段。</li><li>一个 <code>Deployment</code>、<code>ReplicaSet</code> 和三个 <code>Pod</code> 资源被持久化到 <code>Etcd</code> 存储中。</li><li>然后运行了一系列初始化器。</li><li>最后每个 <code>Pod</code> 都被调度到合适的节点。</li></ul><p>然而到目前为止，所有的状态变化仅仅只是针对保存在 <code>Etcd</code> 中的资源记录，接下来的步骤涉及到运行在工作节点之间的 <code>Pod</code> 的分布状况，这是分布式系统（比如：<code>Kubernetes</code>）的关键因素。这些任务都是由 <code>Kubelet</code> 组件完成的，让我们开始吧！</p><p>在 <code>Kubernetes</code> 集群中，每个 <code>Node</code> 节点上都会启动一个 <code>Kubelet</code> 服务进程，该进程用于处理 <code>Scheduler</code> 下发到本节点的任务，管理 <code>Pod</code> 的生命周期，包括挂载卷、容器日志记录、垃圾回收以及其他与 <code>Pod</code> 相关的事件。</p><p>如果换一种思维模式，你可以把 <code>Kubelet</code> 当成一种特殊的 <code>Controller</code>，它每隔 20 秒（可以自定义）向 <code>Kube-Apiserver</code> 通过 <code>NodeName</code> 获取自身 <code>Node</code> 上所要运行的 <code>Pod</code> 清单。一旦获取到了这个清单，它就会通过与自己的内部缓存进行比较来检测新增加的 <code>Pod</code>，如果有差异，就开始同步 <code>Pod</code> 列表。我们来详细分析一下同步过程：</p><ol><li><p>如果 <code>Pod</code> 正在创建， <code>Kubelet</code> 就会记录一些在 <code>Prometheus</code> 中用于追踪 <code>Pod</code> 启动延时的指标。</p></li><li><p>然后生成一个 <code>PodStatus</code> 对象，它表示 <code>Pod</code> 当前阶段的状态。<code>Pod</code> 的状态(<code>Phase</code>) 是 <code>Pod</code> 在其生命周期中的最精简的概要，包括 <code>Pending</code>、<code>Running</code>、<code>Succeeded</code>、<code>Failed</code> 和 <code>Unkown</code> 这几个值。状态的产生过程非常复杂，所以很有必要深入了解一下背后的原理：</p><ul><li><p>首先串行执行一系列 <code>Pod</code> 同步处理器（<code>PodSyncHandlers</code>），每个处理器检查 <code>Pod</code> 是否应该运行在该节点上。当所有的处理器都认为该 <code>Pod</code> 不应该运行在该节点上，则 <code>Pod</code> 的 <code>Phase</code> 值就会变成 <code>PodFailed</code>，并且将该 <code>Pod</code> 从该节点上驱逐出去。例如：当你创建一个 <code>Job</code> 时，如果 <code>Pod</code> 失败重试的时间超过了 <code>spec.activeDeadlineSeconds</code> 设置的值，就会将 <code>Pod</code> 从该节点驱逐出去。</p></li><li><p>接下来，<code>Pod</code> 的 <code>Phase</code> 值由 <code>Init</code> 容器和应用容器的状态共同来决定。因为目前容器还没有启动，容器被视为处于等待阶段，如果 <code>Pod</code> 中至少有一个容器处于等待阶段，则其 <code>Phase</code> 值为 <code>Pending</code>。</p></li><li><p>最后，<code>Pod</code> 的 <code>Condition</code> 字段由 <code>Pod</code> 内所有容器的状态决定。现在我们的容器还没有被容器运行时创建，所以 <code>PodReady</code> 的状态被设置为 <code>False</code>。可以通过 <code>Kubectl</code> 查看：</p></li></ul></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get &lt;PODNAME&gt; -o go-template=<span class="string">'&#123;&#123;range .status.conditions&#125;&#125;&#123;&#123;if eq .type "Ready"&#125;&#125;&#123;&#123;.status&#125;&#125;&#123;&#123;end&#125;&#125;&#123;&#123;end&#125;&#125;'</span></span><br></pre></td></tr></table></figure><ol start="3"><li><p>生成 <code>PodStatus</code> 之后（<code>Pod</code> 中的 <code>Status</code> 字段），<code>Kubelet</code> 就会将它发送到 <code>Pod</code> 的状态管理器，该管理器的任务是通过 <code>Apiserver</code> 异步更新 <code>Etcd</code> 中的记录。</p></li><li><p>接下来运行一系列准入处理器来确保该 <code>Pod</code> 是否具有相应的权限（包括强制执行 <code>AppArmor</code> 配置文件和 <code>NO_NEW_PRIVS</code>），被准入控制器拒绝的 <code>Pod</code> 将一直保持 <code>Pending</code> 状态。</p></li><li><p>如果 <code>Kubelet</code> 启动时指定了 <code>cgroups-per-qos</code> 参数，<code>Kubelet</code> 就会为该 <code>Pod</code> 创建 <code>Cgroup</code> 并进行相应的资源限制。这是为了更方便地对 <code>Pod</code> 进行服务质量（<code>QoS</code>）管理。</p></li><li><p>然后为 <code>Pod</code> 创建相应的目录，包括 <code>Pod</code> 的目录（/var/run/kubelet/pods/<podid>），该 <code>Pod</code> 的卷目录（<code>&lt;podDir&gt;/volumes</code>）和该 Pod 的插件目录（<code>&lt;podDir&gt;/plugins</code>）。</podid></p></li><li><p>卷管理器会挂载 <code>Spec.Volumes</code> 中定义的相关数据卷，然后等待是否挂载成功。根据挂载卷类型的不同，某些 <code>Pod</code> 可能需要等待更长的时间（比如：<code>NFS</code> 卷）。</p></li><li><p>从 <code>Apiserver</code> 中检索 <code>Spec.ImagePullSecrets</code> 中定义的所有 <code>Secret</code>，然后将其注入到容器中。</p></li><li><p>最后通过容器运行时接口（<code>Container Runtime Interface</code>（CRI））开始启动容器（下面会详细描述）。</p></li></ol><p><strong>CRI 与 Pause 容器</strong></p><p>到了这个阶段，大量的初始化工作都已经完成，容器已经准备好开始启动了，而容器是由容器运行时（例如：<code>Docker</code> 和 <code>Rkt</code>）启动的。</p><p>为了更容易扩展，<code>Kubelet</code> 从 1.5.0 开始通过容器运行时接口与容器运行时（<code>Container Runtime</code>）交互。简而言之，<code>CRI</code> 提供了 <code>Kubelet</code> 和特定的运行时之间的抽象接口，它们之间通过<a href="https://github.com/google/protobuf" target="_blank" rel="noopener">协议缓冲区</a>（它像一个更快的 <code>JSON</code>）和 <a href="https://grpc.io/" target="_blank" rel="noopener">gRPC API</a>（一种非常适合执行 <code>Kubernetes</code> 操作的 <code>API</code>）交互。这是一个非常酷的想法，通过使用 <code>Kubelet</code> 和运行时之间定义的契约关系，容器如何编排的具体实现细节已经变得无关紧要。由于不需要修改 <code>Kubernetes</code> 的核心代码，开发者可以以最小的开销添加新的运行时。</p><p>不好意思有点跑题了，让我们继续回到容器启动的阶段。第一次启动 <code>Pod</code> 时，<code>Kubelet</code> 会通过 <code>Remote Procedure Command</code>(RPC) 协议调用 <code>RunPodSandbox</code>。<code>Sandbox</code> 用于描述一组容器，例如：在 <code>Kubernetes</code> 中它表示的是 <code>Pod</code>。<code>Sandbox</code> 是一个很宽泛的概念，所以对于其他没有使用容器的运行时仍然是有意义的（比如：在一个基于 <code>Hypervisor</code> 的运行时中，<code>Sandbox</code> 可能指的就是虚拟机）。</p><p>我们的例子中使用的容器运行时是 <code>Docker</code>，创建 <code>Sandbox</code> 时首先创建的是 <code>Pause</code> 容器。<code>Pause</code> 容器作为同一个 <code>Pod</code> 中所有其他容器的基础容器，它为 <code>Pod</code> 中的每个业务容器提供了大量的 <code>Pod</code> 级别资源，这些资源都是 <code>Linux</code> 命名空间（包括：网络命名空间，<code>IPC</code> 命名空间和 <code>PID</code> 命名空间）。</p><p><code>Pause</code> 容器提供了一种方法来管理所有这些命名空间并允许业务容器共享它们，在同一个网络命名空间中的好处是：同一个 <code>Pod</code> 中的容器可以使用 <code>Localhost</code> 来相互通信。<code>Pause</code> 容器的第二个功能与 <code>PID</code> 命名空间的工作方式相关，在 <code>PID</code> 命名空间中，进程之间形成一个树状结构，一旦某个子进程由于父进程的错误而变成了孤儿进程，其便会被 <code>Init</code> 进程进行收养并最终回收资源。关于 <code>Pause</code> 工作方式的详细信息可以参考：<a href="https://www.ianlewis.org/en/almighty-pause-container" target="_blank" rel="noopener">The Almighty Pause Container</a>。</p><p>一旦创建好了 <code>Pause</code> 容器，下面就会开始检查磁盘状态然后开始启动业务容器。</p><p><strong>CNI 和 Pod 网络</strong></p><p>现在我们的 <code>Pod</code> 已经有了基本的骨架：一个共享所有命名空间以允许业务容器在同一个 <code>Pod</code> 里进行通信的 <code>Pause</code> 容器。但现在还有一个问题，那就是容器的网络是如何建立的？</p><p>当 <code>Kubelet</code> 为 <code>Pod</code> 创建网络时，它会将创建网络的任务交给 <code>CNI</code> 插件。<code>CNI</code> 表示容器网络接口（<code>Container Network Interface</code>），和容器运行时的运行方式类似，它也是一种抽象，允许不同的网络提供商为容器提供不同的网络实现。通过将 <code>JSON</code> 配置文件（默认在 <code>/etc/cni/net.d</code> 路径下）中的数据传送到相关的 <code>CNI</code> 二进制文件（默认在 <code>/opt/cni/bin</code> 路径下）中，<code>CNI</code> 插件可以给 <code>Pause</code> 容器配置相关的网络，然后<code>Pod</code> 中其他的容器都使用 <code>Pause</code> 容器的网络。下面是一个简单的示例配置文件：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="attr">    "cniVersion":</span> <span class="string">"0.3.1"</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "name":</span> <span class="string">"bridge"</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "type":</span> <span class="string">"bridge"</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "bridge":</span> <span class="string">"cnio0"</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "isGateway":</span> <span class="literal">true</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "ipMasq":</span> <span class="literal">true</span><span class="string">,</span></span><br><span class="line"><span class="attr">    "ipam":</span> <span class="string">&#123;</span></span><br><span class="line"><span class="attr">        "type":</span> <span class="string">"host-local"</span><span class="string">,</span></span><br><span class="line"><span class="attr">        "ranges":</span> <span class="string">[</span></span><br><span class="line">          <span class="string">[&#123;"subnet":</span> <span class="string">"$&#123;POD_CIDR&#125;"</span><span class="string">&#125;]</span></span><br><span class="line">        <span class="string">],</span></span><br><span class="line"><span class="attr">        "routes":</span> <span class="string">[&#123;"dst":</span> <span class="string">"0.0.0.0/0"</span><span class="string">&#125;]</span></span><br><span class="line">    <span class="string">&#125;</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure><p><code>CNI</code> 插件还会通过 <code>CNI_ARGS</code> 环境变量为 <code>Pod</code> 指定其他的元数据，包括 <code>Pod</code> 名称和命名空间。</p><p>下面我们以 <code>Bridge</code> 插件举例(步骤会因不同 <code>CNI</code> 插件而异)：</p><ul><li><p>该插件首先会在根网络命名空间（也就是宿主机的网络命名空间）中设置本地 <code>Linux</code> 网桥，以便为该主机上的所有容器提供网络服务。</p></li><li><p>然后它会将一个网络接口（<code>Veth</code> 设备对的一端）插入到 <code>Pause</code> 容器的网络命名空间中，并将另一端连接到网桥上。你可以这样来理解 <code>Veth</code> 设备对：它就像一根很长的管道，一端连接到容器，一端连接到根网络命名空间中，数据包就在管道中进行传播。</p></li><li><p>接下来 <code>JSON</code> 文件中指定的 <code>IPAM Plugin</code> 会为 <code>Pause</code> 容器的网络接口分配一个 <code>IP</code> 并设置相应的路由，现在 <code>Pod</code> 就有了自己的 <code>IP</code>。</p><ul><li><code>IPAM Plugin</code> 的工作方式和 <code>CNI Plugin</code> 类似：通过二进制文件调用并具有标准化的接口，每一个 <code>IPAM Plugin</code> 都必须要确定容器网络接口的 <code>IP</code>、子网以及网关和路由，并将信息返回给 <code>CNI</code> 插件。最常见的 <code>IPAM Plugin</code> 是 <code>host-local</code>，它从预定义的一组地址池中为容器分配 <code>IP</code> 地址。它将地址池的信息以及分配信息保存在主机的文件系统中，从而确保了同一主机上每个容器的 <code>IP</code> 地址的唯一性。</li></ul></li><li><p>最后 <code>Kubelet</code> 会将集群内部的 <code>DNS</code> 服务器的 <code>Cluster IP</code> 地址传给 <code>CNI</code> 插件，然后 <code>CNI</code> 插件将它们写到容器的 <code>/etc/resolv.conf</code> 文件中。</p></li></ul><p>一旦完成了上面的步骤，<code>CNI</code> 插件就会将操作的结果以 <code>JSON</code> 的格式返回给 <code>Kubelet</code>。更多关于 <code>CNI</code> 相关信息可参考 「<a href="https://zhuanlan.zhihu.com/p/27460083" target="_blank" rel="noopener">CNI网络插件指南</a>」一文。</p><p><strong>跨主机容器网络</strong></p><p>到目前为止，我们已经描述了容器如何与宿主机进行通信，但跨主机之间的容器如何通信呢？</p><p>通常情况下使用 <code>Overlay</code> 网络来进行跨主机容器通信，这是一种动态同步多个主机间路由的方法。 其中最常用的 <code>Overlay</code> 网络插件是 <code>Flannel</code>，<code>Flannel</code> 具体的工作方式可以参考 <a href="https://github.com/coreos/flannel" target="_blank" rel="noopener">CoreOS</a> 的文档。</p><p><strong>容器启动</strong></p><p>所有网络都配置完成后，接下来就开始真正启动业务容器了！</p><p>一旦 <code>Sanbox</code> 完成初始化并处于 <code>Active</code> 状态，<code>Kubelet</code> 就可以开始为其创建容器了。首先启动 <code>PodSpec</code> 中定义的 <code>Init</code> 容器，然后再启动业务容器。具体过程如下：</p><ol><li>首先拉取容器的镜像。如果是私有仓库的镜像，就会利用 <code>PodSpec</code> 中指定的 <code>Secret</code> 来拉取该镜像。</li><li>然后通过 <code>CRI</code> 接口创建容器。<code>Kubelet</code> 向 <code>PodSpec</code> 中填充了一个 <code>ContainerConfig</code> 数据结构（在其中定义了命令、镜像、标签、挂载卷、设备、环境变量等），然后通过 <code>Protobufs</code> 发送给 <code>CRI</code> 接口。对于 <code>Docker</code> 来说，它会将这些信息反序列化并填充到自己的配置信息中，然后再发送给 <code>Dockerd</code> 守护进程。在这个过程中，它会将一些元数据标签（例如：容器类型、日志路径、<code>Sandbox ID</code> 等）添加到容器中。</li><li>接下来会使用 <code>CPU</code> 管理器来约束容器，这是 1.8 中新添加的 <code>Alpha</code> 特性，它使用 <code>UpdateContainerResources</code> CRI 方法将容器分配给本节点上的 <code>CPU</code> 资源池。</li><li>最后容器开始真正启动。</li><li>如果 <code>Pod</code> 中配置了容器生命周期钩子（<code>Hook</code>），容器启动之后就会运行这些 <code>Hook</code>。<code>Hook</code> 的类型包括两种：<code>Exec</code>（执行一段命令） 和 <code>HTTP</code>（发送 <code>HTTP</code> 请求）。如果 <code>PostStart Hook</code> 启动的时间过长、挂起或者失败，容器将永远不会变成 <code>Running</code> 状态。</li></ol><h3 id="7-总结">7. 总结</h3><p>如果上面一切顺利，现在你的集群上应该会运行三个容器，所有的网络，数据卷和秘钥都被通过 <code>CRI</code> 接口添加到容器中并配置成功。</p><h3 id="8-原文链接">8. 原文链接</h3><p><a href="https://github.com/jamiehannaford/what-happens-when-k8s" target="_blank" rel="noopener">What happens when … Kubernetes edition!</a></p><blockquote><p>来源：Ryan Yang 的 Blog<br>原文：<a href="http://t.cn/RDhqlnT" target="_blank" rel="noopener">http://t.cn/RDhqlnT</a><br>题图：来自谷歌图片搜索<br>版权：本文版权归原作者所有</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;想象一下，如果我想将 &lt;code&gt;Nginx&lt;/code&gt; 部署到 &lt;code&gt;Kubernetes&lt;/code&gt; 集群，我可能会在终端中输入类似这样的命令：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ kubectl run --image=nginx --replicas=3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后回车。几秒钟后，你就会看到三个 &lt;code&gt;Nginx Pod&lt;/code&gt; 分布在所有的工作节点上。这一切就像变魔术一样，但你并不知道这一切的背后究竟发生了什么事情。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Kubernetes&lt;/code&gt; 的神奇之处在于：它可以通过用户友好的 &lt;code&gt;API&lt;/code&gt; 来处理跨基础架构的 &lt;code&gt;Deployments&lt;/code&gt;，而背后的复杂性被隐藏在简单的抽象中。但为了充分理解它为我们提供的价值，我们需要理解它的内部原理。&lt;/p&gt;
&lt;p&gt;本指南将引导您理解从 &lt;code&gt;Client&lt;/code&gt; 到 &lt;code&gt;Kubelet&lt;/code&gt; 请求的完整生命周期，必要时会通过源代码来说明背后发生了什么。&lt;/p&gt;
&lt;p&gt;这是一份可以在线修改的文档，如果你发现有什么可以改进或重写的，欢迎提供帮助！&lt;/p&gt;
&lt;h3 id=&quot;1-kubectl&quot;&gt;1. Kubectl&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;验证和生成器&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当敲下回车键以后，&lt;code&gt;Kubectl&lt;/code&gt; 首先会执行一些客户端验证操作，以确保不合法的请求（例如：创建不支持的资源或使用格式错误的镜像名称）将会快速失败，也不会发送给 &lt;code&gt;Kube-Apiserver&lt;/code&gt;。通过减少不必要的负载来提高系统性能。&lt;/p&gt;
&lt;p&gt;验证通过之后，&lt;code&gt;Kubectl&lt;/code&gt; 开始将发送给 &lt;code&gt;Kube-Apiserver&lt;/code&gt; 的 &lt;code&gt;HTTP&lt;/code&gt; 请求进行封装。&lt;code&gt;Kube-Apiserver&lt;/code&gt; 与 &lt;code&gt;Etcd&lt;/code&gt; 进行通信，所有尝试访问或更改 &lt;code&gt;Kubernetes&lt;/code&gt; 系统状态的请求都会通过 &lt;code&gt;Kube-Apiserver&lt;/code&gt; 进行，&lt;code&gt;Kubectl&lt;/code&gt; 也不例外。&lt;code&gt;Kubectl&lt;/code&gt; 使用生成器（&lt;a href=&quot;https://kubernetes.io/docs/user-guide/kubectl-conventions/#generators&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Generators&lt;/a&gt;）来构造 &lt;code&gt;HTTP&lt;/code&gt; 请求。生成器是一个用来处理序列化的抽象概念。&lt;/p&gt;
&lt;p&gt;通过 &lt;code&gt;kubectl run&lt;/code&gt; 不仅可以运行 &lt;code&gt;Deployment&lt;/code&gt;，还可以通过指定参数 &lt;code&gt;--generator&lt;/code&gt; 来部署其他多种资源类型。如果没有指定 &lt;code&gt;--generator&lt;/code&gt; 参数的值，&lt;code&gt;Kubectl&lt;/code&gt; 将会自动判断资源的类型。&lt;/p&gt;
&lt;p&gt;例如：带有参数 &lt;code&gt;--restart-policy=Always&lt;/code&gt; 的资源将被部署为 &lt;code&gt;Deployment&lt;/code&gt;，而带有参数 &lt;code&gt;--restart-policy=Never&lt;/code&gt; 的资源将被部署为 &lt;code&gt;Pod&lt;/code&gt;。同时 &lt;code&gt;Kubectl&lt;/code&gt; 也会检查是否需要触发其他操作，例如：记录命令（用来进行回滚或审计）。&lt;/p&gt;
&lt;p&gt;在 &lt;code&gt;Kubectl&lt;/code&gt; 判断出要创建一个 &lt;code&gt;Deployment&lt;/code&gt; 后，它将使用 &lt;code&gt;DeploymentV1Beta1&lt;/code&gt; 生成器从我们提供的参数中生成一个运行时对象。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>如何 10 步 Docker 化一个应用</title>
    <link href="https://www.hi-linux.com/posts/57498.html"/>
    <id>https://www.hi-linux.com/posts/57498.html</id>
    <published>2018-08-01T01:00:00.000Z</published>
    <updated>2018-08-24T08:19:54.208Z</updated>
    
    <content type="html"><![CDATA[<p>本文将讲解如何将应用 Docker 化的一些很实用的技巧和准则，推荐一读。</p><p><strong>一、选择基础镜像</strong></p><p>每种对应技术几乎都有自己的基础镜像，例如：</p><ul><li><a href="https://hub.docker.com/_/java/" target="_blank" rel="noopener">https://hub.docker.com/_/java/</a></li><li><a href="https://hub.docker.com/_/python/" target="_blank" rel="noopener">https://hub.docker.com/_/python/</a></li><li><a href="https://hub.docker.com/_/nginx/" target="_blank" rel="noopener">https://hub.docker.com/_/nginx/</a></li></ul><p>如果不能直接使用这些镜像，我们就需要从基础操作系统镜像开始安装所有的依赖。</p><p>网上大多数教程使用的都是以 Ubuntu（例如：Ubuntu:16.04 ）作为基础镜像，这并不是一个问题，但是我建议优先考虑 Alpine 镜像：</p><ul><li><a href="https://hub.docker.com/_/alpine/" target="_blank" rel="noopener">https://hub.docker.com/_/alpine/</a></li></ul><p>Alpine 是一个非常小的基础镜像（它的容量大约只有 5MB）。</p><blockquote><p>注：在基于 Alpine 的镜像中你无法使用 apt-get 命令。不过你不必担心，因为 Alpine 系统有自己的软件包仓库和包管理工具 apk。关于 apk 的具体使用你可以详细参考：「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247484888&amp;idx=1&amp;sn=bfdd55a668b068ab34251512613e5267&amp;chksm=eac524f1ddb2ade7739c10440af8b33a39b81b8a4a57b628cc6c0d0ec3a82f18b914f4e12641&amp;mpshare=1&amp;scene=23&amp;srcid=0731dgw9eDCTtafPzAd3VxDV%23rd" target="_blank" rel="noopener">Alpine Linux配置使用技巧</a>」一文。</p></blockquote><p><strong>二、安装必要软件包</strong></p><p>这个步骤通常比较琐碎，有一些容易忽略的细节：</p><ul><li><p><code>apt-get update</code> 和 <code>apt-get install</code> 命令应该写在一行（如果使用 Alpine 则对应的是 apk 命令）。这不是一个常见的做法，但是在 Dockerfile 中应该要这么做。否则 <code>apt-get update</code> 命令产出的临时层可能会被缓存，导致构建时没有更新包信息。（具体可参见<a href="https://forums.docker.com/t/dockerfile-run-apt-get-install-all-packages-at-once-or-one-by-one/17191" target="_blank" rel="noopener">此文</a>）。</p></li><li><p>确认是否只安装了实际需要的软件（特别是在生产环境中运行这个容器）。</p></li></ul><blockquote><p>注：我见过有人在他们的镜像中安装了 vim 和其他开发工具。如果这是必要的，应该针对构建、调试和开发环境创建不同的 Dockerfile。这不仅仅关系到镜像大小，还涉及到安全性、可维护性等等。</p></blockquote><a id="more"></a><p><strong>三、添加自定义文件</strong></p><p>一些优化 Dockerfile 的小提示：</p><ul><li><p>理解 COPY 和 ADD 指令的区别，具体可参考<a href="https://docs.docker.com/develop/develop-images/dockerfile_best-practices/#add-or-copy" target="_blank" rel="noopener">此文</a>。</p></li><li><p>尽可能遵照<a href="http://www.pathname.com/fhs/" target="_blank" rel="noopener">文件系统</a>惯例来存放文件。例如：针对解释型应用程序（如：Python），使用 /usr/src 目录。</p></li><li><p>检查添加文件的属性。如果需要可执行权限，没有必要在镜像上新建一个层（ 通过 RUN chmod +x … 指令来增加权限）。你只需要在代码仓库的源文件上修正这些属性即可，即使开发平台是 Windows，也可以参照<a href="https://stackoverflow.com/questions/21691202/how-to-create-file-execute-mode-permissions-in-git-on-windows" target="_blank" rel="noopener">此文</a>给文件增加可执行权限。</p></li></ul><p><strong>四、定义容器运行时的用户权限</strong></p><ul><li>容器中的进程默认情况下是以 root 权限运行的。</li><li>如果容器中的应用程序需要使用特定的用户或组（/etc/passwd 或 /etc/group）来运行时，可以在容器启动时使用 <code>docker run</code> 命令的 <code>--user</code> 参数来指定其固定的 UID 或 GID。</li><li>尽可能避免容器中的进程以 root 权限运行。</li></ul><blockquote><p>注：现在不少热门应用程序镜像都需要用特定的用户 ID 来运行（例如:Elastic Search 需要 uid:gid = 1000:1000），请尽量不要在写出这样的镜像。更多关于容器内运行应用程序的权限说明可参考<a href="https://medium.com/@mccode/understanding-how-uid-and-gid-work-in-docker-containers-c37a01d01cf" target="_blank" rel="noopener">此文</a>。</p></blockquote><p><strong>五、定义暴露的端口</strong></p><p>不要为了暴露特权端口（例如：80）而将容器以 root 权限运行。如果有这样的需求，可以让容器暴露一个非特权端口（例如：8080），然后在启动时进行端口映射。</p><blockquote><p>注：低于 1024 的 TCP / IP 端口号就是特权端口，因为不允许普通用户在这些端口上运行服务。</p></blockquote><p><strong>六、定义入口点（entrypoint）</strong></p><ul><li><p>普通方式：直接运行可执行文件。</p></li><li><p>更好的方式：创建一个 <a href="http://docker-entrypoint.sh" target="_blank" rel="noopener">docker-entrypoint.sh</a> 脚本，这样可以通过环境变量来配置容器的入口点。这也是一个非常普遍的做法，可参考下面这些例子：elasticsearch 的 <a href="https://github.com/elastic/elasticsearch-docker/tree/master/build/elasticsearch/bin" target="_blank" rel="noopener">docker-entrypoint.sh</a> 文件 和 postgres 的 <a href="https://github.com/docker-library/postgres/tree/de8ba87d50de466a1e05e111927d2bc30c2db36d/10" target="_blank" rel="noopener">docker-entrypoint.sh</a> 文件。</p></li></ul><p><strong>七、定义一种配置方式</strong></p><p>每个应用程序都需要参数化，你基本上可以遵循以下两个原则：</p><ul><li><p>使用应用程序特定的配置文件：该方式需要通过文档来说明配置文件的格式、字段、放置位置等等（当运行环境比较复杂，例如：应用程序跨越不同的技术，则不太合适）。</p></li><li><p>使用操作系统环境变量：简单而有效。这也是 <a href="https://12factor.net/zh_cn/config" target="_blank" rel="noopener">12-factors</a> 推荐的方式。</p></li></ul><blockquote><p>注：使用环境变量方式并不意味着您需要丢弃配置文件并重构应用程序的配置机制，你只需要通过 <a href="https://blog.csdn.net/zh515858237/article/details/79218176" target="_blank" rel="noopener">envsubst</a> 命令来替换配置文件模板中的值就可以了（这个流程一般需要在 <a href="http://docker-entrypoint.sh" target="_blank" rel="noopener">docker-entrypoint.sh</a> 文件中完成，因为这需要在容器进程运行前完成）。例如：在 Nginx 配置中使用环境变量，具体方法可参考<a href="https://docs.docker.com/samples/library/nginx/#using-environment-variables-in-nginx-configuration" target="_blank" rel="noopener">此文</a>。</p></blockquote><p>这种方式可以将应用程序的配置文件封装在容器内部。</p><p><strong>八、外部化数据</strong></p><p>关于数据存储有一条黄金法则：绝对不要将任何持久化数据保存到容器内。</p><p>容器的文件系统本身是被设计成临时和短暂的。因此任何由应用程序生成的内容、数据文件和处理结果都应该保存到挂载的卷或者操作系统绑定挂载点上（既：将宿主机操作系统的目录挂载到容器中）。</p><p>如果将数据保存到绑定挂载点，对于要绑定到容器的宿主机上的目录，你需要注意以下几点：</p><ul><li>在宿主机操作系统上创建非特权用户和组。</li><li>所有需要绑定目录的所有者都是该用户。</li><li>根据使用场景给授权（仅针对这个特定的用户和组，其他用户无权访问）。</li><li>容器也以该用户运行。</li><li>容器可以完全控制这些目录。</li></ul><p><strong>九、确保处理好日志</strong></p><p>如果这是一个新的应用程序，并且希望它能够坚持 Docker 约定，就不应该将日志写入任何文件。应用程序应该使用标准输出和标准错误输出日志，这和之前推荐使用环境变量传递参数一样，这也是 12-factors 之一，具体可以参见<a href="https://12factor.net/zh_cn/logs" target="_blank" rel="noopener">这里</a>。</p><p>Docker 会自动捕捉应用程序的标准输出，并可以通过 <code>docker logs</code> 命令查看。有关于 <code>docker logs</code> 的具体使用你可以参考<a href="https://docs.docker.com/engine/reference/commandline/logs/" target="_blank" rel="noopener">这里</a>。</p><p>但是在一些实际场景下你可能会遇到问题，例如：运行一个简单的 Nginx 容器，至少会有两种不同的日志文件：</p><ul><li>HTTP 访问日志（Access Logs）</li><li>错误日志（Error Logs）</li></ul><p>对于这种按照特定结构输出日志的应用，就不太适合将它们的日志输出到标准输出。这种情况下，你需要按持久化的方式处理这些日志，并确保这些日志文件的能正常的轮转。</p><p><strong>十、轮转日志</strong></p><p>如果应用程序将日志写到文件，或者会无限追加内容到文件，就需要关注这些文件的轮转（rotation），这对于防止服务器空间耗尽非常有用的。</p><p>如果使用绑定挂载，我们可以依靠宿主机的一些工具来实现文件轮转功能。例如：logrotate，关于 logrotate 的使用你可以参考<a href="https://www.aerospike.com/docs/operations/configure/log/logrotate.html" target="_blank" rel="noopener">示例一</a>、<a href="https://www.digitalocean.com/community/tutorials/how-to-manage-logfiles-with-logrotate-on-ubuntu-16-04" target="_blank" rel="noopener">示例二</a>。</p><blockquote><p>注：本文在 「<a href="http://t.cn/ReT0AyJ" target="_blank" rel="noopener">如何 Docker 化任意一个应用</a>」的基础上整理和修改，原文地址：<a href="http://t.cn/ReT0AyJ" target="_blank" rel="noopener">http://t.cn/ReT0AyJ</a> 。</p><p>由于微信不允许外部链接，如果你需要访问文中链接可点击文末左下角的阅读原文。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将讲解如何将应用 Docker 化的一些很实用的技巧和准则，推荐一读。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一、选择基础镜像&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;每种对应技术几乎都有自己的基础镜像，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://hub.docker.com/_/java/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://hub.docker.com/_/java/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hub.docker.com/_/python/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://hub.docker.com/_/python/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://hub.docker.com/_/nginx/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://hub.docker.com/_/nginx/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果不能直接使用这些镜像，我们就需要从基础操作系统镜像开始安装所有的依赖。&lt;/p&gt;
&lt;p&gt;网上大多数教程使用的都是以 Ubuntu（例如：Ubuntu:16.04 ）作为基础镜像，这并不是一个问题，但是我建议优先考虑 Alpine 镜像：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://hub.docker.com/_/alpine/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://hub.docker.com/_/alpine/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Alpine 是一个非常小的基础镜像（它的容量大约只有 5MB）。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注：在基于 Alpine 的镜像中你无法使用 apt-get 命令。不过你不必担心，因为 Alpine 系统有自己的软件包仓库和包管理工具 apk。关于 apk 的具体使用你可以详细参考：「&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247484888&amp;amp;idx=1&amp;amp;sn=bfdd55a668b068ab34251512613e5267&amp;amp;chksm=eac524f1ddb2ade7739c10440af8b33a39b81b8a4a57b628cc6c0d0ec3a82f18b914f4e12641&amp;amp;mpshare=1&amp;amp;scene=23&amp;amp;srcid=0731dgw9eDCTtafPzAd3VxDV%23rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Alpine Linux配置使用技巧&lt;/a&gt;」一文。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;二、安装必要软件包&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;这个步骤通常比较琐碎，有一些容易忽略的细节：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;apt-get update&lt;/code&gt; 和 &lt;code&gt;apt-get install&lt;/code&gt; 命令应该写在一行（如果使用 Alpine 则对应的是 apk 命令）。这不是一个常见的做法，但是在 Dockerfile 中应该要这么做。否则 &lt;code&gt;apt-get update&lt;/code&gt; 命令产出的临时层可能会被缓存，导致构建时没有更新包信息。（具体可参见&lt;a href=&quot;https://forums.docker.com/t/dockerfile-run-apt-get-install-all-packages-at-once-or-one-by-one/17191&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;此文&lt;/a&gt;）。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;确认是否只安装了实际需要的软件（特别是在生产环境中运行这个容器）。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;注：我见过有人在他们的镜像中安装了 vim 和其他开发工具。如果这是必要的，应该针对构建、调试和开发环境创建不同的 Dockerfile。这不仅仅关系到镜像大小，还涉及到安全性、可维护性等等。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Docker" scheme="https://www.hi-linux.com/categories/docker/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>浅析从外部访问 Kubernetes 集群中应用的几种方式</title>
    <link href="https://www.hi-linux.com/posts/56619.html"/>
    <id>https://www.hi-linux.com/posts/56619.html</id>
    <published>2018-07-20T01:00:00.000Z</published>
    <updated>2018-08-24T06:08:59.826Z</updated>
    
    <content type="html"><![CDATA[<p>一般情况下，Kubernetes 的 Cluster Network 是属于私有网络，只能在 Cluster Network 内部才能访问部署的应用。那么如何才能将 Kubernetes 集群中的应用暴露到外部网络，为外部用户提供服务呢？本文就来讲一讲从外部网络访问 Kubernetes Cluster 中 Pod 和 Serivce 的几种常用的实现方式。</p><h3 id="pod-和-service-的关系">Pod 和 Service 的关系</h3><p>我们首先来了解一下 Kubernetes 中的 Pod 和 Service 的概念以及两者间的关系。</p><p>Pod (容器组)，英文中 Pod 是豆荚的意思。从名字的含义可以看出，Pod 是一组有依赖关系的容器。Pod 是 Kubernetes 集群中最基本的资源对象，每个 Pod 由一个或多个业务容器和一个根容器 (Pause 容器) 组成。</p><p>Kubernetes 为每个 Pod 分配了唯一的 IP（即：Pod IP），Pod 里的多个容器共享这个 IP。Pod 内的容器除了 IP，还共享相同的网络命名空间、端口、存储卷等，也就是说这些容器之间能通过 Localhost 来通信。Pod 包含的容器都会运行在同一个节点上，也可以同时启动多个相同的 Pod 用于 Failover 或者 Load balance。</p><p>Pod 的生命周期是短暂的，Kubernetes 会根据应用的配置对 Pod 进行创建、销毁并根据监控指标进行伸缩扩容。Kubernetes 在创建 Pod 时可以选择集群中的任何一台空闲的节点上进行，因此其网络地址是不固定的。由于 Pod 的这一特点，一般不建议直接通过 Pod 的地址去访问应用。</p><p>为了解决访问 Pod 不方便直接访问的问题，Kubernetes 采用了 Service 对 Pod 进行封装。Service 是对后端提供服务的一组 Pod 的抽象，Service 会绑定到一个固定的虚拟 IP上。该虚拟 IP 只在 Kubernetes Cluster 中可见，但其实该虚拟 IP 并不对应一个虚拟或者物理设备，而只是 IPtables 中的规则，然后再通过 IPtables 将服务请求路由到后端的 Pod 中。通过这种方式，可以确保服务消费者可以稳定地访问 Pod 提供的服务，而不用关心 Pod 的创建、删除、迁移等变化以及如何用一组 Pod 来进行负载均衡。</p><a id="more"></a><p>实现 Service 这一功能的关键是由 Kubernetes 中的 Kube-Proxy 来完成的。Kube-Proxy 运行在每个节点上，监听 API Server 中服务对象的变化，再通过管理 IPtables 来实现网络的转发。Kube-Proxy 目前支持三种模式：UserSpace、IPtables、IPVS。下面我们来说说这几种模式的异同：</p><ul><li>UserSpace</li></ul><p>UserSpace 是让 Kube-Proxy 在用户空间监听一个端口，所有的 Service 都转发到这个端口，然后 Kube-Proxy 在内部应用层对其进行转发。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc1.png" alt=""></p><p>Kube-Proxy 会为每个 Service 随机监听一个端口 (Proxy Port)，并增加一条 IPtables 规则。从客户端到 ClusterIP:Port 的报文都会被重定向到 Proxy Port，Kube-Proxy 收到报文后，通过 Round Robin (轮询) 或者 Session Affinity（会话亲和力，即同一 Client IP 都走同一链路给同一 Pod 服务）分发给对应的 Pod。</p><p>这种方式最大的缺点显然就是 UserSpace 会造成所有报文都走一遍用户态，造成整体性能下降，这种方在 Kubernetes 1.2 以后已经不再使用了。</p><ul><li>IPtables</li></ul><p>IPtables 方式完全由 IPtables 来实现，这种方式直接使用 IPtables 来做用户态入口，而真正提供服务的是内核的 Netilter。Kube-Proxy 只作为 Controller，这也是目前默认的方式。</p><p>Kube-Proxy 的 IPtables 方式也是支持 Round Robin 和 Session Affinity 特性。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc2.png" alt=""></p><p>Kube-Proxy 监听 Kubernetes Master 增加和删除 Service 以及 Endpoint 的消息。对于每一个 Service，Kube Proxy 创建相应的 IPtables 规则，并将发送到 Service Cluster IP 的流量转发到 Service 后端提供服务的 Pod 的相应端口上。</p><blockquote><p>注：虽然可以通过 Service 的 Cluster IP 和服务端口访问到后端 Pod 提供的服务，但该 Cluster IP 是 Ping 不通的。其原因是 Cluster IP 只是 IPtables 中的规则，并不对应到一个任何网络设备。IPVS 模式的 Cluster IP 是可以 Ping 通的。</p></blockquote><ul><li>IPVS</li></ul><p>Kubernetes 从 1.8 开始增加了 IPVS 支持，IPVS 相对 IPtables 效率会更高一些。使用 IPVS 模式需要在运行 Kube-Proxy 的节点上安装 <code>ipvsadm</code>、<code>ipset</code> 工具包和加载 <code>ip_vs</code> 内核模块。</p><p>当 Kube-Proxy 以 IPVS 代理模式启动时，Kube-Proxy 将验证节点上是否安装了 IPVS 模块，如果未安装，则 Kube-Proxy 将回退到 IPtables 代理模式。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc3.png" alt=""></p><p>这种模式，Kube-Proxy 会监视 Kubernetes Service 对象 和 Endpoints，调用 Netlink 接口以相应地创建 IPVS 规则并定期与 Kubernetes Service 对象 和 Endpoints 对象同步 IPVS 规则，以确保 IPVS 状态与期望一致。访问服务时，流量将被重定向到其中一个后端 Pod。</p><p>与 IPtables 类似，IPVS 基于 Netfilter 的 Hook 功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着 IPVS 可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，IPVS 为负载均衡算法提供了更多选项，例如：rr (轮询调度)、lc (最小连接数)、dh (目标哈希)、sh (源哈希)、sed (最短期望延迟)、nq(不排队调度)等。</p><blockquote><p>注：IPVS 是 LVS 项目的一部分，是一款运行在 Linux Kernel 当中的 4 层负载均衡器，性能异常优秀。使用调优后的内核，可以轻松处理每秒 10 万次以上的转发请求。目前在中大型互联网项目中，IPVS 被广泛的用于承接网站入口处的流量。</p></blockquote><p>了解完 Pod 和 Service 的基本概念后，我们就来具体讲一讲从外部网络访问 Kubernetes Cluster 中 Pod 和 Serivce 的几种常见的实现方式。目前主要包括如下几种：</p><ul><li>hostNetwork</li><li>hostPort</li><li>ClusterIP</li><li>NodePort</li><li>LoadBalancer</li><li>Ingress</li></ul><h3 id="通过-pod-暴露">通过 Pod 暴露</h3><h4 id="hostnetwork-true">hostNetwork: true</h4><p>这是一种直接定义 Pod 网络的方式。</p><p>如果在 Pod 中使用 <code>hostNetwork:true</code> 配置的话，在这种 Pod 中运行的应用程序可以直接看到启动 Pod 主机的网络接口。在主机的所有网络接口上都可以访问到该应用程序，以下是使用主机网络的 Pod 的示例定义：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-hostnetwork</span><br><span class="line">spec:</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx-hostnetwork</span><br><span class="line">      image: nginx:1.7.9</span><br></pre></td></tr></table></figure><p>部署该 Pod：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create  -f nginx-hostnetwork.yml</span><br><span class="line">pod &quot;nginx-hostnetwork&quot; created</span><br></pre></td></tr></table></figure><p>访问该 Pod 所在主机的 80 端口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://$HOST_IP:80</span><br><span class="line">......</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>将看到 Nginx 默认的欢迎页面，说明可以正常访问。</p><p>注意：每次启动这个 Pod 的时候都可能被调度到不同的节点上，这样所有外部访问 Pod 所在节点主机的 IP 也就是不固定的，而且调度 Pod 的时候还需要考虑是否与宿主机上的端口冲突，因此一般情况下除非您知道需要某个特定应用占用特定宿主机上的特定端口时才使用 <code>hostNetwork: true</code> 的方式。</p><p>这种 Pod 的网络模式有一个用处就是可以将网络插件包装在 Pod 中，然后部署在每个宿主机上，这样该 Pod 就可以控制该宿主机上的所有网络。</p><h4 id="hostport">hostPort</h4><p>这也是一种直接定义 Pod 网络的方式。</p><p><code>hostPort</code> 是直接将容器的端口与所调度的节点上的端口路由，这样用户就可以通过宿主机的IP加上 <code>&lt;hostPort&gt;</code> 来访问 Pod 了，如: <code>&lt;hostIP&gt;:&lt;hostPort&gt;</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-hostport</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: nginx-hostport</span><br><span class="line">      image: nginx:1.7.9</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">          hostPort: 8088</span><br></pre></td></tr></table></figure><p>部署该 Pod：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create  -f nginx-hostport.yml</span><br><span class="line">pod &quot;nginx-hostport&quot; created</span><br></pre></td></tr></table></figure><p>访问该 Pod 所在主机的 8088 端口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://$HOST_IP:8088</span><br><span class="line">......</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>将看到 Nginx 默认的欢迎页面，说明可以正常访问。</p><p>这种方式和 <code>hostNetwork: true</code> 有同一个缺点，因为 Pod 重新调度的时候该 Pod 被调度到的宿主机可能会变动，这样 <code>&lt;hostIP&gt;</code> 就变化了，用户必须自己维护一个 Pod 与所在宿主机的对应关系。</p><p>这种网络方式可以用来做 <code>Ingress Controller</code>，外部流量都需要通过 Kubenretes 节点宿主机的 80 和 443 端口。</p><h4 id="port-forward">Port Forward</h4><p>这是一种通过 <code>kubectl port-forward</code> 指令来实现数据转发的方法。<code>kubectl port-forward</code> 命令可以为 Pod 设置端口转发，通过在本机指定监听端口，访问这些端口的请求将会被转发到 Pod 的容器中对应的端口上。</p><p>首先，我们来看下 Kubernetes Port Forward 这种方式的工作机制：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc4.png" alt=""></p><p>使用 Kubectl 创建 Port Forward 后，Kubectl 会主动监听指定的本地端口。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl port-forward pod-name local-port:container-port</span><br></pre></td></tr></table></figure><p>当向 Local-Port 建立端口连接并向该端口发送数据时，数据流向会经过以下步骤：</p><ul><li>数据发往 Kubctl 监听的 Local-Port。</li><li>Kubectl 通过 SPDY 协议将数据发送给 ApiServer。</li><li>ApiServer 与目标节点的 Kubelet 建立连接，并通过 SPDY 协议将数据发送到目标 Pod 的端口上。</li><li>目标节点的 Kubelet 收到数据后，通过 PIPE（STDIN、STDOUT）与 Socat 通信。</li><li>Socat 将 STDIN 的数据发送给 Pod 内部指定的容器端口，并将返回的数据写入到 STDOUT。</li><li>STDOUT 的数据由 Kubelet 接收并按照相反的路径发送回去。</li></ul><blockquote><p>注：SPDY 协议将来可能会被替换为 HTTP/2。</p></blockquote><p>接下来，我们用一个实例来演示如何将本地端口转发到 Pod 中的端口，这里以一个运行了 Nginx 的 Pod 为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE</span><br><span class="line">nginx                       1/1       Running   2          9d</span><br></pre></td></tr></table></figure><p>验证 Nginx 服务器监听的端口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pods nginx --template=&apos;&#123;&#123;(index (index .spec.containers 0).ports 0).containerPort&#125;&#125;&#123;&#123;&quot;\n&quot;&#125;&#125;&apos;</span><br><span class="line">80</span><br></pre></td></tr></table></figure><p>将节点上的 8900 端口转发到 Nginx Pod 的 80 端口：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 执行 Kubectl port-forward 命令的时候需要指定 Pod 名称和端口转发规则。</span><br><span class="line">$ kubectl port-forward nginx 8900:80</span><br><span class="line">Forwarding from 127.0.0.1:8900 -&gt; 80</span><br><span class="line">Forwarding from [::1]:8900 -&gt; 80</span><br></pre></td></tr></table></figure><blockquote><p>注：需要在所有 Kubernetes 节点上都需要安装 Socat，关于 Socat 更详细介绍可参考：「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247485897&amp;idx=1&amp;sn=555846ff170ac7799dde353271c06f98&amp;chksm=eac528e0ddb2a1f674ca4f9e99325fbccf26589a76b43594558eeb83ef5f65b64f84a116974c&amp;mpshare=1&amp;scene=23&amp;srcid=0719C3Onveftz5JQH2g6NLTO%23rd" target="_blank" rel="noopener">Socat 入门教程</a>」 。</p></blockquote><p>验证转发是否成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ curl http://127.0.0.1:8900</span><br><span class="line">......</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>由于这种类型的转发端口是绑定在本地的，这种方式也仅适用于调试服务。</p><h3 id="通过-service-暴露">通过 Service 暴露</h3><p>Service 的类型 ( ServiceType ) 决定了 Service 如何对外提供服务。根据类型不同服务可以只在 Kubernetes Cluster 中可见，也可以暴露到 Cluster 外部。Service 目前有三种类型：ClusterIP、NodePort 和 LoadBalancer。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc5.png" alt=""></p><p>Service 中常见的三种端口的含义：</p><ul><li>port</li></ul><p>Service 暴露在 Cluster IP 上的端口，也就是虚拟 IP 要绑定的端口。port 是提供给集群内部客户访问 Service 的入口。</p><ul><li>nodePort</li></ul><p>nodePort 是 Kubernetes 提供给集群外部客户访问 Service 的入口。</p><ul><li>targetPort</li></ul><p>targetPort 是 Pod 里容器的端口，从 port 和 nodePort 上进入的数据最终会经过 Kube-Proxy 流入到后端 Pod 里容器的端口。如果 targetPort 没有被显式声明，那么会默认转发到 Service 接受请求的端口（和 port 端口的值一样）。</p><p>总的来说，port 和 nodePort 都是 Service 的端口，前者暴露给集群内客户访问服务，后者暴露给集群外客户访问服务。从这两个端口到来的数据都需要经过反向代理 Kube-Proxy 流入后端 Pod 里容器的端口，从而到达 Pod 上的容器内。</p><h4 id="clusterip">ClusterIP</h4><p>ClusterIP 是 Service 的缺省类型，这种类型的服务会自动分配一个只能在集群内部可以访问的虚拟 IP，即：ClusterIP。ClusterIP 为你提供一个集群内部其它应用程序可以访问的服务，外部无法访问。ClusterIP 服务的 YAML 类似这样：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:  </span><br><span class="line">  name: my-nginx</span><br><span class="line">selector:    </span><br><span class="line">  app: my-nginx</span><br><span class="line">spec:</span><br><span class="line">  type: ClusterIP</span><br><span class="line">  ports:  </span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    targetPort: 80</span><br><span class="line">    protocol: TCP</span><br></pre></td></tr></table></figure><p>虽然我们从集群外部不能直接访问一个 ClusterIP 服务，但是你可以使用 Kubernetes Proxy API 来访问它。</p><p>Kubernetes Proxy API 是一种特殊的 API，Kube-APIServer 只是代理这类 API 的 HTTP 请求，然后将请求转发到某个节点上的 Kubelet 进程监听的端口上。最后实际是由该端口上的 REST API 响应请求。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc6.png" alt=""></p><p>在 Master 节点上创建 Kubernetes API 的代理服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl proxy --port=8080</span><br></pre></td></tr></table></figure><p><code>kubectl proxy</code> 默认是监听在 127.0.0.1 上的，如果你需要对外提供访问，可使用一些基本的安全机制。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl proxy --port=8080 --address=192.168.100.211 --accept-hosts=&apos;^192\.168\.100\.*&apos;</span><br></pre></td></tr></table></figure><p>如果需要更多的命令使用帮助，可以使用 <code>kubectl help proxy</code>。</p><p>现在，你可以使用 Kubernetes Proxy API 进行访问。比如：需要访问一个服务，可以使用 <code>/api/v1/namespaces/&lt;NAMESPACE&gt;/services/&lt;SERVICE-NAME&gt;/proxy/</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 适用于 Kubernetes 1.10</span><br><span class="line"></span><br><span class="line">$ kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)       AGE</span><br><span class="line">kubernetes   ClusterIP   10.254.0.1       &lt;none&gt;        443/TCP       10d</span><br><span class="line">my-nginx     ClusterIP   10.254.154.119   &lt;none&gt;        80/TCP        8d</span><br><span class="line"></span><br><span class="line">$ curl http://192.168.100.211:8080/api/v1/namespaces/default/services/my-nginx/proxy/</span><br><span class="line">......</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>如果你需要直接访问一个 Pod，可以使用 <code>/api/v1/namespaces/&lt;NAMESPACE&gt;/pods/&lt;POD-NAME&gt;/proxy/</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 适用于 Kubernetes 1.10</span><br><span class="line"></span><br><span class="line">$ kubectl get pod</span><br><span class="line">NAME                        READY     STATUS    RESTARTS   AGE</span><br><span class="line">my-nginx-86555897f9-5p9c2   1/1       Running   2          8d</span><br><span class="line">my-nginx-86555897f9-ws674   1/1       Running   6          8d</span><br><span class="line"></span><br><span class="line">$ curl http://192.168.100.211:8080/api/v1/namespaces/default/pods/my-nginx-86555897f9-ws674/proxy/</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">&lt;title&gt;Welcome to nginx!&lt;/title&gt;</span><br><span class="line"></span><br><span class="line">&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;</span><br><span class="line">&lt;p&gt;If you see this page, the nginx web server is successfully installed and</span><br><span class="line">working. Further configuration is required.&lt;/p&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>由于访问 Kubernetes Proxy API 要求使用已授权用户运行 kubectl ，因此不应该使用此方法将你的服务公开到公网上或将其用于生产，这种方法主要还是用于调试服务。</p><h4 id="nodeport">NodePort</h4><p>NodePort 在 Kubenretes 里是一个广泛应用的服务暴露方式。基于 ClusterIP 提供的功能，为 Service 在 Kubernetes 集群的每个节点上绑定一个端口，即 NodePort。集群外部可基于任何一个 <code>NodeIP:NodePort</code> 的形式来访问 Service。Service 在每个节点的 NodePort 端口上都是可用的。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc7.png" alt=""></p><p>NodePort 服务与默认的 ClusterIP 服务在 YAML 定义上有两点区别：首先，type 是 NodePort。其次还有一个称为 nodePort 的参数用来指定在节点上打开哪个端口。 如果你不指定这个端口，它会选择一个随机端口。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: my-nginx</span><br><span class="line">  labels:</span><br><span class="line">    name: my-nginx</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">    - name: my-nginx</span><br><span class="line">      image: nginx:1.7.9</span><br><span class="line">      ports:</span><br><span class="line">        - containerPort: 80</span><br></pre></td></tr></table></figure><p><code>nodePort</code> 值的默认范围是 30000-32767，这个值是可以在 API Server 的配置文件中用 <code>--service-node-port-range</code> 来自定义。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: my-nginx</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">      targetPort: 80</span><br><span class="line">      nodePort: 31000</span><br><span class="line">  selector:</span><br><span class="line">    name: my-nginx</span><br></pre></td></tr></table></figure><p>NodePort 类型的服务会在所有的 Kubenretes 节点（运行有 Kube-Proxy 的节点）上统一暴露出一个端口对外提供服务，这样集群外就可以使用 Kubernetes 任意一个节点的 IP 加上指定端口（这里定义的是：31000）访问该服务了。Kube-Proxy 会自动将流量以 Round-Robin 的方式转发给该 Service 的每一个 Pod。</p><p>NodePort 类型的服务并不影响原来虚拟 IP 的访问方式，内部节点依然可以通过 <code>VIP:Port</code> 的方式进行访问。NodePort 这种服务暴露方式也存在一些不足：</p><ul><li>节点上的每个端口只能有一个服务。</li><li>如果节点 IP 地址发生更改，则需要相应机制处理该问题。</li></ul><p>基于以上原因，NodePort 比较适用的场景为演示程序或临时应用，不建议在生产环境中使用这种方法对外暴露服务。</p><h4 id="loadbalancer">LoadBalancer</h4><p>LoadBalancer 是基于 NodePort 和云服务供应商提供的外部负载均衡器，通过这个外部负载均衡器将外部请求转发到各个 <code>NodeIP:NodePort</code> 以实现对外暴露服务。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc8.png" alt=""></p><p><code>LoadBalancer</code> 只能在 Service 上定义。LoadBalancer 是一些特定公有云提供的负载均衡器，需要特定的云服务商支持。比如：AWS、Azure、OpenStack 和 GCE (Google Container Engine) 。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kind: Service</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: my-nginx</span><br><span class="line">spec:</span><br><span class="line">  type: LoadBalancer</span><br><span class="line">  ports:</span><br><span class="line">    - port: 80</span><br><span class="line">  selector:</span><br><span class="line">    name: my-nginx</span><br></pre></td></tr></table></figure><p>查看服务：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get svc my-nginx</span><br><span class="line">NAME       CLUSTER-IP     EXTERNAL-IP     PORT(S)          AGE</span><br><span class="line">my-nginx   10.97.121.42   10.13.242.236   8086:30051/TCP   39s</span><br></pre></td></tr></table></figure><p>集群内部可以使用 ClusterIP 加端口来访问服务，如：10.97.121.42:8086。</p><p>外部可以用以下两种方式访问该服务：</p><ul><li>使用任一节点的 IP 加 30051 端口访问该服务。</li><li>使用 <code>EXTERNAL-IP</code> 来访问，这是一个 VIP，是云供应商提供的负载均衡器 IP，如：10.13.242.236:8086。</li></ul><p>LoadBalancer 这种方式最大的不足就是每个暴露的服务需要使用一个公有云提供的负载均衡器 IP，这可能会付出比较大的成本代价。</p><p>从上面几种 Service 的类型的结论来看，目前 Service 提供的负载均衡功能在使用上有以下限制：</p><ul><li>只提供 4 层负载均衡，不支持 7 层负载均衡功能，比如：不能按需要的匹配规则自定义转发请求。</li><li>使用 NodePort 类型的 Service，需要在集群外部部署一个外部的负载均衡器。</li><li>使用 LoadBalancer 类型的 Service，Kubernetes 必须运行在特定的云服务上。</li></ul><h3 id="通过-ingress-暴露">通过 Ingress 暴露</h3><p>与 Service 不同，Ingress 实际上不是一种服务。相反，它位于多个服务之前，充当集群中的智能路由器或入口点。</p><p><code>Ingress</code> 是自 Kubernetes 1.1 版本后引入的资源类型。Ingress 支持将 Service 暴露到 Kubernetes 集群外，同时可以自定义 Service 的访问策略。Ingress 能够把 Service 配置成外网能够访问的 URL，也支持提供按域名访问的虚拟主机功能。例如，通过负载均衡器实现不同的二级域名到不同 Service 的访问。</p><p><img src="https://www.hi-linux.com/img/linux/k8s-svc9.png" alt=""></p><p>实际上 <code>Ingress</code> 只是一个统称，其由 <code>Ingress</code> 和 <code>Ingress Controller</code> 两部分组成。<code>Ingress</code> 用作将原来需要手动配置的规则抽象成一个 Ingress 对象，使用 YAML 格式的文件来创建和管理。<code>Ingress Controller</code> 用作通过与 Kubernetes API 交互，动态的去感知集群中 Ingress 规则变化。</p><p>使用 <code>Ingress</code> 前必须要先部署 <code>Ingress Controller</code>，<code>Ingress Controller</code> 是以一种插件的形式提供。<code>Ingress Controller</code> 通常是部署在 Kubernetes 之上的 Docker 容器，<code>Ingress Controller</code> 的 Docker 镜像里包含一个像 Nginx 或 HAProxy 的负载均衡器和一个 <code>Ingress Controller</code>。<code>Ingress Controller</code> 会从 Kubernetes 接收所需的 <code>Ingress</code> 配置，然后动态生成一个 Nginx 或 HAProxy 配置文件，并重新启动负载均衡器进程以使更改生效。换句话说，<code>Ingress Controller</code> 是由 Kubernetes 管理的负载均衡器。</p><blockquote><p>注：无论使用何种负载均衡软件（ 比如：Nginx、HAProxy、Traefik等）来实现 Ingress Controller，官方都将其统称为 Ingress Controller。</p></blockquote><p>Kubernetes Ingress 提供了负载均衡器的典型特性：HTTP 路由、粘性会话、SSL 终止、SSL直通、TCP 和 UDP 负载平衡等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ingress</span><br><span class="line">spec:</span><br><span class="line">  backend:</span><br><span class="line">    serviceName: my-nginx-other</span><br><span class="line">    servicePort: 8080</span><br><span class="line">  rules:</span><br><span class="line">  - host: foo.mydomain.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - backend:</span><br><span class="line">          serviceName: my-nginx-foo</span><br><span class="line">          servicePort: 8080</span><br><span class="line">  - host: mydomain.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /bar/*</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: my-nginx-bar</span><br><span class="line">          servicePort: 8080</span><br></pre></td></tr></table></figure><p>外部可通过 <code>foo.yourdomain.com</code> 或者 <code>mydomain.com/bar/</code> 两个不同 URL 来访问对应的后端服务，然后 <code>Ingress Controller</code> 直接将流量转发给后端 Pod，不需再经过 Kube-Proxy 的转发，这种方式比 LoadBalancer 更高效。</p><p>总的来说 Ingress 是一个非常灵活和越来越得到厂商支持的服务暴露方式，包括：Nginx、HAProxy、Traefik、还有各种 Service Mesh，而其它服务暴露方式更适用于服务调试、特殊应用的部署。</p><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RdskHHt" target="_blank" rel="noopener">http://t.cn/RdskHHt</a><br><a href="http://t.cn/RdskYv5" target="_blank" rel="noopener">http://t.cn/RdskYv5</a><br><a href="http://t.cn/Rg5BG4h" target="_blank" rel="noopener">http://t.cn/Rg5BG4h</a><br><a href="http://t.cn/R6CD4ak" target="_blank" rel="noopener">http://t.cn/R6CD4ak</a><br><a href="http://t.cn/Rgcurto" target="_blank" rel="noopener">http://t.cn/Rgcurto</a><br><a href="http://t.cn/RgcdjDw" target="_blank" rel="noopener">http://t.cn/RgcdjDw</a><br><a href="http://t.cn/RgMWBpo" target="_blank" rel="noopener">http://t.cn/RgMWBpo</a><br><a href="http://t.cn/RgMWFM1" target="_blank" rel="noopener">http://t.cn/RgMWFM1</a><br><a href="http://t.cn/RgC3uk8" target="_blank" rel="noopener">http://t.cn/RgC3uk8</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一般情况下，Kubernetes 的 Cluster Network 是属于私有网络，只能在 Cluster Network 内部才能访问部署的应用。那么如何才能将 Kubernetes 集群中的应用暴露到外部网络，为外部用户提供服务呢？本文就来讲一讲从外部网络访问 Kubernetes Cluster 中 Pod 和 Serivce 的几种常用的实现方式。&lt;/p&gt;
&lt;h3 id=&quot;pod-和-service-的关系&quot;&gt;Pod 和 Service 的关系&lt;/h3&gt;
&lt;p&gt;我们首先来了解一下 Kubernetes 中的 Pod 和 Service 的概念以及两者间的关系。&lt;/p&gt;
&lt;p&gt;Pod (容器组)，英文中 Pod 是豆荚的意思。从名字的含义可以看出，Pod 是一组有依赖关系的容器。Pod 是 Kubernetes 集群中最基本的资源对象，每个 Pod 由一个或多个业务容器和一个根容器 (Pause 容器) 组成。&lt;/p&gt;
&lt;p&gt;Kubernetes 为每个 Pod 分配了唯一的 IP（即：Pod IP），Pod 里的多个容器共享这个 IP。Pod 内的容器除了 IP，还共享相同的网络命名空间、端口、存储卷等，也就是说这些容器之间能通过 Localhost 来通信。Pod 包含的容器都会运行在同一个节点上，也可以同时启动多个相同的 Pod 用于 Failover 或者 Load balance。&lt;/p&gt;
&lt;p&gt;Pod 的生命周期是短暂的，Kubernetes 会根据应用的配置对 Pod 进行创建、销毁并根据监控指标进行伸缩扩容。Kubernetes 在创建 Pod 时可以选择集群中的任何一台空闲的节点上进行，因此其网络地址是不固定的。由于 Pod 的这一特点，一般不建议直接通过 Pod 的地址去访问应用。&lt;/p&gt;
&lt;p&gt;为了解决访问 Pod 不方便直接访问的问题，Kubernetes 采用了 Service 对 Pod 进行封装。Service 是对后端提供服务的一组 Pod 的抽象，Service 会绑定到一个固定的虚拟 IP上。该虚拟 IP 只在 Kubernetes Cluster 中可见，但其实该虚拟 IP 并不对应一个虚拟或者物理设备，而只是 IPtables 中的规则，然后再通过 IPtables 将服务请求路由到后端的 Pod 中。通过这种方式，可以确保服务消费者可以稳定地访问 Pod 提供的服务，而不用关心 Pod 的创建、删除、迁移等变化以及如何用一组 Pod 来进行负载均衡。&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>图解 Kubernetes 架构</title>
    <link href="https://www.hi-linux.com/posts/48037.html"/>
    <id>https://www.hi-linux.com/posts/48037.html</id>
    <published>2018-07-10T01:00:00.000Z</published>
    <updated>2018-08-24T06:04:36.227Z</updated>
    
    <content type="html"><![CDATA[<h3 id="kubernetes-整体架构图">Kubernetes 整体架构图</h3><p><img src="https://www.hi-linux.com/img/linux/k8s-arch1.png" alt=""></p><a id="more"></a><h3 id="kubernetes-各组件介绍">Kubernetes 各组件介绍</h3><h4 id="kube-master控制节点">Kube-Master「控制节点」</h4><ul><li>Kube-Master 的工作流程图</li></ul><p><img src="https://www.hi-linux.com/img/linux/k8s-arch2.png" alt=""></p><ol><li>Kubecfg 将特定的请求发送给 Kubernetes Client（比如：创建 Pod 的请求）。</li><li>Kubernetes Client 将请求发送给 API Server。</li><li>API Server 会根据请求的类型选择用何种 REST API 对请求作出处理（比如：创建 Pod 时 Storage 类型是 Pods 时，其对应的就是 REST Storage API）。</li><li>REST Storage API 会对请求作相应的处理并将处理的结果存入高可用键值存储系统 Etcd 中。</li><li>在 API Server 响应 Kubecfg 的请求后，Scheduler 会根据 Kubernetes Client 获取的集群中运行 Pod 及 Minion / Node 信息将未分发的 Pod 分发到可用的 Minion / Node 节点上。</li></ol><h5 id="api-server-资源操作入口">API Server 「资源操作入口」</h5><ul><li><p>API Server 提供了资源对象的唯一操作入口，其它所有组件都必须通过它提供的 API 来操作资源数据。只有 API Server 会与存储通信，其它模块都必须通过 API Server 访问集群状态。</p></li><li><p>API Server 作为 Kubernetes 系统的入口，封装了核心对象的增删改查操作。API Server 以 RESTFul 接口方式提供给外部客户和内部组件调用，API Server 再对相关的资源数据（全量查询 + 变化监听）进行操作，以达到实时完成相关的业务功能。</p></li><li><p>以 API Server 为 Kubernetes 入口的设计主要有以下好处：1. 保证了集群状态访问的安全。2. API Server 隔离了集群状态访问和后端存储实现，这样 API Server 状态访问的方式不会因为后端存储技术 Etcd 的改变而改变，让后端存储方式选择更加灵活，方便了整个架构的扩展。</p></li></ul><h5 id="controller-manager-内部管理控制中心">Controller Manager 「内部管理控制中心」</h5><p>Controller Manager 用于实现 Kubernetes 集群故障检测和恢复的自动化工作。Controller Manager 主要负责执行以下各种控制器：</p><p><img src="https://www.hi-linux.com/img/linux/k8s-arch3.png" alt=""></p><ul><li>Replication Controller</li></ul><p>Replication Controller 的作用主要是定期关联 Replication Controller (RC) 和 Pod，以保证集群中一个 RC (一种资源对象) 所关联的 Pod 副本数始终保持为与预设值一致。</p><ul><li>Node Controller</li></ul><p>Kubelet 在启动时会通过 API Server 注册自身的节点信息，并定时向 API Server 汇报状态信息。API Server 在接收到信息后将信息更新到 Etcd 中。</p><p>Node Controller 通过 API Server 实时获取 Node 的相关信息，实现管理和监控集群中的各个 Node 节点的相关控制功能。</p><ul><li>ResourceQuota Controller</li></ul><p>资源配额管理控制器用于确保指定的资源对象在任何时候都不会超量占用系统上物理资源。</p><ul><li>Namespace Controller</li></ul><p>用户通过 API Server 可以创建新的 Namespace 并保存在 Etcd 中，Namespace Controller 定时通过 API Server 读取这些 Namespace 信息来操作 Namespace。</p><p>比如：Namespace 被 API 标记为优雅删除，则将该 Namespace 状态设置为 Terminating 并保存到 Etcd 中。同时 Namespace Controller 删除该 Namespace 下的 ServiceAccount、RC、Pod 等资源对象。</p><ul><li>Service Account Controller</li></ul><p>Service Account Controller (服务账号控制器)，主要在命名空间内管理 ServiceAccount，以保证名为 default 的 ServiceAccount 在每个命名空间中存在。</p><ul><li>Token Controller</li></ul><p>Token Controller（令牌控制器）作为 Controller Manager 的一部分，主要用作：监听 serviceAccount 的创建和删除动作以及监听 secret 的添加、删除动作。</p><ul><li>Service Controller</li></ul><p>Service Controller 是属于 Kubernetes 集群与外部平台之间的一个接口控制器，Service Controller 主要用作监听 Service 的变化。</p><p>比如：创建的是一个 LoadBalancer 类型的 Service，Service Controller 则要确保外部的云平台上对该 Service 对应的 LoadBalancer 实例被创建、删除以及相应的路由转发表被更新。</p><ul><li>Endpoint Controller</li></ul><p>Endpoints 表示了一个 Service 对应的所有 Pod 副本的访问地址，而 Endpoints Controller 是负责生成和维护所有 Endpoints 对象的控制器。</p><p>Endpoint Controller 负责监听 Service 和对应的 Pod 副本的变化。定期关联 Service 和 Pod (关联信息由 Endpoint 对象维护)，以保证 Service 到 Pod 的映射总是最新的。</p><h5 id="scheduler集群分发调度器">Scheduler「集群分发调度器」</h5><ul><li>Scheduler 主要用于收集和分析当前 Kubernetes 集群中所有 Minion / Node 节点的资源 (包括内存、CPU 等) 负载情况，然后依据资源占用情况分发新建的 Pod 到 Kubernetes 集群中可用的节点。</li><li>Scheduler 会实时监测 Kubernetes 集群中未分发和已分发的所有运行的 Pod。</li><li>Scheduler 会实时监测 Minion / Node 节点信息，由于会频繁查找 Minion/Node 节点，Scheduler 同时会缓存一份最新的信息在本地。</li><li>Scheduler 在分发 Pod 到指定的 Minion / Node 节点后，会把 Pod 相关的信息 Binding 写回 API Server，以方便其它组件使用。</li></ul><h4 id="kube-node服务节点">Kube-Node「服务节点」</h4><ul><li>Kubelet 结构图</li></ul><p><img src="https://www.hi-linux.com/img/linux/k8s-arch4.png" alt=""></p><h5 id="kubelet-节点上的-pod-管家">Kubelet 「节点上的 Pod 管家」</h5><ul><li>负责 Node 节点上 Pod 的创建、修改、监控、删除等全生命周期的管理。</li><li>定时上报本地 Node 的状态信息给 API Server。</li><li>Kubelet 是 Master API Server 和 Minion / Node 之间的桥梁，接收 Master API Server 分配给它的 Commands 和 Work。</li><li>Kubelet 通过 Kube ApiServer 间接与 Etcd 集群交互来读取集群配置信息。</li><li>Kubelet 在 Node 上做的主要工作具体如下：1. 设置容器的环境变量、给容器绑定 Volume、给容器绑定 Port、根据指定的 Pod 运行一个单一容器、给指定的 Pod 创建 Network 容器。2. 同步 Pod 的状态，从 cAdvisor 获取 Container Info、 Pod Info、 Root Info、 Machine info。3. 在容器中运行命令、杀死容器、删除 Pod 的所有容器。</li></ul><h5 id="proxy负载均衡-路由转发">Proxy「负载均衡、路由转发」</h5><ul><li><p>Proxy 是为了解决外部网络能够访问集群中容器提供的应用服务而设计的，Proxy 运行在每个 Minion / Node 上。</p></li><li><p>Proxy 提供 TCP / UDP 两种 Sockets 连接方式 。每创建一个 Service，Proxy 就会从 Etcd 获取 Services 和 Endpoints 的配置信息（也可以从 File 获取），然后根据其配置信息在 Minion / Node 上启动一个 Proxy 的进程并监听相应的服务端口。当外部请求发生时，Proxy 会根据 Load Balancer 将请求分发到后端正确的容器处理。</p></li><li><p>Proxy 不但解决了同一宿主机相同服务端口冲突的问题，还提供了 Service 转发服务端口对外提供服务的能力。Proxy 后端使用随机、轮循等负载均衡算法进行调度。</p></li></ul><h5 id="kubectl-集群管理命令行工具集">Kubectl 「集群管理命令行工具集」</h5><ul><li>Kubectl 是 Kubernetes 的 客户端的工具。 通过 Kubectl 命令对 API Server 进行操作，API Server 响应并返回对应的命令结果，从而达到对 Kubernetes 集群的管理。</li></ul><blockquote><p>本文在 「<a href="http://www.huweihuang.com/article/kubernetes/kubernetes-architecture/" target="_blank" rel="noopener">Kubernetes 总架构图</a>」的基础上整理和修改。</p></blockquote><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RdHiHXX" target="_blank" rel="noopener">http://t.cn/RdHiHXX</a><br><a href="http://t.cn/RdHimG5" target="_blank" rel="noopener">http://t.cn/RdHimG5</a><br><a href="http://t.cn/RdHRDq8" target="_blank" rel="noopener">http://t.cn/RdHRDq8</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;kubernetes-整体架构图&quot;&gt;Kubernetes 整体架构图&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/k8s-arch1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/categories/kubernetes/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Kubernetes" scheme="https://www.hi-linux.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>浏览器数据库 IndexedDB 入门教程</title>
    <link href="https://www.hi-linux.com/posts/32833.html"/>
    <id>https://www.hi-linux.com/posts/32833.html</id>
    <published>2018-07-08T01:00:00.000Z</published>
    <updated>2018-08-24T06:09:49.986Z</updated>
    
    <content type="html"><![CDATA[<h3 id="概述">概述</h3><p>随着浏览器的功能不断增强，越来越多的网站开始考虑，将大量数据储存在客户端，这样可以减少从服务器获取数据，直接从本地获取数据。</p><p>现有的浏览器数据储存方案，都不适合储存大量数据：Cookie 的大小不超过4KB，且每次请求都会发送回服务器；LocalStorage 在 2.5MB 到 10MB 之间（各家浏览器不同），而且不提供搜索功能，不能建立自定义的索引。所以，需要一种新的解决方案，这就是 IndexedDB 诞生的背景。</p><p>通俗地说，IndexedDB 就是浏览器提供的本地数据库，它可以被网页脚本创建和操作。IndexedDB 允许储存大量数据，提供查找接口，还能建立索引。这些都是 LocalStorage 所不具备的。就数据库类型而言，IndexedDB 不属于关系型数据库（不支持 SQL 查询语句），更接近 NoSQL 数据库。</p><a id="more"></a><p>IndexedDB 具有以下特点。</p><p><strong>（1）键值对储存。</strong> IndexedDB 内部采用对象仓库（object store）存放数据。所有类型的数据都可以直接存入，包括 JavaScript 对象。对象仓库中，数据以“键值对”的形式保存，每一个数据记录都有对应的主键，主键是独一无二的，不能有重复，否则会抛出一个错误。</p><p><strong>（2）异步。</strong>  IndexedDB 操作时不会锁死浏览器，用户依然可以进行其他操作，这与 LocalStorage 形成对比，后者的操作是同步的。异步设计是为了防止大量数据的读写，拖慢网页的表现。</p><p><strong>（3）支持事务。</strong> IndexedDB 支持事务（transaction），这意味着一系列操作步骤之中，只要有一步失败，整个事务就都取消，数据库回滚到事务发生之前的状态，不存在只改写一部分数据的情况。</p><p><strong>（4）同源限制</strong> IndexedDB 受到同源限制，每一个数据库对应创建它的域名。网页只能访问自身域名下的数据库，而不能访问跨域的数据库。</p><p><strong>（5）储存空间大</strong> IndexedDB 的储存空间比 LocalStorage 大得多，一般来说不少于 250MB，甚至没有上限。</p><p><strong>（6）支持二进制储存。</strong> IndexedDB 不仅可以储存字符串，还可以储存二进制数据（ArrayBuffer 对象和 Blob 对象）。</p><h3 id="基本概念">基本概念</h3><p>IndexedDB 是一个比较复杂的 API，涉及不少概念。它把不同的实体，抽象成一个个对象接口。学习这个 API，就是学习它的各种对象接口。</p><ul><li>数据库：IDBDatabase 对象</li><li>对象仓库：IDBObjectStore 对象</li><li>索引： IDBIndex 对象</li><li>事务： IDBTransaction 对象</li></ul><p>下面是一些主要的概念。</p><p><strong>（1）数据库</strong></p><p>数据库是一系列相关数据的容器。每个域名（严格的说，是协议 + 域名 + 端口）都可以新建任意多个数据库。</p><p>IndexedDB 数据库有版本的概念。同一个时刻，只能有一个版本的数据库存在。如果要修改数据库结构（新增或删除表、索引或者主键），只能通过升级数据库版本完成。</p><p><strong>（2）对象仓库</strong></p><p>每个数据库包含若干个对象仓库（object store）。它类似于关系型数据库的表格。</p><p><strong>（3）数据记录</strong></p><p>对象仓库保存的是数据记录。每条记录类似于关系型数据库的行，但是只有主键和数据体两部分。主键用来建立默认的索引，必须是不同的，否则会报错。主键可以是数据记录里面的一个属性，也可以指定为一个递增的整数编号。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123; <span class="attr">id</span>: <span class="number">1</span>, <span class="attr">text</span>: <span class="string">'foo'</span> &#125;</span><br></pre></td></tr></table></figure><p>上面的对象中，<code>id</code>属性可以当作主键。</p><p>数据体可以是任意数据类型，不限于对象。</p><p><strong>（4）索引</strong></p><p>为了加速数据的检索，可以在对象仓库里面，为不同的属性建立索引。</p><p><strong>（5）事务</strong></p><p>数据记录的读写和删改，都要通过事务完成。事务对象提供<code>error</code>、<code>abort</code>和<code>complete</code>三个事件，用来监听操作结果。</p><h3 id="操作流程">操作流程</h3><p>IndexedDB 数据库的各种操作，一般是按照下面的流程进行的。</p><h4 id="打开数据库">打开数据库</h4><p>使用 IndexedDB 的第一步是打开数据库，使用<code>indexedDB.open()</code>方法。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> request = <span class="built_in">window</span>.indexedDB.open(databaseName, version);</span><br></pre></td></tr></table></figure><p>这个方法接受两个参数，第一个参数是字符串，表示数据库的名字。如果指定的数据库不存在，就会新建数据库。第二个参数是整数，表示数据库的版本。如果省略，打开已有数据库时，默认为当前版本；新建数据库时，默认为<code>1</code>。</p><p><code>indexedDB.open()</code>方法返回一个 IDBRequest 对象。这个对象通过三种事件<code>error</code>、<code>success</code>、<code>upgradeneeded</code>，处理打开数据库的操作结果。</p><p><strong>（1）error 事件</strong></p><p><code>error</code>事件表示打开数据库失败。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">request.onerror = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">'数据库打开报错'</span>);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>（2）<code>success</code> 事件</strong></p><p><code>success</code>事件表示成功打开数据库。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> db;</span><br><span class="line"></span><br><span class="line">request.onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">  db = request.result;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">'数据库打开成功'</span>);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>这时，通过<code>request</code>对象的<code>result</code>属性拿到数据库对象。</p><p><strong>（3）upgradeneeded 事件</strong></p><p>如果指定的版本号，大于数据库的实际版本号，就会发生数据库升级事件<code>upgradeneeded</code>。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> db;</span><br><span class="line"></span><br><span class="line">request.onupgradeneeded = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">  db = event.target.result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这时通过事件对象的<code>target.result</code>属性，拿到数据库实例。</p><h4 id="新建数据库">新建数据库</h4><p>新建数据库与打开数据库是同一个操作。如果指定的数据库不存在，就会新建。不同之处在于，后续的操作主要在<code>upgradeneeded</code>事件的监听函数里面完成，因为这时版本从无到有，所以会触发这个事件。</p><p>通常，新建数据库以后，第一件事是新建对象仓库（即新建表）。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">request.onupgradeneeded = <span class="function"><span class="keyword">function</span>(<span class="params">event</span>) </span>&#123;</span><br><span class="line">  db = event.target.result;</span><br><span class="line">  <span class="keyword">var</span> objectStore = db.createObjectStore(<span class="string">'person'</span>, &#123; <span class="attr">keyPath</span>: <span class="string">'id'</span> &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码中，数据库新建成功以后，新增一张叫做<code>person</code>的表格，主键是<code>id</code>。</p><p>更好的写法是先判断一下，这张表格是否存在，如果不存在再新建。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">request.onupgradeneeded = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">  db = event.target.result;</span><br><span class="line">  <span class="keyword">var</span> objectStore;</span><br><span class="line">  <span class="keyword">if</span> (!db.objectStoreNames.contains(<span class="string">'person'</span>)) &#123;</span><br><span class="line">    objectStore = db.createObjectStore(<span class="string">'person'</span>, &#123; <span class="attr">keyPath</span>: <span class="string">'id'</span> &#125;);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>主键（key）是默认建立索引的属性。比如，数据记录是<code>{ id: 1, name: '张三' }</code>，那么<code>id</code>属性可以作为主键。主键也可以指定为下一层对象的属性，比如<code>{ foo: { bar: 'baz' } }</code>的<code>foo.bar</code>也可以指定为主键。</p><p>如果数据记录里面没有合适作为主键的属性，那么可以让 IndexedDB 自动生成主键。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> objectStore = db.createObjectStore(</span><br><span class="line">  <span class="string">'person'</span>,</span><br><span class="line">  &#123; <span class="attr">autoIncrement</span>: <span class="literal">true</span> &#125;</span><br><span class="line">);</span><br></pre></td></tr></table></figure><p>上面代码中，指定主键为一个递增的整数。</p><p>新建对象仓库以后，下一步可以新建索引。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">request.onupgradeneeded = <span class="function"><span class="keyword">function</span>(<span class="params">event</span>) </span>&#123;</span><br><span class="line">  db = event.target.result;</span><br><span class="line">  <span class="keyword">var</span> objectStore = db.createObjectStore(<span class="string">'person'</span>, &#123; <span class="attr">keyPath</span>: <span class="string">'id'</span> &#125;);</span><br><span class="line">  objectStore.createIndex(<span class="string">'name'</span>, <span class="string">'name'</span>, &#123; <span class="attr">unique</span>: <span class="literal">false</span> &#125;);</span><br><span class="line">  objectStore.createIndex(<span class="string">'email'</span>, <span class="string">'email'</span>, &#123; <span class="attr">unique</span>: <span class="literal">true</span> &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码中，<code>IDBObject.createIndex()</code>的三个参数分别为索引名称、索引所在的属性、配置对象（说明该属性是否包含重复的值）。</p><h4 id="新增数据">新增数据</h4><p>新增数据指的是向对象仓库写入数据记录。这需要通过事务完成。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">add</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> request = db.transaction([<span class="string">'person'</span>], <span class="string">'readwrite'</span>)</span><br><span class="line">    .objectStore(<span class="string">'person'</span>)</span><br><span class="line">    .add(&#123; <span class="attr">id</span>: <span class="number">1</span>, <span class="attr">name</span>: <span class="string">'张三'</span>, <span class="attr">age</span>: <span class="number">24</span>, <span class="attr">email</span>: <span class="string">'zhangsan@example.com'</span> &#125;);</span><br><span class="line"></span><br><span class="line">  request.onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'数据写入成功'</span>);</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  request.onerror = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'数据写入失败'</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">add();</span><br></pre></td></tr></table></figure><p>上面代码中，写入数据需要新建一个事务。新建时必须指定表格名称和操作模式（“只读”或“读写”）。新建事务以后，通过<code>IDBTransaction.objectStore(name)</code>方法，拿到 IDBObjectStore 对象，再通过表格对象的<code>add()</code>方法，向表格写入一条记录。</p><p>写入操作是一个异步操作，通过监听连接对象的<code>success</code>事件和<code>error</code>事件，了解是否写入成功。</p><h4 id="读取数据">读取数据</h4><p>读取数据也是通过事务完成。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">read</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">   <span class="keyword">var</span> transaction = db.transaction([<span class="string">'person'</span>]);</span><br><span class="line">   <span class="keyword">var</span> objectStore = transaction.objectStore(<span class="string">'person'</span>);</span><br><span class="line">   <span class="keyword">var</span> request = objectStore.get(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">   request.onerror = <span class="function"><span class="keyword">function</span>(<span class="params">event</span>) </span>&#123;</span><br><span class="line">     <span class="built_in">console</span>.log(<span class="string">'事务失败'</span>);</span><br><span class="line">   &#125;;</span><br><span class="line"></span><br><span class="line">   request.onsuccess = <span class="function"><span class="keyword">function</span>(<span class="params"> event</span>) </span>&#123;</span><br><span class="line">      <span class="keyword">if</span> (request.result) &#123;</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="string">'Name: '</span> + request.result.name);</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="string">'Age: '</span> + request.result.age);</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="string">'Email: '</span> + request.result.email);</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="built_in">console</span>.log(<span class="string">'未获得数据记录'</span>);</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">read();</span><br></pre></td></tr></table></figure><p>上面代码中，<code>objectStore.get()</code>方法用于读取数据，参数是主键的值。</p><h4 id="遍历数据">遍历数据</h4><p>遍历数据表格的所有记录，要使用指针对象 IDBCursor。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">readAll</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> objectStore = db.transaction(<span class="string">'person'</span>).objectStore(<span class="string">'person'</span>);</span><br><span class="line"></span><br><span class="line">   objectStore.openCursor().onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">     <span class="keyword">var</span> cursor = event.target.result;</span><br><span class="line"></span><br><span class="line">     <span class="keyword">if</span> (cursor) &#123;</span><br><span class="line">       <span class="built_in">console</span>.log(<span class="string">'Id: '</span> + cursor.key);</span><br><span class="line">       <span class="built_in">console</span>.log(<span class="string">'Name: '</span> + cursor.value.name);</span><br><span class="line">       <span class="built_in">console</span>.log(<span class="string">'Age: '</span> + cursor.value.age);</span><br><span class="line">       <span class="built_in">console</span>.log(<span class="string">'Email: '</span> + cursor.value.email);</span><br><span class="line">       cursor.continue();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="built_in">console</span>.log(<span class="string">'没有更多数据了！'</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">readAll();</span><br></pre></td></tr></table></figure><p>上面代码中，新建指针对象的<code>openCursor()</code>方法是一个异步操作，所以要监听<code>success</code>事件。</p><h4 id="更新数据">更新数据</h4><p>更新数据要使用<code>IDBObject.put()</code>方法。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">update</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> request = db.transaction([<span class="string">'person'</span>], <span class="string">'readwrite'</span>)</span><br><span class="line">    .objectStore(<span class="string">'person'</span>)</span><br><span class="line">    .put(&#123; <span class="attr">id</span>: <span class="number">1</span>, <span class="attr">name</span>: <span class="string">'李四'</span>, <span class="attr">age</span>: <span class="number">35</span>, <span class="attr">email</span>: <span class="string">'lisi@example.com'</span> &#125;);</span><br><span class="line"></span><br><span class="line">  request.onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'数据更新成功'</span>);</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  request.onerror = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'数据更新失败'</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">update();</span><br></pre></td></tr></table></figure><p>上面代码中，<code>put()</code>方法自动更新了主键为<code>1</code>的记录。</p><h4 id="删除数据">删除数据</h4><p><code>IDBObjectStore.delete()</code>方法用于删除记录。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">remove</span>(<span class="params"></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> request = db.transaction([<span class="string">'person'</span>], <span class="string">'readwrite'</span>)</span><br><span class="line">    .objectStore(<span class="string">'person'</span>)</span><br><span class="line">    .delete(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  request.onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'数据删除成功'</span>);</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">remove();</span><br></pre></td></tr></table></figure><h4 id="使用索引">使用索引</h4><p>索引的意义在于，可以让你搜索任意字段，也就是说从任意字段拿到数据记录。如果不建立索引，默认只能搜索主键（即从主键取值）。</p><p>假定新建表格的时候，对<code>name</code>字段建立了索引。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objectStore.createIndex(<span class="string">'name'</span>, <span class="string">'name'</span>, &#123; <span class="attr">unique</span>: <span class="literal">false</span> &#125;);</span><br></pre></td></tr></table></figure><p>现在，就可以从<code>name</code>找到对应的数据记录了。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> transaction = db.transaction([<span class="string">'person'</span>], <span class="string">'readonly'</span>);</span><br><span class="line"><span class="keyword">var</span> store = transaction.objectStore(<span class="string">'person'</span>);</span><br><span class="line"><span class="keyword">var</span> index = store.index(<span class="string">'name'</span>);</span><br><span class="line"><span class="keyword">var</span> request = index.get(<span class="string">'李四'</span>);</span><br><span class="line"></span><br><span class="line">request.onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">e</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> result = e.target.result;</span><br><span class="line">  <span class="keyword">if</span> (result) &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="indexeddb-对象">indexedDB 对象</h3><p>浏览器原生提供<code>indexedDB</code>对象，作为开发者的操作接口。</p><h4 id="indexeddbopen">indexedDB.open()</h4><p><code>indexedDB.open()</code>方法用于打开数据库。这是一个异步操作，但是会立刻返回一个 IDBOpenDBRequest 对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> openRequest = <span class="built_in">window</span>.indexedDB.open(<span class="string">'test'</span>, <span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>上面代码表示，打开一个名为<code>test</code>、版本为<code>1</code>的数据库。如果该数据库不存在，则会新建该数据库。</p><p><code>open()</code>方法的第一个参数是数据库名称，格式为字符串，不可省略；第二个参数是数据库版本，是一个大于<code>0</code>的正整数（<code>0</code>将报错），如果该参数大于当前版本，会触发数据库升级。第二个参数可省略，如果数据库已存在，将打开当前版本的数据库；如果数据库不存在，将创建该版本的数据库，默认版本为<code>1</code>。</p><p>打开数据库是异步操作，通过各种事件通知客户端。下面是有可能触发的4种事件。</p><ul><li><strong>success</strong>：打开成功。</li><li><strong>error</strong>：打开失败。</li><li><strong>upgradeneeded</strong>：第一次打开该数据库，或者数据库版本发生变化。</li><li><strong>blocked</strong>：上一次的数据库连接还未关闭。</li></ul><p>第一次打开数据库时，会先触发<code>upgradeneeded</code>事件，然后触发<code>success</code>事件。</p><p>根据不同的需要，对上面4种事件监听函数。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> openRequest = indexedDB.open(<span class="string">'test'</span>, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">var</span> db;</span><br><span class="line"></span><br><span class="line">openRequest.onupgradeneeded = <span class="function"><span class="keyword">function</span> (<span class="params">e</span>) </span>&#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">'Upgrading...'</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">openRequest.onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">e</span>) </span>&#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">'Success!'</span>);</span><br><span class="line">  db = openRequest.result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">openRequest.onerror = <span class="function"><span class="keyword">function</span> (<span class="params">e</span>) </span>&#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">'Error'</span>);</span><br><span class="line">  <span class="built_in">console</span>.log(e);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码有两个地方需要注意。首先，<code>open()</code>方法返回的是一个对象（IDBOpenDBRequest），监听函数就定义在这个对象上面。其次，<code>success</code>事件发生后，从<code>openRequest.result</code>属性可以拿到已经打开的<code>IndexedDB</code>数据库对象。</p><h4 id="indexeddbdeletedatabase">indexedDB.deleteDatabase()</h4><p><code>indexedDB.deleteDatabase()</code>方法用于删除一个数据库，参数为数据库的名字。它会立刻返回一个<code>IDBOpenDBRequest</code>对象，然后对数据库执行异步删除。删除操作的结果会通过事件通知，<code>IDBOpenDBRequest</code>对象可以监听以下事件。</p><ul><li><code>success</code>：删除成功</li><li><code>error</code>：删除报错</li></ul><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> DBDeleteRequest = <span class="built_in">window</span>.indexedDB.deleteDatabase(<span class="string">'demo'</span>);</span><br><span class="line"></span><br><span class="line">DBDeleteRequest.onerror = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">'Error'</span>);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">DBDeleteRequest.onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">'success'</span>);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>调用<code>deleteDatabase()</code>方法以后，当前数据库的其他已经打开的连接都会接收到<code>versionchange</code>事件。</p><p>注意，删除不存在的数据库并不会报错。</p><h4 id="indexeddbcmp">indexedDB.cmp()</h4><p><code>indexedDB.cmp()</code>方法比较两个值是否为 indexedDB 的相同的主键。它返回一个整数，表示比较的结果：<code>0</code>表示相同，<code>1</code>表示第一个主键大于第二个主键，<code>-1</code>表示第一个主键小于第二个主键。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">window</span>.indexedDB.cmp(<span class="number">1</span>, <span class="number">2</span>) <span class="comment">// -1</span></span><br></pre></td></tr></table></figure><p>注意，这个方法不能用来比较任意的 JavaScript 值。如果参数是布尔值或对象，它会报错。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">window</span>.indexedDB.cmp(<span class="number">1</span>, <span class="literal">true</span>) <span class="comment">// 报错</span></span><br><span class="line"><span class="built_in">window</span>.indexedDB.cmp(&#123;&#125;, &#123;&#125;) <span class="comment">// 报错</span></span><br></pre></td></tr></table></figure><h3 id="idbrequest-对象">IDBRequest 对象</h3><p>IDBRequest 对象表示打开的数据库连接，<code>indexedDB.open()</code>方法和<code>indexedDB.deleteDatabase()</code>方法会返回这个对象。数据库的操作都是通过这个对象完成的。</p><p>这个对象的所有操作都是异步操作，要通过<code>readyState</code>属性判断是否完成，如果为<code>pending</code>就表示操作正在进行，如果为<code>done</code>就表示操作完成，可能成功也可能失败。</p><p>操作完成以后，触发<code>success</code>事件或<code>error</code>事件，这时可以通过<code>result</code>属性和<code>error</code>属性拿到操作结果。如果在<code>pending</code>阶段，就去读取这两个属性，是会报错的。</p><p>IDBRequest 对象有以下属性。</p><ul><li><code>IDBRequest.readyState</code>：等于<code>pending</code>表示操作正在进行，等于<code>done</code>表示操作正在完成。</li><li><code>IDBRequest.result</code>：返回请求的结果。如果请求失败、结果不可用，读取该属性会报错。</li><li><code>IDBRequest.error</code>：请求失败时，返回错误对象。</li><li><code>IDBRequest.source</code>：返回请求的来源（比如索引对象或 ObjectStore）。</li><li><code>IDBRequest.transaction</code>：返回当前请求正在进行的事务，如果不包含事务，返回<code>null</code>。</li><li><code>IDBRequest.onsuccess</code>：指定<code>success</code>事件的监听函数。</li><li><code>IDBRequest.onerror</code>：指定<code>error</code>事件的监听函数。</li></ul><p>IDBOpenDBRequest 对象继承了 IDBRequest 对象，提供了两个额外的事件监听属性。</p><ul><li><code>IDBOpenDBRequest.onblocked</code>：指定<code>blocked</code>事件（<code>upgradeneeded</code>事件触发时，数据库仍然在使用）的监听函数。</li><li><code>IDBOpenDBRequest.onupgradeneeded</code>：<code>upgradeneeded</code>事件的监听函数。</li></ul><h3 id="idbdatabase-对象">IDBDatabase 对象</h3><p>打开数据成功以后，可以从<code>IDBOpenDBRequest</code>对象的<code>result</code>属性上面，拿到一个<code>IDBDatabase</code>对象，它表示连接的数据库。后面对数据库的操作，都通过这个对象完成。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> db;</span><br><span class="line"><span class="keyword">var</span> DBOpenRequest = <span class="built_in">window</span>.indexedDB.open(<span class="string">'demo'</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">DBOpenRequest.onerror = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">'Error'</span>);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">DBOpenRequest.onsuccess = <span class="function"><span class="keyword">function</span>(<span class="params">event</span>) </span>&#123;</span><br><span class="line">  db = DBOpenRequest.result;</span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="属性">属性</h4><p>IDBDatabase 对象有以下属性。</p><ul><li><code>IDBDatabase.name</code>：字符串，数据库名称。</li><li><code>IDBDatabase.version</code>：整数，数据库版本。数据库第一次创建时，该属性为空字符串。</li><li><code>IDBDatabase.objectStoreNames</code>：DOMStringList 对象（字符串的集合），包含当前数据的所有 object store 的名字。</li><li><code>IDBDatabase.onabort</code>：指定 abort 事件（事务中止）的监听函数。</li><li><code>IDBDatabase.onclose</code>：指定 close 事件（数据库意外关闭）的监听函数。</li><li><code>IDBDatabase.onerror</code>：指定 error 事件（访问数据库失败）的监听函数。</li><li><code>IDBDatabase.onversionchange</code>：数据库版本变化时触发（发生<code>upgradeneeded</code>事件，或调用<code>indexedDB.deleteDatabase()</code>）。</li></ul><p>下面是<code>objectStoreNames</code>属性的例子。该属性返回一个DOMStringList 对象，包含了当前数据库所有对象仓库的名称（即表名），可以使用 DOMStringList 对象的<code>contains</code>方法，检查数据库是否包含某个对象仓库。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (!db.objectStoreNames.contains(<span class="string">'firstOS'</span>)) &#123;</span><br><span class="line">  db.createObjectStore(<span class="string">'firstOS'</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面代码先判断某个对象仓库是否存在，如果不存在就创建该对象仓库。</p><h4 id="方法">方法</h4><p>IDBDatabase 对象有以下方法。</p><ul><li><code>IDBDatabase.close()</code>：关闭数据库连接，实际会等所有事务完成后再关闭。</li><li><code>IDBDatabase.createObjectStore()</code>：创建存放数据的对象仓库，类似于传统关系型数据库的表格，返回一个 IDBObjectStore 对象。该方法只能在<code>versionchange</code>事件监听函数中调用。</li><li><code>IDBDatabase.deleteObjectStore()</code>：删除指定的对象仓库。该方法只能在<code>versionchange</code>事件监听函数中调用。</li><li><code>IDBDatabase.transaction()</code>：返回一个 IDBTransaction 事务对象。</li></ul><p>下面是<code>createObjectStore()</code>方法的例子。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> request = <span class="built_in">window</span>.indexedDB.open(<span class="string">'demo'</span>, <span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">request.onupgradeneeded = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> db = event.target.result;</span><br><span class="line"></span><br><span class="line">  db.onerror = <span class="function"><span class="keyword">function</span>(<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'error'</span>);</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> objectStore = db.createObjectStore(<span class="string">'items'</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>上面代码创建了一个名为<code>items</code>的对象仓库，如果该对象仓库已经存在，就会抛出一个错误。为了避免出错，需要用到下文的<code>objectStoreNames</code>属性，检查已有哪些对象仓库。</p><p><code>createObjectStore()</code>方法还可以接受第二个对象参数，用来设置对象仓库的属性。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">db.createObjectStore(<span class="string">'test'</span>, &#123; <span class="attr">keyPath</span>: <span class="string">'email'</span> &#125;);</span><br><span class="line">db.createObjectStore(<span class="string">'test2'</span>, &#123; <span class="attr">autoIncrement</span>: <span class="literal">true</span> &#125;);</span><br></pre></td></tr></table></figure><p>上面代码中，<code>keyPath</code>属性表示主键（由于主键的值不能重复，所以上例存入之前，必须保证数据的<code>email</code>属性值都是不一样的），默认值为<code>null</code>；<code>autoIncrement</code>属性表示，是否使用自动递增的整数作为主键（第一个数据记录为1，第二个数据记录为2，以此类推），默认为<code>false</code>。一般来说，<code>keyPath</code>和<code>autoIncrement</code>属性只要使用一个就够了，如果两个同时使用，表示主键为递增的整数，且对象不得缺少<code>keyPath</code>指定的属性。</p><p>下面是<code>deleteObjectStore()</code>方法的例子。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> dbName = <span class="string">'sampleDB'</span>;</span><br><span class="line"><span class="keyword">var</span> dbVersion = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">var</span> request = indexedDB.open(dbName, dbVersion);</span><br><span class="line"></span><br><span class="line">request.onupgradeneeded = <span class="function"><span class="keyword">function</span>(<span class="params">e</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> db = request.result;</span><br><span class="line">  <span class="keyword">if</span> (e.oldVersion &lt; <span class="number">1</span>) &#123;</span><br><span class="line">    db.createObjectStore(<span class="string">'store1'</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (e.oldVersion &lt; <span class="number">2</span>) &#123;</span><br><span class="line">    db.deleteObjectStore(<span class="string">'store1'</span>);</span><br><span class="line">    db.createObjectStore(<span class="string">'store2'</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>下面是<code>transaction()</code>方法的例子，该方法用于创建一个数据库事务，返回一个 IDBTransaction 对象。向数据库添加数据之前，必须先创建数据库事务。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> t = db.transaction([<span class="string">'items'</span>], <span class="string">'readwrite'</span>);</span><br></pre></td></tr></table></figure><p><code>transaction()</code>方法接受两个参数：第一个参数是一个数组，里面是所涉及的对象仓库，通常是只有一个；第二个参数是一个表示操作类型的字符串。目前，操作类型只有两种：<code>readonly</code>（只读）和<code>readwrite</code>（读写）。添加数据使用<code>readwrite</code>，读取数据使用<code>readonly</code>。第二个参数是可选的，省略时默认为<code>readonly</code>模式。</p><h3 id="idbobjectstore-对象">IDBObjectStore 对象</h3><p>IDBObjectStore 对象对应一个对象仓库（object store）。<code>IDBDatabase.createObjectStore()</code>方法返回的就是一个 IDBObjectStore 对象。</p><p>IDBDatabase 对象的<code>transaction()</code>返回一个事务对象，该对象的<code>objectStore()</code>方法返回 IDBObjectStore 对象，因此可以采用下面的链式写法。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">db.transaction([<span class="string">'test'</span>], <span class="string">'readonly'</span>)</span><br><span class="line">  .objectStore(<span class="string">'test'</span>)</span><br><span class="line">  .get(X)</span><br><span class="line">  .onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">e</span>) </span>&#123;&#125;</span><br></pre></td></tr></table></figure><h4 id="属性">属性</h4><p>IDBObjectStore 对象有以下属性。</p><ul><li><code>IDBObjectStore.indexNames</code>：返回一个类似数组的对象（DOMStringList），包含了当前对象仓库的所有索引。</li><li><code>IDBObjectStore.keyPath</code>：返回当前对象仓库的主键。</li><li><code>IDBObjectStore.name</code>：返回当前对象仓库的名称。</li><li><code>IDBObjectStore.transaction</code>：返回当前对象仓库所属的事务对象。</li><li><code>IDBObjectStore.autoIncrement</code>：布尔值，表示主键是否会自动递增。</li></ul><h4 id="方法">方法</h4><p>IDBObjectStore 对象有以下方法。</p><p><strong>（1）IDBObjectStore.add()</strong></p><p><code>IDBObjectStore.add()</code>用于向对象仓库添加数据，返回一个 IDBRequest 对象。该方法只用于添加数据，如果主键相同会报错，因此更新数据必须使用<code>put()</code>方法。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objectStore.add(value, key)</span><br></pre></td></tr></table></figure><p>该方法接受两个参数，第一个参数是键值，第二个参数是主键，该参数可选，如果省略默认为<code>null</code>。</p><p>创建事务以后，就可以获取对象仓库，然后使用<code>add()</code>方法往里面添加数据了。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> db;</span><br><span class="line"><span class="keyword">var</span> DBOpenRequest = <span class="built_in">window</span>.indexedDB.open(<span class="string">'demo'</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">DBOpenRequest.onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">  db = DBOpenRequest.result;</span><br><span class="line">  <span class="keyword">var</span> transaction = db.transaction([<span class="string">'items'</span>], <span class="string">'readwrite'</span>);</span><br><span class="line"></span><br><span class="line">  transaction.oncomplete = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'transaction success'</span>);</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  transaction.onerror = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'tansaction error: '</span> + transaction.error);</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> objectStore = transaction.objectStore(<span class="string">'items'</span>);</span><br><span class="line">  <span class="keyword">var</span> objectStoreRequest = objectStore.add(&#123; <span class="attr">foo</span>: <span class="number">1</span> &#125;);</span><br><span class="line"></span><br><span class="line">  objectStoreRequest.onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'add data success'</span>);</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p><strong>（2）IDBObjectStore.put()</strong></p><p><code>IDBObjectStore.put()</code>方法用于更新某个主键对应的数据记录，如果对应的键值不存在，则插入一条新的记录。该方法返回一个 IDBRequest 对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objectStore.put(item, key)</span><br></pre></td></tr></table></figure><p>该方法接受两个参数，第一个参数为新数据，第二个参数为主键，该参数可选，且只在自动递增时才有必要提供，因为那时主键不包含在数据值里面。</p><p><strong>（3）IDBObjectStore.clear()</strong></p><p><code>IDBObjectStore.clear()</code>删除当前对象仓库的所有记录。该方法返回一个 IDBRequest 对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objectStore.clear()</span><br></pre></td></tr></table></figure><p>该方法不需要参数。</p><p><strong>（4）IDBObjectStore.delete()</strong></p><p><code>IDBObjectStore.delete()</code>方法用于删除指定主键的记录。该方法返回一个 IDBRequest 对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objectStore.delete(Key)</span><br></pre></td></tr></table></figure><p>该方法的参数为主键的值。</p><p><strong>（5）IDBObjectStore.count()</strong></p><p><code>IDBObjectStore.count()</code>方法用于计算记录的数量。该方法返回一个 IDBRequest 对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IDBObjectStore.count(key)</span><br></pre></td></tr></table></figure><p>不带参数时，该方法返回当前对象仓库的所有记录数量。如果主键或 IDBKeyRange 对象作为参数，则返回对应的记录数量。</p><p><strong>（6）IDBObjectStore.getKey()</strong></p><p><code>IDBObjectStore.getKey()</code>用于获取主键。该方法返回一个 IDBRequest 对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objectStore.getKey(key)</span><br></pre></td></tr></table></figure><p>该方法的参数可以是主键值或 IDBKeyRange 对象。</p><p><strong>（7）IDBObjectStore.get()</strong></p><p><code>IDBObjectStore.get()</code>用于获取主键对应的数据记录。该方法返回一个 IDBRequest 对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objectStore.get(key)</span><br></pre></td></tr></table></figure><p><strong>（8）IDBObjectStore.getAll()</strong></p><p><code>DBObjectStore.getAll()</code>用于获取对象仓库的记录。该方法返回一个 IDBRequest 对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取所有记录</span></span><br><span class="line">objectStore.getAll()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有符合指定主键或 IDBKeyRange 的记录</span></span><br><span class="line">objectStore.getAll(query)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定获取记录的数量</span></span><br><span class="line">objectStore.getAll(query, count)</span><br></pre></td></tr></table></figure><p><strong>（9）IDBObjectStore.getAllKeys()</strong></p><p><code>IDBObjectStore.getAllKeys()</code>用于获取所有符合条件的主键。该方法返回一个 IDBRequest 对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 获取所有记录的主键</span></span><br><span class="line">objectStore.getAllKeys()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 获取所有符合条件的主键</span></span><br><span class="line">objectStore.getAllKeys(query)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定获取主键的数量</span></span><br><span class="line">objectStore.getAllKeys(query, count)</span><br></pre></td></tr></table></figure><p><strong>（10）IDBObjectStore.index()</strong></p><p><code>IDBObjectStore.index()</code>方法返回指定名称的索引对象 IDBIndex。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objectStore.index(name)</span><br></pre></td></tr></table></figure><p>有了索引以后，就可以针对索引所在的属性读取数据。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> t = db.transaction([<span class="string">'people'</span>], <span class="string">'readonly'</span>);</span><br><span class="line"><span class="keyword">var</span> store = t.objectStore(<span class="string">'people'</span>);</span><br><span class="line"><span class="keyword">var</span> index = store.index(<span class="string">'name'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> request = index.get(<span class="string">'foo'</span>);</span><br></pre></td></tr></table></figure><p>上面代码打开对象仓库以后，先用<code>index()</code>方法指定获取<code>name</code>属性的索引，然后用<code>get()</code>方法读取某个<code>name</code>属性(<code>foo</code>)对应的数据。如果<code>name</code>属性不是对应唯一值，这时<code>get()</code>方法有可能取回多个数据对象。另外，<code>get()</code>是异步方法，读取成功以后，只能在<code>success</code>事件的监听函数中处理数据。</p><p><strong>（11）IDBObjectStore.createIndex()</strong></p><p><code>IDBObjectStore.createIndex()</code>方法用于新建当前数据库的一个索引。该方法只能在<code>VersionChange</code>监听函数里面调用。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objectStore.createIndex(indexName, keyPath, objectParameters)</span><br></pre></td></tr></table></figure><p>该方法可以接受三个参数。</p><ul><li>indexName：索引名</li><li>keyPath：主键</li><li>objectParameters：配置对象（可选）</li></ul><p>第三个参数可以配置以下属性。</p><ul><li>unique：如果设为<code>true</code>，将不允许重复的值</li><li>multiEntry：如果设为<code>true</code>，对于有多个值的主键数组，每个值将在索引里面新建一个条目，否则主键数组对应一个条目。</li></ul><p>假定对象仓库中的数据记录都是如下的<code>person</code>类型。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> person = &#123;</span><br><span class="line">  name: name,</span><br><span class="line">  email: email,</span><br><span class="line">  created: <span class="keyword">new</span> <span class="built_in">Date</span>()</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>可以指定这个对象的某个属性来建立索引。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> store = db.createObjectStore(<span class="string">'people'</span>, &#123; <span class="attr">autoIncrement</span>: <span class="literal">true</span> &#125;);</span><br><span class="line"></span><br><span class="line">store.createIndex(<span class="string">'name'</span>, <span class="string">'name'</span>, &#123; <span class="attr">unique</span>: <span class="literal">false</span> &#125;);</span><br><span class="line">store.createIndex(<span class="string">'email'</span>, <span class="string">'email'</span>, &#123; <span class="attr">unique</span>: <span class="literal">true</span> &#125;);</span><br></pre></td></tr></table></figure><p>上面代码告诉索引对象，<code>name</code>属性不是唯一值，<code>email</code>属性是唯一值。</p><p><strong>（12）IDBObjectStore.deleteIndex()</strong></p><p><code>IDBObjectStore.deleteIndex()</code>方法用于删除指定的索引。该方法只能在<code>VersionChange</code>监听函数里面调用。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">objectStore.deleteIndex(indexName)</span><br></pre></td></tr></table></figure><p><strong>（13）IDBObjectStore.openCursor()</strong></p><p><code>IDBObjectStore.openCursor()</code>用于获取一个指针对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IDBObjectStore.openCursor()</span><br></pre></td></tr></table></figure><p>指针对象可以用来遍历数据。该对象也是异步的，有自己的<code>success</code>和<code>error</code>事件，可以对它们指定监听函数。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> t = db.transaction([<span class="string">'test'</span>], <span class="string">'readonly'</span>);</span><br><span class="line"><span class="keyword">var</span> store = t.objectStore(<span class="string">'test'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> cursor = store.openCursor();</span><br><span class="line"></span><br><span class="line">cursor.onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> res = event.target.result;</span><br><span class="line">  <span class="keyword">if</span> (res) &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'Key'</span>, res.key);</span><br><span class="line">    <span class="built_in">console</span>.dir(<span class="string">'Data'</span>, res.value);</span><br><span class="line">    res.continue();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>监听函数接受一个事件对象作为参数，该对象的<code>target.result</code>属性指向当前数据记录。该记录的<code>key</code>和<code>value</code>分别返回主键和键值（即实际存入的数据）。<code>continue()</code>方法将光标移到下一个数据对象，如果当前数据对象已经是最后一个数据了，则光标指向<code>null</code>。</p><p><code>openCursor()</code>方法的第一个参数是主键值，或者一个 IDBKeyRange 对象。如果指定该参数，将只处理包含指定主键的记录；如果省略，将处理所有的记录。该方法还可以接受第二个参数，表示遍历方向，默认值为<code>next</code>，其他可能的值为<code>prev</code>、<code>nextunique</code>和<code>prevunique</code>。后两个值表示如果遇到重复值，会自动跳过。</p><p><strong>（14）IDBObjectStore.openKeyCursor()</strong></p><p><code>IDBObjectStore.openKeyCursor()</code>用于获取一个主键指针对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">IDBObjectStore.openKeyCursor()</span><br></pre></td></tr></table></figure><h3 id="idbtransaction-对象">IDBTransaction 对象</h3><p>IDBTransaction 对象用来异步操作数据库事务，所有的读写操作都要通过这个对象进行。</p><p><code>IDBDatabase.transaction()</code>方法返回的就是一个 IDBTransaction 对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> db;</span><br><span class="line"><span class="keyword">var</span> DBOpenRequest = <span class="built_in">window</span>.indexedDB.open(<span class="string">'demo'</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">DBOpenRequest.onsuccess = <span class="function"><span class="keyword">function</span>(<span class="params">event</span>) </span>&#123;</span><br><span class="line">  db = DBOpenRequest.result;</span><br><span class="line">  <span class="keyword">var</span> transaction = db.transaction([<span class="string">'demo'</span>], <span class="string">'readwrite'</span>);</span><br><span class="line"></span><br><span class="line">  transaction.oncomplete = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'transaction success'</span>);  </span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  transaction.onerror = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'tansaction error: '</span> + transaction.error);</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> objectStore = transaction.objectStore(<span class="string">'demo'</span>);</span><br><span class="line">  <span class="keyword">var</span> objectStoreRequest = objectStore.add(&#123; <span class="attr">foo</span>: <span class="number">1</span> &#125;);</span><br><span class="line"></span><br><span class="line">  objectStoreRequest.onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'add data success'</span>);</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>事务的执行顺序是按照创建的顺序，而不是发出请求的顺序。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> trans1 = db.transaction(<span class="string">'foo'</span>, <span class="string">'readwrite'</span>);</span><br><span class="line"><span class="keyword">var</span> trans2 = db.transaction(<span class="string">'foo'</span>, <span class="string">'readwrite'</span>);</span><br><span class="line"><span class="keyword">var</span> objectStore2 = trans2.objectStore(<span class="string">'foo'</span>)</span><br><span class="line"><span class="keyword">var</span> objectStore1 = trans1.objectStore(<span class="string">'foo'</span>)</span><br><span class="line">objectStore2.put(<span class="string">'2'</span>, <span class="string">'key'</span>);</span><br><span class="line">objectStore1.put(<span class="string">'1'</span>, <span class="string">'key'</span>);</span><br></pre></td></tr></table></figure><p>上面代码中，<code>key</code>对应的键值最终是<code>2</code>，而不是<code>1</code>。因为事务<code>trans1</code>先于<code>trans2</code>创建，所以首先执行。</p><p>注意，事务有可能失败，只有监听到事务的<code>complete</code>事件，才能保证事务操作成功。</p><p>IDBTransaction 对象有以下属性。</p><ul><li><code>IDBTransaction.db</code>：返回当前事务所在的数据库对象 IDBDatabase。</li><li><code>IDBTransaction.error</code>：返回当前事务的错误。如果事务没有结束，或者事务成功结束，或者被手动终止，该方法返回<code>null</code>。</li><li><code>IDBTransaction.mode</code>：返回当前事务的模式，默认是<code>readonly</code>（只读），另一个值是<code>readwrite</code>。</li><li><code>IDBTransaction.objectStoreNames</code>：返回一个类似数组的对象 DOMStringList，成员是当前事务涉及的对象仓库的名字。</li><li><code>IDBTransaction.onabort</code>：指定<code>abort</code>事件（事务中断）的监听函数。</li><li><code>IDBTransaction.oncomplete</code>：指定<code>complete</code>事件（事务成功）的监听函数。</li><li><code>IDBTransaction.onerror</code>：指定<code>error</code>事件（事务失败）的监听函数。</li></ul><p>IDBTransaction 对象有以下方法。</p><ul><li><code>IDBTransaction.abort()</code>：终止当前事务，回滚所有已经进行的变更。</li><li><code>IDBTransaction.objectStore(name)</code>：返回指定名称的对象仓库 IDBObjectStore。</li></ul><h3 id="idbindex-对象">IDBIndex 对象</h3><p>IDBIndex 对象代表数据库的索引，通过这个对象可以获取数据库里面的记录。数据记录的主键默认就是带有索引，IDBIndex 对象主要用于通过除主键以外的其他键，建立索引获取对象。</p><p>IDBIndex 是持久性的键值对存储。只要插入、更新或删除数据记录，引用的对象库中的记录，索引就会自动更新。</p><p><code>IDBObjectStore.index()</code>方法可以获取 IDBIndex 对象。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> transaction = db.transaction([<span class="string">'contactsList'</span>], <span class="string">'readonly'</span>);</span><br><span class="line"><span class="keyword">var</span> objectStore = transaction.objectStore(<span class="string">'contactsList'</span>);</span><br><span class="line"><span class="keyword">var</span> myIndex = objectStore.index(<span class="string">'lName'</span>);</span><br><span class="line"></span><br><span class="line">myIndex.openCursor().onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">event</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> cursor = event.target.result;</span><br><span class="line">  <span class="keyword">if</span> (cursor) &#123;</span><br><span class="line">    <span class="keyword">var</span> tableRow = <span class="built_in">document</span>.createElement(<span class="string">'tr'</span>);</span><br><span class="line">    tableRow.innerHTML =   <span class="string">'&lt;td&gt;'</span> + cursor.value.id + <span class="string">'&lt;/td&gt;'</span></span><br><span class="line">                         + <span class="string">'&lt;td&gt;'</span> + cursor.value.lName + <span class="string">'&lt;/td&gt;'</span></span><br><span class="line">                         + <span class="string">'&lt;td&gt;'</span> + cursor.value.fName + <span class="string">'&lt;/td&gt;'</span></span><br><span class="line">                         + <span class="string">'&lt;td&gt;'</span> + cursor.value.jTitle + <span class="string">'&lt;/td&gt;'</span></span><br><span class="line">                         + <span class="string">'&lt;td&gt;'</span> + cursor.value.company + <span class="string">'&lt;/td&gt;'</span></span><br><span class="line">                         + <span class="string">'&lt;td&gt;'</span> + cursor.value.eMail + <span class="string">'&lt;/td&gt;'</span></span><br><span class="line">                         + <span class="string">'&lt;td&gt;'</span> + cursor.value.phone + <span class="string">'&lt;/td&gt;'</span></span><br><span class="line">                         + <span class="string">'&lt;td&gt;'</span> + cursor.value.age + <span class="string">'&lt;/td&gt;'</span>;</span><br><span class="line">    tableEntry.appendChild(tableRow);</span><br><span class="line"></span><br><span class="line">    cursor.continue();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="string">'Entries all displayed.'</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>IDBIndex 对象有以下属性。</p><ul><li><code>IDBIndex.name</code>：字符串，索引的名称。</li><li><code>IDBIndex.objectStore</code>：索引所在的对象仓库。</li><li><code>IDBIndex.keyPath</code>：索引的主键。</li><li><code>IDBIndex.multiEntry</code>：布尔值，针对<code>keyPath</code>为数组的情况，如果设为<code>true</code>，创建数组时，每个数组成员都会有一个条目，否则每个数组都只有一个条目。</li><li><code>IDBIndex.unique</code>：布尔值，表示创建索引时是否允许相同的主键。</li></ul><p>IDBIndex 对象有以下方法，它们都是异步的，立即返回的都是一个 IDBRequest 对象。</p><ul><li><code>IDBIndex.count()</code>：用来获取记录的数量。它可以接受主键或 KeyRange 对象作为参数，这时只返回符合主键的记录数量，否则返回所有记录的数量。</li><li><code>IDBIndex.get(key)</code>：用来获取符合指定主键的数据记录。</li><li><code>IDBIndex.getKey(key)</code>：用来获取指定的主键。</li><li><code>IDBIndex.getAll()</code>：用来获取所有的数据记录。它可以接受两个参数，都是可选的，第一个参数用来指定主键，第二个参数用来指定返回记录的数量。如果省略这两个参数，则返回所有记录。由于获取成功时，浏览器必须生成所有对象，所以对性能有影响。如果数据集比较大，建议使用 IDBCursor 对象。</li><li><code>IDBIndex.getAllKeys()</code>：该方法与<code>IDBIndex.getAll()</code>方法相似，区别是获取所有主键。</li><li><code>IDBIndex.openCursor()</code>：用来获取一个 IDBCursor 对象，用来遍历索引里面的所有条目。</li><li><code>IDBIndex.openKeyCursor()</code>：该方法与<code>IDBIndex.openCursor()</code>方法相似，区别是遍历所有条目的主键。</li></ul><h3 id="idbcursor-对象">IDBCursor 对象</h3><p>IDBCursor 对象代表指针对象，用来遍历数据仓库（IDBObjectStroe）或索引（IDBIndex）的记录。</p><p>IDBCursor 对象一般通过<code>IDBObjectStore.openCursor()</code>方法获得。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> transaction = db.transaction([<span class="string">'rushAlbumList'</span>], <span class="string">'readonly'</span>);</span><br><span class="line"><span class="keyword">var</span> objectStore = transaction.objectStore(<span class="string">'rushAlbumList'</span>);</span><br><span class="line"></span><br><span class="line">objectStore.openCursor(<span class="literal">null</span>, <span class="string">'next'</span>).onsuccess = <span class="function"><span class="keyword">function</span>(<span class="params">event</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> cursor = event.target.result;</span><br><span class="line">  <span class="keyword">if</span> (cursor) &#123;</span><br><span class="line">    <span class="keyword">var</span> listItem = <span class="built_in">document</span>.createElement(<span class="string">'li'</span>);</span><br><span class="line">      listItem.innerHTML = cursor.value.albumTitle + <span class="string">', '</span> + cursor.value.year;</span><br><span class="line">      list.appendChild(listItem);</span><br><span class="line"></span><br><span class="line">      <span class="built_in">console</span>.log(cursor.source);</span><br><span class="line">      cursor.continue();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="built_in">console</span>.log(<span class="string">'Entries all displayed.'</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>IDBCursor 对象的属性。</p><ul><li><code>IDBCursor.source</code>：返回正在遍历的对象仓库或索引。</li><li><code>IDBCursor.direction</code>：字符串，表示指针遍历的方向。共有四个可能的值：next（从头开始向后遍历）、nextunique（从头开始向后遍历，重复的值只遍历一次）、prev（从尾部开始向前遍历）、prevunique（从尾部开始向前遍历，重复的值只遍历一次）。该属性通过<code>IDBObjectStore.openCursor()</code>方法的第二个参数指定，一旦指定就不能改变了。</li><li><code>IDBCursor.key</code>：返回当前记录的主键。</li><li><code>IDBCursor.value</code>：返回当前记录的数据值。</li><li>IDBCursor.primaryKey：返回当前记录的主键。对于数据仓库（objectStore）来说，这个属性等同于 IDBCursor.key；对于索引，IDBCursor.key 返回索引的位置值，该属性返回数据记录的主键。</li></ul><p>IDBCursor 对象有如下方法。</p><ul><li><code>IDBCursor.advance(n)</code>：指针向前移动 n 个位置。</li><li><code>IDBCursor.continue()</code>：指针向前移动一个位置。它可以接受一个主键作为参数，这时会跳转到这个主键。</li><li><code>IDBCursor.continuePrimaryKey()</code>：该方法需要两个参数，第一个是<code>key</code>，第二个是<code>primaryKey</code>，将指针移到符合这两个参数的位置。</li><li><code>IDBCursor.delete()</code>：用来删除当前位置的记录，返回一个 IDBRequest 对象。该方法不会改变指针的位置。</li><li><code>IDBCursor.update()</code>：用来更新当前位置的记录，返回一个 IDBRequest 对象。它的参数是要写入数据库的新的值。</li></ul><h3 id="idbkeyrange-对象">IDBKeyRange 对象</h3><p>IDBKeyRange 对象代表数据仓库（object store）里面的一组主键。根据这组主键，可以获取数据仓库或主键里面的一组记录。</p><p>IDBKeyRange 可以只包含一个值，也可以指定上限和下限。它有四个静态方法，用来指定主键的范围。</p><ul><li><code>IDBKeyRange.lowerBound()</code>：指定下限。</li><li><code>IDBKeyRange.upperBound()</code>：指定上限。</li><li><code>IDBKeyRange.bound()</code>：同时指定上下限。</li><li><code>IDBKeyRange.only()</code>：指定只包含一个值。</li></ul><p>下面是一些代码实例。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// All keys ≤ x</span></span><br><span class="line"><span class="keyword">var</span> r1 = IDBKeyRange.upperBound(x);</span><br><span class="line"></span><br><span class="line"><span class="comment">// All keys &lt; x</span></span><br><span class="line"><span class="keyword">var</span> r2 = IDBKeyRange.upperBound(x, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// All keys ≥ y</span></span><br><span class="line"><span class="keyword">var</span> r3 = IDBKeyRange.lowerBound(y);</span><br><span class="line"></span><br><span class="line"><span class="comment">// All keys &gt; y</span></span><br><span class="line"><span class="keyword">var</span> r4 = IDBKeyRange.lowerBound(y, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// All keys ≥ x &amp;&amp; ≤ y</span></span><br><span class="line"><span class="keyword">var</span> r5 = IDBKeyRange.bound(x, y);</span><br><span class="line"></span><br><span class="line"><span class="comment">// All keys &gt; x &amp;&amp;&lt; y</span></span><br><span class="line"><span class="keyword">var</span> r6 = IDBKeyRange.bound(x, y, <span class="literal">true</span>, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// All keys &gt; x &amp;&amp; ≤ y</span></span><br><span class="line"><span class="keyword">var</span> r7 = IDBKeyRange.bound(x, y, <span class="literal">true</span>, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// All keys ≥ x &amp;&amp;&lt; y</span></span><br><span class="line"><span class="keyword">var</span> r8 = IDBKeyRange.bound(x, y, <span class="literal">false</span>, <span class="literal">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// The key = z</span></span><br><span class="line"><span class="keyword">var</span> r9 = IDBKeyRange.only(z);</span><br></pre></td></tr></table></figure><p><code>IDBKeyRange.lowerBound()</code>、<code>IDBKeyRange.upperBound()</code>、<code>IDBKeyRange.bound()</code>这三个方法默认包括端点值，可以传入一个布尔值，修改这个属性。</p><p>与之对应，IDBKeyRange 对象有四个只读属性。</p><ul><li><code>IDBKeyRange.lower</code>：返回下限</li><li><code>IDBKeyRange.lowerOpen</code>：布尔值，表示下限是否为开区间（即下限是否排除在范围之外）</li><li><code>IDBKeyRange.upper</code>：返回上限</li><li><code>IDBKeyRange.upperOpen</code>：布尔值，表示上限是否为开区间（即上限是否排除在范围之外）</li></ul><p>IDBKeyRange 实例对象生成以后，将它作为参数输入 IDBObjectStore 或 IDBIndex 对象的<code>openCursor()</code>方法，就可以在所设定的范围内读取数据。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> t = db.transaction([<span class="string">'people'</span>], <span class="string">'readonly'</span>);</span><br><span class="line"><span class="keyword">var</span> store = t.objectStore(<span class="string">'people'</span>);</span><br><span class="line"><span class="keyword">var</span> index = store.index(<span class="string">'name'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> range = IDBKeyRange.bound(<span class="string">'B'</span>, <span class="string">'D'</span>);</span><br><span class="line"></span><br><span class="line">index.openCursor(range).onsuccess = <span class="function"><span class="keyword">function</span> (<span class="params">e</span>) </span>&#123;</span><br><span class="line">  <span class="keyword">var</span> cursor = e.target.result;</span><br><span class="line">  <span class="keyword">if</span> (cursor) &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(cursor.key + <span class="string">':'</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">var</span> field <span class="keyword">in</span> cursor.value) &#123;</span><br><span class="line">      <span class="built_in">console</span>.log(cursor.value[field]);</span><br><span class="line">    &#125;</span><br><span class="line">    cursor.continue();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>IDBKeyRange 有一个实例方法<code>includes(key)</code>，返回一个布尔值，表示某个主键是否包含在当前这个主键组之内。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> keyRangeValue = IDBKeyRange.bound(<span class="string">'A'</span>, <span class="string">'K'</span>, <span class="literal">false</span>, <span class="literal">false</span>);</span><br><span class="line"></span><br><span class="line">keyRangeValue.includes(<span class="string">'F'</span>) <span class="comment">// true</span></span><br><span class="line">keyRangeValue.includes(<span class="string">'W'</span>) <span class="comment">// false</span></span><br></pre></td></tr></table></figure><h3 id="参考链接">参考链接</h3><ul><li>Raymond Camden, <a href="http://net.tutsplus.com/tutorials/javascript-ajax/working-with-indexeddb/" target="_blank" rel="noopener">Working With IndexedDB – Part 1</a></li><li>Raymond Camden, <a href="http://net.tutsplus.com/tutorials/javascript-ajax/working-with-indexeddb-part-2/" target="_blank" rel="noopener">Working With IndexedDB – Part 2</a></li><li>Raymond Camden, <a href="https://code.tutsplus.com/tutorials/working-with-indexeddb-part-3--net-36220" target="_blank" rel="noopener">Working With IndexedDB - Part 3</a></li><li>Tiffany Brown, <a href="http://dev.opera.com/articles/introduction-to-indexeddb/" target="_blank" rel="noopener">An Introduction to IndexedDB</a></li><li>David Fahlander, <a href="https://hacks.mozilla.org/2014/06/breaking-the-borders-of-indexeddb/" target="_blank" rel="noopener">Breaking the Borders of IndexedDB</a></li><li>TutorialsPoint, <a href="https://www.tutorialspoint.com/html5/html5_indexeddb.htm" target="_blank" rel="noopener">HTML5 - IndexedDB</a></li></ul><blockquote><p>来源：阮一峰的网络日志<br>原文：<a href="http://t.cn/RdZa8YW" target="_blank" rel="noopener">http://t.cn/RdZa8YW</a><br>版权：本文版权归原作者所有</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;概述&quot;&gt;概述&lt;/h3&gt;
&lt;p&gt;随着浏览器的功能不断增强，越来越多的网站开始考虑，将大量数据储存在客户端，这样可以减少从服务器获取数据，直接从本地获取数据。&lt;/p&gt;
&lt;p&gt;现有的浏览器数据储存方案，都不适合储存大量数据：Cookie 的大小不超过4KB，且每次请求都会发送回服务器；LocalStorage 在 2.5MB 到 10MB 之间（各家浏览器不同），而且不提供搜索功能，不能建立自定义的索引。所以，需要一种新的解决方案，这就是 IndexedDB 诞生的背景。&lt;/p&gt;
&lt;p&gt;通俗地说，IndexedDB 就是浏览器提供的本地数据库，它可以被网页脚本创建和操作。IndexedDB 允许储存大量数据，提供查找接口，还能建立索引。这些都是 LocalStorage 所不具备的。就数据库类型而言，IndexedDB 不属于关系型数据库（不支持 SQL 查询语句），更接近 NoSQL 数据库。&lt;/p&gt;
    
    </summary>
    
      <category term="IndexedDB" scheme="https://www.hi-linux.com/categories/IndexedDB/"/>
    
    
      <category term="数据库" scheme="https://www.hi-linux.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
      <category term="IndexedDB" scheme="https://www.hi-linux.com/tags/IndexedDB/"/>
    
  </entry>
  
  <entry>
    <title>图解 Docker 常用命令工作原理</title>
    <link href="https://www.hi-linux.com/posts/44544.html"/>
    <id>https://www.hi-linux.com/posts/44544.html</id>
    <published>2018-07-08T01:00:00.000Z</published>
    <updated>2018-08-24T06:05:16.720Z</updated>
    
    <content type="html"><![CDATA[<h3 id="dokcer-常用命令工作原理">Dokcer 常用命令工作原理</h3><ul><li>Docker 常用命令工作原理图</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-01.png" alt=""></p><ul><li>Image Layer（镜像层）</li></ul><p>镜像可以看成是由多个镜像层叠加起来的一个文件系统，镜像层也可以简单理解为一个基本的镜像，而每个镜像层之间通过指针的形式进行叠加。</p><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-02.png" alt=""></p><p>根据上图，镜像层的主要组成部分包括镜像层 ID、镜像层指针 「指向父层」、元数据「 Layer Metadata，包含了 Docker 构建和运行的信息和父层的层次信息」。</p><p>只读层和读写层「Top Layer」的组成部分基本一致，同时读写层可以转换成只读层「 通过<code>docker commit</code> 操作实现」。</p><a id="more"></a><ul><li>Image（镜像，只读层的集合）</li></ul><p>镜像是一堆只读层的统一视角，除了最底层没有指向外，每一层都指向它的父层。统一文件系统（ Union File System）技术能够将不同的层整合成一个文件系统，为这些层提供了一个统一的视角，这样就隐藏了多层的存在。在用户的角度看来，只存在一个文件系统。镜像每一层都是不可写的，都是只读层。</p><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-03.png" alt=""></p><ul><li>Container（容器，一层读写层+多层只读层）</li></ul><p>容器和镜像的区别在于容器的最上面一层是读写层「Top Layer」，在这里并没有区分容器是否在运行。</p><p>运行状态的容器「Running Container」是由一个可读写的文件系统「静态容器」+ 隔离的进程空间和其中的进程构成的。</p><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-04.png" alt=""></p><p>隔离的进程空间中的进程可以对该读写层进行增删改，运行状态容器的进程操作都作用在该读写层上。每个容器只能有一个进程隔离空间。</p><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-05.png" alt=""></p><h3 id="docker-常用命令说明">Docker 常用命令说明</h3><h4 id="标识说明">标识说明</h4><ul><li>Image（统一只读文件系统）</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-06.png" alt=""></p><ul><li>静态容器 （未运行的容器，统一可读写文件系统）</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-07.png" alt=""></p><ul><li>动态容器（运行中的容器，进程空间（包括进程）+ 统一可读写文件系统）</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-08.png" alt=""></p><h4 id="命令说明">命令说明</h4><h5 id="docker-生命周期相关命令">Docker 生命周期相关命令</h5><ul><li>docker create &lt; image-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-09.png" alt=""></p><p>该命令即为在只读文件系统上添加一层可读写层「Top Layer」，并生成可读写文件系统。该命令状态下容器为静态容器，并没有运行。</p><ul><li>docker start | restart &lt; container-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-10.png" alt=""></p><p>该命令即为在可读写文件系统添加一个进程空间和运行的进程，并生成一个动态容器。</p><blockquote><p><code>docker stop</code> 即为 <code>docker start</code> 的逆过程。</p></blockquote><ul><li>docker run &lt; image-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-11.png" alt=""></p><p><code>docker run</code> = <code>docker create</code> + <code>docker start</code></p><p>docker run 流程类似如下：</p><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-12.png" alt=""></p><ul><li>docker stop &lt; container-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-13.png" alt=""></p><p>该指令向运行中的容器发一个 SIGTERM 信号，然后停止所有的进程。即为 <code>docker start</code> 的逆过程。</p><ul><li>docker kill &lt; container-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-14.png" alt=""></p><p>该指令向容器发送一个不友好的 SIGKILL 信号，相当于快速强制关闭容器。与 <code>docker stop</code> 的区别是 <code>docker stop</code> 是先发 SIGTERM 信号来清理进程，然后再发 SIGKILL 信号退出，整个进程是正常关闭的。</p><ul><li>docker pause &lt; container-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-15.png" alt=""></p><p>该指令用作暂停容器中的所有进程，使用 cgroup 的 freezer 顺序暂停容器里的所有进程。</p><blockquote><p>docker unpause 为其逆过程即恢复所有进程，比较少使用。</p></blockquote><ul><li>docker commit &lt; container-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-16.png" alt=""></p><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-17.png" alt=""></p><p>该指令用作把容器的可读写层转化成只读层，即从容器状态「可读写文件系统」变为镜像状态「只读文件系统」，可理解为固化。</p><ul><li>docker build</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-18.png" alt=""></p><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-19.png" alt=""></p><p><code>docker build</code> = <code>docker run</code> 「运行容器 + 进程修改数据」+ <code>docker commit</code>「固化数据」，整个过程不断循环直至生成所需镜像。</p><blockquote><ol><li>循环一次便会形成一个新的层(新镜像 = 原镜像层 + 已固化的可读写层）</li><li>docker build 过程一般通过 dockerfile 文件来实现。</li></ol></blockquote><h5 id="docker-查询类命令">Docker 查询类命令</h5><p>Docker 可查询的对象有：image、container、image/container 中的数据、系统信息（包括容器数、镜像数及其它）。</p><ul><li>docker images</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-20.png" alt=""></p><p>该指令用作列出镜像的顶层镜像（以顶层镜像 ID 来表示整个完整镜像），每个顶层镜像下面隐藏多个镜像层。</p><ul><li>docker images -a</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-21.png" alt=""></p><p>该指令用作列出镜像的所有镜像层。镜像层的排序以每个顶层镜像 ID 为首，依次列出每个镜像下的所有镜像层。</p><ul><li>docker history &lt; image-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-22.png" alt=""></p><p>该指令列出该镜像 ID 下的所有历史镜像。</p><ul><li>docker ps</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-23.png" alt=""></p><p>该指令用作列出所有运行中的容器。</p><ul><li>docker ps -a</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-24.png" alt=""></p><p>该指令用作列出所有容器，包括静态容器和动态容器。</p><ul><li>docker inspect &lt; container-id &gt; or &lt; image-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-25.png" alt=""></p><p>该指令用作提取出容器或镜像中最顶层的元数据。</p><ul><li>docker info</li></ul><p>该指令用作显示 Docker 系统信息，包括镜像和容器数。</p><h5 id="docker-操作类命令">Docker 操作类命令</h5><ul><li>docker rm &lt; container-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-26.png" alt=""><br><br>该指令用作移除容器，默认只能对静态容器（非运行状态的）进行移除。如果要移除运行中的容器，需要使用 <code>-f（force）</code> 参数，即：<code>docker rm -f &lt;container-id&gt;</code>。</p><ul><li>docker rmi &lt; image-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-27.png" alt=""></p><p>该指令作用与 <code>docker rm</code> 类似，用作移除镜像。</p><ul><li>docker exec &lt; running-container-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-28.png" alt=""></p><p>该指令用于在运行状态的容器中执行一个新的进程。</p><ul><li>docker export &lt; container-id &gt;</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-cmd-29.png" alt=""></p><p>该指令用作持久化一个容器，会创建一个 tar 格式的文件。该文件移除了元数据和不必要的层，将多个层整合成了一个层，只保存了当前统一视角看到的内容。</p><p>如果你要持久化一个镜像，可以使用 <code>docker save</code> 指令。它与 <code>docker export</code> 的区别在于其保留了所有元数据和历史层。</p><p>通过 <code>docker export</code> 导出的容器再 <code>docker import</code> 到 Docker 中后，在 <code>docker images –tree</code> 命令只能看到一个镜像。而通过 <code>docker save</code> 保存后的镜像则不同，它能够看到这个镜像构建过程中的所有历史层。</p><p><code>docker export</code> 和 <code>docker save</code> 两者更多的区别可参考「<a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247485543&amp;idx=1&amp;sn=93c14014ae4f01bceb5a59520903b21f&amp;chksm=eac5294eddb2a0580673642e0ed35ce638b4f8b5d37ec6991c75f4705bcfdb50b633bd0474d0&amp;mpshare=1&amp;scene=23&amp;srcid=0708ouNaEe7BDhuhCx3jY8fc%23rd" target="_blank" rel="noopener">Docker 的 save 和 export 命令的区别</a>」一文。</p><blockquote><p>本文在 「<a href="http://www.huweihuang.com/article/docker/docker-commands-principle/" target="_blank" rel="noopener">Docker 常用命令原理图</a>」的基础上整理和修改。</p></blockquote><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RdC2419" target="_blank" rel="noopener">http://t.cn/RdC2419</a><br><a href="http://t.cn/RUfqHh9" target="_blank" rel="noopener">http://t.cn/RUfqHh9</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;dokcer-常用命令工作原理&quot;&gt;Dokcer 常用命令工作原理&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Docker 常用命令工作原理图&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/docker-cmd-01.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Image Layer（镜像层）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;镜像可以看成是由多个镜像层叠加起来的一个文件系统，镜像层也可以简单理解为一个基本的镜像，而每个镜像层之间通过指针的形式进行叠加。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/docker-cmd-02.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;根据上图，镜像层的主要组成部分包括镜像层 ID、镜像层指针 「指向父层」、元数据「 Layer Metadata，包含了 Docker 构建和运行的信息和父层的层次信息」。&lt;/p&gt;
&lt;p&gt;只读层和读写层「Top Layer」的组成部分基本一致，同时读写层可以转换成只读层「 通过&lt;code&gt;docker commit&lt;/code&gt; 操作实现」。&lt;/p&gt;
    
    </summary>
    
      <category term="Docker" scheme="https://www.hi-linux.com/categories/docker/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>图解 Docker 架构</title>
    <link href="https://www.hi-linux.com/posts/13732.html"/>
    <id>https://www.hi-linux.com/posts/13732.html</id>
    <published>2018-07-07T01:00:00.000Z</published>
    <updated>2018-08-24T06:04:46.940Z</updated>
    
    <content type="html"><![CDATA[<h3 id="docker-总架构图">Docker 总架构图</h3><p><img src="https://www.hi-linux.com/img/linux/docker-arch1.jpg" alt=""></p><p>Docker 是一个 C/S 模式的架构，后端是一个松耦合架构，模块各司其职。</p><ol><li><p>用户是使用 Docker Client 与 Docker Daemon 建立通信，并发送请求给后者。</p></li><li><p>Docker Daemon 作为 Docker 架构中的主体部分，首先提供 Docker Server 的功能使其可以接受 Docker Client 的请求。</p></li><li><p>Docker Engine 执行 Docker 内部的一系列工作，每一项工作都是以一个 Job 的形式的存在。</p></li><li><p>Job 的运行过程中，当需要容器镜像时，则从 Docker Registry 中下载镜像，并通过镜像管理驱动 Graphdriver 将下载镜像以 Graph 的形式存储。</p></li><li><p>当需要为 Docker 创建网络环境时，通过网络管理驱动 Networkdriver 创建并配置 Docker容器网络环境。</p></li><li><p>当需要限制 Docker 容器运行资源或执行用户指令等操作时，则通过 Execdriver 来完成。</p></li><li><p>Libcontainer 是一项独立的容器管理包，Networkdriver 以及 Execdriver 都是通过 Libcontainer 来实现具体对容器进行的操作。</p></li></ol><a id="more"></a><h3 id="docker-各模块组件分析">Docker 各模块组件分析</h3><h4 id="docker-client发起请求">Docker Client「发起请求」</h4><ol><li><p>Docker Client 是 和 Docker Daemon 建立通信的客户端。用户使用的可执行文件为 docker（一个命令行可执行文件），docker 命令使用后接参数的形式来实现一个完整的请求命令（例如：<code>docker images</code>，docker 为命令不可变，images 为参数可变）。</p></li><li><p>Docker Client 可以通过以下三种方式和 Docker Daemon 建立通信：<code>tcp://host:port</code>、<code>unix://path_to_socket</code> 和 <code>fd://socketfd</code></p></li><li><p>Docker Client 发送容器管理请求后，由 Docker Daemon 接受并处理请求，当 Docker Client 接收到返回的请求相应并简单处理后，Docker Client 一次完整的生命周期就结束了。(一次完整的请求：发送请求→处理请求→返回结果)，与传统的 C/S 架构请求流程并无不同。</p></li></ol><h4 id="docker-daemon-后台守护进程">Docker Daemon 「后台守护进程」</h4><ol><li>Docker Daemon 架构图</li></ol><p><img src="https://www.hi-linux.com/img/linux/docker-arch2.jpg" alt=""></p><ol start="2"><li>Docker Server 「调度分发请求」</li></ol><ul><li>Docker Server 架构图</li></ul><p><img src="https://www.hi-linux.com/img/linux/docker-arch3.jpg" alt=""></p><ul><li><p>Docker Server 相当于 C/S 架构的服务端。功能为接受并调度分发 Docker Client 发送的请求。接受请求后，Docker Server 通过路由与分发调度，找到相应的 Handler 来执行请求。</p></li><li><p>在 Docker 的启动过程中，通过包 gorilla/mux 创建了一个 mux.Router 来提供请求的路由功能。在 Golang 中 gorilla/mux 是一个强大的 URL 路由器以及调度分发器。该 mux.Router 中添加了众多的路由项，每一个路由项由 HTTP 请求方法（PUT、POST、GET 或DELETE）、URL、Handler 三部分组成。</p></li><li><p>创建完 mux.Router 之后，Docker 将 Server 的监听地址以及 mux.Router 作为参数来创建一个 httpSrv=http.Server{}，最终执行 httpSrv.Serve() 为请求服务。</p></li><li><p>在 Docker Server 的服务过程中，Docker Server 在 listener 上接受 Docker Client 的访问请求，并创建一个全新的 goroutine 来服务该请求。在 goroutine 中，首先读取请求内容并做解析工作，接着找到相应的路由项并调用相应的 Handler 来处理该请求，最后 Handler 处理完请求之后回复该请求。</p></li></ul><ol start="3"><li>Docker Engine</li></ol><ul><li><p>Docker Engine 是 Docker 架构中的运行引擎，同时也 Docker 运行的核心模块。它扮演 Docker Container 存储仓库的角色，并且通过执行 Job 的方式来操纵管理这些容器。</p></li><li><p>在 Docker Engine 数据结构的设计与实现过程中，有一个 Handler 对象。该 Handler 对象存储的都是关于众多特定 Job 的 Handler 处理访问。举例说明: Docker Engine 的Handler 对象中有一项为：{“create”: daemon.ContainerCreate,}，则说明当名为&quot;create&quot; 的 Job 在运行时，执行的是 daemon.ContainerCreate 的 Handler。</p></li></ul><ol start="4"><li>Job</li></ol><ul><li><p>一个 Job 可以认为是 Docker 架构中 Docker Engine 内部最基本的工作执行单元。Docker 可以做的每一项工作，都可以抽象为一个 Job。例如：在容器内部运行一个进程，这是一个 Job；创建一个新的容器，这是一个 Job。 Docker Server 的运行过程也是一个 Job，名为 ServeApi。</p></li><li><p>Job 的设计者，把 Job 设计得与 Unix 进程相仿。比如说：Job 有一个名称、有参数、有环境变量、有标准的输入输出、有错误处理，有返回状态等。</p></li></ul><h4 id="docker-registry-镜像注册中心">Docker Registry 「镜像注册中心」</h4><ol><li><p>Docker Registry 是一个存储容器镜像的仓库（注册中心），可理解为云端镜像仓库。按 Repository 来分类，<code>docker pull</code> 按照 [repository]:[tag] 来精确定义一个具体的 Image。</p></li><li><p>在 Docker 的运行过程中，Docker Daemon 会与 Docker Registry 通信，并实现搜索镜像、下载镜像、上传镜像三个功能，这三个功能对应的 Job 名称分别为： “search”、“pull” 与 “push”。</p></li><li><p>Docker Registry 可分为公有仓库（ Docker Hub）和私有仓库。</p></li></ol><h4 id="graph-docker-内部数据库">Graph 「Docker 内部数据库」</h4><ol><li>Graph 架构图</li></ol><p><img src="https://www.hi-linux.com/img/linux/docker-arch4.jpg" alt=""></p><ol start="2"><li>Repository</li></ol><ul><li>已下载镜像的保管者（包括下载的镜像和通过 Dockerfile 构建的镜像）。</li><li>一个 Repository 表示某类镜像的仓库（例如：Ubuntu），同一个 Repository 内的镜像用 Tag 来区分（表示同一类镜像的不同标签或版本）。一个 Registry 包含多个Repository，一个 Repository 包含同类型的多个 Image。</li><li>镜像的存储类型有 Aufs、Devicemapper、Btrfs、Vfs等。其中 CentOS 系统 7.x 以下版本使用 Devicemapper 的存储类型。</li><li>同时在 Graph 的本地目录中存储有关于每一个的容器镜像具体信息，包含有：该容器镜像的元数据、容器镜像的大小信息、以及该容器镜像所代表的具体 rootfs。</li></ul><ol start="3"><li>GraphDB</li></ol><ul><li>已下载容器镜像之间关系的记录者。</li><li>GraphDB 是一个构建在 SQLite 之上的小型数据库，实现了节点的命名以及节点之间关联关系的记录。</li></ul><h4 id="driver-执行部分">Driver 「执行部分」</h4><p>Driver 是 Docker 架构中的驱动模块。通过 Driver 驱动，Docker 可以实现对 Docker 容器执行环境的定制。即 Graph 负责镜像的存储，Driver 负责容器的执行。</p><h5 id="graphdriver">Graphdriver</h5><ol><li>Graphdriver 架构图</li></ol><p><img src="https://www.hi-linux.com/img/linux/docker-arch5.jpg" alt=""></p><ol start="2"><li>Graphdriver 主要用于完成容器镜像的管理，包括存储与获取。</li><li>存储：<code>docker pull</code> 下载的镜像由 Graphdriver 存储到本地的指定目录（ Graph 中）。</li><li>获取：<code>docker run</code>（create）用镜像来创建容器的时候由 Graphdriver 到本地 Graph中获取镜像。</li></ol><h5 id="networkdriver">Networkdriver</h5><ol><li>Networkdriver 架构图</li></ol><p><img src="https://www.hi-linux.com/img/linux/docker-arch6.jpg" alt=""></p><ol start="2"><li>Networkdriver 的用途是完成 Docker 容器网络环境的配置，其中包括:</li></ol><ul><li>Docker 启动时为 Docker 环境创建网桥。</li><li>Docker 容器创建时为其创建专属虚拟网卡设备。</li><li>Docker 容器分配IP、端口并与宿主机做端口映射，设置容器防火墙策略等。</li></ul><h5 id="execdriver">Execdriver</h5><ol><li>Execdriver 架构图</li></ol><p><img src="https://www.hi-linux.com/img/linux/docker-arch7.jpg" alt=""></p><ol start="2"><li><p>Execdriver 作为 Docker 容器的执行驱动，负责创建容器运行命名空间、容器资源使用的统计与限制、容器内部进程的真正运行等。</p></li><li><p>现在 Execdriver 默认使用 Native 驱动，不依赖于 LXC。</p></li></ol><h4 id="libcontainer-函数库">Libcontainer 「函数库」</h4><ol><li>Libcontainer 架构图</li></ol><p><img src="https://www.hi-linux.com/img/linux/docker-arch8.jpg" alt=""></p><ol start="2"><li><p>Libcontainer 是 Docker 架构中一个使用 Go 语言设计实现的库，设计初衷是希望该库可以不依靠任何依赖，直接访问内核中与容器相关的 API。</p></li><li><p>Docker 可以直接调用 Libcontainer 来操纵容器的 Namespace、Cgroups、Apparmor、网络设备以及防火墙规则等。</p></li><li><p>Libcontainer 提供了一整套标准的接口来满足上层对容器管理的需求。或者说 Libcontainer 屏蔽了 Docker 上层对容器的直接管理。</p></li></ol><h4 id="docker-container-服务交付的最终形式">Docker Container 「服务交付的最终形式」</h4><ol><li>Docker Container 架构</li></ol><p><img src="https://www.hi-linux.com/img/linux/docker-arch9.jpg" alt=""></p><ol start="2"><li><p>Docker Container（ Docker 容器 ）是 Docker 架构中服务交付的最终体现形式。</p></li><li><p>Docker 按照用户的需求与指令，订制相应的 Docker 容器：</p></li></ol><ul><li>用户通过指定容器镜像，使得 Docker 容器可以自定义 rootfs 等文件系统。</li><li>用户通过指定计算资源的配额，使得 Docker 容器使用指定的计算资源。</li><li>用户通过配置网络及其安全策略，使得 Docker 容器拥有独立且安全的网络环境。</li><li>用户通过指定运行的命令，使得 Docker 容器执行指定的工作。</li></ul><blockquote><p>本文在 「<a href="http://www.huweihuang.com/article/docker/docker-architecture/" target="_blank" rel="noopener">Docker 整体架构图</a>」的基础上整理和修改。</p></blockquote><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RdiV7Sn" target="_blank" rel="noopener">http://t.cn/RdiV7Sn</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;docker-总架构图&quot;&gt;Docker 总架构图&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/docker-arch1.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Docker 是一个 C/S 模式的架构，后端是一个松耦合架构，模块各司其职。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;用户是使用 Docker Client 与 Docker Daemon 建立通信，并发送请求给后者。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Docker Daemon 作为 Docker 架构中的主体部分，首先提供 Docker Server 的功能使其可以接受 Docker Client 的请求。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Docker Engine 执行 Docker 内部的一系列工作，每一项工作都是以一个 Job 的形式的存在。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Job 的运行过程中，当需要容器镜像时，则从 Docker Registry 中下载镜像，并通过镜像管理驱动 Graphdriver 将下载镜像以 Graph 的形式存储。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当需要为 Docker 创建网络环境时，通过网络管理驱动 Networkdriver 创建并配置 Docker容器网络环境。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;当需要限制 Docker 容器运行资源或执行用户指令等操作时，则通过 Execdriver 来完成。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Libcontainer 是一项独立的容器管理包，Networkdriver 以及 Execdriver 都是通过 Libcontainer 来实现具体对容器进行的操作。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Docker" scheme="https://www.hi-linux.com/categories/docker/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>浅析 AnyCast 技术</title>
    <link href="https://www.hi-linux.com/posts/26571.html"/>
    <id>https://www.hi-linux.com/posts/26571.html</id>
    <published>2018-06-28T01:00:00.000Z</published>
    <updated>2018-08-24T06:09:39.858Z</updated>
    
    <content type="html"><![CDATA[<p>在讲解任播 (AnyCast) 前，我们先来说说 <code>TCP/IP</code> 协议里常见的几种数据传输方式单播、组播、广播。</p><h3 id="什么是单播">什么是单播</h3><p><img src="https://www.hi-linux.com/img/linux/AnyCast01.png" alt=""></p><ul><li>单播概念</li></ul><p>单播（Unicast）是指封包在计算机网络的传输过程中，目的地址为单一目标的一种传输方式。每次只有两个实体相互通信，发送端和接收端都是唯一确定的。它是现今网络应用最为广泛，通常所使用的网络协议或服务大多采用单播传输，例如一切基于 TCP 的协议。</p><ul><li>单播地址</li></ul><p>在 IPv4 网络中，0.0.0.0 到 223.255.255.255 属于单播地址。</p><ul><li>单播优点</li></ul><ol><li>服务器及时响应客户机的请求。</li><li>服务器针对每个客户不同的请求发送不同的数据，容易实现个性化服务。</li></ol><ul><li>单播缺点</li></ul><ol><li><p>服务器针对每个客户机发送数据流，服务器流量＝客户机数量×客户机流量。在客户数量大、每个客户机流量大的流媒体应用中服务器不堪重负。</p></li><li><p>现有的网络带宽是金字塔结构，城际省际主干带宽仅仅相当于其所有用户带宽之和的 5％。如果全部使用单播协议，将造成网络主干不堪重负。现在的 P2P 应用就已经使主干经常阻塞，只要有 5％ 的客户在全速使用网络，其它用户网速将严重受损，而将主干网络带宽在短时间内扩展 20 倍几乎是不可能。</p></li></ol><a id="more"></a><h3 id="什么是多播">什么是多播</h3><p><img src="https://www.hi-linux.com/img/linux/AnyCast02.png" alt=""></p><ul><li>多播概念</li></ul><p>多播（Multicast，台湾又译作多点发送、多点广播或群播，中国大陆又译作组播）是指把信息同时传递给一组目的地址。它使用策略是最高效的，因为消息在每条网络链路上只需传递一次，而且只有在链路分叉的时候，消息才会被复制。</p><p>与多播相比，常规的点到点的传递被称作单播。当以单播的形式把消息传递给多个接收方时，必须向每个接收者都发送一份数据副本。由此产生的多余副本将导致发送方效率低下，且缺乏可扩展性。不过，许多流行的协议用限制接收者数量的方法弥补了这一不足（例如：XMPP）。</p><ul><li>多播地址</li></ul><p>多播组可以是永久的也可以是临时的。多播组地址中，有一部分由官方分配的，称为永久多播组。永久多播组保持不变的是它的 IP 地址，组中的成员构成可以发生变化。永久多播组中成员的数量都可以是任意的，甚至可以为零。那些没有保留下来供永久组播组使用的 IP 多播地址，可以被临时多播组利用。</p><p>以太网传输单播 IP 报文的时候，目的 MAC 地址使用的是接收者的 MAC 地址。但是在传输多播报文时，传输目的不再是一个具体的接收者，而是一个成员不确定的组。所以使用的是多播 MAC 地址。</p><p>多播 MAC 地址是和多播 IP 地址对应的。IANA 规定，多播 MAC 地址的高 24 位为 0x01005e，低 23 位的 MAC 地址为多播 IP 地址的低 23 位。</p><p>由于 IP 多播地址的后 28 位中只有 23 位被映射到 MAC 地址，这样就会有 32 个 IP多播地址映射到同一 MAC 地址上。</p><p>多播地址分配情况</p><blockquote><ol><li>224.0.0.0 - 224.0.0.255 为预留的多播地址（永久组地址），地址 224.0.0.0 保留不做分配，其它地址供路由协议使用。</li><li>224.0.1.0 - 224.0.1.255 是公用多播地址。</li><li>224.0.2.0 - 238.255.255.255 为用户可用的多播地址（临时组地址），全网范围内有效。</li><li>239.0.0.0 - 239.255.255.255 为本地管理组播地址，仅在特定的本地范围内有效。</li></ol></blockquote><p>永久的多播地址分配情况</p><blockquote><ol><li>224.0.0.0 基准地址（保留）</li><li>224.0.0.1 所有主机的地址 （包括所有路由器地址）</li><li>224.0.0.2 所有多播路由器的地址</li><li>224.0.0.3 不分配</li><li>224.0.0.4 DVMRP 路由器</li><li>224.0.0.5 所有 OSPF 路由器</li><li>224.0.0.6 OSPF DR/BDR</li><li>224.0.0.7 ST 路由器</li><li>224.0.0.8 ST 主机</li><li>224.0.0.9 RIP-2 路由器</li><li>224.0.0.10 Eigrp 路由器</li><li>224.0.0.11 活动代理</li><li>224.0.0.12 DHCP 服务器/中继代理</li><li>224.0.0.13 所有 PIM 路由器</li><li>224.0.0.14 RSVP 封装</li><li>224.0.0.15 所有 CBT 路由器</li><li>224.0.0.16 指定 SBM</li><li>224.0.0.17 所有 SBMS</li><li>224.0.0.18 VRRP</li></ol></blockquote><ul><li>多播优点</li></ul><ol><li>需要相同数据流的客户端加入相同的组共享一条数据流，节省了服务器的负载。</li><li>由于多播协议是根据接受者的需要对数据流进行复制转发，所以服务端的服务总带宽不受客户接入端带宽的限制。IP 协议允许有 2 亿 6 千多万个（268435456）组播，所以其提供的服务可以非常丰富。</li><li>此协议和单播协议一样允许在 Internet 宽带网上传输。</li></ol><ul><li>多播缺点</li></ul><ol><li><p>与单播协议相比没有纠错机制，发生丢包错包后难以弥补，但可以通过一定的容错机制和 QOS 加以弥补。</p></li><li><p>现行网络虽然都支持组播的传输，但在客户认证、QOS（指一个网络中能够利用各种基础技术，为指定的网络通信提供更好的服务能力。QOS 是网络的一种安全机制，用来解决网络延迟和阻塞等问题的一种技术。）等方面还需要完善。</p></li><li><p>尽管 IP 多播是一个非常令人满意的概念模型，但它对于网络内部的状态需求要比仅提供尽力而为服务的 IP 单播模型大得多，在这一点上已经遭到了一些人的批评。更糟的是，到目前为止还没有一种机制能保证 IP 多播模型可以被扩展到足以容纳数以百万计的发送者和多播组的地步，而这往往又是使完全通用的多播应用成为商用互联网中的实际应用的必要条件。</p></li></ol><h3 id="什么是广播">什么是广播</h3><p><img src="https://www.hi-linux.com/img/linux/AnyCast03.png" alt=""></p><ul><li>广播概念</li></ul><p>广播（Broadcast）是指封包在计算机网络中传输时，目的地址为网络中所有设备的一种传输方式。实际上，这里所说的所有设备也是限定在一个范围之中，称为广播域。</p><p>并非所有的计算机网络都支持广播，例如 X.25 网络和帧中继都不支持广播，而且也没有在整个互联网范围中的广播。IPv6 亦不支持广播，广播相应的功能由多播代替。</p><p>通常，广播都是限制在局域网中的。比如：以太网或令牌环网络，因为广播在局域网中造成的影响远比在广域网中小得多。</p><ul><li>广播地址</li></ul><p>以太网和 IPv4 网都用全 1 的地址表示广播，分别是 ff:ff:ff:ff:ff:ff 和 255.255.255.255。</p><ul><li>广播优点</li></ul><ol><li>网络设备简单，维护简单，布网成本低廉。</li><li>由于服务器不用向每个客户机单独发送数据，所以服务器流量负载极低。</li></ol><ul><li>广播缺点</li></ul><ol><li>无法针对每个客户的要求和时间及时提供个性化服务。</li><li>网络允许服务器提供数据的带宽有限，客户端的最大带宽＝服务总带宽。例如有线电视的客户端的线路支持 100 个频道（如果采用数字压缩技术，理论上可以提供 500 个频道），即使服务商有更大的财力配置更多的发送设备、改成光纤主干，也无法超过此极限。也就是说无法向众多客户提供更多样化、更加个性化的服务。</li><li>广播禁止在 Internet 宽带网上传输。</li></ol><h3 id="什么是任播">什么是任播</h3><p><img src="https://www.hi-linux.com/img/linux/AnyCast04.png" alt=""></p><ul><li>任播概念</li></ul><p>Anycast 最初是在 RFC1546 中提出并定义的，根据 RFC1546 的说明 IPv4 的任播地址不同于 IPv4 的单播地址，它建议从 IPv4 的地址空间分配出一块独立的地址空间作为任播地址空间。</p><p>Anycast 提供的是一种无状态的、尽力而为的服务，目前对于 Anycast 的中文译称主要有任播、泛潘、选播等。任播的基本概念是从物理主机设备中分离出的逻辑服务标识符，任播地址可以根据服务类型来分配，使得网络服务担当一个逻辑主机的角色。</p><p>RFC1546 定义的这种任播没有得到广泛的使用，在 1998 年的 RFC2373 规定了 IPv6 寻址体系结构。在这个文档中改进了任播的定义：发送到一个任播地址的报文被传送到由该地址标识的接口之一(根据路由协议的距离量度最近的一个)。RFC2373 定义的 IPv6 的任播模型没有限制路由选择的下部结构，也没有限制可使用该服务的上部协议。</p><p>RFC3513 (废弃了 RFC 2373)中，进一步对任播进行了定义：任播地址被分配给两个以上的接口 (一般指不同 IP 地址的节点) ，而发送到这个地址上的分组被路由到最近的接口。这里最近可以是指路由器跳数、服务器负载、服务器吞吐量、客户和服务器之间的往返时间 (RTT，round trip time)、链路的可用带宽等特征值 (metric) 决定。</p><p>在实际应用中，任播 (Anycast) 是一种网络寻址和路由的策略。Anycast 采用将一个单播地址分配到处于 Internet 中多个不同物理位置的主机上，发送到这个主机的报文被网络路由到路由协议度量的最近的目标主机上。</p><p>例如：在 IP 网络上通过一个 Anycast 地址标识一组提供特定服务的主机，同时服务访问方并不关心提供服务的具体是哪一台主机（比如：DNS 或者镜像服务），访问该地址的报文可以被 IP 网络路由到这一组目标中的任何一台主机上。</p><ul><li>任播与单播、广播和组播区别</li></ul><ol><li>在单播中，在网络地址和网络节点之间存在一一对应的关系。</li><li>在广播和多播中，网络地址和网络节点之间存在一对多的关系。每一个目的地址对应一群接收可以复制信息的节点。</li><li>在任播中，网络地址和网络节点之间存在一对多的关系。每一个地址对应一群接收节点，但在任何给定时间，只有其中之一可以接收到传送端来的信息。</li><li>在互联网中，通常使用边界网关协议来实现任播。</li></ol><ul><li>任播优点</li></ul><ol><li>不同客户端将访问不同目的主机，此过程对客户端透明，从而实现了目的主机的负载均衡。</li><li>当任意目的主机接入的网络出现故障，导致该目的主机不可达时，客户端请求可以在无人为干预的情况下自动被路由到目前可达的最近目的主机，在一定程度上为目标主机提供了冗余性。</li><li>当目的主机受到 DoS 攻击而无法到达时，由于网络不可到达，客户端请求也将路由到其它目的主机上。而在 DDoS 攻击时，由于任播的负载均衡效应，避免了单台目的主机承受所有攻击流量，因此在一定程度上为目的主机提高了安全性。</li><li>因为任播利用路由度量到最近的目的主机，提高了客户端响应速度。</li></ol><ul><li>任播缺点</li></ul><ol><li>使用任播中的共享单播地址不能作为客户端发起请求，因为请求的响应不一定能返回到发起的任播单播地址。因此，目前任播仅适合一些特定的上层协议。从目前的实际应用来看，任播最广泛的应用是 DNS 的部署。</li></ol><h4 id="什么是-bgp-anycast">什么是 BGP Anycast</h4><p>BGP Anycast 就是利用一个或多个 AS 号码在不同的地区广播相同的一个 IP 段。利用 BGP 的寻路原则，越短的 AS Path 会选成最优路径，从而优化了访问速度。</p><p>BGP Anycast 相较于 IP Anycast 多了一个 BGP AS，也就是说宣告的这段 IP 拥有独立的 AS 号，属于独立的自治域。</p><p>阿里的 DNS 就是一个典型的 BGP AnyCast 服务，本质上 BGP Anycast 就是不同服务器用了相同的 IP 地址。</p><h4 id="anycast-的实现原理">AnyCast 的实现原理</h4><ul><li>配置 AnyCast 节点组</li></ul><p><img src="https://www.hi-linux.com/img/linux/AnyCast05.png" alt=""></p><ol><li>配置三个节点的 AnyCast 组</li><li>组的三个节点都使用相同的 IP 地址 10.5.0.1</li></ol><ul><li>配置 AnyCast 的等价路由</li></ul><p><img src="https://www.hi-linux.com/img/linux/AnyCast06.png" alt=""></p><ol><li>客户端接入路由宣告相同目标地址三条路由</li><li>三条路由的度量值 (Metric) 值等价</li><li>可使用静态或动态协议</li></ol><ul><li>配置 AnyCast 的多路由节点</li></ul><p><img src="https://www.hi-linux.com/img/linux/AnyCast07.png" alt=""></p><ol><li>经过之前的配置连续的数据包可被送到不同的任播节点</li></ol><ul><li>AnyCast 不同源节点的路由选择</li></ul><p><img src="https://www.hi-linux.com/img/linux/AnyCast08.png" alt=""></p><ol><li>经过之前的配置来自不同节点的流量可被路由到不同的节点</li><li>接收数据包的服务器有单播路由决定</li><li>所以通讯模型适用于单个请求或单个响应的协议</li></ol><h4 id="域内基于-ipv4-的-anycast-服务">域内基于 IPv4 的 AnyCast 服务</h4><ul><li>地址的选择</li></ul><ol><li>目前的做法是从单播 IP 地址空间分配任播地址。</li><li>指定任播使用的子网（尽量小）。</li></ol><blockquote><p>利于域间的路由宣告。<br>常用的指定是 24。<br>子网有可能不会附加任何接口。</p></blockquote><ul><li>主机的配置</li></ul><ol><li>配置主机接收流量的任播地址（配置与回环口）</li><li>配置每个主机的唯一管理地址</li></ol><p>配置回环口地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ifconfig lo:1 10.5.0.1 netmask 255.255.255.255 up</span><br></pre></td></tr></table></figure><p>查看回环口地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ifconfig lo:1</span><br><span class="line">lo:1: flags=73  mtu 65536</span><br><span class="line">        inet 10.5.0.1  netmask 255.255.255.255</span><br><span class="line">        loop  txqueuelen 1  (Local Loopback)</span><br></pre></td></tr></table></figure><ul><li>服务的配置</li></ul><p>具体的业务配置（例如DNS），这里不详述。</p><ul><li>网络的配置</li></ul><p>A. 域内的配置</p><p>如果配置的服务完全在你的路由域内，则只需要考虑域内配置，条件如下：</p><ol><li>所有的任播节点都在域内</li><li>多个域内的位置</li></ol><blockquote><p>需要配置路由将任播流量传递给服务器。</p></blockquote><p>B. 静态的 IGP 路由配置</p><p><img src="https://www.hi-linux.com/img/linux/AnyCast09.png" alt=""></p><ol><li>在路由器上配置静态路由（主机路由），确保路由通过域传播。</li><li>方案优点: 无需服务中断即可重定向服务器。</li><li>方案缺点: 服务器故障没有状态检测。</li></ol><p>C. 动态的 IGP 路由配置</p><p><img src="https://www.hi-linux.com/img/linux/AnyCast10.png" alt=""></p><ol><li>在任播服务器上运行基于主机路由的守护进程，如 GateD、Zebra 或 Quagga。</li><li>任播服务器本身是路由的发起者。</li><li>当主机停机时，路由被自动撤销。</li><li>方案优点：可检测到任播服务器的状态。</li><li>方案缺点：任播服务器需要自建服务不可用路由自动撤销的机制。</li></ol><h4 id="域外基于-ipv4-的-anycast-服务">域外基于 IPv4 的 AnyCast 服务</h4><ul><li>配置原则</li></ul><ol><li>域外的实现基于 BGP 协议</li><li>域外的配置方式遵守传统 BGP 基本规则</li></ol><ul><li>网络设置步骤</li></ul><ol><li>宣告全球唯一的 AS 号</li><li>宣告直连的网段</li><li>宣告邻居路由的 AS 号和 IP 段</li></ol><ul><li>服务配置步骤</li></ul><ol><li>配置服务去监听任播 IP</li></ol><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="http://t.cn/RBkIvnV" target="_blank" rel="noopener">http://t.cn/RBkIvnV</a><br><a href="http://t.cn/RBkpQfh" target="_blank" rel="noopener">http://t.cn/RBkpQfh</a><br><a href="http://t.cn/8Ftlhp9" target="_blank" rel="noopener">http://t.cn/8Ftlhp9</a><br><a href="http://t.cn/RrAJNQ6" target="_blank" rel="noopener">http://t.cn/RrAJNQ6</a><br><a href="http://t.cn/RGCgN9e" target="_blank" rel="noopener">http://t.cn/RGCgN9e</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在讲解任播 (AnyCast) 前，我们先来说说 &lt;code&gt;TCP/IP&lt;/code&gt; 协议里常见的几种数据传输方式单播、组播、广播。&lt;/p&gt;
&lt;h3 id=&quot;什么是单播&quot;&gt;什么是单播&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/AnyCast01.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单播概念&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;单播（Unicast）是指封包在计算机网络的传输过程中，目的地址为单一目标的一种传输方式。每次只有两个实体相互通信，发送端和接收端都是唯一确定的。它是现今网络应用最为广泛，通常所使用的网络协议或服务大多采用单播传输，例如一切基于 TCP 的协议。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单播地址&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在 IPv4 网络中，0.0.0.0 到 223.255.255.255 属于单播地址。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单播优点&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;服务器及时响应客户机的请求。&lt;/li&gt;
&lt;li&gt;服务器针对每个客户不同的请求发送不同的数据，容易实现个性化服务。&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;单播缺点&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;服务器针对每个客户机发送数据流，服务器流量＝客户机数量×客户机流量。在客户数量大、每个客户机流量大的流媒体应用中服务器不堪重负。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;现有的网络带宽是金字塔结构，城际省际主干带宽仅仅相当于其所有用户带宽之和的 5％。如果全部使用单播协议，将造成网络主干不堪重负。现在的 P2P 应用就已经使主干经常阻塞，只要有 5％ 的客户在全速使用网络，其它用户网速将严重受损，而将主干网络带宽在短时间内扩展 20 倍几乎是不可能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="网络" scheme="https://www.hi-linux.com/categories/%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="网络" scheme="https://www.hi-linux.com/tags/%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>推荐一款支持 SQL/NoSQL 数据库的通用命令行工具 USQL</title>
    <link href="https://www.hi-linux.com/posts/38677.html"/>
    <id>https://www.hi-linux.com/posts/38677.html</id>
    <published>2018-06-22T01:00:00.000Z</published>
    <updated>2018-08-24T06:07:37.700Z</updated>
    
    <content type="html"><![CDATA[<p><code>USQL</code> 是一个使用 Go 语言开发的支持 SQL/NoSQL 数据库的通用命令行工具，支持多种主流的数据库软件。比如：PostgreSQL、MySQL、Oracle Database、SQLite3、Microsoft SQL Server 以及许多其它的数据库（包括 NoSQL 和非关系型数据库）。</p><p><code>USQL</code> 的灵感来自 PostgreSQL 的 PSQL，<code>USQL</code> 支持大多数 PSQL 的核心特性，比如：设置变量、反引号参数。并具有 PSQL 不支持的其它功能，如语法高亮、基于上下文的自动补全和多数据库支持等。</p><p>项目地址：<a href="https://github.com/xo/usql" target="_blank" rel="noopener">https://github.com/xo/usql</a></p><h3 id="安装-usql">安装 USQL</h3><p>由于 <code>USQL</code> 使用 Go 语言开发，具备了良好的跨平台特性。<code>USQL</code> 安装非常简单，官方也提供二进制、Homebrew、Scoop等多种安装方式。这里我们就使用最具通用性的二进制方式安装，以 Linux 平台为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ wget https://github.com/xo/usql/releases/download/v0.7.0/usql-0.7.0-linux-amd64.tar.bz2</span><br><span class="line">$ tar xjvf usql-0.7.0-linux-amd64.tar.bz2</span><br><span class="line">$ sudo mv usql /usr/local/bin</span><br></pre></td></tr></table></figure><p>如果你使用其它平台，可根据实际情况在官方<a href="https://github.com/xo/usql/releases" target="_blank" rel="noopener">下载页面</a>下载对应版本。</p><a id="more"></a><h3 id="usql-用法">USQL 用法</h3><ul><li>USQL 命令行语法</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ usql --help</span><br><span class="line">usql, the universal command-line interface for SQL databases.</span><br><span class="line"></span><br><span class="line">usql 0.7.0</span><br><span class="line">Usage: usql [--command COMMAND] [--file FILE] [--output OUTPUT] [--username USERNAME] [--password] [--no-password] [--no-rc] [--single-transaction] [--set SET] DSN</span><br><span class="line"></span><br><span class="line">Positional arguments:</span><br><span class="line">  DSN                    database url</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --command COMMAND, -c COMMAND</span><br><span class="line">                         run only single command (SQL or internal) and exit</span><br><span class="line">  --file FILE, -f FILE   execute commands from file and exit</span><br><span class="line">  --output OUTPUT, -o OUTPUT</span><br><span class="line">                         output file</span><br><span class="line">  --username USERNAME, -U USERNAME</span><br><span class="line">                         database user name [default: mike]</span><br><span class="line">  --password, -W         force password prompt (should happen automatically)</span><br><span class="line">  --no-password, -w      never prompt for password</span><br><span class="line">  --no-rc, -X            do not read start up file</span><br><span class="line">  --single-transaction, -1</span><br><span class="line">                         execute as a single transaction (if non-interactive)</span><br><span class="line">  --set SET, -v SET      set variable NAME=VALUE</span><br><span class="line">  --help, -h             display this help and exit</span><br><span class="line">  --version              display version and exit</span><br></pre></td></tr></table></figure><ul><li>USQL 支持的反斜杠命令</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">$ usql</span><br><span class="line">Type &quot;help&quot; for help.</span><br><span class="line"></span><br><span class="line">(not connected)=&gt; \?</span><br><span class="line">General</span><br><span class="line">  \q                    quit usql</span><br><span class="line">  \copyright            show usql usage and distribution terms</span><br><span class="line">  \drivers              display information about available database drivers</span><br><span class="line">  \g [FILE] or ;        execute query (and send results to file or |pipe)</span><br><span class="line">  \gexec                execute query and execute each value of the result</span><br><span class="line">  \gset [PREFIX]        execute query and store results in usql variables</span><br><span class="line"></span><br><span class="line">Help</span><br><span class="line">  \? [commands]         show help on backslash commands</span><br><span class="line">  \? options            show help on usql command-line options</span><br><span class="line">  \? variables          show help on special variables</span><br><span class="line"></span><br><span class="line">Query Buffer</span><br><span class="line">  \e [FILE] [LINE]      edit the query buffer (or file) with external editor</span><br><span class="line">  \p                    show the contents of the query buffer</span><br><span class="line">  \raw                  show the raw (non-interpolated) contents of the query buffer</span><br><span class="line">  \r                    reset (clear) the query buffer</span><br><span class="line">  \w FILE               write query buffer to file</span><br><span class="line"></span><br><span class="line">Input/Output</span><br><span class="line">  \echo [STRING]        write string to standard output</span><br><span class="line">  \i FILE               execute commands from file</span><br><span class="line">  \ir FILE              as \i, but relative to location of current script</span><br><span class="line"></span><br><span class="line">Transaction</span><br><span class="line">  \begin                begin a transaction</span><br><span class="line">  \commit               commit current transaction</span><br><span class="line">  \rollback             rollback (abort) current transaction</span><br><span class="line"></span><br><span class="line">Connection</span><br><span class="line">  \c URL                connect to database with url</span><br><span class="line">  \c DRIVER PARAMS...   connect to database with SQL driver and parameters</span><br><span class="line">  \Z                    close database connection</span><br><span class="line">  \password [USERNAME]  change the password for a user</span><br><span class="line">  \conninfo             display information about the current database connection</span><br><span class="line"></span><br><span class="line">Operating System</span><br><span class="line">  \cd [DIR]             change the current working directory</span><br><span class="line">  \setenv NAME [VALUE]  set or unset environment variable</span><br><span class="line">  \! [COMMAND]          execute command in shell or start interactive shell</span><br><span class="line"></span><br><span class="line">Variables</span><br><span class="line">  \prompt [TEXT] NAME   prompt user to set internal variable</span><br><span class="line">  \set [NAME [VALUE]]   set internal variable, or list all if no parameters</span><br><span class="line">  \unset NAME           unset (delete) internal variable</span><br></pre></td></tr></table></figure><ul><li>USQL 目前支持的数据库类型</li></ul><table><thead><tr><th>Database (scheme/driver)</th><th>Protocol Aliases [real driver]</th></tr></thead><tbody><tr><td>Microsoft SQL Server (mssql)</td><td>ms, sqlserver</td></tr><tr><td>MySQL (mysql)</td><td>my, mariadb, maria, percona, aurora</td></tr><tr><td>Oracle (ora)</td><td>or, oracle, oci8, oci</td></tr><tr><td>PostgreSQL (postgres)</td><td>pg, postgresql, pgsql</td></tr><tr><td>SQLite3 (sqlite3)</td><td>sq, sqlite, file</td></tr><tr><td>Amazon Redshift (redshift)</td><td>rs [postgres]</td></tr><tr><td>CockroachDB (cockroachdb)</td><td>cr, cockroach, crdb, cdb [postgres]</td></tr><tr><td>MemSQL (memsql)</td><td>me [mysql]</td></tr><tr><td>TiDB (tidb)</td><td>ti [mysql]</td></tr><tr><td>Vitess (vitess)</td><td>vt [mysql]</td></tr><tr><td>Google Spanner (spanner)</td><td>gs, google, span (not yet public)</td></tr><tr><td>MySQL (mymysql)</td><td>zm, mymy</td></tr><tr><td>PostgreSQL (pgx)</td><td>px</td></tr><tr><td>Apache Avatica (avatica)</td><td>av, phoenix</td></tr><tr><td>Apache Ignite (ignite)</td><td>ig, gridgain</td></tr><tr><td>Cassandra (cql)</td><td>ca, cassandra, datastax, scy, scylla</td></tr><tr><td>ClickHouse (clickhouse)</td><td>ch</td></tr><tr><td>Couchbase (n1ql)</td><td>n1, couchbase</td></tr><tr><td>Cznic QL (ql)</td><td>ql, cznic, cznicql</td></tr><tr><td>Firebird SQL (firebirdsql)</td><td>fb, firebird</td></tr><tr><td>Microsoft ADODB (adodb)</td><td>ad, ado</td></tr><tr><td>ODBC (odbc)</td><td>od</td></tr><tr><td>OLE ODBC (oleodbc)</td><td>oo, ole, oleodbc [adodb]</td></tr><tr><td>Presto (presto)</td><td>pr, prestodb, prestos, prs, prestodbs</td></tr><tr><td>SAP HANA (hdb)</td><td>sa, saphana, sap, hana</td></tr><tr><td>Snowflake (snowflake)</td><td>sf</td></tr><tr><td>VoltDB (voltdb)</td><td>vo, volt, vdb</td></tr></tbody></table><h3 id="usql-使用实例">USQL 使用实例</h3><h4 id="连接到不同数据库">连接到不同数据库</h4><p>这里演示几个常用数据库连接方法，其它数据库也类似：</p><ul><li>连接到一个 MySQL 数据库</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 使用默认信息连接到数据库</span><br><span class="line">$ usql my://</span><br><span class="line"># 使用用户名和密码连接到指定的数据库</span><br><span class="line">$ usql my://user:pass@host/dbname</span><br><span class="line"># 使用用户名、密码和端口连接到指定的数据库</span><br><span class="line">$ usql mysql://user:pass@host:port/dbname</span><br><span class="line"># 使用指定的套接字连接数据库</span><br><span class="line">$ usql /var/run/mysqld/mysqld.sock</span><br></pre></td></tr></table></figure><ul><li>连接到一个 PostgreSQL 数据库</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 使用默认信息连接到数据库</span><br><span class="line">$ usql pg://</span><br><span class="line"># 使用用户名和密码连接到指定的数据库</span><br><span class="line">$ usql pg://user:pass@host/dbname</span><br><span class="line"># 使用用户名、密码和端口连接到指定的数据库</span><br><span class="line">$ usql postgres://user:pass@host:port/dbname</span><br><span class="line"># 使用指定的套接字连接数据库</span><br><span class="line">$ usql /var/run/postgresql</span><br></pre></td></tr></table></figure><ul><li>连接到一个 Oracle 数据库</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 使用默认信息连接到数据库</span><br><span class="line">$ usql or://</span><br><span class="line"># 使用用户名和密码连接到指定的数据库</span><br><span class="line">$ usql or://user:pass@host/sid</span><br><span class="line"># 使用用户名、密码和端口连接到指定的数据库</span><br><span class="line">$ usql oracle://user:pass@host:port/sid</span><br></pre></td></tr></table></figure><ul><li>连接到一个 SQL Server 数据库</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 使用默认信息连接到数据库</span><br><span class="line">$ usql ms://</span><br><span class="line"># 使用用户名和密码连接到指定的数据库</span><br><span class="line">$ usql ms://user:pass@host/dbname</span><br><span class="line"># 使用用户名、密码和端口连接到指定的数据库</span><br><span class="line">$ usql mssql://user:pass@host:port/dbname</span><br><span class="line"># 使用用户名和密码连接到指定实例的数据库</span><br><span class="line">$ usql ms://user:pass@host/instancename/dbname</span><br></pre></td></tr></table></figure><h4 id="执行查询和命令">执行查询和命令</h4><p>以下例子均在 MySQL 数据库环境以执行。</p><ul><li>创建一个名为 test 的数据库，并在这个数据库中建立的名为 test 的表中新增一行数据。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ usql my://root:000000@localhost</span><br><span class="line">Connected with driver mysql (5.7.18-log)</span><br><span class="line">Type &quot;help&quot; for help.</span><br><span class="line"></span><br><span class="line">my:root@localhost=&gt; create database test;</span><br><span class="line">CREATE DATABASE 1</span><br><span class="line">my:root@localhost=&gt; use test;</span><br><span class="line">USE</span><br><span class="line">my:root@localhost=&gt; CREATE TABLE IF NOT EXISTS `test`(</span><br><span class="line">my:root@localhost(&gt;    `test_id` INT,</span><br><span class="line">my:root@localhost(&gt;    `name` VARCHAR(100) NOT NULL</span><br><span class="line">my:root@localhost(&gt; )ENGINE=InnoDB DEFAULT CHARSET=utf8;</span><br><span class="line">CREATE TABLE</span><br><span class="line">my:root@localhost=&gt; insert into test (test_id, name) values (1, &apos;hello&apos;);</span><br><span class="line">INSERT 1</span><br><span class="line">my:root@localhost=&gt; select * from test;</span><br><span class="line">  test_id | name</span><br><span class="line">+---------+-------+</span><br><span class="line">        1 | hello</span><br><span class="line">(1 rows)</span><br></pre></td></tr></table></figure><ul><li>连接到一个 MySQL 数据库并运行一个名为 script.sql 的脚本</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ usql my://root:000000@localhost/wordpress -f script.sql</span><br><span class="line"> id  | post_author |           post_date</span><br><span class="line">+-----+-------------+-------------------------------+</span><br><span class="line">  673 |           1 | 2007-01-22 06:31:27 +0800 CST</span><br><span class="line">  675 |           1 | 2007-01-22 06:36:39 +0800 CST</span><br><span class="line">(2 rows)</span><br></pre></td></tr></table></figure><p>接下来我们再举几个使用内置命令进行操作的例子。</p><ul><li>打印并执行缓充区中的 SQL 语句</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">my:root@localhost=&gt; select *</span><br><span class="line">my:root@localhost-&gt; from test</span><br><span class="line">my:root@localhost-&gt; \p</span><br><span class="line">select *</span><br><span class="line">from test</span><br><span class="line">my:root@localhost-&gt; \g</span><br><span class="line">  test_id | name</span><br><span class="line">+---------+-------+</span><br><span class="line">        1 | hello</span><br><span class="line">(1 rows)</span><br></pre></td></tr></table></figure><ul><li>快速切换到另一个数据库连接</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ms:mike@192.168.100.210:1433/testdb=&gt; \c my://root:000000@localhost/wordpress</span><br><span class="line">Connected with driver mysql (5.7.18-log)</span><br><span class="line">my:root@localhost/wordpress=&gt; select * from wp_postmeta limit 5;</span><br><span class="line">  meta_id | post_id | meta_key | meta_value</span><br><span class="line">+---------+---------+----------+------------+</span><br><span class="line">        1 |       1 | views    |     185245</span><br><span class="line">        2 |       2 | views    |       5659</span><br><span class="line">        3 |       3 | views    |       2197</span><br><span class="line">        4 |       4 | views    |       1739</span><br><span class="line">        5 |       5 | views    |       2002</span><br><span class="line">(5 rows)</span><br></pre></td></tr></table></figure><ul><li>使用变量进行条件查询</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 设置变量</span><br><span class="line">my:root@localhost/wordpress=&gt; \set meta 2179</span><br><span class="line"># 查看当前已设置的变量</span><br><span class="line">my:root@localhost/wordpress=&gt; \set</span><br><span class="line">meta = &apos;2179&apos;</span><br><span class="line"># 使用变量做为条件进行查询</span><br><span class="line">my:root@localhost/wordpress=&gt; select * from wp_postmeta where meta_value=:&apos;meta&apos;;</span><br><span class="line">  meta_id | post_id | meta_key | meta_value</span><br><span class="line">+---------+---------+----------+------------+</span><br><span class="line">      156 |     163 | views    |       2179</span><br><span class="line">      203 |     211 | views    |       2179</span><br><span class="line">(2 rows)</span><br></pre></td></tr></table></figure><blockquote><p>变量调用支持 :NAME、:‘NAME’、和 :“NAME” 这三种方式进行调用。</p></blockquote><ul><li>使用反引号参数</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">my:root@localhost/wordpress=&gt; \echo Welcome `echo $USER` -- &apos;currently:&apos; &quot;(&quot; `date` &quot;)&quot;</span><br><span class="line">Welcome root -- currently: ( Tue Jun 19 13:47:31 CST 2018 )</span><br></pre></td></tr></table></figure><p>反引号参数的执行结果也可以直接设置为一个变量的值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">my:root@localhost=&gt; \set MYVAR `date &quot;+%Y-%m-%d&quot;`</span><br><span class="line">my:root@localhost=&gt; \echo :MYVAR</span><br><span class="line">2018-06-19</span><br><span class="line">my:root@localhost=&gt; select id,post_author,post_date  from wp_posts where post_date &lt; :&apos;MYVAR&apos;  limit 2;</span><br><span class="line">  id  | post_author |           post_date</span><br><span class="line">+-----+-------------+-------------------------------+</span><br><span class="line">  673 |           1 | 2007-01-22 06:31:27 +0800 CST</span><br><span class="line">  675 |           1 | 2007-01-22 06:36:39 +0800 CST</span><br><span class="line">(2 rows)</span><br></pre></td></tr></table></figure><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="https://github.com/xo/usql" target="_blank" rel="noopener">https://github.com/xo/usql</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;code&gt;USQL&lt;/code&gt; 是一个使用 Go 语言开发的支持 SQL/NoSQL 数据库的通用命令行工具，支持多种主流的数据库软件。比如：PostgreSQL、MySQL、Oracle Database、SQLite3、Microsoft SQL Server 以及许多其它的数据库（包括 NoSQL 和非关系型数据库）。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;USQL&lt;/code&gt; 的灵感来自 PostgreSQL 的 PSQL，&lt;code&gt;USQL&lt;/code&gt; 支持大多数 PSQL 的核心特性，比如：设置变量、反引号参数。并具有 PSQL 不支持的其它功能，如语法高亮、基于上下文的自动补全和多数据库支持等。&lt;/p&gt;
&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/xo/usql&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/xo/usql&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装-usql&quot;&gt;安装 USQL&lt;/h3&gt;
&lt;p&gt;由于 &lt;code&gt;USQL&lt;/code&gt; 使用 Go 语言开发，具备了良好的跨平台特性。&lt;code&gt;USQL&lt;/code&gt; 安装非常简单，官方也提供二进制、Homebrew、Scoop等多种安装方式。这里我们就使用最具通用性的二进制方式安装，以 Linux 平台为例：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ wget https://github.com/xo/usql/releases/download/v0.7.0/usql-0.7.0-linux-amd64.tar.bz2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ tar xjvf usql-0.7.0-linux-amd64.tar.bz2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo mv usql /usr/local/bin&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果你使用其它平台，可根据实际情况在官方&lt;a href=&quot;https://github.com/xo/usql/releases&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;下载页面&lt;/a&gt;下载对应版本。&lt;/p&gt;
    
    </summary>
    
      <category term="工具" scheme="https://www.hi-linux.com/categories/%E5%B7%A5%E5%85%B7/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="工具" scheme="https://www.hi-linux.com/tags/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
  <entry>
    <title>推荐几款超好用的 Alfred 插件</title>
    <link href="https://www.hi-linux.com/posts/8397.html"/>
    <id>https://www.hi-linux.com/posts/8397.html</id>
    <published>2018-06-20T01:00:00.000Z</published>
    <updated>2018-08-24T06:08:27.902Z</updated>
    
    <content type="html"><![CDATA[<p>Alfred 可以说是 Mac 上必装的神器，作为 Mac 上最强大的效率工具 Alfred 早已不仅仅是最开始的快速启动与搜索工具。 它的 Workflow 扩展功能，让它成为了一个拥有无限自动化潜力的「工具台」软件，你可以用它来实现你的一切有关自动化的想法。</p><p>下面这张官方图中标出了 Alfred 的所有功能，从中你可以直观的感受到它的强大。</p><p><img src="https://www.hi-linux.com/img/linux/alfred19.png" alt=""></p><h3 id="获取-alfred">获取 Alfred</h3><p>从 Alfred 官网就可以下载 Alfred 安装程序，安装完成即可使用。不过 Alfred 将功能分为免费和付费两大部分：</p><ul><li>免费用户只能使用其 Features 中的功能（即基本搜索和快速启动应用等功能，其实这已满足非重度使用者日常需求）。</li><li>若要使用 Workflows (即自定义插件的工作流)，则需要激活 Powerpack 功能。</li></ul><p>激活 Powerpack 功能的正常途径当然是在官方购买序列号，当然你也可以通过一些技术手段获得此功能，不过还是强烈建议读者购买并使用正版。</p><a id="more"></a><p>激活 Powerpack 后，你可以在设置界面的 Powerpack 子界面中看到类似下图。这样就表明可以使用 Alfred 的所有功能，当然也包括工作流。</p><p><img src="https://www.hi-linux.com/img/linux/alfred1.png" alt=""></p><p>如果你还不知道 Alfred 基本功能如何使用，推荐先读下 「<a href="https://sspai.com/post/34468" target="_blank" rel="noopener">Alfred 3.0 新版详解</a>」 和 「<a href="https://sspai.com/post/27900" target="_blank" rel="noopener">OS X 效率启动器 Alfred 详解与使用技巧</a>」 这两篇入门文章。</p><p>今天主要给大家推荐几款我长期使用且功能异常强大和好用的 Alfred 插件。</p><h3 id="secure-shell">Secure SHell</h3><p>一个可在 Alfred 上快速打开 SSH/SFTP/mosh 链接的插件，其功能非常的强大。</p><p>插件官方地址：<a href="https://github.com/deanishe/alfred-ssh" target="_blank" rel="noopener">https://github.com/deanishe/alfred-ssh</a></p><p>如果你和我一样经常使用 SSH 访问服务器,相信这会是你的不二之选，通过这个插件也弥补了 iTerm2 不方便管理多主机的功能。插件效果如下：</p><p><img src="https://www.hi-linux.com/img/linux/alfred20.gif" alt=""></p><p>该插件默认启动关键字是：<code>ssh</code>。默认情况下，在选中的 SSH 链接上直接回车后就会在终端中访问对应的 SSH 服务器。</p><p>如果在选中的指定链接上配合一些功能键，就可以实现以下更多的功能：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">⌘+↩ — 打开一个 SFTP 链接。</span><br><span class="line">⌥+↩ — 使用 mosh 访问当前 SSH 链接。</span><br><span class="line">⇧+↩ — 使用 Ping 命令测试当前主机的连通性。</span><br><span class="line">^+↩ — 从历史记录中清出之前访问过的连接信息。</span><br></pre></td></tr></table></figure><p>Secure SHell 默认支持从以下文件中读出 SSH 链接配置信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">~/.ssh/config</span><br><span class="line">~/.ssh/known_hosts</span><br><span class="line">/etc/hosts</span><br><span class="line">/etc/ssh/ssh_config</span><br><span class="line">History (用户以前输入的用户名+主机地址)</span><br></pre></td></tr></table></figure><p>Secure SHell 默认情况下已经非常的强大和方便了，如果结合上 SSH 用户配置文件，你会发现更加完美。如果你还不知道如何用 SSH 用户配置文件管理多主机，可参考「<a href="https://mp.weixin.qq.com/s/gaeu6nGxxQPRbbbWbwDiiA" target="_blank" rel="noopener">利用SSH的用户配置文件Config管理SSH会话</a>」一文。</p><h3 id="terminalfinder">TerminalFinder</h3><p>一个可以在 Terminal 和 Finder 间快速切换的插件。</p><p>插件官方地址：<a href="https://github.com/LeEnno/alfred-terminalfinder" target="_blank" rel="noopener">https://github.com/LeEnno/alfred-terminalfinder</a></p><p>TerminalFinder 的主要作用是在 Finder 和 Terminal 间快速切换到当前目录。TerminalFinder 使用非常简单，只需在 Alfred 输入以下关键字就可进行快速切换:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ft: 在 Terminal 打开当前 Finder 中的目录</span><br><span class="line">tf: 在 Finder 打开当前 Terminal 中的目录</span><br><span class="line">fi: 在 iTerm 中打开当前 Finder 中的目录</span><br><span class="line">if: 在 Finder 中打开当前 iTerm 中的目录</span><br><span class="line"></span><br><span class="line">pt: 在 Terminal 打开当前 Path Finder 中的目录</span><br><span class="line">tp: 在 Path Finder 打开当前 Terminal 中的目录</span><br><span class="line">pi: 在 iTerm 中打开当前 Path Finder 中的目录</span><br><span class="line">ip: 在 Path Finder 中打开当前 iTerm 中的目录</span><br></pre></td></tr></table></figure><p>比如你要在 iTerm 中打开当前 Finder 中的目录，在 Alfred 上输入 <code>fi</code> 就可以了。</p><p><img src="https://www.hi-linux.com/img/linux/alfred10.png" alt=""></p><h3 id="password-generator">Password Generator</h3><p>一个快速随机密码生成器。</p><p>插件官方地址：<a href="https://github.com/deanishe/alfred-pwgen" target="_blank" rel="noopener">https://github.com/deanishe/alfred-pwgen</a></p><p>如果你需要经常生成随机密码，Password Generator 会是一个不错的选择。插件效果如下：</p><p><img src="https://www.hi-linux.com/img/linux/alfred21.gif" alt=""></p><p>该插件默认提供：<code>pwgen</code>、<code>pwlen</code>、<code>pwconf</code> 三个启动关键字。<code>pwgen</code> 默认情况下会生成一组随机密码并显示其长度信息，此时只需在选中的条目上直接回车便可以方便将其复制。</p><p><img src="https://www.hi-linux.com/img/linux/alfred16.png" alt=""></p><p>如果你想对密码生成规则进行定制，只需输入 <code>pwconf</code> 就可以进入 Password Generator 配置界面。Password Generator 提供的配置功能非常丰富，可以根据不同需求进行定制。</p><p><img src="https://www.hi-linux.com/img/linux/alfred17.png" alt=""></p><p>如果你想快速生成一个指定长度的随机密码，此时你可以用 <code>pwlen</code> 来实现。比如要生成一个密码长度为 10 位的随机密码，此时只需输入 <code>pwlen 10</code>：</p><p><img src="https://www.hi-linux.com/img/linux/alfred18.png" alt=""></p><blockquote><p><code>pwlen</code> 默认是生成一个长度为 20 位的随机密码的。</p></blockquote><p>Password Generator 功能非常强大，远不止这里介绍的这一点。官方文档对其一些高级功能的使用已经做了详细说明，这里就不在重复累述了。</p><h3 id="new-files">New Files</h3><p>一个可以在任意位置快速创建新文档和文件夹的插件。</p><p>插件官方地址：<a href="https://github.com/cpimhoff/alfred3-newFiles" target="_blank" rel="noopener">https://github.com/cpimhoff/alfred3-newFiles</a></p><p>默认情况下，macOS 不能通过右键菜单来完成新文件的创建。macOS 这样的设计是鼓励用户都通过应用程序来创建所需的文件，而不是通过创建文件来打开应用。如果你之前一直是个 Windows 用户，对这样的设计应该很不习惯。New Files 就是用来解决这个问题的，有了 New Files 你就可以在任意位置新建你所需文件格式的文档。</p><p>该插件默认启动关键字是：<code>new</code>。默认情况下，你可以直接新建 TXT、MD、自定义文件类型这三种类型的文件和目录。其中 TXT、MD 格式也是比较常用的文本格式，插件就将这两种类型的文件直接作为了默认文件类型。真是非常贴心！</p><p><img src="https://www.hi-linux.com/img/linux/alfred2.png" alt=""></p><p>New Files 的功能非常强大，不但可以随时随地的新建需要的文档类型文档，而且还可以让你在新建文档的时候选择新建文档的位置、是否将剪贴版里的内容直接放入到新建的文档中。</p><p>下面我们就来看看如何快速建立一个 MD 格式的文件：</p><p><img src="https://www.hi-linux.com/img/linux/alfred4.png" alt=""></p><p>输入新建文件的文件名：</p><p><img src="https://www.hi-linux.com/img/linux/alfred5.png" alt=""></p><p>选择文件保存的位置：</p><p><img src="https://www.hi-linux.com/img/linux/alfred6.png" alt=""></p><p>并把剪贴版中内容自动粘贴到新建的文件中：</p><p><img src="https://www.hi-linux.com/img/linux/alfred7.png" alt=""></p><p>完成文件创建后，插件还会自动用文件类型默认关联的程序打开新建的文件。</p><p><img src="https://www.hi-linux.com/img/linux/alfred8.png" alt=""></p><p>经过以上简单几步，一个指定文件格式且包含剪贴版里内容的文件就建立完成了，有没有很方便呢？</p><p>如果你要自定义常用的文件类型也是可以的，直接在插件的设置界面进行增加即可。</p><p><img src="https://www.hi-linux.com/img/linux/alfred3.png" alt=""></p><p>如果你要自定义常用的文件存放路径当然也是可以的，同样在插件的设置界面进行增加即可了。</p><p><img src="https://www.hi-linux.com/img/linux/alfred9.png" alt=""></p><h3 id="recent-items">Recent Items</h3><p>显示最近访问过的文件、文件夹、应用程序、文档、下载内容等。</p><p>插件官方地址：<a href="http://t.cn/RmFG5Zy" target="_blank" rel="noopener">http://t.cn/RmFG5Zy</a></p><p>该插件默认启动关键字是：<code>rec</code>。通过以下关键字可以快速访问相对应的类型。</p><p><img src="https://www.hi-linux.com/img/linux/alfred11.png" alt=""></p><p>各关键字对应的分类说明：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">now: 显示当前使用过的文档</span><br><span class="line">fol: 显示最近使用过的目录</span><br><span class="line">apps: 显示最近使用过的应用程序</span><br><span class="line">docs: 显示最近使用过的文档</span><br><span class="line">dow: 显示最近下载过的文件</span><br><span class="line">fav: 显示最近收藏过的项目</span><br><span class="line">c1: 插件默认预留关键字用作自定义分类项目</span><br><span class="line">c2: 插件默认预留关键字用作自定义分类项目</span><br></pre></td></tr></table></figure><p>默认情况下在选中条目上直接回车会打开文件或者目录，如果在选中的条目上配合一些功能键，就可以实现以下更多的功能：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">⌘+↩ — 在 Finder 中显示文件或文件夹</span><br><span class="line">⌥+↩ — 将文件/目录的路径传递到打开/保存对话框或者Finder窗口</span><br><span class="line">⇧+↩ — 快速预览选中的文件或者目录</span><br><span class="line">^+↩ — 将选中的文件或者目录加入或移出收藏夹</span><br></pre></td></tr></table></figure><p>比如你要在 Alfred 中显示最近使用过的应用，在 Alfred 上输入 <code>rec apps</code> 就可以了。</p><p><img src="https://www.hi-linux.com/img/linux/alfred12.png" alt=""></p><h3 id="hidden-files">Hidden Files</h3><p>一个可以在 Finder 中快速切换是否显示隐藏文件的插件。</p><p>该插件默认启动关键字是：<code>hide</code>。使用非常简单，效果如下图：</p><p><img src="https://www.hi-linux.com/img/linux/alfred13.png" alt=""></p><p>此时在 Finder 中打开的目录中就会显示隐藏文件。</p><p><img src="https://www.hi-linux.com/img/linux/alfred14.png" alt=""></p><p>在显示隐藏文件状态下，再次输入 <code>hide</code> 就可以在 Finder 中不显示隐藏文件。</p><p><img src="https://www.hi-linux.com/img/linux/alfred15.png" alt=""></p><p>通常我都在是终端下查看隐藏文件的，这个插件相对使用频率就要低得多了。插件下载地址暂时也找不到了，如果你觉得插件对你有用，可留下联系方式单独发给你！</p><p>此次推荐的几款 Alfred 插件就介绍完了。如果你有什么更好用的 Alfred 插件推荐，欢迎积极留言哟！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Alfred 可以说是 Mac 上必装的神器，作为 Mac 上最强大的效率工具 Alfred 早已不仅仅是最开始的快速启动与搜索工具。 它的 Workflow 扩展功能，让它成为了一个拥有无限自动化潜力的「工具台」软件，你可以用它来实现你的一切有关自动化的想法。&lt;/p&gt;
&lt;p&gt;下面这张官方图中标出了 Alfred 的所有功能，从中你可以直观的感受到它的强大。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://www.hi-linux.com/img/linux/alfred19.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;获取-alfred&quot;&gt;获取 Alfred&lt;/h3&gt;
&lt;p&gt;从 Alfred 官网就可以下载 Alfred 安装程序，安装完成即可使用。不过 Alfred 将功能分为免费和付费两大部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;免费用户只能使用其 Features 中的功能（即基本搜索和快速启动应用等功能，其实这已满足非重度使用者日常需求）。&lt;/li&gt;
&lt;li&gt;若要使用 Workflows (即自定义插件的工作流)，则需要激活 Powerpack 功能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;激活 Powerpack 功能的正常途径当然是在官方购买序列号，当然你也可以通过一些技术手段获得此功能，不过还是强烈建议读者购买并使用正版。&lt;/p&gt;
    
    </summary>
    
      <category term="macOS" scheme="https://www.hi-linux.com/categories/macOS/"/>
    
    
      <category term="macOS" scheme="https://www.hi-linux.com/tags/macOS/"/>
    
      <category term="Alfred" scheme="https://www.hi-linux.com/tags/Alfred/"/>
    
  </entry>
  
  <entry>
    <title>又一款命令行下交互式 Docker 容器管理工具 Dockly</title>
    <link href="https://www.hi-linux.com/posts/45929.html"/>
    <id>https://www.hi-linux.com/posts/45929.html</id>
    <published>2018-06-19T01:00:00.000Z</published>
    <updated>2018-08-24T06:04:55.783Z</updated>
    
    <content type="html"><![CDATA[<p>在不久前给大家推荐过一款命令行下交互式 <code>Docker</code> 容器管理工具 <a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247485950&amp;idx=1&amp;sn=fe6639a5c9e7c3950157701c203212b5&amp;chksm=eac528d7ddb2a1c1e8b4984cbf7ed14a0217714c1ce0d7359ac97f97f3f86d9beff53589bdf8#rd" target="_blank" rel="noopener">Dry</a>，今天再给大家介绍一款与其类似的 <code>Docker</code> 容器管理工具 <code>Dockly</code>。</p><p><code>Dockly</code> 是一个使用 <code>Node.js</code> 编写的开源软件，功能上和 <a href="https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;mid=2247485950&amp;idx=1&amp;sn=fe6639a5c9e7c3950157701c203212b5&amp;chksm=eac528d7ddb2a1c1e8b4984cbf7ed14a0217714c1ce0d7359ac97f97f3f86d9beff53589bdf8#rd" target="_blank" rel="noopener">Dry</a> 类似。<code>Dockly</code> 可以很方便的通过 <code>NPM</code> 进行安装，并支持在 Linux、macOS 和 Windows 上运行。</p><p>项目地址：<a href="https://github.com/lirantal/dockly" target="_blank" rel="noopener">https://github.com/lirantal/dockly</a></p><h3 id="安装-dockly">安装 Dockly</h3><p>由于 <code>Dockly</code> 是使用 <code>Node.js</code> 开发的， <code>Dockly</code> 对 <code>Node.js</code> 版本有以下要求: <code>Node.js</code> &gt;= 7.6，<code>NPM</code> &gt;= 3.6。目前比较流行的几个 Linux 发行版默认仓库中的 <code>Node.js</code> 版本都比较低，比如 Ubuntu 16.04 中的  <code>Node.js</code> 版本为 4.x。</p><p>在安装 <code>Dockly</code> 前，我们首先需要用官方源来安装较新版本的 <code>Node.js</code>。这里以 Ubuntu 为例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -</span><br><span class="line">$ sudo apt-get install -y nodejs</span><br></pre></td></tr></table></figure><p>其它发行版本的安装方法可参考<a href="https://nodejs.org/en/download/package-manager/" target="_blank" rel="noopener">官方安装文档</a>，这里就不再详述了。在完成 <code>Node.js</code> 的安装后， 接下来我们只需用 <code>NPM</code> 工具就可以很容易的完成 <code>Dockly</code> 的安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo npm install -g dockly</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="运行-dockly">运行 Dockly</h3><p>启动 <code>Dockly</code> 的方法很简单，只需在命令行下直接运行就可以了。它会通过默认的 Unix Socket 自动连接到本地 <code>Docker</code> 守护进程。如果你 <code>Docker</code> 的 Unix Socket 不在默认位置，可以使用 <code>-s</code> 或 <code>--socketPath</code> 参数指定。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ dockly</span><br></pre></td></tr></table></figure><p><img src="https://www.hi-linux.com/img/linux/dockly1.png" alt=""></p><h3 id="使用-dockly">使用 Dockly</h3><p><code>Dockly</code> 默认运行界面会显示所有容器的一些基本信息，比如：容器的 ID、容器的名称、容器所使用的镜像、容器运行的命令、容器的状态等信息。</p><p><img src="https://www.hi-linux.com/img/linux/dockly2.png" alt=""></p><ul><li>管理 Docker 容器</li></ul><p><code>Dockly</code> 对容器管理提供了查看、重启、删除容器等操作。</p><p>在选定容器上按下 <code>i</code> 键即可查看容器的相关信息。</p><p><img src="https://www.hi-linux.com/img/linux/dockly4.png" alt=""></p><p>在选定容器上按下 <code>m</code> 键即可对容器进行一些管理。</p><p><img src="https://www.hi-linux.com/img/linux/dockly5.png" alt=""></p><ul><li>查看 Docker 容器日志</li></ul><p>如果你在 <code>Dockly</code> 主界面选中的容器上单击回车键，你就可以实时查看该容器的日志。</p><p><img src="https://www.hi-linux.com/img/linux/dockly3.png" alt=""></p><ul><li>其它操作</li></ul><p><code>Dockly</code> 操作主要通过下方提供的快捷菜单来实现，主要有以下这些：</p><ol><li>按下 <code>=</code> 键即可对当前界面进行刷新。</li><li>按下 <code>l</code> 键即可快速进入当前容器的 Shell。</li><li>按下 <code>r</code> 键即可快速重启当前容器。</li><li>按下 <code>s</code> 键即可快速停止当前容器。</li><li>按下 <code>v</code> 键即可快速进入查看模式。</li><li>按下 <code>q</code> 键即可退出 Dockly。</li><li>按下 <code>h</code> 键即可查询帮助来获得更详细的使用说明。</li></ol><p><code>Dockly</code> 虽然目前在整体功能上并没有 <code>Dry</code> 强大，但是在命令行下交互式  <code>Docker</code> 管理工具上也算给了我们更多一种选择吧。指不定哪一天就发展强大了呢，开源世界一切皆有可能的。有兴趣的同学不妨试一下！</p><h3 id="参考文档">参考文档</h3><p><a href="http://www.google.com" target="_blank" rel="noopener">http://www.google.com</a><br><a href="https://github.com/lirantal/dockly" target="_blank" rel="noopener">https://github.com/lirantal/dockly</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在不久前给大家推荐过一款命令行下交互式 &lt;code&gt;Docker&lt;/code&gt; 容器管理工具 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247485950&amp;amp;idx=1&amp;amp;sn=fe6639a5c9e7c3950157701c203212b5&amp;amp;chksm=eac528d7ddb2a1c1e8b4984cbf7ed14a0217714c1ce0d7359ac97f97f3f86d9beff53589bdf8#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Dry&lt;/a&gt;，今天再给大家介绍一款与其类似的 &lt;code&gt;Docker&lt;/code&gt; 容器管理工具 &lt;code&gt;Dockly&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Dockly&lt;/code&gt; 是一个使用 &lt;code&gt;Node.js&lt;/code&gt; 编写的开源软件，功能上和 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzI3MTI2NzkxMA==&amp;amp;mid=2247485950&amp;amp;idx=1&amp;amp;sn=fe6639a5c9e7c3950157701c203212b5&amp;amp;chksm=eac528d7ddb2a1c1e8b4984cbf7ed14a0217714c1ce0d7359ac97f97f3f86d9beff53589bdf8#rd&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Dry&lt;/a&gt; 类似。&lt;code&gt;Dockly&lt;/code&gt; 可以很方便的通过 &lt;code&gt;NPM&lt;/code&gt; 进行安装，并支持在 Linux、macOS 和 Windows 上运行。&lt;/p&gt;
&lt;p&gt;项目地址：&lt;a href=&quot;https://github.com/lirantal/dockly&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/lirantal/dockly&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;安装-dockly&quot;&gt;安装 Dockly&lt;/h3&gt;
&lt;p&gt;由于 &lt;code&gt;Dockly&lt;/code&gt; 是使用 &lt;code&gt;Node.js&lt;/code&gt; 开发的， &lt;code&gt;Dockly&lt;/code&gt; 对 &lt;code&gt;Node.js&lt;/code&gt; 版本有以下要求: &lt;code&gt;Node.js&lt;/code&gt; &amp;gt;= 7.6，&lt;code&gt;NPM&lt;/code&gt; &amp;gt;= 3.6。目前比较流行的几个 Linux 发行版默认仓库中的 &lt;code&gt;Node.js&lt;/code&gt; 版本都比较低，比如 Ubuntu 16.04 中的  &lt;code&gt;Node.js&lt;/code&gt; 版本为 4.x。&lt;/p&gt;
&lt;p&gt;在安装 &lt;code&gt;Dockly&lt;/code&gt; 前，我们首先需要用官方源来安装较新版本的 &lt;code&gt;Node.js&lt;/code&gt;。这里以 Ubuntu 为例：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo curl -sL https://deb.nodesource.com/setup_8.x | sudo -E bash -&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo apt-get install -y nodejs&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;其它发行版本的安装方法可参考&lt;a href=&quot;https://nodejs.org/en/download/package-manager/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;官方安装文档&lt;/a&gt;，这里就不再详述了。在完成 &lt;code&gt;Node.js&lt;/code&gt; 的安装后， 接下来我们只需用 &lt;code&gt;NPM&lt;/code&gt; 工具就可以很容易的完成 &lt;code&gt;Dockly&lt;/code&gt; 的安装。&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo npm install -g dockly&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="Docker" scheme="https://www.hi-linux.com/categories/docker/"/>
    
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
      <category term="Docker" scheme="https://www.hi-linux.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>使用 Nginx 的 image_filter 模块来构建动态缩略图服务器</title>
    <link href="https://www.hi-linux.com/posts/47787.html"/>
    <id>https://www.hi-linux.com/posts/47787.html</id>
    <published>2018-06-15T01:00:00.000Z</published>
    <updated>2018-08-24T06:01:06.571Z</updated>
    
    <content type="html"><![CDATA[<p>在以前我们实现缩略图机制通常是在当用户上传一张图片后，后端程序会固定将图片生成前端页面需要的不同大小缩略图。不管前端页面是否有使用，后端都会先产生好，这样做明显有以下缺陷：</p><ul><li>占用过多的磁盘空间大小</li><li>前端页面需要更多样格式的缩略图时，需要单独处理。</li></ul><p>当出现第二个问题时会比较麻烦，后端程序就需要将系统的全部图片重新产生一次所需的缩略图。这个过程非常耗时，也比较耗费系统性能。</p><p>现在比较流行的做法是改成透过 URL 定义长宽来即时生成所需的缩略图，实现的方式也有多种多样，本文将介绍使用 Nginx 的 <code>image_filter</code> 模块来实现动态生成缩略图。</p><p>从 Nginx 0.7.54 以后的版本，提供了一个 <code>http_image_filter_module</code> 的集成图片处理模块。该模块可以实现实时缩放图片、旋转图片、验证图片有效性以及获取图片宽高以及图片类型信息等功能。</p><h3 id="安装-image_filter-模块">安装 image_filter 模块</h3><p><code>http_image_filter_module</code> 模块在默认情况下是没有包含在官方预编译包中的，如果需要使用 <code>http_image_filter_module</code> 模块需要重新编译 Nginx。</p><ul><li>查看是否已安装 image_filter 模块</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nginx -V</span><br></pre></td></tr></table></figure><p>如果返回的结果中含有 <code>--with-http_image_filter_module</code> 字样就表明已经安装了 <code>image_filter</code> 模块。</p><a id="more"></a><ul><li>编译 Nginx 并加入 image_filter 模块</li></ul><p>启用 <code>http_image_filter_module</code> 模块的方法是比较简单的，由于<code>http_image_filter_module</code> 扩展需要 GD 库支持，首先需要安装相关依赖环境：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># CentOS / RHEL</span><br><span class="line">$ sudo yum -y install gd-devel</span><br><span class="line"></span><br><span class="line"># Ubuntu / Debian</span><br><span class="line">$ sudo apt-get install libgd2-xpm-dev</span><br></pre></td></tr></table></figure><p>其次在原来的 Nginx 编译参数上加上 <code>--with-http_image_filter_module</code> 参数后重新编译 Nginx 就行了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo wget http://nginx.org/download/nginx-1.15.0.tar.gz</span><br><span class="line">$ sudo tar xzvf nginx-1.15.0.tar.gz</span><br><span class="line">$ cd nginx-1.15.0/</span><br><span class="line">$ sudo ./configure --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt=&apos;-g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC&apos; --with-ld-opt=&apos;-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie&apos; --with-http_image_filter_module</span><br><span class="line">$ make &amp;&amp; make install</span><br></pre></td></tr></table></figure><ul><li>验证是否安装成功</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ nginx -V</span><br><span class="line">nginx version: nginx/1.15.0</span><br><span class="line">built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.4)</span><br><span class="line">built with OpenSSL 1.0.2g  1 Mar 2016</span><br><span class="line">TLS SNI support enabled</span><br><span class="line">configure arguments: --prefix=/etc/nginx --sbin-path=/usr/sbin/nginx --modules-path=/usr/lib/nginx/modules --conf-path=/etc/nginx/nginx.conf --error-log-path=/var/log/nginx/error.log --http-log-path=/var/log/nginx/access.log --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock --http-client-body-temp-path=/var/cache/nginx/client_temp --http-proxy-temp-path=/var/cache/nginx/proxy_temp --http-fastcgi-temp-path=/var/cache/nginx/fastcgi_temp --http-uwsgi-temp-path=/var/cache/nginx/uwsgi_temp --http-scgi-temp-path=/var/cache/nginx/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt=&apos;-g -O2 -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC&apos; --with-ld-opt=&apos;-Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie&apos; --with-http_image_filter_module</span><br></pre></td></tr></table></figure><p>如果返回的结果中含有 <code>--with-http_image_filter_module</code> 字样就表明安装成功了。</p><h3 id="image_filter-模块常用参数说明">image_filter 模块常用参数说明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 关闭模块</span><br><span class="line">image_filter off;  </span><br><span class="line"></span><br><span class="line"># 确保图片是 JPEG、GIF、PNG、或者 WebP，否则返回 415 错误    </span><br><span class="line">image_filter test;  </span><br><span class="line"></span><br><span class="line"># 输出有关图像的 json 格式，例如 &#123; &quot;img&quot; : &#123; &quot;width&quot;: 100, &quot;height&quot;: 100, &quot;type&quot;: &quot;gif&quot; &#125; &#125; ，如果出错显示：&#123;&#125;  </span><br><span class="line">image_filter size;  </span><br><span class="line"></span><br><span class="line"># 将图像旋转指定度数，参数可以包括变量。单独或一起与 resize crop 一起使用。  </span><br><span class="line">image_filter rotate 90|180|270;</span><br><span class="line"> </span><br><span class="line"># 按比例减少图像到指定大小，要减少一个维度，可以将另一个维度指定为 &quot;-&quot; 来表示, 出错返回 415 错误。参数值可包含变量，可以与 rotate 一起使用。  </span><br><span class="line">image_filter resize width height;  </span><br><span class="line"></span><br><span class="line"># 按比例将图像缩小到较大的一侧，并在另一侧产生无关边缘。其它和 rotate 一样。</span><br><span class="line">image_filter crop width height;</span><br><span class="line"></span><br><span class="line"># 设置读取图像缓冲的最大大小，超过则报 415 错误。</span><br><span class="line">image_filter_buffer 10M;</span><br><span class="line">  </span><br><span class="line"># 如果启用，最终图像将交错。对于JPEG，最终图像将采用 &quot;逐行JPEG&quot; 格式。</span><br><span class="line">image_filter_interlace on;</span><br><span class="line"></span><br><span class="line"># 设置转换的 JPEG 图像的质量。可接受的值是从 1 到 100 的范围内。较小的值通常意味着图像质量越低，以达到减少数据传输。推荐的最大值为 95，参数值可以包含变量。  </span><br><span class="line">image_filter_jpeg_quality 95;</span><br><span class="line"></span><br><span class="line"># 增加最终图像的清晰度，锐度百分比可以超过 100。零值将禁用锐化。参数值可以包含变量。 </span><br><span class="line">image_filter_sharpen 100;</span><br><span class="line"> </span><br><span class="line"># 定义在使用调色板指定的颜色转换 GIF 图像或 PNG 图像时是否应保留透明度。透明度的降低会导致质量更好的图像，PNG 中 的 alpha 通道透明度始终保留。</span><br><span class="line">image_filter_transparency on;</span><br></pre></td></tr></table></figure><h3 id="使用-image_filter-模块动态生成缩略图">使用 image_filter 模块动态生成缩略图</h3><ul><li>配置 image_filter 模块</li></ul><p>这里给出一个简单的例子，在你的 Nginx 配置文件中 Server 配置段增加如下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim nginx.conf</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">server &#123;</span><br><span class="line">  listen 80 default_server;</span><br><span class="line">  listen [::]:80 default_server;</span><br><span class="line">  server_name www.hi-linux.com;</span><br><span class="line"> </span><br><span class="line">  location ~ ^/([0-9]+)/(.*)$ &#123;</span><br><span class="line">    set $width $1;</span><br><span class="line">    set $path $2;</span><br><span class="line">    set $EXPIRE_TIME 1d;</span><br><span class="line">    set $JPG_QUALITY 95;</span><br><span class="line">    rewrite ^ /$path break;</span><br><span class="line">    image_filter resize $width -;</span><br><span class="line">    image_filter_buffer 100M;</span><br><span class="line">    image_filter_jpeg_quality $&#123;JPG_QUALITY&#125;;</span><br><span class="line">    expires $&#123;EXPIRE_TIME&#125;;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br></pre></td></tr></table></figure><ul><li>重载 Nginx 配置</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nginx -s reload</span><br></pre></td></tr></table></figure><ul><li>生成缩略图</li></ul><p>通过在图片请求地址的文件名前加上 <code>/缩放大小/</code>，就可以访问到你想要的任意尺寸的缩略图。例如：要生成缩略图的图片地址为：<code>http://192.168.100.210/01.jpg</code>，现在只需要将请求修改为：<code>http://192.168.100.210/200/01.jpg</code> 将会自动返回对应的缩略图。</p><h3 id="给-image_filter-模块加入缓存机制">给 image_filter 模块加入缓存机制</h3><p>默认情况下，<code>image_filter</code> 模块都是实时生成缩略图的。每一次请求 Nginx 都会需要处理，如果有 10000 次请求，就会处理 10000 次。这样就会在高并发请求时给 Nginx 服务器造成压力。</p><p>要解决这个问题，我们可以通过 Nginx 的 <code>Proxy Cache</code> 机制让 Nginx 在相同请求的情况下只产生一次缩略图，并且将缩略图放到缓存目录中以减少短时间内的不同请求所产生的运算次数。</p><p>由于 <code>image_filter</code> 模块无法跟 <code>Proxy Cache</code> 同时处理，要实现这个功能必须要将请求拆成两个 Host 来达成。具体配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim nginx.conf</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">proxy_cache_path /data keys_zone=cache_zone:10m;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">  # Internal image resizing server.</span><br><span class="line">  server_name localhost;</span><br><span class="line">  listen 8888;</span><br><span class="line"></span><br><span class="line">  location ~ ^/([0-9]+)/(.*)$ &#123;</span><br><span class="line">    set $width $1;</span><br><span class="line">    set $path $2;</span><br><span class="line">    set $JPG_QUALITY 95;</span><br><span class="line">    rewrite ^ /$path break;</span><br><span class="line">    #proxy_pass $&#123;IMAGE_HOST&#125;;</span><br><span class="line">    image_filter resize $width -;</span><br><span class="line">    image_filter_buffer 100M;</span><br><span class="line">    image_filter_jpeg_quality $&#123;JPG_QUALITY&#125;;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">server &#123;</span><br><span class="line">  listen 80 default_server;</span><br><span class="line">  listen [::]:80 default_server;</span><br><span class="line">  server_name www.hi-linux.com;</span><br><span class="line"></span><br><span class="line">  location ~ ^/([0-9]+)/(.*)$ &#123;</span><br><span class="line">    set $width $1;</span><br><span class="line">    set $path $2;</span><br><span class="line">    set $EXPIRE_TIME 1d;</span><br><span class="line">    rewrite ^ /$path break;</span><br><span class="line">    proxy_pass http://127.0.0.1:8888/$width/$path;</span><br><span class="line">    proxy_cache cache_zone;</span><br><span class="line">    proxy_cache_key $uri;</span><br><span class="line">    proxy_cache_valid 200 302 24h;</span><br><span class="line">    proxy_cache_valid 404 1m;</span><br><span class="line">    # expire time for browser</span><br><span class="line">    expires $&#123;EXPIRE_TIME&#125;;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="使用-docker-容器快速构建">使用 Docker 容器快速构建</h3><p>如果你和我一样比较偏爱使用 Docker，可以直接使用 <code>nginx-image-resizer</code> 项目。<code>nginx-image-resizer</code> 项目是一个用 Docker 实现的实时图像调整大小和缓存的容器项目，<code>nginx-image-resizer</code> 支持 Amazon S3 和 Minio 作为存储。</p><p>项目官方地址：<a href="https://github.com/appleboy/nginx-image-resizer" target="_blank" rel="noopener">https://github.com/appleboy/nginx-image-resizer</a></p><p><img src="https://www.hi-linux.com/img/linux/nginx-image-resizer.png" alt=""></p><p>下面给一个使用 Minio 作为存储的 docker-compose 示例：</p><ul><li>编写 docker-compose 配置文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ vim docker-compose.yml</span><br><span class="line"></span><br><span class="line">version: &apos;2&apos;</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  minio:</span><br><span class="line">    image: minio/minio</span><br><span class="line">    container_name: minio</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;9000:9000&quot;</span><br><span class="line">    volumes:</span><br><span class="line">      - minio-data:/data</span><br><span class="line">    environment:</span><br><span class="line">      MINIO_ACCESS_KEY: YOUR_MINIO_ACCESS_KEY</span><br><span class="line">      MINIO_SECRET_KEY: YOUR_MINIO_SECRET_KEY</span><br><span class="line">    command: server /data</span><br><span class="line"> </span><br><span class="line">  image-resizer:</span><br><span class="line">    image: appleboy/nginx-image-resizer</span><br><span class="line">    container_name: image-resizer</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;80:80&quot;</span><br><span class="line">    environment:</span><br><span class="line">      IMAGE_HOST: http://minio:9000</span><br><span class="line">      NGINX_HOST: localhost</span><br><span class="line"> </span><br><span class="line">volumes:</span><br><span class="line">  minio-data:</span><br></pre></td></tr></table></figure><blockquote><p>注：MINIO_ACCESS_KEY 和 MINIO_SECRET_KEY 分别为 Minio 的登陆帐号和密码，请根据实际情况修改。</p></blockquote><ul><li>启动相关容器</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose up -d</span><br></pre></td></tr></table></figure><ul><li>生成缩略图</li></ul><p>生成缩略图的方法和前面方法类似，但由于这里使用了 Minio 做为文件存储服务，所以需要先在 Minio 中上传图片文件。在 Minio 中上传文件的方法大致分为以下几步：</p><ol><li>登陆 Minio Web 管理界面， Web 管理界面访问地址默认为：<code>http://HostIP:9000</code>。</li><li>创建 bucket，这里我们创建一个名为 test 的 bucket。</li><li>设置名为 test 的 bucket 的访问权限。</li><li>在名为 test 的 bucket 中上传图片文件。</li></ol><p>完成以上几步后，和前面类似直接在访问的 URL 中加入需缩放图片比例即可生成对应缩略图。例如：访问：<code>http://192.168.100.210/600/test/test1.jpg</code> 将会自动返回对应的缩略图。</p><h3 id="其它相关说明">其它相关说明</h3><p>利用 Nginx 的 <code>image_filter</code> 模块不仅仅能生成缩略图，你还可以通过修改配置来实现其它的功能，比如图片旋转、图片裁剪、图片透明化等等。关于 <code>image_filter</code> 模块的更多使用方法可参考<a href="http://nginx.org/cn/docs/http/ngx_http_image_filter_module.html" target="_blank" rel="noopener">官方文档</a>。</p><h3 id="参考文档">参考文档</h3><p><a href="https://www.google.com" target="_blank" rel="noopener">https://www.google.com</a><br><a href="http://t.cn/RBG2QWx" target="_blank" rel="noopener">http://t.cn/RBG2QWx</a><br><a href="http://t.cn/RBGyUbz" target="_blank" rel="noopener">http://t.cn/RBGyUbz</a><br><a href="http://t.cn/R7R9bZU" target="_blank" rel="noopener">http://t.cn/R7R9bZU</a><br><a href="http://t.cn/RBGyxil" target="_blank" rel="noopener">http://t.cn/RBGyxil</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在以前我们实现缩略图机制通常是在当用户上传一张图片后，后端程序会固定将图片生成前端页面需要的不同大小缩略图。不管前端页面是否有使用，后端都会先产生好，这样做明显有以下缺陷：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;占用过多的磁盘空间大小&lt;/li&gt;
&lt;li&gt;前端页面需要更多样格式的缩略图时，需要单独处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当出现第二个问题时会比较麻烦，后端程序就需要将系统的全部图片重新产生一次所需的缩略图。这个过程非常耗时，也比较耗费系统性能。&lt;/p&gt;
&lt;p&gt;现在比较流行的做法是改成透过 URL 定义长宽来即时生成所需的缩略图，实现的方式也有多种多样，本文将介绍使用 Nginx 的 &lt;code&gt;image_filter&lt;/code&gt; 模块来实现动态生成缩略图。&lt;/p&gt;
&lt;p&gt;从 Nginx 0.7.54 以后的版本，提供了一个 &lt;code&gt;http_image_filter_module&lt;/code&gt; 的集成图片处理模块。该模块可以实现实时缩放图片、旋转图片、验证图片有效性以及获取图片宽高以及图片类型信息等功能。&lt;/p&gt;
&lt;h3 id=&quot;安装-image-filter-模块&quot;&gt;安装 image_filter 模块&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;http_image_filter_module&lt;/code&gt; 模块在默认情况下是没有包含在官方预编译包中的，如果需要使用 &lt;code&gt;http_image_filter_module&lt;/code&gt; 模块需要重新编译 Nginx。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;查看是否已安装 image_filter 模块&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ nginx -V&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果返回的结果中含有 &lt;code&gt;--with-http_image_filter_module&lt;/code&gt; 字样就表明已经安装了 &lt;code&gt;image_filter&lt;/code&gt; 模块。&lt;/p&gt;
    
    </summary>
    
      <category term="Nginx" scheme="https://www.hi-linux.com/categories/nginx/"/>
    
    
      <category term="Nginx" scheme="https://www.hi-linux.com/tags/Nginx/"/>
    
      <category term="Linux" scheme="https://www.hi-linux.com/tags/Linux/"/>
    
  </entry>
  
</feed>
